{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OmarAbdelwahab\\AppData\\Anaconda2\\envs\\gl-env\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import graphlab\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to oaabde01@louisville.edu and will expire on September 22, 2017.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: C:\\Users\\OMARAB~1\\AppData\\Local\\Temp\\graphlab_server_1483659100.log.0\n"
     ]
    }
   ],
   "source": [
    "products = graphlab.SFrame('amazon_baby.gl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BASE_DIR = \"/home/graphlab_create/data/blogs\" # NOTE: Update BASE_DIR to your own directory path\n",
    "class TrainSentences(object):\n",
    "    \"\"\"\n",
    "    Iterator class that returns Sentences from texts files in a input directory\n",
    "    \"\"\"\n",
    "    RE_WIHTE_SPACES = re.compile(\"\\s+\")\n",
    "    STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "    def __init__(self, dirname):\n",
    "        \"\"\"\n",
    "        Initialize a TrainSentences object with a input directory that contains text files for training\n",
    "        :param dirname: directory name which contains the text files        \n",
    "        \"\"\"\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Sentences iterator that return sentences parsed from files in the input directory.\n",
    "        Each sentences is returned as list of words\n",
    "        \"\"\"\n",
    "        #First iterate  on all files in the input directory\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            # read line from file (Without reading the entire file)\n",
    "            for line in file(os.path.join(self.dirname, fname), \"rb\"):\n",
    "                # split the read line into sentences using NLTK\n",
    "                for s in txt2sentences(line, is_html=True):\n",
    "                    # split the sentence into words using regex\n",
    "                    w =txt2words(s, lower=True, is_html=False, remove_stop_words=False,\n",
    "                                                 remove_none_english_chars=True)\n",
    "                    #skip short sentneces with less than 3 words\n",
    "                    if len(w) < 3:\n",
    "                        continue\n",
    "                    yield w\n",
    "\n",
    "def txt2sentences(txt, is_html=False, remove_none_english_chars=True):\n",
    "    \"\"\"\n",
    "    Split the English text into sentences using NLTK\n",
    "    :param txt: input text.\n",
    "    :param is_html: If True thenremove HTML tags using BeautifulSoup\n",
    "    :param remove_none_english_chars: if True then remove non-english chars from text\n",
    "    :return: string in which each line consists of single sentence from the original input text.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    if is_html:\n",
    "        txt = BeautifulSoup(txt).get_text()\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    # split text into sentences using nltk packages\n",
    "    for s in tokenizer.tokenize(txt):\n",
    "        if remove_none_english_chars:\n",
    "            #remove none English chars\n",
    "            s = re.sub(\"[^a-zA-Z]\", \" \", s)\n",
    "        yield s\n",
    "    \n",
    "def txt2words(txt, lower=True, is_html=False, remove_none_english_chars=True, remove_stop_words=True):\n",
    "    \"\"\"\n",
    "    Split text into words list\n",
    "    :param txt: the input text\n",
    "    :param lower: if to make the  text to lowercase or not.\n",
    "    :param is_html: If True then  remove HTML tags using BeautifulSoup\n",
    "    :param remove_none_english_chars: if True then remove non-english chars from text\n",
    "    :param remove_stop_words: if True then remove stop words from text\n",
    "    :return: words list create from the input text according to the input parameters.\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    if is_html:\n",
    "        txt = BeautifulSoup(txt).get_text()\n",
    "    if lower:\n",
    "        txt = txt.lower()\n",
    "    if remove_none_english_chars:\n",
    "        txt = re.sub(\"[^a-zA-Z]\", \" \", txt)\n",
    "\n",
    "    words = TrainSentences.RE_WIHTE_SPACES.split(txt.strip().lower())\n",
    "    if remove_stop_words:\n",
    "        #remove stop words from text\n",
    "        words = [w for w in words if w not in TrainSentences.STOP_WORDS]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import average\n",
    "import graphlab as gl\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "class DeepTextAnalyzer(object):\n",
    "    def __init__(self, word2vec_model):\n",
    "        \"\"\"\n",
    "        Construct a DeepTextAnalyzer using the input Word2Vec model\n",
    "        :param word2vec_model: a trained Word2Vec model\n",
    "        \"\"\"\n",
    "        self._model = word2vec_model\n",
    "\n",
    "    def txt2vectors(self,txt, is_html):\n",
    "        \"\"\"\n",
    "        Convert input text into an iterator that returns the corresponding vector representation of each\n",
    "        word in the text, if it exists in the Word2Vec model\n",
    "        :param txt: input text\n",
    "        :param is_html: if True, then extract the text from the input HTML\n",
    "        :return: iterator of vectors created from the words in the text using the Word2Vec model.\n",
    "        \"\"\"\n",
    "        words = txt2words(txt,is_html=is_html, lower=True, remove_none_english_chars=True)\n",
    "        words = [w for w in words if w in self._model]\n",
    "        #yield self._model[txt]\n",
    "        if len(words) != 0:\n",
    "            for w in words:\n",
    "                yield self._model[w]\n",
    "\n",
    "\n",
    "    def txt2avg_vector(self, txt, is_html):\n",
    "        \"\"\"\n",
    "        Calculate the average vector representation of the input text\n",
    "        :param txt: input text\n",
    "        :param is_html: is the text is a HTML\n",
    "        :return the average vector of the vector representations of the words in the text  \n",
    "        \"\"\"\n",
    "        ##vector = self.txt2vectors(txt,is_html=is_html)\n",
    "        ##return vector\n",
    "        vectors = self.txt2vectors(txt,is_html=is_html)\n",
    "        vectors_sum = next(vectors, None)\n",
    "        if vectors_sum is None:\n",
    "            return None\n",
    "        count =1.0\n",
    "        for v in vectors:\n",
    "            count += 1\n",
    "            vectors_sum = np.add(vectors_sum,v)\n",
    "        \n",
    "        #calculate the average vector and replace +infy and -inf with numeric values \n",
    "        avg_vector = np.nan_to_num(vectors_sum/count)\n",
    "        return avg_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import graphlab\n",
    "import os\n",
    "import gensim\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "def split_sentence(sentence):\n",
    "    return re.split('\\W+', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to oaabde01@louisville.edu and will expire on September 22, 2017.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: C:\\Users\\OMARAB~1\\AppData\\Local\\Temp\\graphlab_server_1484259696.log.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\OmarAbdelwahab\\full_train_2016.tsv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\OmarAbdelwahab\\full_train_2016.tsv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.067003 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.067003 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[long,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\OmarAbdelwahab\\full_train_2016.tsv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\OmarAbdelwahab\\full_train_2016.tsv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 11798 lines in 0.035 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 11798 lines in 0.035 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets = graphlab.SFrame('full_train_2016.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdb_w2v = Word2Vec.load('C:/Users/OmarAbdelwahab/Documents/RESEARCH/SEMEVAL2016/TwitterSentimentAnalysisTask/Train-Trial-Data/2016Data/Train+dev-2016/Needed/Train/word2vecmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lol.', 0.5225681066513062),\n",
       " (':)', 0.4609519839286804),\n",
       " (':).', 0.43749451637268066),\n",
       " (';-)', 0.4341832399368286),\n",
       " ('alot,', 0.4135943651199341),\n",
       " ('no.', 0.4129267930984497),\n",
       " (';)', 0.41199108958244324),\n",
       " ('poops,', 0.41076964139938354),\n",
       " ('haha', 0.40914398431777954),\n",
       " ('deserv', 0.4071287512779236)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_w2v.most_similar(\"lol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets['1gram features'] = gl.text_analytics.count_ngrams(tweets['Tweet'], 1)\n",
    "tweets['2gram features'] = gl.text_analytics.count_ngrams(tweets['Tweet'], 2)\n",
    "tweets['3gram features'] = gl.text_analytics.count_ngrams(tweets['Tweet'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Runtime Exception. Runtime Exception: 110. Fail executing the lambda function. The lambda worker may have run out of memory or crashed because it captured objects that cannot be properly serialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e5f9082b1ec8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDeepTextAnalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimdb_w2v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'vectors'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtxt2avg_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_html\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\OmarAbdelwahab\\AppData\\Anaconda2\\envs\\gl-env\\lib\\site-packages\\graphlab\\data_structures\\sarray.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn, dtype, skip_undefined, seed)\u001b[0m\n\u001b[0;32m   1892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mcython_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1894\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mSArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_proxy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__proxy__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_undefined\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1895\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\OmarAbdelwahab\\AppData\\Anaconda2\\envs\\gl-env\\lib\\site-packages\\graphlab\\cython\\context.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_cython_trace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                 \u001b[1;31m# To hide cython trace, we re-raise from here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[1;31m# To show the full trace, we do nothing and let exception propagate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Runtime Exception. Runtime Exception: 110. Fail executing the lambda function. The lambda worker may have run out of memory or crashed because it captured objects that cannot be properly serialized."
     ]
    }
   ],
   "source": [
    "dt = DeepTextAnalyzer(imdb_w2v)\n",
    "tweets['vectors'] = tweets['Tweet'].apply(lambda p: dt.txt2avg_vector(p, is_html=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">TweetID</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">UserID</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Sentiment</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Tweet</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">1gram features</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">264183816548130816</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">15140428</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">ga by my hous hit<br>$NUM.$NUM i m go to ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'on': 1L, 'hit': 1L,<br>'hous': 1L, 'i': 1L,  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">264249301910310912</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">18516728</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">iranian gener say israel<br>s iron dome can t deal ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 1L, 'deal': 1L,<br>'we': 1L, 'say': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">264105751826538497</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">147088367</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">with j davlar $NUMth.<br>main rival are team ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'week': 1L, 'it': 1L,<br>'an': 1L, 'are': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">264094586689953794</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">332474633</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">talk about act s amp;<br>amp; sat s, decid   i ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'me': 1L, 'and': 1L,<br>'about': 2L, 'decid': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">254941790757601280</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">557103111</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">they may have a superbowl<br>in dalla, but dalla a ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 1L, 'a': 2L,<br>'dalla': 2L, 'superbo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">264169034155696130</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">382403760</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">im bring the ne load of<br>candi tomorrow, i just ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'load': 1L, 'all': 1L,<br>'just': 1L, 'get': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">263192091700654080</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">344222239</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">appl software, retail<br>chief out in overhaul: ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'on': 1L, 'francisco':<br>1L, 'heads': 1L, 'san': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">263398998675693568</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">812957996</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">AT USER AT USER AT USER i<br>just watch it sridevi s ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'on': 1L, 'u': 1L,<br>'from': 1L, 'just': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">260200142420992000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">332530284</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">livewir nadal confirm for<br>mexican open in febru ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'me': 1L, 'play': 1L,<br>'nadal': 2L, 'mexican': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">264087629237202944</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">61903760</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">AT USER i didnt want to<br>just pop up... but ye ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 1L, 'have': 1L,<br>'just': 1L, 'pop': 1L, ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "<table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">2gram features</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">3gram features</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">vectors</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'to chapel': 1L, 'num<br>num': 1L, 'chapel hill': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'num num i': 1L, 'hous<br>hit num': 1L, 'hill on ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0424087643623,<br>0.119673058391, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'with their': 1L, 'dome<br>can': 1L, 'talk po': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'end up find': 1L, 'talk<br>po that': 1L, 'may end ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0252824444324,<br>0.0970393121243, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'main rival': 1L, 'j<br>davlar': 1L, 'end to': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'rival are team': 1L,<br>'po end to': 1L, 'are ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0193239226937,<br>0.0230387803167, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'act s': 1L, 'everyth<br>about': 1L, 'to go': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'colleg ne me': 1L, 'sat<br>s decid': 1L, 'colleg ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0176480095834,<br>0.133513972163, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'ain t': 1L, 'at user':<br>2L, 'have a': 1L, 'dalla ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'dalla ain t': 1L, 'a<br>superbowl not': 1L, 'at ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0516797862947,<br>-0.00137295853347, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'load of': 1L, 'ne<br>load': 1L, 'it doesn': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'tomorrow i just': 1L,<br>'of candi tomorrow': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.000500957190525,<br>0.0506830215454, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'inc ceo': 1L, 'overhaul<br>san': 1L, 'monday ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'appl inc ceo': 1L,<br>'francisco appl inc': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0708317831159,<br>0.117349348962, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'nums sun': 1L, 'sun<br>morn': 1L, 'at user': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'sridevi s comeback':<br>1L, 'at user i': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0301081500947,<br>0.114462018013, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'mexican open': 1L,<br>'nadal is': 1L, 'febr ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'rafael nadal is': 1L,<br>'confirm for mexican': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0961522832513,<br>0.0568600408733, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'tell her': 1L, 'to<br>just': 1L, 'just pop': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'hill next wednesday':<br>1L, 'chapel hill next': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0420787446201,<br>0.0605648420751, ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[10 rows x 8 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tTweetID\tint\n",
       "\tUserID\tint\n",
       "\tSentiment\tstr\n",
       "\tTweet\tstr\n",
       "\t1gram features\tdict\n",
       "\t2gram features\tdict\n",
       "\t3gram features\tdict\n",
       "\tvectors\tarray\n",
       "\n",
       "Rows: 10\n",
       "\n",
       "Data:\n",
       "+--------------------+-----------+-----------+-------------------------------+\n",
       "|      TweetID       |   UserID  | Sentiment |             Tweet             |\n",
       "+--------------------+-----------+-----------+-------------------------------+\n",
       "| 264183816548130816 |  15140428 |  positive | ga by my hous hit $NUM.$NU... |\n",
       "| 264249301910310912 |  18516728 |  negative | iranian gener say israel s... |\n",
       "| 264105751826538497 | 147088367 |  positive | with j davlar $NUMth. main... |\n",
       "| 264094586689953794 | 332474633 |  negative | talk about act s amp; amp;... |\n",
       "| 254941790757601280 | 557103111 |  negative | they may have a superbowl ... |\n",
       "| 264169034155696130 | 382403760 |  neutral  | im bring the ne load of ca... |\n",
       "| 263192091700654080 | 344222239 |  neutral  | appl software, retail chie... |\n",
       "| 263398998675693568 | 812957996 |  positive | AT USER AT USER AT USER i ... |\n",
       "| 260200142420992000 | 332530284 |  neutral  | livewir nadal confirm for ... |\n",
       "| 264087629237202944 |  61903760 |  positive | AT USER i didnt want to ju... |\n",
       "+--------------------+-----------+-----------+-------------------------------+\n",
       "+-------------------------------+-------------------------------+\n",
       "|         1gram features        |         2gram features        |\n",
       "+-------------------------------+-------------------------------+\n",
       "| {'on': 1L, 'hit': 1L, 'hou... | {'to chapel': 1L, 'num num... |\n",
       "| {'and': 1L, 'deal': 1L, 'w... | {'with their': 1L, 'dome c... |\n",
       "| {'week': 1L, 'it': 1L, 'an... | {'main rival': 1L, 'j davl... |\n",
       "| {'me': 1L, 'and': 1L, 'abo... | {'act s': 1L, 'everyth abo... |\n",
       "| {'and': 1L, 'a': 2L, 'dall... | {'ain t': 1L, 'at user': 2... |\n",
       "| {'load': 1L, 'all': 1L, 'j... | {'load of': 1L, 'ne load':... |\n",
       "| {'on': 1L, 'francisco': 1L... | {'inc ceo': 1L, 'overhaul ... |\n",
       "| {'on': 1L, 'u': 1L, 'from'... | {'nums sun': 1L, 'sun morn... |\n",
       "| {'me': 1L, 'play': 1L, 'na... | {'mexican open': 1L, 'nada... |\n",
       "| {'and': 1L, 'have': 1L, 'j... | {'tell her': 1L, 'to just'... |\n",
       "+-------------------------------+-------------------------------+\n",
       "+-------------------------------+-------------------------------+\n",
       "|         3gram features        |            vectors            |\n",
       "+-------------------------------+-------------------------------+\n",
       "| {'num num i': 1L, 'hous hi... | [-0.0424087643623, 0.11967... |\n",
       "| {'end up find': 1L, 'talk ... | [-0.0252824444324, 0.09703... |\n",
       "| {'rival are team': 1L, 'po... | [-0.0193239226937, 0.02303... |\n",
       "| {'colleg ne me': 1L, 'sat ... | [-0.0176480095834, 0.13351... |\n",
       "| {'dalla ain t': 1L, 'a sup... | [-0.0516797862947, -0.0013... |\n",
       "| {'tomorrow i just': 1L, 'o... | [-0.000500957190525, 0.050... |\n",
       "| {'appl inc ceo': 1L, 'fran... | [0.0708317831159, 0.117349... |\n",
       "| {'sridevi s comeback': 1L,... | [-0.0301081500947, 0.11446... |\n",
       "| {'rafael nadal is': 1L, 'c... | [0.0961522832513, 0.056860... |\n",
       "| {'hill next wednesday': 1L... | [0.0420787446201, 0.060564... |\n",
       "+-------------------------------+-------------------------------+\n",
       "[10 rows x 8 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets['word_count'] = graphlab.text_analytics.count_words(tweets['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = graphlab.text_analytics.tf_idf(tweets['word_count'])\n",
    "tweets['tfidf'] = tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ignore all neutral tweets\n",
    "tweets_pos_neg = tweets[tweets['Sentiment'] != 'neutral']\n",
    "tweets_neutral = tweets[tweets['Sentiment'] == 'neutral']\n",
    "train_data_pos_neg,test_data_pos_neg = tweets_pos_neg.random_split(.8, seed=0)\n",
    "feature_set1 = ['tfidf','1gram features','2gram features','3gram features','vectors_pos_neg','vectors_pos_neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2559\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 87428\n",
      "PROGRESS: Number of coefficients    : 87429\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy | Validation-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000059  | 1.232822     | 0.933568          | 0.788310            |\n",
      "PROGRESS: | 2         | 3        | 0.000059  | 1.360907     | 0.969129          | 0.800948            |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n"
     ]
    }
   ],
   "source": [
    "sentiment_model_word2vec_ =graphlab.logistic_classifier.create(train_data_pos_neg,\n",
    "                                                     target='Sentiment',\n",
    "                                                     features=feature_set1,\n",
    "                                                     max_iterations=100,\n",
    "                                                     l1_penalty=10,\n",
    "                                                     l2_penalty=1000.0,\n",
    "                                                     class_weights = 'auto',\n",
    "                                                     validation_set=test_data_pos_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_neg_model_w2v = Word2Vec.load('C:/Users/OmarAbdelwahab/Documents/RESEARCH/SEMEVAL2016/TwitterSentimentAnalysisTask/Train-Trial-Data/2016Data/Train+dev-2016/Needed/Train/pos_neg_model_w2v')\n",
    "pos_neutral_model_w2v = Word2Vec.load('C:/Users/OmarAbdelwahab/Documents/RESEARCH/SEMEVAL2016/TwitterSentimentAnalysisTask/Train-Trial-Data/2016Data/Train+dev-2016/Needed/Train/pos_neutral_model_w2v')\n",
    "neutral_neg_model_w2v = Word2Vec.load('C:/Users/OmarAbdelwahab/Documents/RESEARCH/SEMEVAL2016/TwitterSentimentAnalysisTask/Train-Trial-Data/2016Data/Train+dev-2016/Needed/Train/neutral_neg_model_w2v')\n",
    "positive_nonpositive_model_w2v = Word2Vec.load('C:/Users/OmarAbdelwahab/Documents/RESEARCH/SEMEVAL2016/TwitterSentimentAnalysisTask/Train-Trial-Data/2016Data/Train+dev-2016/Needed/Train/positive_nonpositive_model_w2v')\n",
    "negative_nonnegative_model_w2v = Word2Vec.load('C:/Users/OmarAbdelwahab/Documents/RESEARCH/SEMEVAL2016/TwitterSentimentAnalysisTask/Train-Trial-Data/2016Data/Train+dev-2016/Needed/Train/negative_nonnegative_model_w2v')\n",
    "neutral_nonneutral_model_w2v = Word2Vec.load('C:/Users/OmarAbdelwahab/Documents/RESEARCH/SEMEVAL2016/TwitterSentimentAnalysisTask/Train-Trial-Data/2016Data/Train+dev-2016/Needed/Train/neutral_nonneutral_model_w2v')\n",
    "doc2vec_reviewsonly_dm = Doc2Vec.load('C:/Python27/doc2vec_dm_reviewsonly')\n",
    "doc2vec_tweetsonly_dm = Doc2Vec.load('C:/Python27/doc2vec_dm_tweetsonly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = DeepTextAnalyzer(pos_neg_model_w2v)\n",
    "tweets['vectors_pos_neg'] = tweets['Tweet'].apply(lambda p: dt.txt2avg_vector(p, is_html=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = DeepTextAnalyzer(pos_neutral_model_w2v)\n",
    "tweets['vectors_pos_neutral'] = tweets['Tweet'].apply(lambda p: dt.txt2avg_vector(p, is_html=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = DeepTextAnalyzer(neutral_neg_model_w2v)\n",
    "tweets['vectors_neutral_neg'] = tweets['Tweet'].apply(lambda p: dt.txt2avg_vector(p, is_html=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for loop for looping over the k segments for the K-Fold cross validation\n",
    "def k_fold_cross_validation(k, l2_penalty, data, output):\n",
    "    n = len(data)\n",
    "    k = 10 # 10-fold cross-validation\n",
    "    RSSTotal = 0\n",
    "    Accuracy = []\n",
    "    fscores = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        start = (n*i)/k\n",
    "        end = (n*(i+1))/k\n",
    "        Valid_shuffled = data[start:end+1]\n",
    "        train_shuffled = data[0:start].append(data[end+1:n])\n",
    "        feature_set_negative_ornot = ['tfidf','1gram features','2gram features','3gram features','vectors_neg_ornot']\n",
    "        model =graphlab.logistic_classifier.create(train_shuffled,\n",
    "                                                     target='NegativeorNot',\n",
    "                                                     features=feature_set_negative_ornot,\n",
    "                                                     max_iterations=100,\n",
    "                                                     l1_penalty=0,\n",
    "                                                     l2_penalty=l2_penalty,\n",
    "                                                     class_weights = 'auto',\n",
    "                                                     validation_set=None)\n",
    "        modelresults = model.evaluate(Valid_shuffled)\n",
    "        fscores.append(modelresults['f1_score'])\n",
    "        Accuracy.append(modelresults['accuracy'])\n",
    "        ##predictiondifference_squared = np.multiply(np.subtract(modelpredictions,Valid_shuffled['Sentiment']),np.subtract(modelpredictions,Valid_shuffled['Sentiment']))\n",
    "        ##RSS = predictiondifference_squared.sum()\n",
    "        ##RSSTotal = RSSTotal + RSS\n",
    "        ##print modelpredictions\n",
    "    \n",
    "    ##RSSAvg = RSSTotal/k\n",
    "    return (sum(Accuracy)/len(Accuracy),sum(fscores)/len(fscores))\n",
    "    #return max(fscores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79948\n",
      "PROGRESS: Number of coefficients    : 79949\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.941629     | 0.783326          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.052701     | 0.741207          |\n",
      "PROGRESS: | 3         | 4        | 0.000301  | 1.172782     | 0.775510          |\n",
      "PROGRESS: | 4         | 5        | 0.000301  | 1.300868     | 0.766826          |\n",
      "PROGRESS: | 5         | 6        | 0.000301  | 1.415946     | 0.769865          |\n",
      "PROGRESS: | 6         | 7        | 0.000301  | 1.527018     | 0.772471          |\n",
      "PROGRESS: | 10        | 11       | 0.000301  | 1.966311     | 0.781589          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79969\n",
      "PROGRESS: Number of coefficients    : 79970\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.931620     | 0.783666          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.044695     | 0.741095          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.198798     | 0.770634          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.308871     | 0.766290          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.417943     | 0.769765          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.530019     | 0.769765          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.957303     | 0.775847          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80034\n",
      "PROGRESS: Number of coefficients    : 80035\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.912607     | 0.778888          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.025683     | 0.751520          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.174782     | 0.766725          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.280854     | 0.766725          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.387924     | 0.771069          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.498000     | 0.772806          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.933288     | 0.779322          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79911\n",
      "PROGRESS: Number of coefficients    : 79912\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.887590     | 0.782798          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 0.996664     | 0.751086          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.143762     | 0.761077          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.254836     | 0.761946          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.361906     | 0.764553          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.468977     | 0.767159          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.894262     | 0.777585          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79950\n",
      "PROGRESS: Number of coefficients    : 79951\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.931624     | 0.785838          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.038694     | 0.756733          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.145765     | 0.767159          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.254838     | 0.766725          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.365912     | 0.767159          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.473985     | 0.767159          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.909274     | 0.772806          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80098\n",
      "PROGRESS: Number of coefficients    : 80099\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.944631     | 0.775847          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.057706     | 0.750652          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.163778     | 0.762381          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.276852     | 0.761512          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.386926     | 0.763684          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.500001     | 0.763684          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.937292     | 0.768028          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79658\n",
      "PROGRESS: Number of coefficients    : 79659\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.939627     | 0.782363          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.048700     | 0.759774          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.157771     | 0.765856          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.265844     | 0.768028          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.375917     | 0.770200          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.486992     | 0.770200          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.916278     | 0.777585          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80441\n",
      "PROGRESS: Number of coefficients    : 80442\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.969647     | 0.784535          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.078719     | 0.749783          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.189793     | 0.767593          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.304871     | 0.762381          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.419948     | 0.763249          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.537026     | 0.765856          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.986324     | 0.775847          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79909\n",
      "PROGRESS: Number of coefficients    : 79910\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.953636     | 0.777150          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.063709     | 0.755430          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.174784     | 0.764987          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.285858     | 0.767159          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.391929     | 0.771069          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.506004     | 0.771937          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.945297     | 0.777150          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79835\n",
      "PROGRESS: Number of coefficients    : 79836\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.944628     | 0.782458          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.054702     | 0.758576          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.160772     | 0.763352          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.264842     | 0.764221          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.370913     | 0.767694          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.478984     | 0.768129          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.914274     | 0.770734          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79948\n",
      "PROGRESS: Number of coefficients    : 79949\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.915610     | 0.783326          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.023683     | 0.741641          |\n",
      "PROGRESS: | 3         | 4        | 0.000301  | 1.135757     | 0.775510          |\n",
      "PROGRESS: | 4         | 5        | 0.000301  | 1.248833     | 0.766826          |\n",
      "PROGRESS: | 5         | 6        | 0.000301  | 1.359907     | 0.770300          |\n",
      "PROGRESS: | 6         | 7        | 0.000301  | 1.467980     | 0.772471          |\n",
      "PROGRESS: | 10        | 11       | 0.000301  | 1.909273     | 0.782023          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79969\n",
      "PROGRESS: Number of coefficients    : 79970\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.911607     | 0.783666          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.020679     | 0.740660          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.171781     | 0.771503          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.278852     | 0.766725          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.386925     | 0.769331          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.497998     | 0.769331          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.932287     | 0.775413          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80034\n",
      "PROGRESS: Number of coefficients    : 80035\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.899600     | 0.778888          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.006671     | 0.752389          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.153769     | 0.766290          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.261840     | 0.766290          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.369912     | 0.771069          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.478985     | 0.772372          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.920280     | 0.779322          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79911\n",
      "PROGRESS: Number of coefficients    : 79912\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.906601     | 0.782798          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.011671     | 0.750652          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.164775     | 0.761512          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.274848     | 0.761946          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.381918     | 0.764987          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.487991     | 0.766725          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.924280     | 0.776281          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79950\n",
      "PROGRESS: Number of coefficients    : 79951\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.944628     | 0.785838          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.059705     | 0.757602          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.171779     | 0.767159          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.277850     | 0.766725          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.384921     | 0.767159          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.491994     | 0.767159          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.920278     | 0.772806          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80098\n",
      "PROGRESS: Number of coefficients    : 80099\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.934622     | 0.775847          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.043695     | 0.750217          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.155769     | 0.762381          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.263842     | 0.761512          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.370913     | 0.764118          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.480986     | 0.764118          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.918278     | 0.767593          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79658\n",
      "PROGRESS: Number of coefficients    : 79659\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.933620     | 0.782363          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.041692     | 0.759774          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.149764     | 0.765856          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.257836     | 0.768028          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.365908     | 0.770200          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.477983     | 0.769765          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.918277     | 0.776281          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80441\n",
      "PROGRESS: Number of coefficients    : 80442\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.947630     | 0.784535          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.056703     | 0.750217          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.164776     | 0.767593          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.278852     | 0.762381          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.388926     | 0.763684          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.492995     | 0.766290          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.925283     | 0.776281          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79909\n",
      "PROGRESS: Number of coefficients    : 79910\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.940621     | 0.777150          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.058700     | 0.755430          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.167772     | 0.764987          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.283849     | 0.767159          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.394924     | 0.770634          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.502995     | 0.772806          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.940288     | 0.776716          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79835\n",
      "PROGRESS: Number of coefficients    : 79836\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.948633     | 0.782458          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.055704     | 0.758576          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.163777     | 0.763786          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.272848     | 0.764655          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.382921     | 0.768563          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.487992     | 0.768129          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.920280     | 0.771168          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79948\n",
      "PROGRESS: Number of coefficients    : 79949\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.909606     | 0.783326          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.022682     | 0.741641          |\n",
      "PROGRESS: | 3         | 4        | 0.000301  | 1.131753     | 0.775510          |\n",
      "PROGRESS: | 4         | 5        | 0.000301  | 1.244829     | 0.767694          |\n",
      "PROGRESS: | 5         | 6        | 0.000301  | 1.355904     | 0.769865          |\n",
      "PROGRESS: | 6         | 7        | 0.000301  | 1.461974     | 0.772471          |\n",
      "PROGRESS: | 10        | 11       | 0.000301  | 1.901268     | 0.781589          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79969\n",
      "PROGRESS: Number of coefficients    : 79970\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.905604     | 0.783666          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.013676     | 0.741095          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.165776     | 0.771069          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.274849     | 0.767159          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.384922     | 0.769331          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.493996     | 0.769765          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.923281     | 0.775847          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80034\n",
      "PROGRESS: Number of coefficients    : 80035\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.905604     | 0.778888          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.013676     | 0.751955          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.166778     | 0.767159          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.277853     | 0.766290          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.393929     | 0.771503          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.506005     | 0.772806          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79911\n",
      "PROGRESS: Number of coefficients    : 79912\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.898598     | 0.782798          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.009673     | 0.751520          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.156770     | 0.761512          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.267845     | 0.761512          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.382922     | 0.765421          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.492995     | 0.766725          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.930286     | 0.776281          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79950\n",
      "PROGRESS: Number of coefficients    : 79951\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.927609     | 0.785838          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.036682     | 0.758036          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.145754     | 0.767593          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.253826     | 0.766725          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.360897     | 0.766725          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.469971     | 0.767159          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.918269     | 0.772372          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80098\n",
      "PROGRESS: Number of coefficients    : 80099\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.936620     | 0.775847          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.045692     | 0.750217          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.155767     | 0.762381          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.266839     | 0.761077          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.375913     | 0.763684          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.484986     | 0.764553          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.916274     | 0.767593          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79658\n",
      "PROGRESS: Number of coefficients    : 79659\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.937625     | 0.782363          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.046697     | 0.759774          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.159775     | 0.765856          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.267845     | 0.767159          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.373916     | 0.770200          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.482988     | 0.770634          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.919279     | 0.776281          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80441\n",
      "PROGRESS: Number of coefficients    : 80442\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.933623     | 0.784535          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.041695     | 0.750217          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.152768     | 0.767593          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.265844     | 0.762381          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.373916     | 0.763684          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.492995     | 0.766290          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.921281     | 0.776281          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79909\n",
      "PROGRESS: Number of coefficients    : 79910\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.945629     | 0.777150          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.054703     | 0.755430          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.162774     | 0.764553          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.272847     | 0.767159          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.381921     | 0.770634          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.488992     | 0.772806          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.925283     | 0.777150          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79835\n",
      "PROGRESS: Number of coefficients    : 79836\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.947632     | 0.782458          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.054702     | 0.758576          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.166777     | 0.763786          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.276850     | 0.764655          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.385923     | 0.768563          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.498998     | 0.768997          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.939292     | 0.771168          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79948\n",
      "PROGRESS: Number of coefficients    : 79949\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.906605     | 0.783326          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.020678     | 0.742076          |\n",
      "PROGRESS: | 3         | 4        | 0.000301  | 1.133754     | 0.775510          |\n",
      "PROGRESS: | 4         | 5        | 0.000301  | 1.240827     | 0.767694          |\n",
      "PROGRESS: | 5         | 6        | 0.000301  | 1.349898     | 0.770300          |\n",
      "PROGRESS: | 6         | 7        | 0.000301  | 1.459973     | 0.773339          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79969\n",
      "PROGRESS: Number of coefficients    : 79970\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.892594     | 0.783666          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 0.999666     | 0.741529          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.148764     | 0.771503          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.258839     | 0.767159          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.365910     | 0.769765          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.472983     | 0.769765          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80034\n",
      "PROGRESS: Number of coefficients    : 80035\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.906603     | 0.778888          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.016678     | 0.751955          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.162775     | 0.767159          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.273849     | 0.766725          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.379919     | 0.771069          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.489994     | 0.774109          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79911\n",
      "PROGRESS: Number of coefficients    : 79912\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.898600     | 0.782798          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.005672     | 0.751086          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.156773     | 0.762381          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.262843     | 0.761946          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.372916     | 0.765421          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.483991     | 0.767159          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79950\n",
      "PROGRESS: Number of coefficients    : 79951\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.940627     | 0.785838          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.049699     | 0.758036          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.157773     | 0.767159          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.268845     | 0.766725          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.375917     | 0.766290          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.486993     | 0.767159          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80098\n",
      "PROGRESS: Number of coefficients    : 80099\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.931619     | 0.775847          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.044696     | 0.749783          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.153768     | 0.762381          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.265842     | 0.761077          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.373915     | 0.762815          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.483987     | 0.764553          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.918278     | 0.768028          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79658\n",
      "PROGRESS: Number of coefficients    : 79659\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.929620     | 0.782363          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.038692     | 0.759774          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.146764     | 0.765856          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.254837     | 0.766725          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.366910     | 0.770200          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.474982     | 0.771069          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.906270     | 0.775847          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80441\n",
      "PROGRESS: Number of coefficients    : 80442\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.921613     | 0.784535          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.029684     | 0.750652          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.136756     | 0.767593          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.242827     | 0.762381          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.351900     | 0.763684          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.460973     | 0.766290          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.895262     | 0.776716          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79909\n",
      "PROGRESS: Number of coefficients    : 79910\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.942628     | 0.777150          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.049699     | 0.755430          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.159772     | 0.764553          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.268845     | 0.768028          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.378919     | 0.770200          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.483989     | 0.773675          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.915277     | 0.776716          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79835\n",
      "PROGRESS: Number of coefficients    : 79836\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.935611     | 0.782458          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.042682     | 0.758576          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.150754     | 0.763786          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.262829     | 0.764221          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.369900     | 0.768563          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.479973     | 0.768997          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.909260     | 0.772471          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79948\n",
      "PROGRESS: Number of coefficients    : 79949\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.905600     | 0.783326          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.013673     | 0.741641          |\n",
      "PROGRESS: | 3         | 4        | 0.000301  | 1.120743     | 0.775510          |\n",
      "PROGRESS: | 4         | 5        | 0.000301  | 1.229815     | 0.767694          |\n",
      "PROGRESS: | 5         | 6        | 0.000301  | 1.336887     | 0.770734          |\n",
      "PROGRESS: | 6         | 7        | 0.000301  | 1.446961     | 0.773339          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79969\n",
      "PROGRESS: Number of coefficients    : 79970\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.894595     | 0.783666          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.000667     | 0.741964          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.147764     | 0.771503          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.257838     | 0.767593          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.366910     | 0.769331          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.477984     | 0.768897          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80034\n",
      "PROGRESS: Number of coefficients    : 80035\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.902603     | 0.778888          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.011675     | 0.751955          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.163776     | 0.767593          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.270847     | 0.766725          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.382923     | 0.770634          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.491996     | 0.774109          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79911\n",
      "PROGRESS: Number of coefficients    : 79912\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.893595     | 0.782798          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.000665     | 0.751086          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.144763     | 0.762815          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.250834     | 0.762815          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.356903     | 0.765421          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.463974     | 0.767159          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79950\n",
      "PROGRESS: Number of coefficients    : 79951\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.945630     | 0.785838          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.054704     | 0.758036          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.162776     | 0.766725          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.271849     | 0.765856          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.377918     | 0.766725          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.487992     | 0.766725          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80098\n",
      "PROGRESS: Number of coefficients    : 80099\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.931620     | 0.775847          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.042694     | 0.750217          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.148766     | 0.762381          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.258839     | 0.761077          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.368912     | 0.762381          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.478985     | 0.764553          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79658\n",
      "PROGRESS: Number of coefficients    : 79659\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.931622     | 0.782363          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.038692     | 0.759774          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.144762     | 0.765856          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.253835     | 0.766290          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.361908     | 0.769765          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.467979     | 0.770634          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.909272     | 0.777150          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80441\n",
      "PROGRESS: Number of coefficients    : 80442\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.932620     | 0.784535          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.038691     | 0.750652          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.149765     | 0.767593          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.257837     | 0.762381          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.362906     | 0.763684          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.471979     | 0.765856          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79909\n",
      "PROGRESS: Number of coefficients    : 79910\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.920614     | 0.777150          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.029687     | 0.755430          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.139761     | 0.764553          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.247832     | 0.768028          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.357906     | 0.769765          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.463976     | 0.773675          |\n",
      "PROGRESS: | 10        | 11       | 0.000200  | 1.894263     | 0.775413          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79835\n",
      "PROGRESS: Number of coefficients    : 79836\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.927617     | 0.782458          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.036691     | 0.758576          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.143762     | 0.763786          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.247831     | 0.764655          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.352902     | 0.768997          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.467977     | 0.769865          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79948\n",
      "PROGRESS: Number of coefficients    : 79949\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.902602     | 0.783326          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.012676     | 0.742510          |\n",
      "PROGRESS: | 3         | 4        | 0.000301  | 1.121750     | 0.775076          |\n",
      "PROGRESS: | 4         | 5        | 0.000301  | 1.239827     | 0.767694          |\n",
      "PROGRESS: | 5         | 6        | 0.000301  | 1.349901     | 0.770300          |\n",
      "PROGRESS: | 6         | 7        | 0.000301  | 1.456974     | 0.773339          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79969\n",
      "PROGRESS: Number of coefficients    : 79970\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.900601     | 0.783666          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.004671     | 0.741529          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.146765     | 0.771503          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.254836     | 0.766725          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.360907     | 0.770200          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.470982     | 0.769331          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80034\n",
      "PROGRESS: Number of coefficients    : 80035\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.915610     | 0.778888          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.022681     | 0.751955          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.169779     | 0.767593          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.276849     | 0.766725          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.385924     | 0.770634          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.491993     | 0.774978          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79911\n",
      "PROGRESS: Number of coefficients    : 79912\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.895595     | 0.782798          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.007670     | 0.751086          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.153768     | 0.762815          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.259837     | 0.763249          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.373915     | 0.764987          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.481986     | 0.768028          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79950\n",
      "PROGRESS: Number of coefficients    : 79951\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.943627     | 0.785838          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.052700     | 0.758036          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.165776     | 0.766725          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.277852     | 0.765856          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.385923     | 0.767159          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.494995     | 0.767593          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80098\n",
      "PROGRESS: Number of coefficients    : 80099\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.924617     | 0.775847          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.035693     | 0.750217          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.144763     | 0.762381          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.254838     | 0.761077          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.362909     | 0.761946          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.469983     | 0.764553          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79658\n",
      "PROGRESS: Number of coefficients    : 79659\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.943628     | 0.782363          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.056704     | 0.759774          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.161775     | 0.766290          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.273850     | 0.766290          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.379920     | 0.769765          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.490993     | 0.770634          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80441\n",
      "PROGRESS: Number of coefficients    : 80442\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.934622     | 0.784535          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.044696     | 0.750652          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.155770     | 0.767159          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.262841     | 0.762381          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.371914     | 0.763684          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.476984     | 0.765421          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79909\n",
      "PROGRESS: Number of coefficients    : 79910\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.923616     | 0.777150          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.025683     | 0.755430          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.133755     | 0.764553          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.241828     | 0.768028          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.346898     | 0.769765          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.455969     | 0.774544          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79835\n",
      "PROGRESS: Number of coefficients    : 79836\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.942627     | 0.782458          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.050699     | 0.758576          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.158772     | 0.763786          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.265842     | 0.765523          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.375917     | 0.768997          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.482988     | 0.768997          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79948\n",
      "PROGRESS: Number of coefficients    : 79949\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.915610     | 0.783326          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.029687     | 0.742510          |\n",
      "PROGRESS: | 3         | 4        | 0.000301  | 1.139760     | 0.775076          |\n",
      "PROGRESS: | 4         | 5        | 0.000301  | 1.249833     | 0.767260          |\n",
      "PROGRESS: | 5         | 6        | 0.000301  | 1.358906     | 0.770300          |\n",
      "PROGRESS: | 6         | 7        | 0.000301  | 1.462975     | 0.772905          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79969\n",
      "PROGRESS: Number of coefficients    : 79970\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.893598     | 0.783666          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.003669     | 0.741529          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.150767     | 0.771069          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.261841     | 0.766725          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.369913     | 0.770200          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.475983     | 0.770200          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80034\n",
      "PROGRESS: Number of coefficients    : 80035\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.905604     | 0.778888          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.017681     | 0.751955          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.166778     | 0.767593          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.278853     | 0.766725          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.386924     | 0.770634          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.495998     | 0.774978          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79911\n",
      "PROGRESS: Number of coefficients    : 79912\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.907605     | 0.782798          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.014677     | 0.750652          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.163777     | 0.762815          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.271847     | 0.763249          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.381922     | 0.765421          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.490994     | 0.768028          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79950\n",
      "PROGRESS: Number of coefficients    : 79951\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.944629     | 0.785838          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.052703     | 0.758036          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.163776     | 0.766725          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.272849     | 0.765856          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.381921     | 0.767593          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.488993     | 0.768462          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80098\n",
      "PROGRESS: Number of coefficients    : 80099\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.933624     | 0.775847          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.044697     | 0.750217          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.155770     | 0.762381          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.260841     | 0.761077          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.369913     | 0.761946          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.477986     | 0.764987          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79658\n",
      "PROGRESS: Number of coefficients    : 79659\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.926616     | 0.782363          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.037691     | 0.759774          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.146762     | 0.766725          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.257837     | 0.766290          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.366910     | 0.769765          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.472982     | 0.770634          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80441\n",
      "PROGRESS: Number of coefficients    : 80442\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.931620     | 0.784535          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.037691     | 0.751086          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.144763     | 0.767159          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.250835     | 0.762381          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.364911     | 0.763249          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.470980     | 0.765421          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79909\n",
      "PROGRESS: Number of coefficients    : 79910\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.939626     | 0.777150          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.046697     | 0.755864          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.155771     | 0.764553          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.265844     | 0.768028          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.375917     | 0.769765          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.483989     | 0.775413          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79835\n",
      "PROGRESS: Number of coefficients    : 79836\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.944629     | 0.782458          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.054702     | 0.758142          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.163774     | 0.763786          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.269845     | 0.765089          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.373914     | 0.768997          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.479986     | 0.768997          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79948\n",
      "PROGRESS: Number of coefficients    : 79949\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.901600     | 0.783326          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.009673     | 0.742076          |\n",
      "PROGRESS: | 3         | 4        | 0.000301  | 1.119747     | 0.775510          |\n",
      "PROGRESS: | 4         | 5        | 0.000301  | 1.227818     | 0.767694          |\n",
      "PROGRESS: | 5         | 6        | 0.000301  | 1.335890     | 0.770734          |\n",
      "PROGRESS: | 6         | 7        | 0.000301  | 1.451967     | 0.773339          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79969\n",
      "PROGRESS: Number of coefficients    : 79970\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.889592     | 0.783666          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.003669     | 0.742398          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.144763     | 0.771503          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.250833     | 0.766725          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.352902     | 0.771069          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.464977     | 0.770634          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80034\n",
      "PROGRESS: Number of coefficients    : 80035\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.911607     | 0.778888          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.025682     | 0.751955          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.174782     | 0.768028          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.279853     | 0.766290          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.389927     | 0.771069          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.500000     | 0.774978          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79911\n",
      "PROGRESS: Number of coefficients    : 79912\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.905601     | 0.782798          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.012672     | 0.750652          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.162772     | 0.762381          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.269843     | 0.763249          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.379917     | 0.765856          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.488991     | 0.768028          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79950\n",
      "PROGRESS: Number of coefficients    : 79951\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.925616     | 0.785838          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.033687     | 0.758036          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.141761     | 0.767159          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.249831     | 0.765856          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.356903     | 0.768028          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.465976     | 0.768462          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80098\n",
      "PROGRESS: Number of coefficients    : 80099\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.936624     | 0.775847          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.049699     | 0.750217          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.156771     | 0.762381          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.262842     | 0.760643          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.378920     | 0.761946          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.484989     | 0.764987          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79658\n",
      "PROGRESS: Number of coefficients    : 79659\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.938625     | 0.782363          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.046696     | 0.758905          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.156769     | 0.766725          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.267844     | 0.766290          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.376916     | 0.769765          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.485990     | 0.770634          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80441\n",
      "PROGRESS: Number of coefficients    : 80442\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.933618     | 0.784535          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.043691     | 0.751520          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.150763     | 0.767159          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.261838     | 0.762381          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.368907     | 0.763249          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.479982     | 0.764987          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79909\n",
      "PROGRESS: Number of coefficients    : 79910\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.946631     | 0.777150          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.048700     | 0.755864          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.154771     | 0.764553          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.264843     | 0.768028          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.374918     | 0.770200          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.481988     | 0.775847          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79835\n",
      "PROGRESS: Number of coefficients    : 79836\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.944630     | 0.782458          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.054703     | 0.757707          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.162775     | 0.764221          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.272849     | 0.765089          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.377918     | 0.768997          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.484991     | 0.768997          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79948\n",
      "PROGRESS: Number of coefficients    : 79949\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.913609     | 0.783326          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.020681     | 0.741641          |\n",
      "PROGRESS: | 3         | 4        | 0.000301  | 1.123750     | 0.775510          |\n",
      "PROGRESS: | 4         | 5        | 0.000301  | 1.238825     | 0.766826          |\n",
      "PROGRESS: | 5         | 6        | 0.000301  | 1.346897     | 0.770734          |\n",
      "PROGRESS: | 6         | 7        | 0.000301  | 1.455970     | 0.773339          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79969\n",
      "PROGRESS: Number of coefficients    : 79970\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.908605     | 0.783666          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.016677     | 0.742832          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.162774     | 0.771937          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.270847     | 0.766725          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.375916     | 0.771503          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.486990     | 0.771503          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80034\n",
      "PROGRESS: Number of coefficients    : 80035\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.909607     | 0.778888          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.019678     | 0.752389          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.165776     | 0.767159          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.276850     | 0.765856          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.386924     | 0.770634          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.496996     | 0.774978          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79911\n",
      "PROGRESS: Number of coefficients    : 79912\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.895595     | 0.782798          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.005669     | 0.750652          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.152767     | 0.762381          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.260839     | 0.762815          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.370912     | 0.765856          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.481987     | 0.767593          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79950\n",
      "PROGRESS: Number of coefficients    : 79951\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.932622     | 0.785838          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.038693     | 0.758036          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.147765     | 0.767159          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.259839     | 0.765856          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.368912     | 0.766725          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.479988     | 0.768028          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80098\n",
      "PROGRESS: Number of coefficients    : 80099\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.935625     | 0.775847          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.046697     | 0.750217          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.156770     | 0.761946          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.267844     | 0.760643          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.379919     | 0.762381          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.490993     | 0.764987          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79658\n",
      "PROGRESS: Number of coefficients    : 79659\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.935626     | 0.782363          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.045700     | 0.758471          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.155773     | 0.766290          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.265845     | 0.766725          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.371916     | 0.769765          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.482991     | 0.770634          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80441\n",
      "PROGRESS: Number of coefficients    : 80442\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.932620     | 0.784535          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.040693     | 0.751520          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.149766     | 0.767159          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.258838     | 0.762381          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.363909     | 0.763684          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.470979     | 0.764553          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79909\n",
      "PROGRESS: Number of coefficients    : 79910\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.950634     | 0.777150          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.059707     | 0.755864          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.169781     | 0.764553          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.276851     | 0.768028          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.388926     | 0.770200          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.498999     | 0.775413          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79835\n",
      "PROGRESS: Number of coefficients    : 79836\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.926617     | 0.782458          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.032687     | 0.757707          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.141761     | 0.764221          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.251834     | 0.765089          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.357904     | 0.768563          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.466977     | 0.768997          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79948\n",
      "PROGRESS: Number of coefficients    : 79949\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.902600     | 0.783326          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.020679     | 0.740773          |\n",
      "PROGRESS: | 3         | 4        | 0.000301  | 1.126750     | 0.775944          |\n",
      "PROGRESS: | 4         | 5        | 0.000301  | 1.235823     | 0.766826          |\n",
      "PROGRESS: | 5         | 6        | 0.000301  | 1.345896     | 0.771602          |\n",
      "PROGRESS: | 6         | 7        | 0.000301  | 1.453967     | 0.773773          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79969\n",
      "PROGRESS: Number of coefficients    : 79970\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.894595     | 0.783666          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.001667     | 0.742832          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.148766     | 0.771937          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.257838     | 0.766290          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.363908     | 0.771069          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.472982     | 0.771503          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80034\n",
      "PROGRESS: Number of coefficients    : 80035\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.904602     | 0.778888          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.012676     | 0.753258          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.169780     | 0.767159          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.277852     | 0.766290          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.383923     | 0.770200          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.492995     | 0.775847          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79911\n",
      "PROGRESS: Number of coefficients    : 79912\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000301  | 0.892596     | 0.782798          |\n",
      "PROGRESS: | 2         | 3        | 0.000301  | 1.004671     | 0.750652          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.151768     | 0.762381          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.257839     | 0.763249          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.372916     | 0.765421          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.480990     | 0.767593          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79950\n",
      "PROGRESS: Number of coefficients    : 79951\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.946632     | 0.785838          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.055704     | 0.758036          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.161775     | 0.767593          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.269847     | 0.766725          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.382923     | 0.766725          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.489993     | 0.768028          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80098\n",
      "PROGRESS: Number of coefficients    : 80099\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.930619     | 0.775847          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.034688     | 0.750652          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.142761     | 0.761946          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.251834     | 0.761077          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.361906     | 0.762815          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.473981     | 0.764987          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79658\n",
      "PROGRESS: Number of coefficients    : 79659\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.932622     | 0.782363          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.041696     | 0.758471          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.149767     | 0.766290          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.259840     | 0.767159          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.366911     | 0.769765          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.479988     | 0.770634          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80441\n",
      "PROGRESS: Number of coefficients    : 80442\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.926616     | 0.784535          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.030685     | 0.751520          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.137757     | 0.767159          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.246830     | 0.761946          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.354902     | 0.764118          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.464975     | 0.764118          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79909\n",
      "PROGRESS: Number of coefficients    : 79910\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.934624     | 0.777150          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.043697     | 0.755864          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.150768     | 0.764987          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.261842     | 0.768462          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.368913     | 0.770200          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.476986     | 0.774978          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79835\n",
      "PROGRESS: Number of coefficients    : 79836\n",
      "PROGRESS: Starting Accelerated Gradient (FISTA)\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Tuning step size. First iteration could take longer than subsequent iterations.\n",
      "PROGRESS: | 1         | 2        | 0.000200  | 0.925616     | 0.782458          |\n",
      "PROGRESS: | 2         | 3        | 0.000200  | 1.033688     | 0.758142          |\n",
      "PROGRESS: | 3         | 4        | 0.000200  | 1.143761     | 0.763786          |\n",
      "PROGRESS: | 4         | 5        | 0.000200  | 1.254836     | 0.764655          |\n",
      "PROGRESS: | 5         | 6        | 0.000200  | 1.361908     | 0.768129          |\n",
      "PROGRESS: | 6         | 7        | 0.000200  | 1.470981     | 0.768997          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "0\n",
      "0.789883268482\n",
      "6\n",
      "0.849604221636\n",
      "10.0\n",
      "70.0\n"
     ]
    }
   ],
   "source": [
    "penalty = []\n",
    "acc = []\n",
    "fsco = []\n",
    "for i in np.linspace(10, 100, num=10):\n",
    "    (maxAccuracy,maxfscores) = k_fold_cross_validation(10, i, train_data_pos_neg, 'Sentiment')\n",
    "    penalty.append(i)\n",
    "    acc.append(maxAccuracy)\n",
    "    fsco.append(maxfscores)\n",
    "    #print i\n",
    "\n",
    "print acc.index(max(acc))\n",
    "print max(acc)\n",
    "print fsco.index(max(fsco))\n",
    "print max(fsco)\n",
    "print penalty[acc.index(max(acc))]\n",
    "print penalty[fsco.index(max(fsco))]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244879\n",
      "PROGRESS: Number of coefficients    : 244880\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 3        | 0.000117  | 0.377255     | 0.990407          |\n",
      "PROGRESS: | 2         | 5        | 1.000000  | 0.676453     | 0.998947          |\n",
      "PROGRESS: | 3         | 6        | 1.000000  | 0.865578     | 0.867337          |\n",
      "PROGRESS: | 4         | 8        | 1.000000  | 1.182793     | 0.999181          |\n",
      "PROGRESS: | 5         | 9        | 1.000000  | 1.376923     | 0.999415          |\n",
      "PROGRESS: | 6         | 13       | 0.250000  | 1.877253     | 0.999532          |\n",
      "PROGRESS: | 10        | 21       | 1.000000  | 3.080058     | 0.999532          |\n",
      "PROGRESS: | 11        | 22       | 1.000000  | 3.281189     | 0.999532          |\n",
      "PROGRESS: | 15        | 27       | 1.000000  | 4.152773     | 0.999298          |\n",
      "PROGRESS: | 20        | 34       | 1.000000  | 5.329558     | 0.999532          |\n",
      "PROGRESS: | 25        | 40       | 1.000000  | 6.395269     | 0.999649          |\n",
      "PROGRESS: | 30        | 46       | 1.000000  | 7.515015     | 0.999649          |\n",
      "PROGRESS: | 35        | 51       | 1.000000  | 8.519681     | 0.999532          |\n",
      "PROGRESS: | 40        | 59       | 1.000000  | 9.860583     | 0.999532          |\n",
      "PROGRESS: | 45        | 68       | 1.000000  | 11.308544    | 0.999532          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244633\n",
      "PROGRESS: Number of coefficients    : 244634\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 3        | 0.000117  | 0.361241     | 0.990523          |\n",
      "PROGRESS: | 2         | 5        | 1.000000  | 0.681457     | 0.999181          |\n",
      "PROGRESS: | 3         | 6        | 1.000000  | 0.870583     | 0.866854          |\n",
      "PROGRESS: | 4         | 8        | 1.000000  | 1.173786     | 0.999298          |\n",
      "PROGRESS: | 5         | 9        | 1.000000  | 1.372916     | 0.999415          |\n",
      "PROGRESS: | 6         | 13       | 0.250000  | 1.895267     | 0.999532          |\n",
      "PROGRESS: | 10        | 21       | 1.000000  | 3.091062     | 0.999532          |\n",
      "PROGRESS: | 11        | 22       | 1.000000  | 3.287194     | 0.999532          |\n",
      "PROGRESS: | 15        | 30       | 3.000000  | 4.476989     | 0.999415          |\n",
      "PROGRESS: | 20        | 37       | 1.000000  | 5.661777     | 0.999532          |\n",
      "PROGRESS: | 25        | 43       | 1.000000  | 6.741496     | 0.999532          |\n",
      "PROGRESS: | 30        | 50       | 1.000000  | 7.915277     | 0.999532          |\n",
      "PROGRESS: | 35        | 56       | 1.000000  | 8.973985     | 0.999415          |\n",
      "PROGRESS: | 40        | 62       | 1.000000  | 10.043699    | 0.999532          |\n",
      "PROGRESS: | 45        | 70       | 1.000000  | 11.356572    | 0.999532          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244617\n",
      "PROGRESS: Number of coefficients    : 244618\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 3        | 0.000117  | 0.344233     | 0.991342          |\n",
      "PROGRESS: | 2         | 7        | 3.000000  | 0.840561     | 0.991927          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 1.143765     | 0.998713          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.332892     | 0.999181          |\n",
      "PROGRESS: | 5         | 11       | 1.000000  | 1.524022     | 0.999649          |\n",
      "PROGRESS: | 6         | 12       | 1.000000  | 1.715146     | 0.999532          |\n",
      "PROGRESS: | 10        | 18       | 1.000000  | 2.692797     | 0.999532          |\n",
      "PROGRESS: | 11        | 19       | 1.000000  | 2.886933     | 0.999532          |\n",
      "PROGRESS: | 15        | 24       | 1.000000  | 3.776519     | 0.999532          |\n",
      "PROGRESS: | 20        | 32       | 1.000000  | 5.089394     | 0.999415          |\n",
      "PROGRESS: | 25        | 41       | 0.250000  | 6.473320     | 0.999532          |\n",
      "PROGRESS: | 30        | 49       | 1.000000  | 7.780189     | 0.999532          |\n",
      "PROGRESS: | 35        | 58       | 0.500000  | 9.199137     | 0.999532          |\n",
      "PROGRESS: | 40        | 71       | 1.000000  | 11.022351    | 0.999532          |\n",
      "PROGRESS: | 45        | 78       | 1.000000  | 12.195136    | 0.999532          |\n",
      "PROGRESS: | 50        | 89       | 0.500000  | 13.769180    | 0.999532          |\n",
      "PROGRESS: | 51        | 91       | 1.000000  | 14.086393    | 0.999532          |\n",
      "PROGRESS: | 55        | 95       | 1.000000  | 14.863916    | 0.999532          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245269\n",
      "PROGRESS: Number of coefficients    : 245270\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 3        | 0.000117  | 0.339227     | 0.991459          |\n",
      "PROGRESS: | 2         | 7        | 3.000000  | 0.878594     | 0.991927          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 1.194797     | 0.998830          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.381926     | 0.999298          |\n",
      "PROGRESS: | 5         | 11       | 1.000000  | 1.557041     | 0.999766          |\n",
      "PROGRESS: | 6         | 12       | 1.000000  | 1.744164     | 0.999766          |\n",
      "PROGRESS: | 10        | 18       | 1.000000  | 2.734828     | 0.999766          |\n",
      "PROGRESS: | 11        | 19       | 1.000000  | 2.928954     | 0.999766          |\n",
      "PROGRESS: | 15        | 26       | 0.500000  | 4.026686     | 0.999766          |\n",
      "PROGRESS: | 20        | 38       | 0.500000  | 5.717814     | 0.999883          |\n",
      "PROGRESS: | 25        | 47       | 1.000000  | 7.104738     | 0.999532          |\n",
      "PROGRESS: | 30        | 55       | 1.000000  | 8.404606     | 0.999766          |\n",
      "PROGRESS: | 35        | 63       | 0.500000  | 9.714578     | 0.999766          |\n",
      "PROGRESS: | 40        | 72       | 0.500000  | 11.107503    | 0.999766          |\n",
      "PROGRESS: | 45        | 84       | 1.000000  | 12.808641    | 0.999766          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245817\n",
      "PROGRESS: Number of coefficients    : 245818\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 3        | 0.000117  | 0.342233     | 0.990991          |\n",
      "PROGRESS: | 2         | 5        | 1.000000  | 0.633422     | 0.999532          |\n",
      "PROGRESS: | 3         | 6        | 1.000000  | 0.823554     | 0.864163          |\n",
      "PROGRESS: | 4         | 8        | 1.000000  | 1.133762     | 0.999649          |\n",
      "PROGRESS: | 5         | 9        | 1.000000  | 1.319883     | 0.999766          |\n",
      "PROGRESS: | 6         | 13       | 0.250000  | 1.858160     | 0.999766          |\n",
      "PROGRESS: | 10        | 21       | 1.000000  | 3.094982     | 0.999766          |\n",
      "PROGRESS: | 11        | 22       | 1.000000  | 3.293111     | 0.999766          |\n",
      "PROGRESS: | 15        | 29       | 0.500000  | 4.397852     | 0.999766          |\n",
      "PROGRESS: | 20        | 41       | 1.000000  | 6.153383     | 0.999532          |\n",
      "PROGRESS: | 25        | 50       | 1.000000  | 7.590344     | 0.999415          |\n",
      "PROGRESS: | 30        | 56       | 1.000000  | 8.709095     | 0.999766          |\n",
      "PROGRESS: | 35        | 65       | 1.000000  | 10.092014    | 0.999766          |\n",
      "PROGRESS: | 40        | 75       | 0.500000  | 11.602018    | 0.999766          |\n",
      "PROGRESS: | 45        | 80       | 0.500000  | 12.559656    | 0.999766          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245213\n",
      "PROGRESS: Number of coefficients    : 245214\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 3        | 0.000117  | 0.376250     | 0.989822          |\n",
      "PROGRESS: | 2         | 5        | 1.000000  | 0.674450     | 0.998947          |\n",
      "PROGRESS: | 3         | 6        | 1.000000  | 0.857575     | 0.869794          |\n",
      "PROGRESS: | 4         | 8        | 1.000000  | 1.152770     | 0.999298          |\n",
      "PROGRESS: | 5         | 9        | 1.000000  | 1.350906     | 0.999532          |\n",
      "PROGRESS: | 6         | 13       | 0.250000  | 1.879256     | 0.999649          |\n",
      "PROGRESS: | 10        | 21       | 1.000000  | 3.087059     | 0.999649          |\n",
      "PROGRESS: | 11        | 22       | 1.000000  | 3.271185     | 0.999649          |\n",
      "PROGRESS: | 15        | 28       | 1.000000  | 4.248832     | 0.999532          |\n",
      "PROGRESS: | 20        | 34       | 1.000000  | 5.331557     | 0.999649          |\n",
      "PROGRESS: | 25        | 42       | 1.000000  | 6.617413     | 0.999298          |\n",
      "PROGRESS: | 30        | 48       | 1.000000  | 7.705139     | 0.999766          |\n",
      "PROGRESS: | 35        | 57       | 1.000000  | 9.104071     | 0.999649          |\n",
      "PROGRESS: | 40        | 66       | 1.000000  | 10.531024    | 0.999649          |\n",
      "PROGRESS: | 45        | 75       | 1.000000  | 12.025021    | 0.999649          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 243924\n",
      "PROGRESS: Number of coefficients    : 243925\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 3        | 0.000117  | 0.379253     | 0.990757          |\n",
      "PROGRESS: | 2         | 5        | 1.000000  | 0.673452     | 0.998947          |\n",
      "PROGRESS: | 3         | 6        | 1.000000  | 0.863581     | 0.860653          |\n",
      "PROGRESS: | 4         | 8        | 1.000000  | 1.180792     | 0.999181          |\n",
      "PROGRESS: | 5         | 9        | 1.000000  | 1.373920     | 0.999415          |\n",
      "PROGRESS: | 6         | 13       | 0.250000  | 1.863243     | 0.999532          |\n",
      "PROGRESS: | 10        | 21       | 1.000000  | 3.099066     | 0.999532          |\n",
      "PROGRESS: | 11        | 22       | 1.000000  | 3.298198     | 0.999532          |\n",
      "PROGRESS: | 15        | 28       | 1.000000  | 4.312876     | 0.999532          |\n",
      "PROGRESS: | 20        | 34       | 1.000000  | 5.399599     | 0.999532          |\n",
      "PROGRESS: | 25        | 40       | 1.000000  | 6.485324     | 0.999532          |\n",
      "PROGRESS: | 30        | 48       | 1.000000  | 7.821218     | 0.999532          |\n",
      "PROGRESS: | 35        | 54       | 1.000000  | 8.902938     | 0.999532          |\n",
      "PROGRESS: | 40        | 62       | 0.250000  | 10.180788    | 0.999532          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244864\n",
      "PROGRESS: Number of coefficients    : 244865\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 3        | 0.000117  | 0.358241     | 0.989587          |\n",
      "PROGRESS: | 2         | 5        | 1.000000  | 0.663447     | 0.998830          |\n",
      "PROGRESS: | 3         | 6        | 1.000000  | 0.854575     | 0.865216          |\n",
      "PROGRESS: | 4         | 8        | 1.000000  | 1.151769     | 0.999181          |\n",
      "PROGRESS: | 5         | 9        | 1.000000  | 1.338896     | 0.999415          |\n",
      "PROGRESS: | 6         | 13       | 0.250000  | 1.838229     | 0.999532          |\n",
      "PROGRESS: | 10        | 21       | 1.000000  | 3.053039     | 0.999532          |\n",
      "PROGRESS: | 11        | 22       | 1.000000  | 3.242162     | 0.999532          |\n",
      "PROGRESS: | 15        | 28       | 1.000000  | 4.237831     | 0.999415          |\n",
      "PROGRESS: | 20        | 35       | 0.500000  | 5.409608     | 0.999532          |\n",
      "PROGRESS: | 25        | 41       | 1.000000  | 6.485324     | 0.999532          |\n",
      "PROGRESS: | 30        | 49       | 1.000000  | 7.802207     | 0.999532          |\n",
      "PROGRESS: | 35        | 56       | 1.000000  | 8.981993     | 0.999532          |\n",
      "PROGRESS: | 40        | 64       | 0.500000  | 10.264847    | 0.999532          |\n",
      "PROGRESS: | 45        | 74       | 1.000000  | 11.774852    | 0.999532          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 243778\n",
      "PROGRESS: Number of coefficients    : 243779\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 3        | 0.000117  | 0.335229     | 0.990874          |\n",
      "PROGRESS: | 2         | 5        | 1.000000  | 0.636424     | 0.998947          |\n",
      "PROGRESS: | 3         | 6        | 1.000000  | 0.831555     | 0.867439          |\n",
      "PROGRESS: | 4         | 8        | 1.000000  | 1.129756     | 0.999298          |\n",
      "PROGRESS: | 5         | 9        | 1.000000  | 1.315885     | 0.999532          |\n",
      "PROGRESS: | 6         | 13       | 0.250000  | 1.850238     | 0.999649          |\n",
      "PROGRESS: | 10        | 21       | 1.000000  | 3.062041     | 0.999649          |\n",
      "PROGRESS: | 11        | 22       | 1.000000  | 3.261176     | 0.999649          |\n",
      "PROGRESS: | 15        | 28       | 1.000000  | 4.292863     | 0.999649          |\n",
      "PROGRESS: | 20        | 34       | 1.000000  | 5.384590     | 0.999532          |\n",
      "PROGRESS: | 25        | 41       | 1.000000  | 6.577391     | 0.999649          |\n",
      "PROGRESS: | 30        | 50       | 0.500000  | 7.997334     | 0.999649          |\n",
      "PROGRESS: | 35        | 57       | 0.500000  | 9.249173     | 0.999649          |\n",
      "PROGRESS: | 40        | 67       | 1.000000  | 10.796200    | 0.999649          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 246249\n",
      "PROGRESS: Number of coefficients    : 246250\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 3        | 0.000117  | 0.336227     | 0.990173          |\n",
      "PROGRESS: | 2         | 5        | 1.000000  | 0.637430     | 0.998947          |\n",
      "PROGRESS: | 3         | 6        | 1.000000  | 0.866578     | 0.859148          |\n",
      "PROGRESS: | 4         | 8        | 1.000000  | 1.165780     | 0.999181          |\n",
      "PROGRESS: | 5         | 9        | 1.000000  | 1.352902     | 0.999415          |\n",
      "PROGRESS: | 6         | 13       | 0.250000  | 1.863242     | 0.999532          |\n",
      "PROGRESS: | 10        | 21       | 1.000000  | 3.049037     | 0.999532          |\n",
      "PROGRESS: | 11        | 22       | 1.000000  | 3.244165     | 0.999532          |\n",
      "PROGRESS: | 15        | 28       | 1.000000  | 4.226822     | 0.999532          |\n",
      "PROGRESS: | 20        | 36       | 0.500000  | 5.509678     | 0.999532          |\n",
      "PROGRESS: | 25        | 43       | 1.000000  | 6.686460     | 0.999532          |\n",
      "PROGRESS: | 30        | 51       | 1.000000  | 7.981322     | 0.999532          |\n",
      "PROGRESS: | 35        | 59       | 0.500000  | 9.278191     | 0.999532          |\n",
      "PROGRESS: | 40        | 67       | 1.000000  | 10.566048    | 0.999532          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244879\n",
      "PROGRESS: Number of coefficients    : 244880\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000058  | 0.437295     | 0.990407          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.721486     | 0.998713          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.906607     | 0.999532          |\n",
      "PROGRESS: | 4         | 10       | 0.500000  | 1.308873     | 0.999298          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.612079     | 0.999532          |\n",
      "PROGRESS: | 6         | 14       | 1.000000  | 1.906272     | 0.999532          |\n",
      "PROGRESS: | 10        | 20       | 0.500000  | 2.883923     | 0.999532          |\n",
      "PROGRESS: | 11        | 22       | 1.000000  | 3.175117     | 0.999181          |\n",
      "PROGRESS: | 15        | 29       | 0.250000  | 4.274855     | 0.999298          |\n",
      "PROGRESS: | 20        | 36       | 1.000000  | 5.436632     | 0.999649          |\n",
      "PROGRESS: | 25        | 45       | 0.250000  | 6.807542     | 0.999532          |\n",
      "PROGRESS: | 30        | 54       | 0.500000  | 8.189466     | 0.999532          |\n",
      "PROGRESS: | 35        | 61       | 0.500000  | 9.370251     | 0.999532          |\n",
      "PROGRESS: | 40        | 69       | 1.000000  | 10.654105    | 0.999532          |\n",
      "PROGRESS: | 45        | 75       | 1.000000  | 11.724824    | 0.999532          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244633\n",
      "PROGRESS: Number of coefficients    : 244634\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000059  | 0.450305     | 0.990523          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.754505     | 0.998830          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.937626     | 0.999532          |\n",
      "PROGRESS: | 4         | 10       | 0.500000  | 1.346900     | 0.999064          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.644097     | 0.999532          |\n",
      "PROGRESS: | 6         | 14       | 1.000000  | 1.937292     | 0.999532          |\n",
      "PROGRESS: | 10        | 18       | 1.000000  | 2.704808     | 0.999298          |\n",
      "PROGRESS: | 11        | 20       | 1.000000  | 3.024017     | 0.999415          |\n",
      "PROGRESS: | 15        | 24       | 1.000000  | 3.892599     | 0.999415          |\n",
      "PROGRESS: | 20        | 31       | 1.000000  | 5.117414     | 0.999532          |\n",
      "PROGRESS: | 25        | 39       | 0.500000  | 6.415281     | 0.999532          |\n",
      "PROGRESS: | 30        | 47       | 1.000000  | 7.709139     | 0.999532          |\n",
      "PROGRESS: | 35        | 57       | 0.500000  | 9.244166     | 0.999532          |\n",
      "PROGRESS: | 40        | 65       | 1.000000  | 10.558038    | 0.999532          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244617\n",
      "PROGRESS: Number of coefficients    : 244618\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000059  | 0.446302     | 0.991342          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.756506     | 0.998830          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.945631     | 0.999532          |\n",
      "PROGRESS: | 4         | 10       | 0.500000  | 1.347901     | 0.999181          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.651103     | 0.999532          |\n",
      "PROGRESS: | 6         | 14       | 1.000000  | 1.949304     | 0.999532          |\n",
      "PROGRESS: | 10        | 18       | 1.000000  | 2.732824     | 0.999298          |\n",
      "PROGRESS: | 11        | 20       | 1.000000  | 3.031023     | 0.999415          |\n",
      "PROGRESS: | 15        | 24       | 1.000000  | 3.828553     | 0.999532          |\n",
      "PROGRESS: | 20        | 31       | 1.000000  | 5.007345     | 0.999532          |\n",
      "PROGRESS: | 25        | 41       | 0.500000  | 6.505340     | 0.999532          |\n",
      "PROGRESS: | 30        | 48       | 1.000000  | 7.679127     | 0.999532          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245269\n",
      "PROGRESS: Number of coefficients    : 245270\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000059  | 0.426280     | 0.991459          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.720480     | 0.999064          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.925609     | 0.999766          |\n",
      "PROGRESS: | 4         | 10       | 0.500000  | 1.322874     | 0.999649          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.634086     | 0.999766          |\n",
      "PROGRESS: | 6         | 14       | 1.000000  | 1.931285     | 0.999766          |\n",
      "PROGRESS: | 10        | 18       | 1.000000  | 2.688790     | 0.999532          |\n",
      "PROGRESS: | 11        | 20       | 1.000000  | 2.996994     | 0.999766          |\n",
      "PROGRESS: | 15        | 24       | 1.000000  | 3.761500     | 0.999532          |\n",
      "PROGRESS: | 20        | 31       | 1.000000  | 4.928277     | 0.999766          |\n",
      "PROGRESS: | 25        | 45       | 1.000000  | 6.889584     | 0.999766          |\n",
      "PROGRESS: | 30        | 53       | 1.000000  | 8.164436     | 0.999766          |\n",
      "PROGRESS: | 35        | 62       | 1.000000  | 9.560368     | 0.999766          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245817\n",
      "PROGRESS: Number of coefficients    : 245818\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000059  | 0.463302     | 0.990991          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.757503     | 0.999415          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.940623     | 0.999766          |\n",
      "PROGRESS: | 4         | 10       | 0.500000  | 1.361901     | 0.999649          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.675113     | 0.999766          |\n",
      "PROGRESS: | 6         | 14       | 1.000000  | 1.967305     | 0.999766          |\n",
      "PROGRESS: | 10        | 18       | 1.000000  | 2.757833     | 0.999532          |\n",
      "PROGRESS: | 11        | 20       | 1.000000  | 3.059036     | 0.999766          |\n",
      "PROGRESS: | 15        | 24       | 1.000000  | 3.829550     | 0.999766          |\n",
      "PROGRESS: | 20        | 31       | 1.000000  | 5.020342     | 0.999766          |\n",
      "PROGRESS: | 25        | 38       | 0.500000  | 6.203129     | 0.999766          |\n",
      "PROGRESS: | 30        | 46       | 1.000000  | 7.508999     | 0.999766          |\n",
      "PROGRESS: | 35        | 52       | 1.000000  | 8.598728     | 0.999766          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245213\n",
      "PROGRESS: Number of coefficients    : 245214\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000058  | 0.460310     | 0.989822          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.752505     | 0.998947          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.933627     | 0.999649          |\n",
      "PROGRESS: | 4         | 10       | 0.500000  | 1.324885     | 0.999415          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.646099     | 0.999649          |\n",
      "PROGRESS: | 6         | 14       | 1.000000  | 1.936291     | 0.999649          |\n",
      "PROGRESS: | 10        | 21       | 0.250000  | 2.996999     | 0.999649          |\n",
      "PROGRESS: | 11        | 25       | 0.250000  | 3.505338     | 0.999649          |\n",
      "PROGRESS: | 15        | 34       | 0.250000  | 4.822218     | 0.999649          |\n",
      "PROGRESS: | 20        | 39       | 0.250000  | 5.791862     | 0.999766          |\n",
      "PROGRESS: | 25        | 47       | 1.000000  | 7.080723     | 0.999649          |\n",
      "PROGRESS: | 30        | 55       | 0.500000  | 8.433622     | 0.999649          |\n",
      "PROGRESS: | 35        | 65       | 1.000000  | 10.052702    | 0.999649          |\n",
      "PROGRESS: | 40        | 73       | 1.000000  | 11.481659    | 0.999649          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 243924\n",
      "PROGRESS: Number of coefficients    : 243925\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000059  | 0.449305     | 0.990757          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.755503     | 0.998713          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.943633     | 0.999532          |\n",
      "PROGRESS: | 4         | 10       | 0.500000  | 1.359910     | 0.999298          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.650106     | 0.999532          |\n",
      "PROGRESS: | 6         | 14       | 1.000000  | 1.948299     | 0.999532          |\n",
      "PROGRESS: | 10        | 18       | 1.000000  | 2.758838     | 0.999415          |\n",
      "PROGRESS: | 11        | 20       | 1.000000  | 3.094067     | 0.999532          |\n",
      "PROGRESS: | 15        | 24       | 1.000000  | 3.870585     | 0.999649          |\n",
      "PROGRESS: | 20        | 33       | 0.500000  | 5.234491     | 0.999415          |\n",
      "PROGRESS: | 25        | 45       | 0.500000  | 6.960640     | 0.999532          |\n",
      "PROGRESS: | 30        | 56       | 0.500000  | 8.604738     | 0.999532          |\n",
      "PROGRESS: | 35        | 63       | 1.000000  | 9.802537     | 0.999532          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244864\n",
      "PROGRESS: Number of coefficients    : 244865\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000059  | 0.439297     | 0.989587          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.732492     | 0.998713          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.920615     | 0.999532          |\n",
      "PROGRESS: | 4         | 10       | 0.500000  | 1.320886     | 0.999181          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.619082     | 0.999532          |\n",
      "PROGRESS: | 6         | 14       | 1.000000  | 1.912278     | 0.999532          |\n",
      "PROGRESS: | 10        | 20       | 0.500000  | 2.884926     | 0.999532          |\n",
      "PROGRESS: | 11        | 22       | 1.000000  | 3.176122     | 0.999181          |\n",
      "PROGRESS: | 15        | 30       | 1.000000  | 4.416947     | 0.999649          |\n",
      "PROGRESS: | 20        | 37       | 1.000000  | 5.599741     | 0.999415          |\n",
      "PROGRESS: | 25        | 46       | 0.500000  | 6.988663     | 0.999532          |\n",
      "PROGRESS: | 30        | 55       | 0.500000  | 8.374586     | 0.999532          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 243778\n",
      "PROGRESS: Number of coefficients    : 243779\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000059  | 0.465312     | 0.990874          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.764513     | 0.998830          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.950634     | 0.999649          |\n",
      "PROGRESS: | 4         | 10       | 0.500000  | 1.346901     | 0.999298          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.634095     | 0.999649          |\n",
      "PROGRESS: | 6         | 14       | 1.000000  | 1.934293     | 0.999649          |\n",
      "PROGRESS: | 10        | 18       | 1.000000  | 2.692798     | 0.999415          |\n",
      "PROGRESS: | 11        | 20       | 1.000000  | 2.994996     | 0.999532          |\n",
      "PROGRESS: | 15        | 24       | 1.000000  | 3.753506     | 0.999649          |\n",
      "PROGRESS: | 20        | 32       | 1.000000  | 5.056373     | 0.999532          |\n",
      "PROGRESS: | 25        | 41       | 1.000000  | 6.420280     | 0.999649          |\n",
      "PROGRESS: | 30        | 47       | 1.000000  | 7.500003     | 0.999649          |\n",
      "PROGRESS: | 35        | 55       | 0.500000  | 8.769850     | 0.999649          |\n",
      "PROGRESS: | 40        | 62       | 1.000000  | 9.919613     | 0.999649          |\n",
      "PROGRESS: | 45        | 67       | 1.000000  | 10.871250    | 0.999649          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 246249\n",
      "PROGRESS: Number of coefficients    : 246250\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000058  | 0.447299     | 0.990173          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.752503     | 0.998830          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.941629     | 0.999532          |\n",
      "PROGRESS: | 4         | 10       | 0.500000  | 1.343898     | 0.999298          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.627083     | 0.999532          |\n",
      "PROGRESS: | 6         | 14       | 1.000000  | 1.917277     | 0.999532          |\n",
      "PROGRESS: | 10        | 18       | 1.000000  | 2.682790     | 0.999298          |\n",
      "PROGRESS: | 11        | 20       | 1.000000  | 2.987996     | 0.999415          |\n",
      "PROGRESS: | 15        | 24       | 1.000000  | 3.775522     | 0.999532          |\n",
      "PROGRESS: | 20        | 33       | 0.500000  | 5.162442     | 0.999649          |\n",
      "PROGRESS: | 25        | 38       | 0.500000  | 6.185126     | 0.999532          |\n",
      "PROGRESS: | 30        | 46       | 1.000000  | 7.461974     | 0.999532          |\n",
      "PROGRESS: | 35        | 56       | 0.250000  | 9.001007     | 0.999532          |\n",
      "PROGRESS: | 40        | 61       | 0.250000  | 9.994663     | 0.999532          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244879\n",
      "PROGRESS: Number of coefficients    : 244880\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000058  | 0.451301     | 0.990407          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.744497     | 0.998596          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.939630     | 0.999181          |\n",
      "PROGRESS: | 4         | 9        | 1.000000  | 1.229822     | 0.998830          |\n",
      "PROGRESS: | 5         | 10       | 1.000000  | 1.422950     | 0.999181          |\n",
      "PROGRESS: | 6         | 12       | 1.000000  | 1.728154     | 0.999532          |\n",
      "PROGRESS: | 10        | 19       | 1.000000  | 2.843895     | 0.999415          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.142097     | 0.999298          |\n",
      "PROGRESS: | 15        | 27       | 1.000000  | 4.141765     | 0.999415          |\n",
      "PROGRESS: | 20        | 33       | 1.000000  | 5.223484     | 0.999415          |\n",
      "PROGRESS: | 25        | 40       | 1.000000  | 6.403270     | 0.999532          |\n",
      "PROGRESS: | 30        | 48       | 1.000000  | 7.697138     | 0.999532          |\n",
      "PROGRESS: | 35        | 55       | 1.000000  | 8.906939     | 0.999532          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244633\n",
      "PROGRESS: Number of coefficients    : 244634\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000059  | 0.433289     | 0.990523          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.721483     | 0.998713          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.929619     | 0.999181          |\n",
      "PROGRESS: | 4         | 9        | 1.000000  | 1.234826     | 0.998830          |\n",
      "PROGRESS: | 5         | 10       | 1.000000  | 1.420950     | 0.999064          |\n",
      "PROGRESS: | 6         | 12       | 1.000000  | 1.721150     | 0.999532          |\n",
      "PROGRESS: | 10        | 21       | 0.750000  | 3.041030     | 0.999415          |\n",
      "PROGRESS: | 11        | 23       | 1.000000  | 3.339234     | 0.999298          |\n",
      "PROGRESS: | 15        | 27       | 1.000000  | 4.117748     | 0.999298          |\n",
      "PROGRESS: | 20        | 33       | 1.000000  | 5.194464     | 0.999532          |\n",
      "PROGRESS: | 25        | 40       | 1.000000  | 6.392262     | 0.999532          |\n",
      "PROGRESS: | 30        | 48       | 1.000000  | 7.666111     | 0.999532          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244617\n",
      "PROGRESS: Number of coefficients    : 244618\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000059  | 0.477319     | 0.991342          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.786528     | 0.998830          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.970648     | 0.999181          |\n",
      "PROGRESS: | 4         | 9        | 1.000000  | 1.266849     | 0.998830          |\n",
      "PROGRESS: | 5         | 10       | 1.000000  | 1.459973     | 0.999064          |\n",
      "PROGRESS: | 6         | 12       | 1.000000  | 1.771181     | 0.999532          |\n",
      "PROGRESS: | 10        | 19       | 1.000000  | 2.858905     | 0.999415          |\n",
      "PROGRESS: | 11        | 22       | 0.500000  | 3.263177     | 0.999415          |\n",
      "PROGRESS: | 15        | 26       | 0.500000  | 4.021684     | 0.999415          |\n",
      "PROGRESS: | 20        | 32       | 1.000000  | 5.092396     | 0.999415          |\n",
      "PROGRESS: | 25        | 40       | 0.250000  | 6.386258     | 0.999532          |\n",
      "PROGRESS: | 30        | 45       | 0.250000  | 7.357908     | 0.999532          |\n",
      "PROGRESS: | 35        | 52       | 1.000000  | 8.574718     | 0.999532          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245269\n",
      "PROGRESS: Number of coefficients    : 245270\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000059  | 0.471317     | 0.991459          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.772516     | 0.998947          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.965643     | 0.999415          |\n",
      "PROGRESS: | 4         | 9        | 1.000000  | 1.282859     | 0.998947          |\n",
      "PROGRESS: | 5         | 10       | 1.000000  | 1.463980     | 0.999415          |\n",
      "PROGRESS: | 6         | 12       | 1.000000  | 1.755173     | 0.999766          |\n",
      "PROGRESS: | 10        | 19       | 1.000000  | 2.863910     | 0.999649          |\n",
      "PROGRESS: | 11        | 22       | 0.500000  | 3.265176     | 0.999649          |\n",
      "PROGRESS: | 15        | 26       | 0.500000  | 4.055708     | 0.999649          |\n",
      "PROGRESS: | 20        | 34       | 0.500000  | 5.358575     | 0.999649          |\n",
      "PROGRESS: | 25        | 45       | 0.500000  | 6.941630     | 0.999766          |\n",
      "PROGRESS: | 30        | 53       | 0.500000  | 8.223486     | 0.999766          |\n",
      "PROGRESS: | 35        | 58       | 0.500000  | 9.187124     | 0.999766          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245817\n",
      "PROGRESS: Number of coefficients    : 245818\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000059  | 0.436296     | 0.990991          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.728487     | 0.999298          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.916612     | 0.999766          |\n",
      "PROGRESS: | 4         | 9        | 1.000000  | 1.224820     | 0.999181          |\n",
      "PROGRESS: | 5         | 10       | 1.000000  | 1.420948     | 0.999649          |\n",
      "PROGRESS: | 6         | 12       | 1.000000  | 1.741164     | 0.999766          |\n",
      "PROGRESS: | 10        | 19       | 1.000000  | 2.818880     | 0.999766          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.117080     | 0.999649          |\n",
      "PROGRESS: | 15        | 26       | 1.000000  | 4.012675     | 0.999766          |\n",
      "PROGRESS: | 20        | 32       | 1.000000  | 5.117413     | 0.999766          |\n",
      "PROGRESS: | 25        | 42       | 0.500000  | 6.616412     | 0.999766          |\n",
      "PROGRESS: | 30        | 50       | 0.250000  | 7.897269     | 0.999766          |\n",
      "PROGRESS: | 35        | 58       | 0.500000  | 9.174121     | 0.999766          |\n",
      "PROGRESS: | 40        | 66       | 1.000000  | 10.488996    | 0.999766          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245213\n",
      "PROGRESS: Number of coefficients    : 245214\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000058  | 0.468313     | 0.989822          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.765510     | 0.998947          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.951634     | 0.999298          |\n",
      "PROGRESS: | 4         | 9        | 1.000000  | 1.247832     | 0.998947          |\n",
      "PROGRESS: | 5         | 10       | 1.000000  | 1.441962     | 0.999298          |\n",
      "PROGRESS: | 6         | 12       | 1.000000  | 1.735160     | 0.999649          |\n",
      "PROGRESS: | 10        | 19       | 1.000000  | 2.837897     | 0.999532          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.138096     | 0.999415          |\n",
      "PROGRESS: | 15        | 26       | 1.000000  | 4.021685     | 0.999298          |\n",
      "PROGRESS: | 20        | 35       | 0.500000  | 5.431629     | 0.999649          |\n",
      "PROGRESS: | 25        | 44       | 0.500000  | 6.872582     | 0.999649          |\n",
      "PROGRESS: | 30        | 51       | 0.500000  | 8.074390     | 0.999649          |\n",
      "PROGRESS: | 35        | 63       | 1.000000  | 9.781525     | 0.999649          |\n",
      "PROGRESS: | 40        | 73       | 1.000000  | 11.362577    | 0.999649          |\n",
      "PROGRESS: | 45        | 83       | 1.000000  | 12.895602    | 0.999649          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 243924\n",
      "PROGRESS: Number of coefficients    : 243925\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000059  | 0.452301     | 0.990757          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.757510     | 0.998596          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.940628     | 0.999181          |\n",
      "PROGRESS: | 4         | 9        | 1.000000  | 1.239829     | 0.998713          |\n",
      "PROGRESS: | 5         | 10       | 1.000000  | 1.428952     | 0.999064          |\n",
      "PROGRESS: | 6         | 12       | 1.000000  | 1.729156     | 0.999532          |\n",
      "PROGRESS: | 10        | 19       | 1.000000  | 2.826887     | 0.999415          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.128088     | 0.999415          |\n",
      "PROGRESS: | 15        | 26       | 1.000000  | 3.980655     | 0.999415          |\n",
      "PROGRESS: | 20        | 33       | 1.000000  | 5.165447     | 0.999532          |\n",
      "PROGRESS: | 25        | 42       | 1.000000  | 6.563381     | 0.999532          |\n",
      "PROGRESS: | 30        | 48       | 1.000000  | 7.642096     | 0.999532          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244864\n",
      "PROGRESS: Number of coefficients    : 244865\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000059  | 0.463313     | 0.989587          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.760512     | 0.998713          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.945633     | 0.999181          |\n",
      "PROGRESS: | 4         | 9        | 1.000000  | 1.249837     | 0.998713          |\n",
      "PROGRESS: | 5         | 10       | 1.000000  | 1.436966     | 0.999064          |\n",
      "PROGRESS: | 6         | 12       | 1.000000  | 1.730157     | 0.999532          |\n",
      "PROGRESS: | 10        | 19       | 1.000000  | 2.839895     | 0.999415          |\n",
      "PROGRESS: | 11        | 22       | 0.500000  | 3.269179     | 0.999298          |\n",
      "PROGRESS: | 15        | 27       | 1.000000  | 4.220814     | 0.999298          |\n",
      "PROGRESS: | 20        | 33       | 1.000000  | 5.341561     | 0.999532          |\n",
      "PROGRESS: | 25        | 38       | 1.000000  | 6.347237     | 0.999532          |\n",
      "PROGRESS: | 30        | 45       | 1.000000  | 7.554037     | 0.999532          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 243778\n",
      "PROGRESS: Number of coefficients    : 243779\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000059  | 0.446299     | 0.990874          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.747498     | 0.998830          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.938627     | 0.999298          |\n",
      "PROGRESS: | 4         | 9        | 1.000000  | 1.237826     | 0.998830          |\n",
      "PROGRESS: | 5         | 10       | 1.000000  | 1.432957     | 0.999181          |\n",
      "PROGRESS: | 6         | 12       | 1.000000  | 1.729157     | 0.999649          |\n",
      "PROGRESS: | 10        | 19       | 1.000000  | 2.825884     | 0.999532          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.118080     | 0.999415          |\n",
      "PROGRESS: | 15        | 26       | 1.000000  | 4.025686     | 0.999298          |\n",
      "PROGRESS: | 20        | 33       | 1.000000  | 5.230490     | 0.999532          |\n",
      "PROGRESS: | 25        | 41       | 0.500000  | 6.536359     | 0.999532          |\n",
      "PROGRESS: | 30        | 51       | 0.250000  | 8.075389     | 0.999649          |\n",
      "PROGRESS: | 35        | 56       | 0.250000  | 9.085057     | 0.999649          |\n",
      "PROGRESS: | 40        | 66       | 1.000000  | 10.644101    | 0.999649          |\n",
      "PROGRESS: | 45        | 76       | 1.000000  | 12.168115    | 0.999649          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 246249\n",
      "PROGRESS: Number of coefficients    : 246250\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000058  | 0.471315     | 0.990173          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.777518     | 0.998596          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.967647     | 0.999181          |\n",
      "PROGRESS: | 4         | 9        | 1.000000  | 1.267848     | 0.998713          |\n",
      "PROGRESS: | 5         | 10       | 1.000000  | 1.454973     | 0.999181          |\n",
      "PROGRESS: | 6         | 12       | 1.000000  | 1.758171     | 0.999532          |\n",
      "PROGRESS: | 10        | 19       | 1.000000  | 2.881922     | 0.999415          |\n",
      "PROGRESS: | 11        | 22       | 0.500000  | 3.296197     | 0.999415          |\n",
      "PROGRESS: | 15        | 26       | 0.500000  | 4.082723     | 0.999415          |\n",
      "PROGRESS: | 20        | 31       | 0.500000  | 5.055374     | 0.999298          |\n",
      "PROGRESS: | 25        | 37       | 1.000000  | 6.123083     | 0.999415          |\n",
      "PROGRESS: | 30        | 46       | 0.500000  | 7.578051     | 0.999415          |\n",
      "PROGRESS: | 35        | 53       | 1.000000  | 8.818883     | 0.999532          |\n",
      "PROGRESS: | 40        | 64       | 1.000000  | 10.445966    | 0.999532          |\n",
      "PROGRESS: | 45        | 71       | 1.000000  | 11.605739    | 0.999532          |\n",
      "PROGRESS: | 50        | 79       | 1.000000  | 12.919612    | 0.999532          |\n",
      "PROGRESS: | 51        | 81       | 1.000000  | 13.228822    | 0.999532          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244879\n",
      "PROGRESS: Number of coefficients    : 244880\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000058  | 0.435292     | 0.990407          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.734494     | 0.998596          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.928623     | 0.913898          |\n",
      "PROGRESS: | 4         | 9        | 1.000000  | 1.237828     | 0.998830          |\n",
      "PROGRESS: | 5         | 10       | 1.000000  | 1.435963     | 0.998830          |\n",
      "PROGRESS: | 6         | 13       | 0.500000  | 1.841228     | 0.998947          |\n",
      "PROGRESS: | 10        | 19       | 1.000000  | 2.822882     | 0.998947          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.137093     | 0.999415          |\n",
      "PROGRESS: | 15        | 25       | 1.000000  | 3.960644     | 0.999298          |\n",
      "PROGRESS: | 20        | 33       | 0.500000  | 5.334560     | 0.999298          |\n",
      "PROGRESS: | 25        | 41       | 0.250000  | 6.640428     | 0.999298          |\n",
      "PROGRESS: | 30        | 52       | 0.500000  | 8.260509     | 0.999298          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244633\n",
      "PROGRESS: Number of coefficients    : 244634\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000059  | 0.445307     | 0.990523          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.741499     | 0.998713          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.945635     | 0.922078          |\n",
      "PROGRESS: | 4         | 9        | 1.000000  | 1.241831     | 0.998947          |\n",
      "PROGRESS: | 5         | 10       | 1.000000  | 1.435966     | 0.998947          |\n",
      "PROGRESS: | 6         | 13       | 0.500000  | 1.841229     | 0.998947          |\n",
      "PROGRESS: | 10        | 19       | 1.000000  | 2.838894     | 0.999064          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.158111     | 0.999415          |\n",
      "PROGRESS: | 15        | 25       | 1.000000  | 3.950637     | 0.999298          |\n",
      "PROGRESS: | 20        | 31       | 1.000000  | 5.014344     | 0.999415          |\n",
      "PROGRESS: | 25        | 39       | 0.500000  | 6.332225     | 0.999298          |\n",
      "PROGRESS: | 30        | 44       | 0.500000  | 7.304873     | 0.999298          |\n",
      "PROGRESS: | 35        | 53       | 1.000000  | 8.722817     | 0.999298          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244617\n",
      "PROGRESS: Number of coefficients    : 244618\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000059  | 0.434293     | 0.991342          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.741494     | 0.998830          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.943632     | 0.912601          |\n",
      "PROGRESS: | 4         | 9        | 1.000000  | 1.255836     | 0.998830          |\n",
      "PROGRESS: | 5         | 10       | 1.000000  | 1.449965     | 0.998830          |\n",
      "PROGRESS: | 6         | 13       | 0.500000  | 1.856239     | 0.999181          |\n",
      "PROGRESS: | 10        | 19       | 1.000000  | 2.890926     | 0.998947          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.227151     | 0.999415          |\n",
      "PROGRESS: | 15        | 25       | 1.000000  | 4.022681     | 0.998947          |\n",
      "PROGRESS: | 20        | 34       | 0.500000  | 5.441629     | 0.999415          |\n",
      "PROGRESS: | 25        | 41       | 0.500000  | 6.649434     | 0.999298          |\n",
      "PROGRESS: | 30        | 52       | 1.000000  | 8.290530     | 0.999298          |\n",
      "PROGRESS: | 35        | 60       | 1.000000  | 9.611407     | 0.999298          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245269\n",
      "PROGRESS: Number of coefficients    : 245270\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000059  | 0.452303     | 0.991459          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.752500     | 0.998830          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.946637     | 0.910612          |\n",
      "PROGRESS: | 4         | 9        | 1.000000  | 1.248832     | 0.998947          |\n",
      "PROGRESS: | 5         | 10       | 1.000000  | 1.448965     | 0.999064          |\n",
      "PROGRESS: | 6         | 13       | 0.500000  | 1.858242     | 0.999181          |\n",
      "PROGRESS: | 10        | 19       | 1.000000  | 2.866914     | 0.999298          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.153107     | 0.999649          |\n",
      "PROGRESS: | 15        | 25       | 1.000000  | 3.971650     | 0.999532          |\n",
      "PROGRESS: | 20        | 33       | 1.000000  | 5.277521     | 0.999649          |\n",
      "PROGRESS: | 25        | 39       | 1.000000  | 6.351239     | 0.999649          |\n",
      "PROGRESS: | 30        | 47       | 1.000000  | 7.672119     | 0.999649          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245817\n",
      "PROGRESS: Number of coefficients    : 245818\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000059  | 0.475321     | 0.990991          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.772516     | 0.999298          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.978654     | 0.913654          |\n",
      "PROGRESS: | 4         | 9        | 1.000000  | 1.278855     | 0.999298          |\n",
      "PROGRESS: | 5         | 10       | 1.000000  | 1.470984     | 0.999298          |\n",
      "PROGRESS: | 6         | 13       | 0.500000  | 1.874255     | 0.999415          |\n",
      "PROGRESS: | 10        | 19       | 1.000000  | 2.873921     | 0.999298          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.179119     | 0.999766          |\n",
      "PROGRESS: | 15        | 25       | 1.000000  | 3.962645     | 0.999532          |\n",
      "PROGRESS: | 20        | 32       | 1.000000  | 5.158444     | 0.999766          |\n",
      "PROGRESS: | 25        | 42       | 1.000000  | 6.663446     | 0.999766          |\n",
      "PROGRESS: | 30        | 52       | 0.500000  | 8.162443     | 0.999766          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245213\n",
      "PROGRESS: Number of coefficients    : 245214\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000058  | 0.462312     | 0.989822          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.773518     | 0.998947          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.964644     | 0.914249          |\n",
      "PROGRESS: | 4         | 9        | 1.000000  | 1.279857     | 0.999064          |\n",
      "PROGRESS: | 5         | 10       | 1.000000  | 1.475985     | 0.999064          |\n",
      "PROGRESS: | 6         | 13       | 0.500000  | 1.895269     | 0.999181          |\n",
      "PROGRESS: | 10        | 19       | 1.000000  | 2.867913     | 0.999064          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.184124     | 0.999532          |\n",
      "PROGRESS: | 15        | 25       | 1.000000  | 3.966649     | 0.999415          |\n",
      "PROGRESS: | 20        | 32       | 1.000000  | 5.161444     | 0.999532          |\n",
      "PROGRESS: | 25        | 39       | 1.000000  | 6.372248     | 0.999532          |\n",
      "PROGRESS: | 30        | 46       | 1.000000  | 7.592061     | 0.999532          |\n",
      "PROGRESS: | 35        | 53       | 0.500000  | 8.763845     | 0.999532          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 243924\n",
      "PROGRESS: Number of coefficients    : 243925\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000059  | 0.467312     | 0.990757          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.774519     | 0.998596          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.981657     | 0.910729          |\n",
      "PROGRESS: | 4         | 9        | 1.000000  | 1.285861     | 0.998713          |\n",
      "PROGRESS: | 5         | 10       | 1.000000  | 1.485992     | 0.998713          |\n",
      "PROGRESS: | 6         | 13       | 0.500000  | 1.900275     | 0.998830          |\n",
      "PROGRESS: | 10        | 19       | 1.000000  | 2.875923     | 0.999064          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.163115     | 0.999415          |\n",
      "PROGRESS: | 15        | 27       | 5.000000  | 4.146766     | 0.998713          |\n",
      "PROGRESS: | 20        | 35       | 1.000000  | 5.518685     | 0.999298          |\n",
      "PROGRESS: | 25        | 41       | 1.000000  | 6.622418     | 0.999298          |\n",
      "PROGRESS: | 30        | 48       | 1.000000  | 7.824220     | 0.999298          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244864\n",
      "PROGRESS: Number of coefficients    : 244865\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000059  | 0.458307     | 0.989587          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.756505     | 0.998596          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.940629     | 0.913537          |\n",
      "PROGRESS: | 4         | 9        | 1.000000  | 1.243829     | 0.998713          |\n",
      "PROGRESS: | 5         | 10       | 1.000000  | 1.441961     | 0.998713          |\n",
      "PROGRESS: | 6         | 13       | 0.500000  | 1.851238     | 0.998830          |\n",
      "PROGRESS: | 10        | 23       | 0.371295  | 3.257173     | 0.999415          |\n",
      "PROGRESS: | 11        | 24       | 0.371295  | 3.447299     | 0.999415          |\n",
      "PROGRESS: | 15        | 28       | 0.371295  | 4.235832     | 0.999415          |\n",
      "PROGRESS: | 20        | 34       | 1.000000  | 5.314545     | 0.998947          |\n",
      "PROGRESS: | 25        | 41       | 1.000000  | 6.476323     | 0.999298          |\n",
      "PROGRESS: | 30        | 48       | 1.000000  | 7.660112     | 0.999298          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 243778\n",
      "PROGRESS: Number of coefficients    : 243779\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000059  | 0.450300     | 0.990874          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.762510     | 0.998830          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.973649     | 0.916228          |\n",
      "PROGRESS: | 4         | 9        | 1.000000  | 1.267844     | 0.998830          |\n",
      "PROGRESS: | 5         | 10       | 1.000000  | 1.462976     | 0.998830          |\n",
      "PROGRESS: | 6         | 13       | 0.500000  | 1.867249     | 0.998947          |\n",
      "PROGRESS: | 10        | 19       | 1.000000  | 2.914945     | 0.999181          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.261178     | 0.999532          |\n",
      "PROGRESS: | 15        | 25       | 1.000000  | 4.059713     | 0.999415          |\n",
      "PROGRESS: | 20        | 34       | 0.500000  | 5.517679     | 0.999298          |\n",
      "PROGRESS: | 25        | 41       | 1.000000  | 6.756508     | 0.999415          |\n",
      "PROGRESS: | 30        | 50       | 1.000000  | 8.233493     | 0.999415          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 246249\n",
      "PROGRESS: Number of coefficients    : 246250\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000058  | 0.448305     | 0.990173          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.761508     | 0.998596          |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.951637     | 0.906528          |\n",
      "PROGRESS: | 4         | 9        | 1.000000  | 1.250834     | 0.998713          |\n",
      "PROGRESS: | 5         | 10       | 1.000000  | 1.445964     | 0.998713          |\n",
      "PROGRESS: | 6         | 13       | 0.500000  | 1.866247     | 0.998947          |\n",
      "PROGRESS: | 10        | 19       | 1.000000  | 2.865910     | 0.998947          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.168112     | 0.999415          |\n",
      "PROGRESS: | 15        | 25       | 1.000000  | 3.936627     | 0.999415          |\n",
      "PROGRESS: | 20        | 33       | 1.000000  | 5.236494     | 0.999415          |\n",
      "PROGRESS: | 25        | 45       | 0.500000  | 6.959641     | 0.999415          |\n",
      "PROGRESS: | 30        | 52       | 1.000000  | 8.157439     | 0.999415          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244879\n",
      "PROGRESS: Number of coefficients    : 244880\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.578378     | 0.990407          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.884583     | 0.998362          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.078711     | 0.998596          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.382914     | 0.998830          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.692120     | 0.998830          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.884252     | 0.998947          |\n",
      "PROGRESS: | 10        | 23       | 1.000000  | 3.330211     | 0.999298          |\n",
      "PROGRESS: | 11        | 24       | 1.000000  | 3.529351     | 0.999415          |\n",
      "PROGRESS: | 15        | 30       | 1.000000  | 4.536017     | 0.999532          |\n",
      "PROGRESS: | 20        | 39       | 0.500000  | 5.989990     | 0.999181          |\n",
      "PROGRESS: | 25        | 44       | 0.500000  | 6.956630     | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244633\n",
      "PROGRESS: Number of coefficients    : 244634\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.539360     | 0.990523          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.855571     | 0.998362          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.053705     | 0.998596          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.351905     | 0.999181          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.652101     | 0.998947          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.851235     | 0.999064          |\n",
      "PROGRESS: | 10        | 21       | 1.000000  | 3.068052     | 0.999415          |\n",
      "PROGRESS: | 11        | 22       | 1.000000  | 3.260178     | 0.999415          |\n",
      "PROGRESS: | 15        | 28       | 1.000000  | 4.276852     | 0.999415          |\n",
      "PROGRESS: | 20        | 36       | 1.000000  | 5.590733     | 0.999298          |\n",
      "PROGRESS: | 25        | 42       | 1.000000  | 6.653439     | 0.999181          |\n",
      "PROGRESS: | 30        | 51       | 1.000000  | 8.131423     | 0.999181          |\n",
      "PROGRESS: | 35        | 59       | 1.000000  | 9.478325     | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244617\n",
      "PROGRESS: Number of coefficients    : 244618\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.560382     | 0.991342          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.862582     | 0.998245          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.047705     | 0.998713          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.333894     | 0.999181          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.641095     | 0.998830          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.826223     | 0.999064          |\n",
      "PROGRESS: | 10        | 22       | 3.000000  | 3.154107     | 0.999415          |\n",
      "PROGRESS: | 11        | 24       | 1.000000  | 3.466313     | 0.999298          |\n",
      "PROGRESS: | 15        | 32       | 1.000000  | 4.682129     | 0.999532          |\n",
      "PROGRESS: | 20        | 38       | 1.000000  | 5.760843     | 0.999298          |\n",
      "PROGRESS: | 25        | 47       | 0.500000  | 7.210811     | 0.999298          |\n",
      "PROGRESS: | 30        | 54       | 0.500000  | 8.372587     | 0.999298          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245269\n",
      "PROGRESS: Number of coefficients    : 245270\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.556378     | 0.991459          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.868579     | 0.998362          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.063713     | 0.998830          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.370915     | 0.999415          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.714149     | 0.998947          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.913278     | 0.999298          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 3.042034     | 0.999532          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.246167     | 0.999766          |\n",
      "PROGRESS: | 15        | 26       | 1.000000  | 4.186792     | 0.999532          |\n",
      "PROGRESS: | 20        | 32       | 1.000000  | 5.312546     | 0.999532          |\n",
      "PROGRESS: | 25        | 42       | 1.000000  | 6.884591     | 0.999532          |\n",
      "PROGRESS: | 30        | 52       | 0.500000  | 8.445631     | 0.999532          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245817\n",
      "PROGRESS: Number of coefficients    : 245818\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.565378     | 0.990991          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.869579     | 0.999064          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.063709     | 0.999181          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.376922     | 0.999415          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.683120     | 0.999298          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.878255     | 0.999532          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 2.989994     | 0.999766          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.205138     | 0.999649          |\n",
      "PROGRESS: | 15        | 26       | 1.000000  | 4.110742     | 0.999766          |\n",
      "PROGRESS: | 20        | 32       | 1.000000  | 5.219481     | 0.999532          |\n",
      "PROGRESS: | 25        | 41       | 0.500000  | 6.659437     | 0.999766          |\n",
      "PROGRESS: | 30        | 53       | 0.250000  | 8.370582     | 0.999649          |\n",
      "PROGRESS: | 35        | 58       | 0.250000  | 9.338225     | 0.999649          |\n",
      "PROGRESS: | 40        | 63       | 0.250000  | 10.318878    | 0.999649          |\n",
      "PROGRESS: | 45        | 68       | 0.250000  | 11.281522    | 0.999649          |\n",
      "PROGRESS: | 50        | 73       | 0.250000  | 12.265179    | 0.999649          |\n",
      "PROGRESS: | 51        | 74       | 0.250000  | 12.469314    | 0.999649          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245213\n",
      "PROGRESS: Number of coefficients    : 245214\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.524350     | 0.989822          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.816546     | 0.998479          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 0.992662     | 0.998947          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.302869     | 0.999181          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.589060     | 0.999181          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.773184     | 0.999181          |\n",
      "PROGRESS: | 10        | 21       | 1.000000  | 3.003006     | 0.999532          |\n",
      "PROGRESS: | 11        | 22       | 1.000000  | 3.193129     | 0.999532          |\n",
      "PROGRESS: | 15        | 28       | 1.000000  | 4.179787     | 0.999649          |\n",
      "PROGRESS: | 20        | 36       | 1.000000  | 5.460645     | 0.999532          |\n",
      "PROGRESS: | 25        | 46       | 1.000000  | 6.961646     | 0.999532          |\n",
      "PROGRESS: | 30        | 54       | 1.000000  | 8.230487     | 0.999532          |\n",
      "PROGRESS: | 35        | 63       | 1.000000  | 9.626418     | 0.999532          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 243924\n",
      "PROGRESS: Number of coefficients    : 243925\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.530354     | 0.990757          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.835558     | 0.998245          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.019682     | 0.998713          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.321885     | 0.998947          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.618080     | 0.998713          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.810208     | 0.999064          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 2.879921     | 0.999415          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.075054     | 0.999532          |\n",
      "PROGRESS: | 15        | 26       | 1.000000  | 3.948637     | 0.999298          |\n",
      "PROGRESS: | 20        | 32       | 1.000000  | 5.002335     | 0.999298          |\n",
      "PROGRESS: | 25        | 39       | 1.000000  | 6.211145     | 0.999298          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244864\n",
      "PROGRESS: Number of coefficients    : 244865\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.548368     | 0.989587          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.843565     | 0.998245          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.030689     | 0.998596          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.317882     | 0.998830          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.602071     | 0.998713          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.794197     | 0.998947          |\n",
      "PROGRESS: | 10        | 22       | 5.000000  | 3.078051     | 0.999532          |\n",
      "PROGRESS: | 11        | 23       | 5.000000  | 3.265181     | 0.999298          |\n",
      "PROGRESS: | 15        | 29       | 1.000000  | 4.237828     | 0.999532          |\n",
      "PROGRESS: | 20        | 34       | 1.000000  | 5.187458     | 0.999181          |\n",
      "PROGRESS: | 25        | 41       | 1.000000  | 6.368252     | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 243778\n",
      "PROGRESS: Number of coefficients    : 243779\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.549368     | 0.990874          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.845566     | 0.998479          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.034693     | 0.998596          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.325887     | 0.998947          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.620082     | 0.998830          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.805205     | 0.998947          |\n",
      "PROGRESS: | 10        | 23       | 1.000000  | 3.201133     | 0.999532          |\n",
      "PROGRESS: | 11        | 24       | 1.000000  | 3.400272     | 0.999532          |\n",
      "PROGRESS: | 15        | 30       | 1.000000  | 4.389931     | 0.999766          |\n",
      "PROGRESS: | 20        | 39       | 0.500000  | 5.780859     | 0.999415          |\n",
      "PROGRESS: | 25        | 45       | 1.000000  | 6.846566     | 0.999415          |\n",
      "PROGRESS: | 30        | 52       | 1.000000  | 8.009343     | 0.999415          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 246249\n",
      "PROGRESS: Number of coefficients    : 246250\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.566383     | 0.990173          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.867581     | 0.998128          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.071715     | 0.998947          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.364914     | 0.999181          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.662110     | 0.998713          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.851239     | 0.999064          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 2.936965     | 0.999298          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.128089     | 0.999532          |\n",
      "PROGRESS: | 15        | 26       | 1.000000  | 4.003676     | 0.999415          |\n",
      "PROGRESS: | 20        | 35       | 0.250000  | 5.390598     | 0.999298          |\n",
      "PROGRESS: | 25        | 40       | 0.250000  | 6.390264     | 0.999298          |\n",
      "PROGRESS: | 30        | 45       | 0.250000  | 7.327889     | 0.999298          |\n",
      "PROGRESS: | 35        | 50       | 0.250000  | 8.280525     | 0.999298          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244879\n",
      "PROGRESS: Number of coefficients    : 244880\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.571378     | 0.990407          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.874584     | 0.998362          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.055701     | 0.998713          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.348897     | 0.998713          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.647092     | 0.998830          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.839222     | 0.998830          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 2.938956     | 0.999298          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.136088     | 0.999298          |\n",
      "PROGRESS: | 15        | 26       | 1.000000  | 4.021679     | 0.999298          |\n",
      "PROGRESS: | 20        | 34       | 1.000000  | 5.310541     | 0.999181          |\n",
      "PROGRESS: | 25        | 41       | 0.500000  | 6.487320     | 0.999181          |\n",
      "PROGRESS: | 30        | 47       | 1.000000  | 7.580052     | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244633\n",
      "PROGRESS: Number of coefficients    : 244634\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.543363     | 0.990523          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.866577     | 0.998362          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.059708     | 0.998596          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.357906     | 0.998947          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.651103     | 0.998947          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.843230     | 0.998947          |\n",
      "PROGRESS: | 10        | 19       | 1.000000  | 2.863912     | 0.999298          |\n",
      "PROGRESS: | 11        | 20       | 1.000000  | 3.050035     | 0.999298          |\n",
      "PROGRESS: | 15        | 25       | 1.000000  | 3.961645     | 0.999415          |\n",
      "PROGRESS: | 20        | 35       | 1.000000  | 5.466645     | 0.999181          |\n",
      "PROGRESS: | 25        | 43       | 1.000000  | 6.759512     | 0.999181          |\n",
      "PROGRESS: | 30        | 51       | 1.000000  | 8.053372     | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244617\n",
      "PROGRESS: Number of coefficients    : 244618\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.569382     | 0.991342          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.898600     | 0.998245          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.088729     | 0.998830          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.379925     | 0.998830          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.672116     | 0.998947          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.865249     | 0.998947          |\n",
      "PROGRESS: | 10        | 19       | 1.000000  | 2.856910     | 0.999415          |\n",
      "PROGRESS: | 11        | 20       | 1.000000  | 3.046033     | 0.999298          |\n",
      "PROGRESS: | 15        | 25       | 1.000000  | 3.938631     | 0.999415          |\n",
      "PROGRESS: | 20        | 33       | 0.500000  | 5.226487     | 0.999181          |\n",
      "PROGRESS: | 25        | 39       | 1.000000  | 6.305207     | 0.999181          |\n",
      "PROGRESS: | 30        | 47       | 1.000000  | 7.581057     | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245269\n",
      "PROGRESS: Number of coefficients    : 245270\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.578390     | 0.991459          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.867579     | 0.998479          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.050699     | 0.999064          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.351904     | 0.998830          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.654107     | 0.999298          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.842230     | 0.999064          |\n",
      "PROGRESS: | 10        | 19       | 1.000000  | 2.825887     | 0.999649          |\n",
      "PROGRESS: | 11        | 20       | 1.000000  | 3.028020     | 0.999532          |\n",
      "PROGRESS: | 15        | 25       | 1.000000  | 3.910608     | 0.999649          |\n",
      "PROGRESS: | 20        | 33       | 1.000000  | 5.195469     | 0.999415          |\n",
      "PROGRESS: | 25        | 42       | 1.000000  | 6.583389     | 0.999415          |\n",
      "PROGRESS: | 30        | 51       | 0.500000  | 7.946301     | 0.999415          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245817\n",
      "PROGRESS: Number of coefficients    : 245818\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.538361     | 0.990991          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.839562     | 0.999181          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.025689     | 0.999298          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.328888     | 0.999181          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.618079     | 0.999298          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.804203     | 0.999298          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 2.904937     | 0.999766          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.100072     | 0.999532          |\n",
      "PROGRESS: | 15        | 26       | 1.000000  | 3.997668     | 0.999766          |\n",
      "PROGRESS: | 20        | 34       | 0.500000  | 5.299534     | 0.999649          |\n",
      "PROGRESS: | 25        | 44       | 0.500000  | 6.835561     | 0.999649          |\n",
      "PROGRESS: | 30        | 54       | 0.250000  | 8.363575     | 0.999649          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245213\n",
      "PROGRESS: Number of coefficients    : 245214\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.577386     | 0.989822          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.886591     | 0.998479          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.082724     | 0.999181          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.394930     | 0.998947          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.701139     | 0.999181          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.903271     | 0.999064          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 3.067046     | 0.999298          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.266177     | 0.999532          |\n",
      "PROGRESS: | 15        | 26       | 1.000000  | 4.172785     | 0.999415          |\n",
      "PROGRESS: | 20        | 34       | 1.000000  | 5.489664     | 0.999415          |\n",
      "PROGRESS: | 25        | 42       | 1.000000  | 6.832559     | 0.999415          |\n",
      "PROGRESS: | 30        | 54       | 1.000000  | 8.574717     | 0.999415          |\n",
      "PROGRESS: | 35        | 62       | 1.000000  | 9.885593     | 0.999415          |\n",
      "PROGRESS: | 40        | 73       | 1.000000  | 11.537692    | 0.999415          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 243924\n",
      "PROGRESS: Number of coefficients    : 243925\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.564380     | 0.990757          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.865578     | 0.998362          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.055706     | 0.998830          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.351904     | 0.998596          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.656104     | 0.998830          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.858241     | 0.998830          |\n",
      "PROGRESS: | 10        | 20       | 5.000000  | 3.011012     | 0.999532          |\n",
      "PROGRESS: | 11        | 21       | 5.000000  | 3.204138     | 0.998947          |\n",
      "PROGRESS: | 15        | 27       | 1.000000  | 4.181788     | 0.999181          |\n",
      "PROGRESS: | 20        | 37       | 0.500000  | 5.682794     | 0.999181          |\n",
      "PROGRESS: | 25        | 42       | 0.500000  | 6.668448     | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244864\n",
      "PROGRESS: Number of coefficients    : 244865\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.545366     | 0.989587          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.855571     | 0.998245          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.040701     | 0.998713          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.341895     | 0.998596          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.647099     | 0.998713          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.845234     | 0.998713          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 2.955973     | 0.999298          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.141098     | 0.999532          |\n",
      "PROGRESS: | 15        | 26       | 1.000000  | 4.045703     | 0.999298          |\n",
      "PROGRESS: | 20        | 34       | 1.000000  | 5.329556     | 0.999181          |\n",
      "PROGRESS: | 25        | 42       | 1.000000  | 6.607405     | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 243778\n",
      "PROGRESS: Number of coefficients    : 243779\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.549373     | 0.990874          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.842564     | 0.998479          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.029688     | 0.998713          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.332891     | 0.998713          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.639097     | 0.998830          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.825218     | 0.998830          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 2.924953     | 0.999532          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.113080     | 0.999532          |\n",
      "PROGRESS: | 15        | 26       | 1.000000  | 3.999667     | 0.999415          |\n",
      "PROGRESS: | 20        | 33       | 1.000000  | 5.194466     | 0.999415          |\n",
      "PROGRESS: | 25        | 40       | 0.500000  | 6.378254     | 0.999298          |\n",
      "PROGRESS: | 30        | 49       | 0.250000  | 7.774186     | 0.999298          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 246249\n",
      "PROGRESS: Number of coefficients    : 246250\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.550370     | 0.990173          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.848568     | 0.998128          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.057707     | 0.998830          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.364916     | 0.998713          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.668114     | 0.998947          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.862245     | 0.998947          |\n",
      "PROGRESS: | 10        | 19       | 1.000000  | 2.845903     | 0.999415          |\n",
      "PROGRESS: | 11        | 20       | 1.000000  | 3.039033     | 0.999415          |\n",
      "PROGRESS: | 15        | 25       | 1.000000  | 3.930624     | 0.999181          |\n",
      "PROGRESS: | 20        | 32       | 1.000000  | 5.147437     | 0.999181          |\n",
      "PROGRESS: | 25        | 42       | 1.000000  | 6.670452     | 0.999181          |\n",
      "PROGRESS: | 30        | 48       | 1.000000  | 7.747171     | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244879\n",
      "PROGRESS: Number of coefficients    : 244880\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.540365     | 0.990407          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.842565     | 0.998479          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.030693     | 0.998830          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.316878     | 0.998713          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.607072     | 0.998830          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.817215     | 0.998830          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 2.922956     | 0.999298          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.120085     | 0.999532          |\n",
      "PROGRESS: | 15        | 27       | 1.000000  | 4.129757     | 0.999064          |\n",
      "PROGRESS: | 20        | 33       | 1.000000  | 5.216478     | 0.998830          |\n",
      "PROGRESS: | 25        | 42       | 1.000000  | 6.702472     | 0.999181          |\n",
      "PROGRESS: | 30        | 48       | 1.000000  | 7.790195     | 0.999181          |\n",
      "PROGRESS: | 35        | 56       | 1.000000  | 9.146101     | 0.999181          |\n",
      "PROGRESS: | 40        | 62       | 1.000000  | 10.259840    | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244633\n",
      "PROGRESS: Number of coefficients    : 244634\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.568382     | 0.990523          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.863582     | 0.998245          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.056710     | 0.998596          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.391935     | 0.998830          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.693134     | 0.998947          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.876255     | 0.998947          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 2.970986     | 0.999298          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.167114     | 0.999532          |\n",
      "PROGRESS: | 15        | 27       | 1.000000  | 4.190799     | 0.999415          |\n",
      "PROGRESS: | 20        | 34       | 1.000000  | 5.380591     | 0.999181          |\n",
      "PROGRESS: | 25        | 41       | 1.000000  | 6.568379     | 0.999181          |\n",
      "PROGRESS: | 30        | 52       | 1.000000  | 8.202473     | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244617\n",
      "PROGRESS: Number of coefficients    : 244618\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.571391     | 0.991342          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.868579     | 0.998245          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.058706     | 0.998830          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.366916     | 0.998713          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.656104     | 0.998830          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.847234     | 0.998830          |\n",
      "PROGRESS: | 10        | 21       | 5.000000  | 3.034032     | 0.999064          |\n",
      "PROGRESS: | 11        | 22       | 5.000000  | 3.229155     | 0.999064          |\n",
      "PROGRESS: | 15        | 27       | 1.000000  | 4.112745     | 0.999064          |\n",
      "PROGRESS: | 20        | 34       | 1.000000  | 5.301536     | 0.999181          |\n",
      "PROGRESS: | 25        | 42       | 1.000000  | 6.608407     | 0.999181          |\n",
      "PROGRESS: | 30        | 50       | 1.000000  | 7.883258     | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245269\n",
      "PROGRESS: Number of coefficients    : 245270\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.582386     | 0.991459          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.873584     | 0.998479          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.071713     | 0.999181          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.366913     | 0.998713          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.660110     | 0.998947          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.839227     | 0.998947          |\n",
      "PROGRESS: | 10        | 21       | 5.000000  | 3.049035     | 0.999532          |\n",
      "PROGRESS: | 11        | 22       | 5.000000  | 3.252171     | 0.999766          |\n",
      "PROGRESS: | 15        | 27       | 1.000000  | 4.132758     | 0.999415          |\n",
      "PROGRESS: | 20        | 37       | 0.500000  | 5.659774     | 0.999415          |\n",
      "PROGRESS: | 25        | 44       | 1.000000  | 6.880586     | 0.999415          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245817\n",
      "PROGRESS: Number of coefficients    : 245818\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.590396     | 0.990991          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.894596     | 0.999181          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.095732     | 0.999415          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.388931     | 0.999064          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.686125     | 0.999298          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.898265     | 0.999298          |\n",
      "PROGRESS: | 10        | 21       | 5.000000  | 3.117076     | 0.999298          |\n",
      "PROGRESS: | 11        | 22       | 5.000000  | 3.307211     | 0.999298          |\n",
      "PROGRESS: | 15        | 28       | 1.000000  | 4.293862     | 0.999766          |\n",
      "PROGRESS: | 20        | 35       | 0.500000  | 5.446633     | 0.999649          |\n",
      "PROGRESS: | 25        | 42       | 1.000000  | 6.641427     | 0.999649          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245213\n",
      "PROGRESS: Number of coefficients    : 245214\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.563380     | 0.989822          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.873584     | 0.998596          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.085724     | 0.999181          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.383924     | 0.998830          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.676122     | 0.999064          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.859242     | 0.999064          |\n",
      "PROGRESS: | 10        | 21       | 5.000000  | 3.057040     | 0.999649          |\n",
      "PROGRESS: | 11        | 22       | 5.000000  | 3.249173     | 0.954960          |\n",
      "PROGRESS: | 15        | 29       | 0.500000  | 4.340895     | 0.999415          |\n",
      "PROGRESS: | 20        | 36       | 1.000000  | 5.535695     | 0.999415          |\n",
      "PROGRESS: | 25        | 45       | 1.000000  | 6.926623     | 0.999415          |\n",
      "PROGRESS: | 30        | 52       | 1.000000  | 8.092398     | 0.999415          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 243924\n",
      "PROGRESS: Number of coefficients    : 243925\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.559374     | 0.990757          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.881590     | 0.998479          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.072716     | 0.998830          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.384927     | 0.998479          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.721148     | 0.998713          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.922281     | 0.998713          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 3.017012     | 0.999532          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.204141     | 0.999532          |\n",
      "PROGRESS: | 15        | 26       | 1.000000  | 4.085726     | 0.999181          |\n",
      "PROGRESS: | 20        | 34       | 0.500000  | 5.388596     | 0.999181          |\n",
      "PROGRESS: | 25        | 41       | 0.500000  | 6.581393     | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244864\n",
      "PROGRESS: Number of coefficients    : 244865\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.555373     | 0.989587          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.856575     | 0.998245          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.048702     | 0.998947          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.358906     | 0.998479          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.649101     | 0.998713          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.838228     | 0.998713          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 2.933956     | 0.999298          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.122083     | 0.999298          |\n",
      "PROGRESS: | 15        | 26       | 1.000000  | 4.008672     | 0.998947          |\n",
      "PROGRESS: | 20        | 33       | 1.000000  | 5.211476     | 0.999181          |\n",
      "PROGRESS: | 25        | 41       | 1.000000  | 6.498335     | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 243778\n",
      "PROGRESS: Number of coefficients    : 243779\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.569379     | 0.990874          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.868583     | 0.998479          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.062710     | 0.998830          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.376920     | 0.998596          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.686130     | 0.998830          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.874254     | 0.998830          |\n",
      "PROGRESS: | 10        | 21       | 5.000000  | 3.059041     | 0.999649          |\n",
      "PROGRESS: | 11        | 22       | 5.000000  | 3.270180     | 0.993682          |\n",
      "PROGRESS: | 15        | 27       | 1.000000  | 4.176785     | 0.999181          |\n",
      "PROGRESS: | 20        | 37       | 0.500000  | 5.670785     | 0.999298          |\n",
      "PROGRESS: | 25        | 42       | 0.500000  | 6.627421     | 0.999298          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 246249\n",
      "PROGRESS: Number of coefficients    : 246250\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.548367     | 0.990173          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.847573     | 0.998362          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.035698     | 0.998947          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.350907     | 0.998479          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.639095     | 0.998713          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.835227     | 0.998713          |\n",
      "PROGRESS: | 10        | 21       | 5.000000  | 3.046032     | 0.999532          |\n",
      "PROGRESS: | 11        | 22       | 5.000000  | 3.259176     | 0.652433          |\n",
      "PROGRESS: | 15        | 28       | 1.000000  | 4.257843     | 0.999298          |\n",
      "PROGRESS: | 20        | 38       | 0.500000  | 5.759844     | 0.999298          |\n",
      "PROGRESS: | 25        | 50       | 1.000000  | 7.504004     | 0.999181          |\n",
      "PROGRESS: | 30        | 57       | 1.000000  | 8.716810     | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244879\n",
      "PROGRESS: Number of coefficients    : 244880\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.550370     | 0.990407          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.862578     | 0.998245          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.052708     | 0.998947          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.341900     | 0.998713          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.642096     | 0.998830          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.829220     | 0.998830          |\n",
      "PROGRESS: | 10        | 21       | 5.000000  | 3.052042     | 0.999532          |\n",
      "PROGRESS: | 11        | 22       | 5.000000  | 3.256172     | 0.853182          |\n",
      "PROGRESS: | 15        | 28       | 1.000000  | 4.288860     | 0.999181          |\n",
      "PROGRESS: | 20        | 37       | 1.000000  | 5.690798     | 0.999181          |\n",
      "PROGRESS: | 25        | 44       | 1.000000  | 6.883595     | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244633\n",
      "PROGRESS: Number of coefficients    : 244634\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.554375     | 0.990523          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.879586     | 0.998362          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.076721     | 0.998830          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.382925     | 0.998830          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.663112     | 0.998947          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.881260     | 0.998947          |\n",
      "PROGRESS: | 10        | 21       | 5.000000  | 3.114079     | 0.999532          |\n",
      "PROGRESS: | 11        | 22       | 5.000000  | 3.307209     | 0.795718          |\n",
      "PROGRESS: | 15        | 28       | 1.000000  | 4.307875     | 0.999064          |\n",
      "PROGRESS: | 20        | 36       | 1.000000  | 5.599737     | 0.999181          |\n",
      "PROGRESS: | 25        | 44       | 1.000000  | 6.900602     | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244617\n",
      "PROGRESS: Number of coefficients    : 244618\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.567377     | 0.991342          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.873581     | 0.998245          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.055706     | 0.998830          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.354903     | 0.998713          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.662109     | 0.998830          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.857238     | 0.998830          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 2.956972     | 0.999415          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.158103     | 0.999532          |\n",
      "PROGRESS: | 15        | 27       | 1.000000  | 4.159772     | 0.999415          |\n",
      "PROGRESS: | 20        | 35       | 0.500000  | 5.463640     | 0.999181          |\n",
      "PROGRESS: | 25        | 41       | 1.000000  | 6.523349     | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245269\n",
      "PROGRESS: Number of coefficients    : 245270\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.556373     | 0.991459          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.861577     | 0.998479          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.075722     | 0.999181          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.393935     | 0.998713          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.717149     | 0.998947          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.899268     | 0.998947          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 3.029025     | 0.999766          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.228157     | 0.999766          |\n",
      "PROGRESS: | 15        | 27       | 1.000000  | 4.221818     | 0.999649          |\n",
      "PROGRESS: | 20        | 36       | 0.500000  | 5.597736     | 0.999415          |\n",
      "PROGRESS: | 25        | 42       | 1.000000  | 6.704471     | 0.999415          |\n",
      "PROGRESS: | 30        | 49       | 1.000000  | 7.893264     | 0.999415          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245817\n",
      "PROGRESS: Number of coefficients    : 245818\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.554369     | 0.990991          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.864578     | 0.999181          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.057708     | 0.999415          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.373916     | 0.999064          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.679126     | 0.999298          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.876253     | 0.999298          |\n",
      "PROGRESS: | 10        | 21       | 5.000000  | 3.084056     | 0.999766          |\n",
      "PROGRESS: | 11        | 22       | 5.000000  | 3.282188     | 0.793846          |\n",
      "PROGRESS: | 15        | 28       | 1.000000  | 4.290861     | 0.999649          |\n",
      "PROGRESS: | 20        | 35       | 1.000000  | 5.483659     | 0.999649          |\n",
      "PROGRESS: | 25        | 42       | 1.000000  | 6.669446     | 0.999649          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245213\n",
      "PROGRESS: Number of coefficients    : 245214\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.558378     | 0.989822          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.868584     | 0.998713          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.053706     | 0.999298          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.345899     | 0.998830          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.646098     | 0.999064          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.836228     | 0.999064          |\n",
      "PROGRESS: | 10        | 21       | 5.000000  | 3.038028     | 0.999649          |\n",
      "PROGRESS: | 11        | 22       | 5.000000  | 3.223152     | 0.759008          |\n",
      "PROGRESS: | 15        | 28       | 1.000000  | 4.214816     | 0.999298          |\n",
      "PROGRESS: | 20        | 39       | 0.500000  | 5.812876     | 0.999415          |\n",
      "PROGRESS: | 25        | 47       | 1.000000  | 7.119749     | 0.999415          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 243924\n",
      "PROGRESS: Number of coefficients    : 243925\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.567381     | 0.990757          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.861579     | 0.998479          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.046702     | 0.998947          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.352906     | 0.998479          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.660115     | 0.998713          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.846234     | 0.998713          |\n",
      "PROGRESS: | 10        | 21       | 1.000000  | 3.052039     | 0.999415          |\n",
      "PROGRESS: | 11        | 22       | 1.000000  | 3.245165     | 0.999532          |\n",
      "PROGRESS: | 15        | 27       | 1.000000  | 4.127757     | 0.999181          |\n",
      "PROGRESS: | 20        | 35       | 1.000000  | 5.428627     | 0.999181          |\n",
      "PROGRESS: | 25        | 43       | 1.000000  | 6.717487     | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244864\n",
      "PROGRESS: Number of coefficients    : 244865\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.547368     | 0.989587          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.841564     | 0.998245          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.032693     | 0.998830          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.337894     | 0.998479          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.634093     | 0.998713          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.818215     | 0.998713          |\n",
      "PROGRESS: | 10        | 21       | 5.000000  | 3.020020     | 0.999298          |\n",
      "PROGRESS: | 11        | 22       | 5.000000  | 3.217150     | 0.999649          |\n",
      "PROGRESS: | 15        | 29       | 1.000000  | 4.328889     | 0.999181          |\n",
      "PROGRESS: | 20        | 36       | 0.500000  | 5.524686     | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 243778\n",
      "PROGRESS: Number of coefficients    : 243779\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.559377     | 0.990874          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.858570     | 0.998479          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.039695     | 0.998947          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.368912     | 0.998596          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.658106     | 0.998830          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.849236     | 0.998830          |\n",
      "PROGRESS: | 10        | 21       | 5.000000  | 3.097063     | 0.999649          |\n",
      "PROGRESS: | 11        | 22       | 5.000000  | 3.289191     | 0.654382          |\n",
      "PROGRESS: | 15        | 28       | 1.000000  | 4.278853     | 0.999415          |\n",
      "PROGRESS: | 20        | 37       | 0.500000  | 5.660779     | 0.999298          |\n",
      "PROGRESS: | 25        | 45       | 0.250000  | 6.935622     | 0.999298          |\n",
      "PROGRESS: | 30        | 50       | 0.250000  | 7.899267     | 0.999298          |\n",
      "PROGRESS: | 35        | 56       | 1.000000  | 8.978987     | 0.999298          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 246249\n",
      "PROGRESS: Number of coefficients    : 246250\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.570384     | 0.990173          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.882591     | 0.998362          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.082733     | 0.999064          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.377927     | 0.998479          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 1.664116     | 0.998713          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.862243     | 0.998713          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 2.949973     | 0.999532          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.148105     | 0.999532          |\n",
      "PROGRESS: | 15        | 28       | 1.000000  | 4.235830     | 0.998596          |\n",
      "PROGRESS: | 20        | 35       | 1.000000  | 5.389597     | 0.999415          |\n",
      "PROGRESS: | 25        | 43       | 1.000000  | 6.708477     | 0.999181          |\n",
      "PROGRESS: | 30        | 55       | 1.000000  | 8.435625     | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244879\n",
      "PROGRESS: Number of coefficients    : 244880\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.549372     | 0.990407          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.903607     | 0.998362          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.098733     | 0.998947          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.394934     | 0.998713          |\n",
      "PROGRESS: | 5         | 11       | 1.000000  | 1.580055     | 0.998830          |\n",
      "PROGRESS: | 6         | 12       | 1.000000  | 1.777185     | 0.998830          |\n",
      "PROGRESS: | 10        | 17       | 1.000000  | 2.657777     | 0.999181          |\n",
      "PROGRESS: | 11        | 18       | 1.000000  | 2.851905     | 0.999532          |\n",
      "PROGRESS: | 15        | 23       | 1.000000  | 3.740494     | 0.999064          |\n",
      "PROGRESS: | 20        | 30       | 1.000000  | 4.930290     | 0.998947          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244633\n",
      "PROGRESS: Number of coefficients    : 244634\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.544364     | 0.990523          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.844567     | 0.998479          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.042703     | 0.998830          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.352901     | 0.998830          |\n",
      "PROGRESS: | 5         | 11       | 1.000000  | 1.534023     | 0.998947          |\n",
      "PROGRESS: | 6         | 12       | 1.000000  | 1.718150     | 0.998947          |\n",
      "PROGRESS: | 10        | 17       | 1.000000  | 2.631757     | 0.999064          |\n",
      "PROGRESS: | 11        | 18       | 1.000000  | 2.855906     | 0.999532          |\n",
      "PROGRESS: | 15        | 24       | 1.000000  | 3.873586     | 0.998362          |\n",
      "PROGRESS: | 20        | 33       | 0.500000  | 5.301538     | 0.999064          |\n",
      "PROGRESS: | 25        | 40       | 1.000000  | 6.487329     | 0.998947          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244617\n",
      "PROGRESS: Number of coefficients    : 244618\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.542364     | 0.991342          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.857576     | 0.998245          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.044696     | 0.998830          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.327886     | 0.998713          |\n",
      "PROGRESS: | 5         | 11       | 1.000000  | 1.516012     | 0.998830          |\n",
      "PROGRESS: | 6         | 12       | 1.000000  | 1.717148     | 0.998830          |\n",
      "PROGRESS: | 10        | 17       | 1.000000  | 2.592729     | 0.999415          |\n",
      "PROGRESS: | 11        | 18       | 1.000000  | 2.785863     | 0.999532          |\n",
      "PROGRESS: | 15        | 24       | 0.500000  | 3.819548     | 0.999181          |\n",
      "PROGRESS: | 20        | 32       | 0.500000  | 5.116411     | 0.998947          |\n",
      "PROGRESS: | 25        | 44       | 0.500000  | 6.854575     | 0.998947          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245269\n",
      "PROGRESS: Number of coefficients    : 245270\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.559377     | 0.991459          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.853569     | 0.998596          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.053707     | 0.999064          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.348901     | 0.998713          |\n",
      "PROGRESS: | 5         | 11       | 1.000000  | 1.556039     | 0.998947          |\n",
      "PROGRESS: | 6         | 12       | 1.000000  | 1.740164     | 0.998947          |\n",
      "PROGRESS: | 10        | 17       | 1.000000  | 2.651772     | 0.999532          |\n",
      "PROGRESS: | 11        | 18       | 1.000000  | 2.843898     | 0.999766          |\n",
      "PROGRESS: | 15        | 25       | 0.500000  | 3.970647     | 0.998830          |\n",
      "PROGRESS: | 20        | 34       | 1.000000  | 5.396600     | 0.999181          |\n",
      "PROGRESS: | 25        | 42       | 0.500000  | 6.706470     | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245817\n",
      "PROGRESS: Number of coefficients    : 245818\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.573387     | 0.990991          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.899600     | 0.999064          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.085723     | 0.999415          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.387925     | 0.999064          |\n",
      "PROGRESS: | 5         | 11       | 1.000000  | 1.580058     | 0.999298          |\n",
      "PROGRESS: | 6         | 12       | 1.000000  | 1.779188     | 0.999298          |\n",
      "PROGRESS: | 10        | 17       | 1.000000  | 2.682790     | 0.999415          |\n",
      "PROGRESS: | 11        | 18       | 1.000000  | 2.884930     | 0.999766          |\n",
      "PROGRESS: | 15        | 23       | 1.000000  | 3.772516     | 0.998830          |\n",
      "PROGRESS: | 20        | 31       | 0.500000  | 5.093402     | 0.999649          |\n",
      "PROGRESS: | 25        | 36       | 0.500000  | 6.063044     | 0.999649          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245213\n",
      "PROGRESS: Number of coefficients    : 245214\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.543365     | 0.989822          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.838560     | 0.998713          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.045699     | 0.999064          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.338897     | 0.998830          |\n",
      "PROGRESS: | 5         | 11       | 1.000000  | 1.538027     | 0.999064          |\n",
      "PROGRESS: | 6         | 12       | 1.000000  | 1.734159     | 0.999064          |\n",
      "PROGRESS: | 10        | 17       | 1.000000  | 2.612746     | 0.999181          |\n",
      "PROGRESS: | 11        | 18       | 1.000000  | 2.807879     | 0.999649          |\n",
      "PROGRESS: | 15        | 23       | 1.000000  | 3.693462     | 0.998830          |\n",
      "PROGRESS: | 20        | 30       | 1.000000  | 4.868251     | 0.999181          |\n",
      "PROGRESS: | 25        | 36       | 1.000000  | 5.962979     | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 243924\n",
      "PROGRESS: Number of coefficients    : 243925\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.544363     | 0.990757          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.845567     | 0.998596          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.030688     | 0.998830          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.324887     | 0.998479          |\n",
      "PROGRESS: | 5         | 11       | 1.000000  | 1.509011     | 0.998713          |\n",
      "PROGRESS: | 6         | 12       | 1.000000  | 1.699132     | 0.998713          |\n",
      "PROGRESS: | 10        | 17       | 1.000000  | 2.571717     | 0.999532          |\n",
      "PROGRESS: | 11        | 18       | 1.000000  | 2.763842     | 0.999415          |\n",
      "PROGRESS: | 15        | 24       | 1.000000  | 3.770514     | 0.999181          |\n",
      "PROGRESS: | 20        | 30       | 1.000000  | 4.841228     | 0.998947          |\n",
      "PROGRESS: | 25        | 39       | 1.000000  | 6.222148     | 0.998947          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244864\n",
      "PROGRESS: Number of coefficients    : 244865\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.553372     | 0.989587          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.854569     | 0.998245          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.048700     | 0.998713          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.355904     | 0.998479          |\n",
      "PROGRESS: | 5         | 11       | 1.000000  | 1.555036     | 0.998713          |\n",
      "PROGRESS: | 6         | 12       | 1.000000  | 1.749169     | 0.998713          |\n",
      "PROGRESS: | 10        | 17       | 1.000000  | 2.634762     | 0.998947          |\n",
      "PROGRESS: | 11        | 18       | 1.000000  | 2.851905     | 0.999415          |\n",
      "PROGRESS: | 15        | 22       | 1.000000  | 3.670450     | 0.998947          |\n",
      "PROGRESS: | 20        | 31       | 0.500000  | 5.074387     | 0.999064          |\n",
      "PROGRESS: | 25        | 36       | 0.500000  | 6.043031     | 0.999064          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 243778\n",
      "PROGRESS: Number of coefficients    : 243779\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.590395     | 0.990874          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.896601     | 0.998479          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.083723     | 0.998830          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.383927     | 0.998596          |\n",
      "PROGRESS: | 5         | 11       | 1.000000  | 1.588061     | 0.998830          |\n",
      "PROGRESS: | 6         | 12       | 1.000000  | 1.801203     | 0.998830          |\n",
      "PROGRESS: | 10        | 17       | 1.000000  | 2.687792     | 0.999415          |\n",
      "PROGRESS: | 11        | 18       | 1.000000  | 2.894936     | 0.999649          |\n",
      "PROGRESS: | 15        | 23       | 1.000000  | 3.780521     | 0.998596          |\n",
      "PROGRESS: | 20        | 31       | 1.000000  | 5.052367     | 0.999181          |\n",
      "PROGRESS: | 25        | 37       | 1.000000  | 6.116079     | 0.999064          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 246249\n",
      "PROGRESS: Number of coefficients    : 246250\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.562376     | 0.990173          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.861573     | 0.998362          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.064712     | 0.998830          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.368915     | 0.998479          |\n",
      "PROGRESS: | 5         | 11       | 1.000000  | 1.554035     | 0.998713          |\n",
      "PROGRESS: | 6         | 12       | 1.000000  | 1.742163     | 0.998713          |\n",
      "PROGRESS: | 10        | 17       | 1.000000  | 2.659775     | 0.999532          |\n",
      "PROGRESS: | 11        | 18       | 1.000000  | 2.855906     | 0.999532          |\n",
      "PROGRESS: | 15        | 23       | 1.000000  | 3.733491     | 0.999064          |\n",
      "PROGRESS: | 20        | 33       | 0.500000  | 5.249500     | 0.999064          |\n",
      "PROGRESS: | 25        | 40       | 0.500000  | 6.436296     | 0.999064          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244879\n",
      "PROGRESS: Number of coefficients    : 244880\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.564374     | 0.990407          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.876583     | 0.998362          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.075716     | 0.998713          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.364912     | 0.998713          |\n",
      "PROGRESS: | 5         | 11       | 1.000000  | 1.557037     | 0.998830          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.855240     | 0.998830          |\n",
      "PROGRESS: | 10        | 17       | 1.000000  | 2.641759     | 0.998830          |\n",
      "PROGRESS: | 11        | 19       | 1.000000  | 2.966977     | 0.998947          |\n",
      "PROGRESS: | 15        | 23       | 1.000000  | 3.754505     | 0.999064          |\n",
      "PROGRESS: | 20        | 33       | 0.500000  | 5.276518     | 0.998947          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244633\n",
      "PROGRESS: Number of coefficients    : 244634\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.542364     | 0.990523          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.868581     | 0.998479          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.062710     | 0.998830          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.386930     | 0.998830          |\n",
      "PROGRESS: | 5         | 11       | 1.000000  | 1.593062     | 0.998947          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.953303     | 0.998947          |\n",
      "PROGRESS: | 10        | 17       | 1.000000  | 2.762842     | 0.998947          |\n",
      "PROGRESS: | 11        | 19       | 1.000000  | 3.078053     | 0.999181          |\n",
      "PROGRESS: | 15        | 23       | 1.000000  | 3.940627     | 0.998596          |\n",
      "PROGRESS: | 20        | 32       | 1.000000  | 5.538695     | 0.998947          |\n",
      "PROGRESS: | 25        | 41       | 1.000000  | 7.178794     | 0.998947          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244617\n",
      "PROGRESS: Number of coefficients    : 244618\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.601401     | 0.991342          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.913613     | 0.998128          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.111744     | 0.998713          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.423950     | 0.998713          |\n",
      "PROGRESS: | 5         | 11       | 1.000000  | 1.623085     | 0.998830          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.929291     | 0.998830          |\n",
      "PROGRESS: | 10        | 19       | 5.000000  | 2.964977     | 0.999064          |\n",
      "PROGRESS: | 11        | 21       | 1.000000  | 3.289197     | 0.999298          |\n",
      "PROGRESS: | 15        | 26       | 1.000000  | 4.181793     | 0.998713          |\n",
      "PROGRESS: | 20        | 33       | 1.000000  | 5.388594     | 0.998947          |\n",
      "PROGRESS: | 25        | 40       | 1.000000  | 6.603402     | 0.998947          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245269\n",
      "PROGRESS: Number of coefficients    : 245270\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.528356     | 0.991459          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.852571     | 0.998596          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.054702     | 0.998830          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.372918     | 0.998713          |\n",
      "PROGRESS: | 5         | 11       | 1.000000  | 1.565043     | 0.998947          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.884257     | 0.998947          |\n",
      "PROGRESS: | 10        | 19       | 5.000000  | 2.875922     | 0.999181          |\n",
      "PROGRESS: | 11        | 22       | 5.000000  | 3.289196     | 0.999766          |\n",
      "PROGRESS: | 15        | 27       | 1.000000  | 4.166778     | 0.999064          |\n",
      "PROGRESS: | 20        | 35       | 1.000000  | 5.481660     | 0.999181          |\n",
      "PROGRESS: | 25        | 43       | 1.000000  | 6.771515     | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245817\n",
      "PROGRESS: Number of coefficients    : 245818\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.552372     | 0.990991          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.855572     | 0.998947          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.043698     | 0.999181          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.346898     | 0.999064          |\n",
      "PROGRESS: | 5         | 11       | 1.000000  | 1.545030     | 0.999298          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.837230     | 0.999298          |\n",
      "PROGRESS: | 10        | 17       | 1.000000  | 2.614743     | 0.999415          |\n",
      "PROGRESS: | 11        | 19       | 1.000000  | 2.912947     | 0.999649          |\n",
      "PROGRESS: | 15        | 23       | 1.000000  | 3.708474     | 0.999532          |\n",
      "PROGRESS: | 20        | 31       | 1.000000  | 5.011346     | 0.999532          |\n",
      "PROGRESS: | 25        | 37       | 1.000000  | 6.087060     | 0.999532          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 245213\n",
      "PROGRESS: Number of coefficients    : 245214\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.573387     | 0.989822          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.883596     | 0.998713          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.078720     | 0.999064          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.397934     | 0.998830          |\n",
      "PROGRESS: | 5         | 11       | 1.000000  | 1.586059     | 0.998947          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.884258     | 0.999064          |\n",
      "PROGRESS: | 10        | 17       | 1.000000  | 2.667780     | 0.998947          |\n",
      "PROGRESS: | 11        | 18       | 1.000000  | 2.857910     | 0.999532          |\n",
      "PROGRESS: | 15        | 22       | 1.000000  | 3.656440     | 0.999064          |\n",
      "PROGRESS: | 20        | 30       | 1.000000  | 4.980322     | 0.999181          |\n",
      "PROGRESS: | 25        | 40       | 0.500000  | 6.540366     | 0.999181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 243924\n",
      "PROGRESS: Number of coefficients    : 243925\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.582389     | 0.990757          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.899600     | 0.998362          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.093731     | 0.998713          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.386929     | 0.998479          |\n",
      "PROGRESS: | 5         | 11       | 1.000000  | 1.563046     | 0.998713          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.860243     | 0.998713          |\n",
      "PROGRESS: | 10        | 17       | 1.000000  | 2.657778     | 0.999064          |\n",
      "PROGRESS: | 11        | 18       | 1.000000  | 2.849904     | 0.999532          |\n",
      "PROGRESS: | 15        | 23       | 1.000000  | 3.751506     | 0.998830          |\n",
      "PROGRESS: | 20        | 29       | 1.000000  | 4.823218     | 0.998947          |\n",
      "PROGRESS: | 25        | 37       | 1.000000  | 6.132089     | 0.998947          |\n",
      "PROGRESS: | 30        | 45       | 1.000000  | 7.443965     | 0.998947          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 244864\n",
      "PROGRESS: Number of coefficients    : 244865\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.588392     | 0.989587          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.907607     | 0.998128          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.097736     | 0.998596          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.397940     | 0.998479          |\n",
      "PROGRESS: | 5         | 11       | 1.000000  | 1.584056     | 0.998713          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.892264     | 0.998713          |\n",
      "PROGRESS: | 10        | 18       | 1.000000  | 2.804872     | 0.998713          |\n",
      "PROGRESS: | 11        | 20       | 1.000000  | 3.133090     | 0.999181          |\n",
      "PROGRESS: | 15        | 25       | 1.000000  | 4.035693     | 0.998362          |\n",
      "PROGRESS: | 20        | 33       | 1.000000  | 5.435624     | 0.999064          |\n",
      "PROGRESS: | 25        | 40       | 0.500000  | 6.662445     | 0.998947          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8547\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 243778\n",
      "PROGRESS: Number of coefficients    : 243779\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.599402     | 0.990874          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.890594     | 0.998479          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.079724     | 0.998713          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.407939     | 0.998596          |\n",
      "PROGRESS: | 5         | 11       | 1.000000  | 1.610075     | 0.998830          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.913278     | 0.998713          |\n",
      "PROGRESS: | 10        | 17       | 1.000000  | 2.673784     | 0.999181          |\n",
      "PROGRESS: | 11        | 18       | 1.000000  | 2.861908     | 0.999532          |\n",
      "PROGRESS: | 15        | 22       | 1.000000  | 3.655437     | 0.999064          |\n",
      "PROGRESS: | 20        | 34       | 1.000000  | 5.390599     | 0.999064          |\n",
      "PROGRESS: | 25        | 45       | 1.000000  | 7.022684     | 0.999064          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8548\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 246249\n",
      "PROGRESS: Number of coefficients    : 246250\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000029  | 0.556372     | 0.990173          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.858573     | 0.998362          |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.053703     | 0.998596          |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.369914     | 0.998479          |\n",
      "PROGRESS: | 5         | 11       | 1.000000  | 1.555040     | 0.998713          |\n",
      "PROGRESS: | 6         | 13       | 1.000000  | 1.848236     | 0.998596          |\n",
      "PROGRESS: | 10        | 17       | 1.000000  | 2.641763     | 0.998947          |\n",
      "PROGRESS: | 11        | 18       | 1.000000  | 2.828889     | 0.999415          |\n",
      "PROGRESS: | 15        | 22       | 1.000000  | 3.618417     | 0.998830          |\n",
      "PROGRESS: | 20        | 32       | 1.000000  | 5.118415     | 0.998947          |\n",
      "PROGRESS: | 25        | 38       | 1.000000  | 6.187130     | 0.998947          |\n",
      "PROGRESS: | 30        | 47       | 1.000000  | 7.630090     | 0.998947          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "0\n",
      "0.859152360396\n",
      "0\n",
      "0.0204395789324\n",
      "1000.0\n",
      "1000.0\n",
      "l2 penalty\n"
     ]
    }
   ],
   "source": [
    "penalty = []\n",
    "acc = []\n",
    "fsco = []\n",
    "for i in np.linspace(1000, 10000, num=10):\n",
    "    (maxAccuracy,maxfscores) = k_fold_cross_validation(10, i, train_data_positive_ornot, 'Sentiment')\n",
    "    penalty.append(i)\n",
    "    acc.append(maxAccuracy)\n",
    "    fsco.append(maxfscores)\n",
    "    #print i\n",
    "\n",
    "print acc.index(max(acc))\n",
    "print max(acc)\n",
    "print fsco.index(max(fsco))\n",
    "print max(fsco)\n",
    "print penalty[acc.index(max(acc))]\n",
    "print penalty[fsco.index(max(fsco))]\n",
    "print \"l2 penalty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79948\n",
      "PROGRESS: Number of coefficients    : 79949\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000217  | 0.175117     | 0.992618          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.301203     | 0.997829          |\n",
      "PROGRESS: | 3         | 9        | 0.500000  | 0.482322     | 0.992184          |\n",
      "PROGRESS: | 4         | 12       | 0.500000  | 0.645430     | 0.999566          |\n",
      "PROGRESS: | 5         | 13       | 0.500000  | 0.728486     | 0.999566          |\n",
      "PROGRESS: | 6         | 14       | 0.500000  | 0.816545     | 0.999132          |\n",
      "PROGRESS: | 10        | 18       | 0.500000  | 1.127751     | 0.999132          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79969\n",
      "PROGRESS: Number of coefficients    : 79970\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000217  | 0.175119     | 0.992181          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.288193     | 0.997828          |\n",
      "PROGRESS: | 3         | 9        | 0.500000  | 0.440295     | 0.994353          |\n",
      "PROGRESS: | 4         | 12       | 0.500000  | 0.603402     | 0.999566          |\n",
      "PROGRESS: | 5         | 13       | 0.500000  | 0.669449     | 0.999566          |\n",
      "PROGRESS: | 6         | 14       | 0.500000  | 0.739494     | 0.999131          |\n",
      "PROGRESS: | 10        | 18       | 0.500000  | 1.036693     | 0.999131          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80034\n",
      "PROGRESS: Number of coefficients    : 80035\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000217  | 0.165110     | 0.991746          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.276184     | 0.997828          |\n",
      "PROGRESS: | 3         | 9        | 0.500000  | 0.427285     | 0.993484          |\n",
      "PROGRESS: | 4         | 12       | 0.500000  | 0.582387     | 0.999131          |\n",
      "PROGRESS: | 5         | 13       | 0.500000  | 0.653435     | 0.999131          |\n",
      "PROGRESS: | 6         | 14       | 0.500000  | 0.726485     | 0.999131          |\n",
      "PROGRESS: | 10        | 18       | 0.500000  | 1.011675     | 0.999131          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79911\n",
      "PROGRESS: Number of coefficients    : 79912\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000217  | 0.167114     | 0.991312          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.278188     | 0.997394          |\n",
      "PROGRESS: | 3         | 9        | 0.500000  | 0.439294     | 0.993484          |\n",
      "PROGRESS: | 4         | 12       | 0.500000  | 0.621415     | 0.999131          |\n",
      "PROGRESS: | 5         | 13       | 0.500000  | 0.701468     | 0.999566          |\n",
      "PROGRESS: | 6         | 14       | 0.500000  | 0.775517     | 0.999566          |\n",
      "PROGRESS: | 10        | 18       | 0.500000  | 1.078722     | 0.999131          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79950\n",
      "PROGRESS: Number of coefficients    : 79951\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000217  | 0.162107     | 0.990877          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.278185     | 0.997828          |\n",
      "PROGRESS: | 3         | 9        | 0.500000  | 0.420280     | 0.998262          |\n",
      "PROGRESS: | 4         | 12       | 0.500000  | 0.572381     | 0.999131          |\n",
      "PROGRESS: | 5         | 13       | 0.500000  | 0.642426     | 0.999131          |\n",
      "PROGRESS: | 6         | 14       | 0.500000  | 0.719478     | 0.999131          |\n",
      "PROGRESS: | 10        | 18       | 0.500000  | 1.018680     | 0.999131          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80098\n",
      "PROGRESS: Number of coefficients    : 80099\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000217  | 0.174116     | 0.992181          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.287191     | 0.997828          |\n",
      "PROGRESS: | 3         | 9        | 0.500000  | 0.434288     | 0.998262          |\n",
      "PROGRESS: | 4         | 12       | 0.500000  | 0.585388     | 0.999131          |\n",
      "PROGRESS: | 5         | 13       | 0.500000  | 0.657437     | 0.999566          |\n",
      "PROGRESS: | 6         | 14       | 0.500000  | 0.736490     | 0.999566          |\n",
      "PROGRESS: | 10        | 18       | 0.500000  | 1.067711     | 0.999131          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79658\n",
      "PROGRESS: Number of coefficients    : 79659\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000217  | 0.164110     | 0.991312          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.273182     | 0.997828          |\n",
      "PROGRESS: | 3         | 9        | 0.500000  | 0.423282     | 0.993918          |\n",
      "PROGRESS: | 4         | 12       | 0.500000  | 0.578385     | 0.999131          |\n",
      "PROGRESS: | 5         | 13       | 0.500000  | 0.647432     | 0.999566          |\n",
      "PROGRESS: | 6         | 14       | 0.500000  | 0.719480     | 0.999131          |\n",
      "PROGRESS: | 10        | 18       | 0.500000  | 1.021681     | 0.999131          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80441\n",
      "PROGRESS: Number of coefficients    : 80442\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000217  | 0.165112     | 0.990443          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.278186     | 0.998697          |\n",
      "PROGRESS: | 3         | 9        | 0.500000  | 0.424284     | 0.998697          |\n",
      "PROGRESS: | 4         | 12       | 0.500000  | 0.582387     | 1.000000          |\n",
      "PROGRESS: | 5         | 13       | 0.500000  | 0.657438     | 1.000000          |\n",
      "PROGRESS: | 6         | 14       | 0.500000  | 0.732489     | 1.000000          |\n",
      "PROGRESS: | 10        | 18       | 0.500000  | 1.042695     | 1.000000          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79909\n",
      "PROGRESS: Number of coefficients    : 79910\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000217  | 0.170114     | 0.991312          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.280186     | 0.997394          |\n",
      "PROGRESS: | 3         | 9        | 0.500000  | 0.431286     | 0.997394          |\n",
      "PROGRESS: | 4         | 12       | 0.500000  | 0.581388     | 0.999131          |\n",
      "PROGRESS: | 5         | 13       | 0.500000  | 0.652433     | 0.999566          |\n",
      "PROGRESS: | 6         | 14       | 0.500000  | 0.725484     | 0.999566          |\n",
      "PROGRESS: | 10        | 18       | 0.500000  | 1.025684     | 0.999131          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79835\n",
      "PROGRESS: Number of coefficients    : 79836\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000217  | 0.171113     | 0.992618          |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.280185     | 0.997829          |\n",
      "PROGRESS: | 3         | 9        | 0.500000  | 0.433292     | 0.995224          |\n",
      "PROGRESS: | 4         | 12       | 0.500000  | 0.580387     | 0.999566          |\n",
      "PROGRESS: | 5         | 13       | 0.500000  | 0.654437     | 1.000000          |\n",
      "PROGRESS: | 6         | 14       | 0.500000  | 0.726483     | 0.999566          |\n",
      "PROGRESS: | 10        | 18       | 0.500000  | 1.025686     | 0.999566          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "{'f1_score': 0.8476190476190475, 'auc': 0.838357588357588, 'recall': 0.978021978021978, 'precision': 0.7478991596638656, 'log_loss': 0.4896195479867756, 'roc_curve': Columns:\n",
      "\tthreshold\tfloat\n",
      "\tfpr\tfloat\n",
      "\ttpr\tfloat\n",
      "\tp\tint\n",
      "\tn\tint\n",
      "\n",
      "Rows: 100001\n",
      "\n",
      "Data:\n",
      "+-----------+-----+-----+-----+----+\n",
      "| threshold | fpr | tpr |  p  | n  |\n",
      "+-----------+-----+-----+-----+----+\n",
      "|    0.0    | 1.0 | 1.0 | 182 | 74 |\n",
      "|   1e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   2e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   3e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   4e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   5e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   6e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   7e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   8e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   9e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "+-----------+-----+-----+-----+----+\n",
      "[100001 rows x 5 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns., 'confusion_matrix': Columns:\n",
      "\ttarget_label\tstr\n",
      "\tpredicted_label\tstr\n",
      "\tcount\tint\n",
      "\n",
      "Rows: 4\n",
      "\n",
      "Data:\n",
      "+--------------+-----------------+-------+\n",
      "| target_label | predicted_label | count |\n",
      "+--------------+-----------------+-------+\n",
      "|   negative   |     negative    |   14  |\n",
      "|   negative   |     positive    |   60  |\n",
      "|   positive   |     negative    |   4   |\n",
      "|   positive   |     positive    |  178  |\n",
      "+--------------+-----------------+-------+\n",
      "[4 rows x 3 columns]\n",
      ", 'accuracy': 0.75}\n",
      "1000.0\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79948\n",
      "PROGRESS: Number of coefficients    : 79949\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000109  | 0.205136     | 0.992618          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.335222     | 0.998263          |\n",
      "PROGRESS: | 3         | 10       | 0.500000  | 0.509338     | 0.998697          |\n",
      "PROGRESS: | 4         | 13       | 0.500000  | 0.672449     | 0.998263          |\n",
      "PROGRESS: | 5         | 15       | 1.000000  | 0.781520     | 0.998263          |\n",
      "PROGRESS: | 6         | 17       | 1.000000  | 0.896596     | 0.998263          |\n",
      "PROGRESS: | 10        | 22       | 1.000000  | 1.221813     | 0.998263          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79969\n",
      "PROGRESS: Number of coefficients    : 79970\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000109  | 0.200136     | 0.992181          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.316211     | 0.998262          |\n",
      "PROGRESS: | 3         | 10       | 0.500000  | 0.486326     | 0.998262          |\n",
      "PROGRESS: | 4         | 12       | 1.000000  | 0.602403     | 0.998262          |\n",
      "PROGRESS: | 5         | 14       | 1.000000  | 0.720481     | 0.998262          |\n",
      "PROGRESS: | 6         | 15       | 1.000000  | 0.793531     | 0.998262          |\n",
      "PROGRESS: | 10        | 21       | 1.000000  | 1.172783     | 0.998262          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80034\n",
      "PROGRESS: Number of coefficients    : 80035\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000109  | 0.228153     | 0.991746          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.340228     | 0.998697          |\n",
      "PROGRESS: | 3         | 10       | 0.500000  | 0.516344     | 0.998697          |\n",
      "PROGRESS: | 4         | 13       | 0.500000  | 0.681454     | 0.998262          |\n",
      "PROGRESS: | 5         | 15       | 1.000000  | 0.807538     | 0.998262          |\n",
      "PROGRESS: | 6         | 17       | 1.000000  | 0.941626     | 0.998262          |\n",
      "PROGRESS: | 10        | 22       | 1.000000  | 1.320880     | 0.998262          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79911\n",
      "PROGRESS: Number of coefficients    : 79912\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000109  | 0.234157     | 0.991312          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.360241     | 0.998262          |\n",
      "PROGRESS: | 3         | 10       | 0.500000  | 0.537360     | 0.998697          |\n",
      "PROGRESS: | 4         | 12       | 1.000000  | 0.658439     | 0.998262          |\n",
      "PROGRESS: | 5         | 14       | 1.000000  | 0.789528     | 0.998262          |\n",
      "PROGRESS: | 6         | 15       | 1.000000  | 0.874585     | 0.998262          |\n",
      "PROGRESS: | 10        | 21       | 1.000000  | 1.311875     | 0.998262          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79950\n",
      "PROGRESS: Number of coefficients    : 79951\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000109  | 0.236156     | 0.990877          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.366245     | 0.998697          |\n",
      "PROGRESS: | 3         | 10       | 0.500000  | 0.539361     | 0.998262          |\n",
      "PROGRESS: | 4         | 12       | 1.000000  | 0.662442     | 0.998697          |\n",
      "PROGRESS: | 5         | 14       | 1.000000  | 0.792529     | 0.998697          |\n",
      "PROGRESS: | 6         | 15       | 1.000000  | 0.879588     | 0.998697          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 1.230821     | 0.998697          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80098\n",
      "PROGRESS: Number of coefficients    : 80099\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000109  | 0.211140     | 0.992181          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.322214     | 0.998697          |\n",
      "PROGRESS: | 3         | 10       | 0.500000  | 0.482320     | 0.999131          |\n",
      "PROGRESS: | 4         | 12       | 1.000000  | 0.595395     | 0.998697          |\n",
      "PROGRESS: | 5         | 14       | 1.000000  | 0.711474     | 0.999131          |\n",
      "PROGRESS: | 6         | 15       | 1.000000  | 0.782520     | 0.999131          |\n",
      "PROGRESS: | 10        | 23       | 5.000000  | 1.232820     | 0.999131          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79658\n",
      "PROGRESS: Number of coefficients    : 79659\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000109  | 0.198130     | 0.991312          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.312206     | 0.998697          |\n",
      "PROGRESS: | 3         | 10       | 0.500000  | 0.468312     | 0.998697          |\n",
      "PROGRESS: | 4         | 12       | 1.000000  | 0.583388     | 0.998697          |\n",
      "PROGRESS: | 5         | 14       | 1.000000  | 0.695463     | 0.998697          |\n",
      "PROGRESS: | 6         | 15       | 1.000000  | 0.766510     | 0.998697          |\n",
      "PROGRESS: | 10        | 21       | 1.000000  | 1.156770     | 0.999131          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80441\n",
      "PROGRESS: Number of coefficients    : 80442\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000109  | 0.202136     | 0.990443          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.310208     | 0.999131          |\n",
      "PROGRESS: | 3         | 10       | 0.500000  | 0.460307     | 0.999131          |\n",
      "PROGRESS: | 4         | 12       | 1.000000  | 0.571380     | 0.999131          |\n",
      "PROGRESS: | 5         | 14       | 1.000000  | 0.689460     | 0.999566          |\n",
      "PROGRESS: | 6         | 15       | 1.000000  | 0.760508     | 0.999566          |\n",
      "PROGRESS: | 10        | 21       | 1.000000  | 1.140761     | 0.999566          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79909\n",
      "PROGRESS: Number of coefficients    : 79910\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000109  | 0.214144     | 0.991312          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.323215     | 0.998262          |\n",
      "PROGRESS: | 3         | 10       | 0.500000  | 0.482321     | 0.998697          |\n",
      "PROGRESS: | 4         | 12       | 1.000000  | 0.605404     | 0.998262          |\n",
      "PROGRESS: | 5         | 14       | 1.000000  | 0.724482     | 0.998697          |\n",
      "PROGRESS: | 6         | 15       | 1.000000  | 0.805537     | 0.998697          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 1.151768     | 0.999131          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79835\n",
      "PROGRESS: Number of coefficients    : 79836\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000109  | 0.199133     | 0.992618          |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 0.314210     | 0.998697          |\n",
      "PROGRESS: | 3         | 10       | 0.500000  | 0.468313     | 0.998697          |\n",
      "PROGRESS: | 4         | 12       | 1.000000  | 0.584390     | 0.998697          |\n",
      "PROGRESS: | 5         | 14       | 1.000000  | 0.700467     | 0.998697          |\n",
      "PROGRESS: | 6         | 15       | 1.000000  | 0.778520     | 0.998697          |\n",
      "PROGRESS: | 10        | 21       | 1.000000  | 1.147767     | 0.999132          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "{'f1_score': 0.8468899521531099, 'auc': 0.8328259578259581, 'recall': 0.9725274725274725, 'precision': 0.75, 'log_loss': 0.533551789174679, 'roc_curve': Columns:\n",
      "\tthreshold\tfloat\n",
      "\tfpr\tfloat\n",
      "\ttpr\tfloat\n",
      "\tp\tint\n",
      "\tn\tint\n",
      "\n",
      "Rows: 100001\n",
      "\n",
      "Data:\n",
      "+-----------+-----+-----+-----+----+\n",
      "| threshold | fpr | tpr |  p  | n  |\n",
      "+-----------+-----+-----+-----+----+\n",
      "|    0.0    | 1.0 | 1.0 | 182 | 74 |\n",
      "|   1e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   2e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   3e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   4e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   5e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   6e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   7e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   8e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   9e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "+-----------+-----+-----+-----+----+\n",
      "[100001 rows x 5 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns., 'confusion_matrix': Columns:\n",
      "\ttarget_label\tstr\n",
      "\tpredicted_label\tstr\n",
      "\tcount\tint\n",
      "\n",
      "Rows: 4\n",
      "\n",
      "Data:\n",
      "+--------------+-----------------+-------+\n",
      "| target_label | predicted_label | count |\n",
      "+--------------+-----------------+-------+\n",
      "|   negative   |     negative    |   15  |\n",
      "|   negative   |     positive    |   59  |\n",
      "|   positive   |     negative    |   5   |\n",
      "|   positive   |     positive    |  177  |\n",
      "+--------------+-----------------+-------+\n",
      "[4 rows x 3 columns]\n",
      ", 'accuracy': 0.75}\n",
      "3162.27766017\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79948\n",
      "PROGRESS: Number of coefficients    : 79949\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000030  | 0.243162     | 0.992618          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.352234     | 0.995224          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.422282     | 0.998263          |\n",
      "PROGRESS: | 4         | 11       | 1.000000  | 0.532355     | 0.997395          |\n",
      "PROGRESS: | 5         | 13       | 1.000000  | 0.642429     | 0.998263          |\n",
      "PROGRESS: | 6         | 15       | 1.000000  | 0.760506     | 0.998263          |\n",
      "PROGRESS: | 10        | 21       | 1.000000  | 1.139760     | 0.998263          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79969\n",
      "PROGRESS: Number of coefficients    : 79970\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000030  | 0.255172     | 0.992181          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.373248     | 0.995222          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.445297     | 0.998262          |\n",
      "PROGRESS: | 4         | 11       | 1.000000  | 0.560374     | 0.997828          |\n",
      "PROGRESS: | 5         | 13       | 1.000000  | 0.672449     | 0.997828          |\n",
      "PROGRESS: | 6         | 15       | 1.000000  | 0.781522     | 0.998262          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 1.119745     | 0.997828          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80034\n",
      "PROGRESS: Number of coefficients    : 80035\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000030  | 0.260175     | 0.991746          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.373250     | 0.995222          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.448300     | 0.998697          |\n",
      "PROGRESS: | 4         | 11       | 1.000000  | 0.564379     | 0.997828          |\n",
      "PROGRESS: | 5         | 13       | 1.000000  | 0.675451     | 0.997828          |\n",
      "PROGRESS: | 6         | 15       | 1.000000  | 0.786526     | 0.998262          |\n",
      "PROGRESS: | 10        | 21       | 1.000000  | 1.177785     | 0.998262          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79911\n",
      "PROGRESS: Number of coefficients    : 79912\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000030  | 0.247164     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.364243     | 0.995656          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.441295     | 0.998262          |\n",
      "PROGRESS: | 4         | 11       | 1.000000  | 0.554369     | 0.998262          |\n",
      "PROGRESS: | 5         | 13       | 1.000000  | 0.671448     | 0.998262          |\n",
      "PROGRESS: | 6         | 15       | 1.000000  | 0.779520     | 0.998262          |\n",
      "PROGRESS: | 10        | 21       | 1.000000  | 1.170780     | 0.997828          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79950\n",
      "PROGRESS: Number of coefficients    : 79951\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000030  | 0.263175     | 0.990877          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.384256     | 0.995656          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.458305     | 0.998697          |\n",
      "PROGRESS: | 4         | 11       | 1.000000  | 0.585390     | 0.997828          |\n",
      "PROGRESS: | 5         | 13       | 1.000000  | 0.700468     | 0.997828          |\n",
      "PROGRESS: | 6         | 15       | 1.000000  | 0.820546     | 0.997828          |\n",
      "PROGRESS: | 10        | 22       | 1.000000  | 1.235825     | 0.996525          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80098\n",
      "PROGRESS: Number of coefficients    : 80099\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000030  | 0.260173     | 0.992181          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.382256     | 0.996959          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.451299     | 0.998262          |\n",
      "PROGRESS: | 4         | 11       | 1.000000  | 0.566377     | 0.997828          |\n",
      "PROGRESS: | 5         | 13       | 1.000000  | 0.673448     | 0.997828          |\n",
      "PROGRESS: | 6         | 15       | 1.000000  | 0.785522     | 0.998697          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 1.114743     | 0.998262          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79658\n",
      "PROGRESS: Number of coefficients    : 79659\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000030  | 0.241159     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.346232     | 0.996525          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.423281     | 0.998697          |\n",
      "PROGRESS: | 4         | 11       | 1.000000  | 0.540359     | 0.998262          |\n",
      "PROGRESS: | 5         | 13       | 1.000000  | 0.652434     | 0.998262          |\n",
      "PROGRESS: | 6         | 15       | 1.000000  | 0.765509     | 0.998262          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 1.097732     | 0.998262          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80441\n",
      "PROGRESS: Number of coefficients    : 80442\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000030  | 0.248167     | 0.990443          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.365243     | 0.996090          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.447299     | 0.998697          |\n",
      "PROGRESS: | 4         | 11       | 1.000000  | 0.558373     | 0.998262          |\n",
      "PROGRESS: | 5         | 13       | 1.000000  | 0.671447     | 0.998262          |\n",
      "PROGRESS: | 6         | 15       | 1.000000  | 0.780522     | 0.998262          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 1.126755     | 0.997828          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79909\n",
      "PROGRESS: Number of coefficients    : 79910\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000030  | 0.250168     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.365243     | 0.996090          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.435291     | 0.998262          |\n",
      "PROGRESS: | 4         | 11       | 1.000000  | 0.547365     | 0.997828          |\n",
      "PROGRESS: | 5         | 13       | 1.000000  | 0.657439     | 0.997828          |\n",
      "PROGRESS: | 6         | 15       | 1.000000  | 0.769513     | 0.997828          |\n",
      "PROGRESS: | 10        | 21       | 5.000000  | 1.145768     | 0.873154          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79835\n",
      "PROGRESS: Number of coefficients    : 79836\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000030  | 0.252158     | 0.992618          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.365234     | 0.996526          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.445287     | 0.998697          |\n",
      "PROGRESS: | 4         | 11       | 1.000000  | 0.561365     | 0.997395          |\n",
      "PROGRESS: | 5         | 13       | 1.000000  | 0.681443     | 0.997829          |\n",
      "PROGRESS: | 6         | 15       | 1.000000  | 0.798523     | 0.997829          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 1.161764     | 0.997829          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "{'f1_score': 0.8443396226415094, 'auc': 0.830969705969706, 'recall': 0.9835164835164835, 'precision': 0.7396694214876033, 'log_loss': 0.5874017325925834, 'roc_curve': Columns:\n",
      "\tthreshold\tfloat\n",
      "\tfpr\tfloat\n",
      "\ttpr\tfloat\n",
      "\tp\tint\n",
      "\tn\tint\n",
      "\n",
      "Rows: 100001\n",
      "\n",
      "Data:\n",
      "+-----------+-----+-----+-----+----+\n",
      "| threshold | fpr | tpr |  p  | n  |\n",
      "+-----------+-----+-----+-----+----+\n",
      "|    0.0    | 1.0 | 1.0 | 182 | 74 |\n",
      "|   1e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   2e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   3e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   4e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   5e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   6e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   7e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   8e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   9e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "+-----------+-----+-----+-----+----+\n",
      "[100001 rows x 5 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns., 'confusion_matrix': Columns:\n",
      "\ttarget_label\tstr\n",
      "\tpredicted_label\tstr\n",
      "\tcount\tint\n",
      "\n",
      "Rows: 4\n",
      "\n",
      "Data:\n",
      "+--------------+-----------------+-------+\n",
      "| target_label | predicted_label | count |\n",
      "+--------------+-----------------+-------+\n",
      "|   negative   |     negative    |   11  |\n",
      "|   negative   |     positive    |   63  |\n",
      "|   positive   |     positive    |  179  |\n",
      "|   positive   |     negative    |   3   |\n",
      "+--------------+-----------------+-------+\n",
      "[4 rows x 3 columns]\n",
      ", 'accuracy': 0.7421875}\n",
      "10000.0\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79948\n",
      "PROGRESS: Number of coefficients    : 79949\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000013  | 0.247166     | 0.992618          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.368247     | 0.994355          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.447300     | 0.994789          |\n",
      "PROGRESS: | 4         | 11       | 1.000000  | 0.571382     | 0.994789          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 0.650435     | 0.994789          |\n",
      "PROGRESS: | 6         | 14       | 1.000000  | 0.780522     | 0.994789          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 1.192797     | 0.992618          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79969\n",
      "PROGRESS: Number of coefficients    : 79970\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000013  | 0.244164     | 0.992181          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.360243     | 0.994353          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.431291     | 0.994787          |\n",
      "PROGRESS: | 4         | 11       | 1.000000  | 0.545364     | 0.994787          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 0.618413     | 0.994787          |\n",
      "PROGRESS: | 6         | 14       | 1.000000  | 0.747501     | 0.994787          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80034\n",
      "PROGRESS: Number of coefficients    : 80035\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000013  | 0.244165     | 0.991746          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.355240     | 0.993484          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.427287     | 0.995656          |\n",
      "PROGRESS: | 4         | 11       | 1.000000  | 0.541363     | 0.995656          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 0.614411     | 0.995656          |\n",
      "PROGRESS: | 6         | 14       | 1.000000  | 0.731491     | 0.995656          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79911\n",
      "PROGRESS: Number of coefficients    : 79912\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000013  | 0.244163     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.361241     | 0.993918          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.436292     | 0.995656          |\n",
      "PROGRESS: | 4         | 11       | 1.000000  | 0.545365     | 0.995222          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 0.616411     | 0.995222          |\n",
      "PROGRESS: | 6         | 14       | 1.000000  | 0.735492     | 0.995222          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79950\n",
      "PROGRESS: Number of coefficients    : 79951\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000013  | 0.255171     | 0.990877          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.366246     | 0.993918          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.434290     | 0.994353          |\n",
      "PROGRESS: | 4         | 11       | 1.000000  | 0.545364     | 0.994353          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 0.622415     | 0.993918          |\n",
      "PROGRESS: | 6         | 14       | 1.000000  | 0.739495     | 0.993918          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80098\n",
      "PROGRESS: Number of coefficients    : 80099\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000013  | 0.239163     | 0.992181          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.351235     | 0.993918          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.422284     | 0.994353          |\n",
      "PROGRESS: | 4         | 11       | 1.000000  | 0.538361     | 0.993918          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 0.616412     | 0.993918          |\n",
      "PROGRESS: | 6         | 14       | 1.000000  | 0.734493     | 0.993918          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79658\n",
      "PROGRESS: Number of coefficients    : 79659\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000013  | 0.254172     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.369248     | 0.994353          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.442295     | 0.996525          |\n",
      "PROGRESS: | 4         | 11       | 1.000000  | 0.553370     | 0.996090          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 0.632425     | 0.996090          |\n",
      "PROGRESS: | 6         | 14       | 1.000000  | 0.745497     | 0.995656          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80441\n",
      "PROGRESS: Number of coefficients    : 80442\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000013  | 0.234159     | 0.990443          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.354236     | 0.994353          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.433291     | 0.994787          |\n",
      "PROGRESS: | 4         | 11       | 1.000000  | 0.564378     | 0.994787          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 0.640429     | 0.994787          |\n",
      "PROGRESS: | 6         | 14       | 1.000000  | 0.758507     | 0.994787          |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 1.170785     | 0.993050          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79909\n",
      "PROGRESS: Number of coefficients    : 79910\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000013  | 0.243163     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.359240     | 0.993918          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.433291     | 0.994787          |\n",
      "PROGRESS: | 4         | 11       | 1.000000  | 0.550368     | 0.994787          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 0.625418     | 0.994353          |\n",
      "PROGRESS: | 6         | 14       | 1.000000  | 0.742496     | 0.994353          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79835\n",
      "PROGRESS: Number of coefficients    : 79836\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000013  | 0.237159     | 0.992618          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.353235     | 0.995658          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.424282     | 0.996092          |\n",
      "PROGRESS: | 4         | 11       | 1.000000  | 0.539360     | 0.996526          |\n",
      "PROGRESS: | 5         | 12       | 1.000000  | 0.624417     | 0.996526          |\n",
      "PROGRESS: | 6         | 14       | 1.000000  | 0.743496     | 0.996526          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "{'f1_score': 0.8443396226415094, 'auc': 0.8259207009207006, 'recall': 0.9835164835164835, 'precision': 0.7396694214876033, 'log_loss': 0.6402246370120775, 'roc_curve': Columns:\n",
      "\tthreshold\tfloat\n",
      "\tfpr\tfloat\n",
      "\ttpr\tfloat\n",
      "\tp\tint\n",
      "\tn\tint\n",
      "\n",
      "Rows: 100001\n",
      "\n",
      "Data:\n",
      "+-----------+-----+-----+-----+----+\n",
      "| threshold | fpr | tpr |  p  | n  |\n",
      "+-----------+-----+-----+-----+----+\n",
      "|    0.0    | 1.0 | 1.0 | 182 | 74 |\n",
      "|   1e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   2e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   3e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   4e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   5e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   6e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   7e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   8e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   9e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "+-----------+-----+-----+-----+----+\n",
      "[100001 rows x 5 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns., 'confusion_matrix': Columns:\n",
      "\ttarget_label\tstr\n",
      "\tpredicted_label\tstr\n",
      "\tcount\tint\n",
      "\n",
      "Rows: 4\n",
      "\n",
      "Data:\n",
      "+--------------+-----------------+-------+\n",
      "| target_label | predicted_label | count |\n",
      "+--------------+-----------------+-------+\n",
      "|   negative   |     negative    |   11  |\n",
      "|   negative   |     positive    |   63  |\n",
      "|   positive   |     positive    |  179  |\n",
      "|   positive   |     negative    |   3   |\n",
      "+--------------+-----------------+-------+\n",
      "[4 rows x 3 columns]\n",
      ", 'accuracy': 0.7421875}\n",
      "31622.7766017\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79948\n",
      "PROGRESS: Number of coefficients    : 79949\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000005  | 0.268180     | 0.992618          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.393264     | 0.992618          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.478318     | 0.992618          |\n",
      "PROGRESS: | 4         | 13       | 21.000000 | 0.698466     | 0.993053          |\n",
      "PROGRESS: | 5         | 17       | 21.000000 | 0.909606     | 0.991750          |\n",
      "PROGRESS: | 6         | 19       | 1.000000  | 1.033690     | 0.991750          |\n",
      "PROGRESS: | 10        | 25       | 1.000000  | 1.462976     | 0.989579          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79969\n",
      "PROGRESS: Number of coefficients    : 79970\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000005  | 0.262174     | 0.992181          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.391261     | 0.993050          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.467313     | 0.993050          |\n",
      "PROGRESS: | 4         | 13       | 21.000000 | 0.689460     | 0.993484          |\n",
      "PROGRESS: | 5         | 17       | 21.000000 | 0.917613     | 0.992615          |\n",
      "PROGRESS: | 6         | 19       | 1.000000  | 1.051702     | 0.991312          |\n",
      "PROGRESS: | 10        | 28       | 0.031500  | 1.601069     | 0.991312          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80034\n",
      "PROGRESS: Number of coefficients    : 80035\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000005  | 0.250166     | 0.991746          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.365245     | 0.992181          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.444297     | 0.992181          |\n",
      "PROGRESS: | 4         | 13       | 21.000000 | 0.643430     | 0.993050          |\n",
      "PROGRESS: | 5         | 17       | 21.000000 | 0.848565     | 0.990877          |\n",
      "PROGRESS: | 6         | 19       | 1.000000  | 0.973650     | 0.990443          |\n",
      "PROGRESS: | 10        | 23       | 1.000000  | 1.306870     | 0.990443          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79911\n",
      "PROGRESS: Number of coefficients    : 79912\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000005  | 0.290196     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.423283     | 0.992615          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.507339     | 0.993050          |\n",
      "PROGRESS: | 4         | 13       | 21.000000 | 0.718479     | 0.993050          |\n",
      "PROGRESS: | 5         | 17       | 21.000000 | 0.918613     | 0.991312          |\n",
      "PROGRESS: | 6         | 19       | 1.000000  | 1.032690     | 0.989574          |\n",
      "PROGRESS: | 10        | 23       | 1.000000  | 1.339895     | 0.989574          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79950\n",
      "PROGRESS: Number of coefficients    : 79951\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000005  | 0.251169     | 0.990877          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.367244     | 0.992181          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.445297     | 0.992181          |\n",
      "PROGRESS: | 4         | 13       | 21.000000 | 0.642428     | 0.992181          |\n",
      "PROGRESS: | 5         | 17       | 21.000000 | 0.840560     | 0.990877          |\n",
      "PROGRESS: | 6         | 19       | 1.000000  | 0.959641     | 0.989574          |\n",
      "PROGRESS: | 10        | 27       | 0.500000  | 1.432956     | 0.989574          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80098\n",
      "PROGRESS: Number of coefficients    : 80099\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000005  | 0.258173     | 0.992181          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.368248     | 0.992181          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.447300     | 0.992181          |\n",
      "PROGRESS: | 4         | 12       | 5.000000  | 0.599401     | 0.992181          |\n",
      "PROGRESS: | 5         | 15       | 5.000000  | 0.753506     | 0.992181          |\n",
      "PROGRESS: | 6         | 18       | 5.000000  | 0.907607     | 0.992181          |\n",
      "PROGRESS: | 10        | 23       | 1.000000  | 1.258843     | 0.990877          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79658\n",
      "PROGRESS: Number of coefficients    : 79659\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000005  | 0.249167     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.359242     | 0.992181          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.431287     | 0.992615          |\n",
      "PROGRESS: | 4         | 12       | 5.000000  | 0.584389     | 0.993050          |\n",
      "PROGRESS: | 5         | 14       | 1.000000  | 0.703470     | 0.993050          |\n",
      "PROGRESS: | 6         | 16       | 1.000000  | 0.816544     | 0.992181          |\n",
      "PROGRESS: | 10        | 22       | 1.000000  | 1.202803     | 0.990443          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80441\n",
      "PROGRESS: Number of coefficients    : 80442\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000005  | 0.243164     | 0.990443          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.372250     | 0.992181          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.449301     | 0.992181          |\n",
      "PROGRESS: | 4         | 13       | 21.000000 | 0.666449     | 0.992181          |\n",
      "PROGRESS: | 5         | 17       | 21.000000 | 0.869581     | 0.991312          |\n",
      "PROGRESS: | 6         | 19       | 1.000000  | 1.002670     | 0.990443          |\n",
      "PROGRESS: | 10        | 28       | 0.082058  | 1.533024     | 0.990443          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79909\n",
      "PROGRESS: Number of coefficients    : 79910\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000005  | 0.299200     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.437293     | 0.992615          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.526352     | 0.992615          |\n",
      "PROGRESS: | 4         | 13       | 21.000000 | 0.724485     | 0.992615          |\n",
      "PROGRESS: | 5         | 16       | 5.000000  | 0.882589     | 0.992615          |\n",
      "PROGRESS: | 6         | 18       | 1.000000  | 0.999668     | 0.992181          |\n",
      "PROGRESS: | 10        | 23       | 1.000000  | 1.341896     | 0.990443          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79835\n",
      "PROGRESS: Number of coefficients    : 79836\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000005  | 0.253171     | 0.992618          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.387263     | 0.993053          |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 0.484324     | 0.993487          |\n",
      "PROGRESS: | 4         | 13       | 21.000000 | 0.689461     | 0.993487          |\n",
      "PROGRESS: | 5         | 17       | 21.000000 | 0.885591     | 0.992618          |\n",
      "PROGRESS: | 6         | 19       | 1.000000  | 0.995665     | 0.991316          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "{'f1_score': 0.8408551068883611, 'auc': 0.8148203148203146, 'recall': 0.9725274725274725, 'precision': 0.7405857740585774, 'log_loss': 0.6728139039456633, 'roc_curve': Columns:\n",
      "\tthreshold\tfloat\n",
      "\tfpr\tfloat\n",
      "\ttpr\tfloat\n",
      "\tp\tint\n",
      "\tn\tint\n",
      "\n",
      "Rows: 100001\n",
      "\n",
      "Data:\n",
      "+-----------+-----+-----+-----+----+\n",
      "| threshold | fpr | tpr |  p  | n  |\n",
      "+-----------+-----+-----+-----+----+\n",
      "|    0.0    | 1.0 | 1.0 | 182 | 74 |\n",
      "|   1e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   2e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   3e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   4e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   5e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   6e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   7e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   8e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   9e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "+-----------+-----+-----+-----+----+\n",
      "[100001 rows x 5 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns., 'confusion_matrix': Columns:\n",
      "\ttarget_label\tstr\n",
      "\tpredicted_label\tstr\n",
      "\tcount\tint\n",
      "\n",
      "Rows: 4\n",
      "\n",
      "Data:\n",
      "+--------------+-----------------+-------+\n",
      "| target_label | predicted_label | count |\n",
      "+--------------+-----------------+-------+\n",
      "|   negative   |     negative    |   12  |\n",
      "|   negative   |     positive    |   62  |\n",
      "|   positive   |     positive    |  177  |\n",
      "|   positive   |     negative    |   5   |\n",
      "+--------------+-----------------+-------+\n",
      "[4 rows x 3 columns]\n",
      ", 'accuracy': 0.73828125}\n",
      "100000.0\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79948\n",
      "PROGRESS: Number of coefficients    : 79949\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000002  | 0.259174     | 0.992618          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.377254     | 0.992618          |\n",
      "PROGRESS: | 3         | 10       | 1.000000  | 0.501335     | 0.992618          |\n",
      "PROGRESS: | 4         | 12       | 1.000000  | 0.622418     | 0.992618          |\n",
      "PROGRESS: | 5         | 15       | 5.000000  | 0.782525     | 0.992618          |\n",
      "PROGRESS: | 6         | 18       | 5.000000  | 0.953637     | 0.992184          |\n",
      "PROGRESS: | 10        | 23       | 1.000000  | 1.303870     | 0.988276          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79969\n",
      "PROGRESS: Number of coefficients    : 79970\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000002  | 0.248166     | 0.992181          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.363244     | 0.993050          |\n",
      "PROGRESS: | 3         | 10       | 1.000000  | 0.478320     | 0.993050          |\n",
      "PROGRESS: | 4         | 13       | 5.000000  | 0.644432     | 0.993050          |\n",
      "PROGRESS: | 5         | 16       | 5.000000  | 0.796532     | 0.993050          |\n",
      "PROGRESS: | 6         | 17       | 5.000000  | 0.870582     | 0.992615          |\n",
      "PROGRESS: | 10        | 22       | 1.000000  | 1.197800     | 0.990009          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80034\n",
      "PROGRESS: Number of coefficients    : 80035\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000002  | 0.239159     | 0.991746          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.351236     | 0.992181          |\n",
      "PROGRESS: | 3         | 10       | 1.000000  | 0.472315     | 0.992181          |\n",
      "PROGRESS: | 4         | 12       | 1.000000  | 0.584392     | 0.992181          |\n",
      "PROGRESS: | 5         | 15       | 5.000000  | 0.735490     | 0.992181          |\n",
      "PROGRESS: | 6         | 18       | 5.000000  | 0.898599     | 0.992181          |\n",
      "PROGRESS: | 10        | 23       | 1.000000  | 1.253836     | 0.989574          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79911\n",
      "PROGRESS: Number of coefficients    : 79912\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000002  | 0.245167     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.363247     | 0.991312          |\n",
      "PROGRESS: | 3         | 10       | 1.000000  | 0.475317     | 0.991312          |\n",
      "PROGRESS: | 4         | 12       | 1.000000  | 0.589393     | 0.991312          |\n",
      "PROGRESS: | 5         | 14       | 1.000000  | 0.700469     | 0.991312          |\n",
      "PROGRESS: | 6         | 17       | 5.000000  | 0.862577     | 0.991746          |\n",
      "PROGRESS: | 10        | 24       | 1.000000  | 1.278854     | 0.989574          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79950\n",
      "PROGRESS: Number of coefficients    : 79951\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000002  | 0.258172     | 0.990877          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.374250     | 0.991312          |\n",
      "PROGRESS: | 3         | 10       | 1.000000  | 0.486325     | 0.991312          |\n",
      "PROGRESS: | 4         | 13       | 5.000000  | 0.644430     | 0.991312          |\n",
      "PROGRESS: | 5         | 16       | 5.000000  | 0.805537     | 0.991312          |\n",
      "PROGRESS: | 6         | 17       | 5.000000  | 0.880589     | 0.990877          |\n",
      "PROGRESS: | 10        | 24       | 5.000000  | 1.335890     | 0.978714          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80098\n",
      "PROGRESS: Number of coefficients    : 80099\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000002  | 0.248165     | 0.992181          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.360239     | 0.992181          |\n",
      "PROGRESS: | 3         | 10       | 1.000000  | 0.479320     | 0.992181          |\n",
      "PROGRESS: | 4         | 13       | 5.000000  | 0.632422     | 0.992181          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 5         | 18       | 25.000000 | 0.863577     | 0.992181          |\n",
      "PROGRESS: | 6         | 20       | 1.000000  | 0.979653     | 0.988705          |\n",
      "PROGRESS: | 10        | 30       | 0.047937  | 1.545032     | 0.989574          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79658\n",
      "PROGRESS: Number of coefficients    : 79659\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000002  | 0.248166     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.365244     | 0.991746          |\n",
      "PROGRESS: | 3         | 10       | 1.000000  | 0.477320     | 0.991746          |\n",
      "PROGRESS: | 4         | 12       | 1.000000  | 0.589394     | 0.991746          |\n",
      "PROGRESS: | 5         | 15       | 5.000000  | 0.742495     | 0.992181          |\n",
      "PROGRESS: | 6         | 18       | 5.000000  | 0.901602     | 0.991746          |\n",
      "PROGRESS: | 10        | 23       | 1.000000  | 1.238826     | 0.989140          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80441\n",
      "PROGRESS: Number of coefficients    : 80442\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000002  | 0.256171     | 0.990443          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.381256     | 0.990877          |\n",
      "PROGRESS: | 3         | 10       | 1.000000  | 0.505337     | 0.990877          |\n",
      "PROGRESS: | 4         | 13       | 5.000000  | 0.662443     | 0.990877          |\n",
      "PROGRESS: | 5         | 16       | 5.000000  | 0.825551     | 0.990877          |\n",
      "PROGRESS: | 6         | 17       | 5.000000  | 0.911609     | 0.990877          |\n",
      "PROGRESS: | 10        | 22       | 1.000000  | 1.283856     | 0.990009          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79909\n",
      "PROGRESS: Number of coefficients    : 79910\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000002  | 0.260176     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.385258     | 0.991746          |\n",
      "PROGRESS: | 3         | 10       | 1.000000  | 0.504337     | 0.991746          |\n",
      "PROGRESS: | 4         | 13       | 5.000000  | 0.660441     | 0.991746          |\n",
      "PROGRESS: | 5         | 17       | 21.000000 | 0.849567     | 0.991746          |\n",
      "PROGRESS: | 6         | 19       | 1.000000  | 0.975651     | 0.991312          |\n",
      "PROGRESS: | 10        | 24       | 1.000000  | 1.327887     | 0.990009          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79835\n",
      "PROGRESS: Number of coefficients    : 79836\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000002  | 0.243162     | 0.992618          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.357239     | 0.992618          |\n",
      "PROGRESS: | 3         | 10       | 1.000000  | 0.467312     | 0.992618          |\n",
      "PROGRESS: | 4         | 13       | 5.000000  | 0.617413     | 0.993053          |\n",
      "PROGRESS: | 5         | 16       | 5.000000  | 0.775518     | 0.992618          |\n",
      "PROGRESS: | 6         | 17       | 5.000000  | 0.844564     | 0.992184          |\n",
      "PROGRESS: | 10        | 23       | 1.000000  | 1.232823     | 0.990447          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "{'f1_score': 0.8448687350835321, 'auc': 0.8113676863676865, 'recall': 0.9725274725274725, 'precision': 0.7468354430379747, 'log_loss': 0.6862583196576263, 'roc_curve': Columns:\n",
      "\tthreshold\tfloat\n",
      "\tfpr\tfloat\n",
      "\ttpr\tfloat\n",
      "\tp\tint\n",
      "\tn\tint\n",
      "\n",
      "Rows: 100001\n",
      "\n",
      "Data:\n",
      "+-----------+-----+-----+-----+----+\n",
      "| threshold | fpr | tpr |  p  | n  |\n",
      "+-----------+-----+-----+-----+----+\n",
      "|    0.0    | 1.0 | 1.0 | 182 | 74 |\n",
      "|   1e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   2e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   3e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   4e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   5e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   6e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   7e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   8e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   9e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "+-----------+-----+-----+-----+----+\n",
      "[100001 rows x 5 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns., 'confusion_matrix': Columns:\n",
      "\ttarget_label\tstr\n",
      "\tpredicted_label\tstr\n",
      "\tcount\tint\n",
      "\n",
      "Rows: 4\n",
      "\n",
      "Data:\n",
      "+--------------+-----------------+-------+\n",
      "| target_label | predicted_label | count |\n",
      "+--------------+-----------------+-------+\n",
      "|   negative   |     negative    |   14  |\n",
      "|   negative   |     positive    |   60  |\n",
      "|   positive   |     positive    |  177  |\n",
      "|   positive   |     negative    |   5   |\n",
      "+--------------+-----------------+-------+\n",
      "[4 rows x 3 columns]\n",
      ", 'accuracy': 0.74609375}\n",
      "316227.766017\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79948\n",
      "PROGRESS: Number of coefficients    : 79949\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.259174     | 0.992618          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.373248     | 0.992618          |\n",
      "PROGRESS: | 3         | 11       | 5.000000  | 0.530352     | 0.992618          |\n",
      "PROGRESS: | 4         | 14       | 5.000000  | 0.695463     | 0.992618          |\n",
      "PROGRESS: | 5         | 18       | 21.000000 | 0.906606     | 0.992184          |\n",
      "PROGRESS: | 6         | 19       | 21.000000 | 0.980657     | 0.832827          |\n",
      "PROGRESS: | 10        | 25       | 1.000000  | 1.377919     | 0.988710          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79969\n",
      "PROGRESS: Number of coefficients    : 79970\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.250166     | 0.992181          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.365244     | 0.992181          |\n",
      "PROGRESS: | 3         | 11       | 5.000000  | 0.512344     | 0.992181          |\n",
      "PROGRESS: | 4         | 14       | 5.000000  | 0.664444     | 0.992615          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 5         | 19       | 25.000000 | 0.898600     | 0.992181          |\n",
      "PROGRESS: | 6         | 22       | 0.500000  | 1.055703     | 0.990009          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80034\n",
      "PROGRESS: Number of coefficients    : 80035\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.263177     | 0.991746          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.377253     | 0.992181          |\n",
      "PROGRESS: | 3         | 11       | 5.000000  | 0.529354     | 0.992181          |\n",
      "PROGRESS: | 4         | 14       | 5.000000  | 0.684458     | 0.992181          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 5         | 19       | 25.000000 | 0.908607     | 0.990877          |\n",
      "PROGRESS: | 6         | 20       | 25.000000 | 0.979653     | 0.723719          |\n",
      "PROGRESS: | 10        | 26       | 1.000000  | 1.374921     | 0.989140          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79911\n",
      "PROGRESS: Number of coefficients    : 79912\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.251168     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.369247     | 0.991312          |\n",
      "PROGRESS: | 3         | 11       | 5.000000  | 0.522352     | 0.991312          |\n",
      "PROGRESS: | 4         | 14       | 5.000000  | 0.688459     | 0.991312          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 5         | 19       | 25.000000 | 0.942630     | 0.990877          |\n",
      "PROGRESS: | 6         | 20       | 25.000000 | 1.018681     | 0.722415          |\n",
      "PROGRESS: | 10        | 25       | 1.000000  | 1.372917     | 0.988705          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79950\n",
      "PROGRESS: Number of coefficients    : 79951\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.238158     | 0.990877          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.357239     | 0.991312          |\n",
      "PROGRESS: | 3         | 11       | 5.000000  | 0.511342     | 0.991312          |\n",
      "PROGRESS: | 4         | 14       | 5.000000  | 0.658442     | 0.991312          |\n",
      "PROGRESS: | 5         | 17       | 5.000000  | 0.818546     | 0.990877          |\n",
      "PROGRESS: | 6         | 19       | 1.000000  | 0.938626     | 0.990877          |\n",
      "PROGRESS: | 10        | 23       | 1.000000  | 1.238826     | 0.988271          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80098\n",
      "PROGRESS: Number of coefficients    : 80099\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.251169     | 0.992181          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.367246     | 0.992181          |\n",
      "PROGRESS: | 3         | 11       | 5.000000  | 0.521350     | 0.992181          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 4         | 16       | 25.000000 | 0.769514     | 0.992181          |\n",
      "PROGRESS: | 5         | 17       | 25.000000 | 0.841562     | 0.908775          |\n",
      "PROGRESS: | 6         | 18       | 25.000000 | 0.913610     | 0.289314          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79658\n",
      "PROGRESS: Number of coefficients    : 79659\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.251168     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.367246     | 0.991312          |\n",
      "PROGRESS: | 3         | 12       | 21.000000 | 0.569382     | 0.991312          |\n",
      "PROGRESS: | 4         | 16       | 21.000000 | 0.768513     | 0.991746          |\n",
      "PROGRESS: | 5         | 19       | 5.000000  | 0.931622     | 0.990443          |\n",
      "PROGRESS: | 6         | 20       | 5.000000  | 1.010674     | 0.955256          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80441\n",
      "PROGRESS: Number of coefficients    : 80442\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.255171     | 0.990443          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.374252     | 0.990443          |\n",
      "PROGRESS: | 3         | 11       | 5.000000  | 0.527354     | 0.990443          |\n",
      "PROGRESS: | 4         | 14       | 5.000000  | 0.695463     | 0.990443          |\n",
      "PROGRESS: | 5         | 18       | 21.000000 | 0.887593     | 0.991312          |\n",
      "PROGRESS: | 6         | 20       | 1.000000  | 1.007672     | 0.990009          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79909\n",
      "PROGRESS: Number of coefficients    : 79910\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.257172     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.372248     | 0.991312          |\n",
      "PROGRESS: | 3         | 11       | 5.000000  | 0.530354     | 0.991312          |\n",
      "PROGRESS: | 4         | 15       | 21.000000 | 0.719479     | 0.991312          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.875584     | 0.991312          |\n",
      "PROGRESS: | 6         | 19       | 5.000000  | 0.947633     | 0.986968          |\n",
      "PROGRESS: | 10        | 25       | 1.000000  | 1.326884     | 0.990009          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79835\n",
      "PROGRESS: Number of coefficients    : 79836\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.261175     | 0.992618          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.380254     | 0.992618          |\n",
      "PROGRESS: | 3         | 11       | 5.000000  | 0.533355     | 0.992618          |\n",
      "PROGRESS: | 4         | 14       | 5.000000  | 0.683454     | 0.992618          |\n",
      "PROGRESS: | 5         | 18       | 21.000000 | 0.871580     | 0.991316          |\n",
      "PROGRESS: | 6         | 20       | 1.000000  | 0.980653     | 0.990881          |\n",
      "PROGRESS: | 10        | 24       | 1.000000  | 1.284857     | 0.987842          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "{'f1_score': 0.8388625592417062, 'auc': 0.806912681912682, 'recall': 0.9725274725274725, 'precision': 0.7375, 'log_loss': 0.6907600774354615, 'roc_curve': Columns:\n",
      "\tthreshold\tfloat\n",
      "\tfpr\tfloat\n",
      "\ttpr\tfloat\n",
      "\tp\tint\n",
      "\tn\tint\n",
      "\n",
      "Rows: 100001\n",
      "\n",
      "Data:\n",
      "+-----------+-----+-----+-----+----+\n",
      "| threshold | fpr | tpr |  p  | n  |\n",
      "+-----------+-----+-----+-----+----+\n",
      "|    0.0    | 1.0 | 1.0 | 182 | 74 |\n",
      "|   1e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   2e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   3e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   4e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   5e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   6e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   7e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   8e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   9e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "+-----------+-----+-----+-----+----+\n",
      "[100001 rows x 5 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns., 'confusion_matrix': Columns:\n",
      "\ttarget_label\tstr\n",
      "\tpredicted_label\tstr\n",
      "\tcount\tint\n",
      "\n",
      "Rows: 4\n",
      "\n",
      "Data:\n",
      "+--------------+-----------------+-------+\n",
      "| target_label | predicted_label | count |\n",
      "+--------------+-----------------+-------+\n",
      "|   negative   |     negative    |   11  |\n",
      "|   negative   |     positive    |   63  |\n",
      "|   positive   |     positive    |  177  |\n",
      "|   positive   |     negative    |   5   |\n",
      "+--------------+-----------------+-------+\n",
      "[4 rows x 3 columns]\n",
      ", 'accuracy': 0.734375}\n",
      "1000000.0\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79948\n",
      "PROGRESS: Number of coefficients    : 79949\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.251168     | 0.992618          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.367248     | 0.992618          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.598400     | 0.992618          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.711476     | 0.992618          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.870583     | 0.992184          |\n",
      "PROGRESS: | 6         | 21       | 5.000000  | 1.027688     | 0.989579          |\n",
      "PROGRESS: | 10        | 26       | 1.000000  | 1.382925     | 0.988710          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79969\n",
      "PROGRESS: Number of coefficients    : 79970\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.253169     | 0.992181          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.370247     | 0.992181          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.611409     | 0.992181          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.725484     | 0.992181          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.881589     | 0.992615          |\n",
      "PROGRESS: | 6         | 21       | 5.000000  | 1.040694     | 0.990877          |\n",
      "PROGRESS: | 10        | 26       | 1.000000  | 1.379920     | 0.989574          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80034\n",
      "PROGRESS: Number of coefficients    : 80035\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.247164     | 0.991746          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.362242     | 0.991746          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.590394     | 0.991746          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.701468     | 0.991746          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.853568     | 0.991746          |\n",
      "PROGRESS: | 6         | 21       | 5.000000  | 1.003670     | 0.989574          |\n",
      "PROGRESS: | 10        | 26       | 1.000000  | 1.343897     | 0.989140          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79911\n",
      "PROGRESS: Number of coefficients    : 79912\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.254171     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.379255     | 0.991312          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.618415     | 0.991312          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.736492     | 0.991312          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.892597     | 0.991312          |\n",
      "PROGRESS: | 6         | 21       | 5.000000  | 1.049703     | 0.989140          |\n",
      "PROGRESS: | 10        | 26       | 1.000000  | 1.391931     | 0.988705          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79950\n",
      "PROGRESS: Number of coefficients    : 79951\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.248165     | 0.990877          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.373251     | 0.991312          |\n",
      "PROGRESS: | 3         | 12       | 21.000000 | 0.562375     | 0.991312          |\n",
      "PROGRESS: | 4         | 14       | 1.000000  | 0.673450     | 0.990877          |\n",
      "PROGRESS: | 5         | 17       | 5.000000  | 0.826554     | 0.990443          |\n",
      "PROGRESS: | 6         | 20       | 5.000000  | 0.979655     | 0.989574          |\n",
      "PROGRESS: | 10        | 25       | 1.000000  | 1.333891     | 0.988271          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80098\n",
      "PROGRESS: Number of coefficients    : 80099\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.252170     | 0.992181          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.361244     | 0.992181          |\n",
      "PROGRESS: | 3         | 12       | 21.000000 | 0.557374     | 0.992181          |\n",
      "PROGRESS: | 4         | 15       | 5.000000  | 0.724484     | 0.992181          |\n",
      "PROGRESS: | 5         | 19       | 21.000000 | 0.923619     | 0.990877          |\n",
      "PROGRESS: | 6         | 21       | 1.000000  | 1.037692     | 0.988705          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79658\n",
      "PROGRESS: Number of coefficients    : 79659\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.244163     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.356237     | 0.991312          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.596397     | 0.991312          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.709474     | 0.991312          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.858574     | 0.990877          |\n",
      "PROGRESS: | 6         | 20       | 1.000000  | 0.974651     | 0.990877          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80441\n",
      "PROGRESS: Number of coefficients    : 80442\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.242162     | 0.990443          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.356237     | 0.990443          |\n",
      "PROGRESS: | 3         | 12       | 21.000000 | 0.561373     | 0.990443          |\n",
      "PROGRESS: | 4         | 14       | 1.000000  | 0.677450     | 0.990443          |\n",
      "PROGRESS: | 5         | 17       | 5.000000  | 0.836557     | 0.990877          |\n",
      "PROGRESS: | 6         | 20       | 5.000000  | 0.987658     | 0.990009          |\n",
      "PROGRESS: | 10        | 25       | 1.000000  | 1.325882     | 0.989140          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79909\n",
      "PROGRESS: Number of coefficients    : 79910\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.244163     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.356238     | 0.991312          |\n",
      "PROGRESS: | 3         | 12       | 21.000000 | 0.551367     | 0.991312          |\n",
      "PROGRESS: | 4         | 15       | 5.000000  | 0.701470     | 0.991312          |\n",
      "PROGRESS: | 5         | 19       | 21.000000 | 0.891593     | 0.990877          |\n",
      "PROGRESS: | 6         | 20       | 21.000000 | 0.966645     | 0.908341          |\n",
      "PROGRESS: | 10        | 32       | 0.250000  | 1.589061     | 0.989574          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79835\n",
      "PROGRESS: Number of coefficients    : 79836\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.240161     | 0.992618          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.358241     | 0.992618          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.588393     | 0.992618          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.702470     | 0.992618          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.856573     | 0.991750          |\n",
      "PROGRESS: | 6         | 21       | 5.000000  | 1.008674     | 0.991316          |\n",
      "PROGRESS: | 10        | 27       | 1.000000  | 1.383923     | 0.989579          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "{'f1_score': 0.8448687350835321, 'auc': 0.8088803088803088, 'recall': 0.9725274725274725, 'precision': 0.7468354430379747, 'log_loss': 0.6924318308870876, 'roc_curve': Columns:\n",
      "\tthreshold\tfloat\n",
      "\tfpr\tfloat\n",
      "\ttpr\tfloat\n",
      "\tp\tint\n",
      "\tn\tint\n",
      "\n",
      "Rows: 100001\n",
      "\n",
      "Data:\n",
      "+-----------+-----+-----+-----+----+\n",
      "| threshold | fpr | tpr |  p  | n  |\n",
      "+-----------+-----+-----+-----+----+\n",
      "|    0.0    | 1.0 | 1.0 | 182 | 74 |\n",
      "|   1e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   2e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   3e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   4e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   5e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   6e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   7e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   8e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   9e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "+-----------+-----+-----+-----+----+\n",
      "[100001 rows x 5 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns., 'confusion_matrix': Columns:\n",
      "\ttarget_label\tstr\n",
      "\tpredicted_label\tstr\n",
      "\tcount\tint\n",
      "\n",
      "Rows: 4\n",
      "\n",
      "Data:\n",
      "+--------------+-----------------+-------+\n",
      "| target_label | predicted_label | count |\n",
      "+--------------+-----------------+-------+\n",
      "|   negative   |     negative    |   14  |\n",
      "|   negative   |     positive    |   60  |\n",
      "|   positive   |     positive    |  177  |\n",
      "|   positive   |     negative    |   5   |\n",
      "+--------------+-----------------+-------+\n",
      "[4 rows x 3 columns]\n",
      ", 'accuracy': 0.74609375}\n",
      "3162277.66017\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79948\n",
      "PROGRESS: Number of coefficients    : 79949\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.249165     | 0.992618          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.367247     | 0.992618          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.600400     | 0.992618          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.714477     | 0.992618          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.865577     | 0.991316          |\n",
      "PROGRESS: | 6         | 20       | 1.000000  | 0.980653     | 0.990013          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79969\n",
      "PROGRESS: Number of coefficients    : 79970\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.244165     | 0.992181          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.357239     | 0.992181          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.596398     | 0.992181          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.710474     | 0.992181          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.863577     | 0.992181          |\n",
      "PROGRESS: | 6         | 20       | 1.000000  | 0.977655     | 0.991312          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80034\n",
      "PROGRESS: Number of coefficients    : 80035\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.248166     | 0.991746          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.363243     | 0.991746          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.594400     | 0.991746          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.706473     | 0.991746          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.856573     | 0.990443          |\n",
      "PROGRESS: | 6         | 20       | 1.000000  | 0.968647     | 0.989574          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79911\n",
      "PROGRESS: Number of coefficients    : 79912\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.260173     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.370250     | 0.991312          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.604404     | 0.991312          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.730488     | 0.991312          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.890596     | 0.990877          |\n",
      "PROGRESS: | 6         | 20       | 1.000000  | 1.005670     | 0.989574          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79950\n",
      "PROGRESS: Number of coefficients    : 79951\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.248166     | 0.990877          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.365243     | 0.990877          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.596398     | 0.990877          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.711475     | 0.990877          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.862576     | 0.990443          |\n",
      "PROGRESS: | 6         | 20       | 1.000000  | 0.981655     | 0.989574          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80098\n",
      "PROGRESS: Number of coefficients    : 80099\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.245165     | 0.992181          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.359239     | 0.992181          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.583390     | 0.992181          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.694462     | 0.992181          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.844563     | 0.992181          |\n",
      "PROGRESS: | 6         | 21       | 5.000000  | 1.002669     | 0.989140          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79658\n",
      "PROGRESS: Number of coefficients    : 79659\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.251170     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.371248     | 0.991312          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.602403     | 0.991312          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.722482     | 0.991312          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.872584     | 0.988705          |\n",
      "PROGRESS: | 6         | 21       | 0.500000  | 1.022683     | 0.988705          |\n",
      "PROGRESS: | 10        | 25       | 0.500000  | 1.323884     | 0.989140          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80441\n",
      "PROGRESS: Number of coefficients    : 80442\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.249169     | 0.990443          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.360241     | 0.990443          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.587392     | 0.990443          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.695464     | 0.990443          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.849567     | 0.990009          |\n",
      "PROGRESS: | 6         | 20       | 1.000000  | 0.968647     | 0.990443          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79909\n",
      "PROGRESS: Number of coefficients    : 79910\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.252170     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.368247     | 0.991312          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.601401     | 0.991312          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.712476     | 0.991312          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.866579     | 0.991312          |\n",
      "PROGRESS: | 6         | 21       | 5.000000  | 1.018679     | 0.990009          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79835\n",
      "PROGRESS: Number of coefficients    : 79836\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.245163     | 0.992618          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.359238     | 0.992618          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.598399     | 0.992618          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.708473     | 0.992618          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.858571     | 0.992184          |\n",
      "PROGRESS: | 6         | 20       | 1.000000  | 0.968645     | 0.991316          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "{'f1_score': 0.8448687350835321, 'auc': 0.8092886842886841, 'recall': 0.9725274725274725, 'precision': 0.7468354430379747, 'log_loss': 0.6929197259340437, 'roc_curve': Columns:\n",
      "\tthreshold\tfloat\n",
      "\tfpr\tfloat\n",
      "\ttpr\tfloat\n",
      "\tp\tint\n",
      "\tn\tint\n",
      "\n",
      "Rows: 100001\n",
      "\n",
      "Data:\n",
      "+-----------+-----+-----+-----+----+\n",
      "| threshold | fpr | tpr |  p  | n  |\n",
      "+-----------+-----+-----+-----+----+\n",
      "|    0.0    | 1.0 | 1.0 | 182 | 74 |\n",
      "|   1e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   2e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   3e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   4e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   5e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   6e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   7e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   8e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   9e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "+-----------+-----+-----+-----+----+\n",
      "[100001 rows x 5 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns., 'confusion_matrix': Columns:\n",
      "\ttarget_label\tstr\n",
      "\tpredicted_label\tstr\n",
      "\tcount\tint\n",
      "\n",
      "Rows: 4\n",
      "\n",
      "Data:\n",
      "+--------------+-----------------+-------+\n",
      "| target_label | predicted_label | count |\n",
      "+--------------+-----------------+-------+\n",
      "|   negative   |     negative    |   14  |\n",
      "|   negative   |     positive    |   60  |\n",
      "|   positive   |     positive    |  177  |\n",
      "|   positive   |     negative    |   5   |\n",
      "+--------------+-----------------+-------+\n",
      "[4 rows x 3 columns]\n",
      ", 'accuracy': 0.74609375}\n",
      "10000000.0\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79948\n",
      "PROGRESS: Number of coefficients    : 79949\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.264176     | 0.992618          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.380255     | 0.992618          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.621413     | 0.992618          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.733488     | 0.992184          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.886592     | 0.988710          |\n",
      "PROGRESS: | 6         | 21       | 0.500000  | 1.037692     | 0.988710          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79969\n",
      "PROGRESS: Number of coefficients    : 79970\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.238158     | 0.992181          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.350234     | 0.992181          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.589393     | 0.992181          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.707473     | 0.992181          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.861575     | 0.990009          |\n",
      "PROGRESS: | 6         | 21       | 0.500000  | 1.010674     | 0.989140          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80034\n",
      "PROGRESS: Number of coefficients    : 80035\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.244163     | 0.991746          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.362243     | 0.991746          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.597399     | 0.991746          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.710476     | 0.992181          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.863578     | 0.989574          |\n",
      "PROGRESS: | 6         | 21       | 0.500000  | 1.018680     | 0.988705          |\n",
      "PROGRESS: | 10        | 27       | 1.000000  | 1.391928     | 0.989140          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79911\n",
      "PROGRESS: Number of coefficients    : 79912\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.247164     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.364243     | 0.991312          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.602400     | 0.991312          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.716477     | 0.991312          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.868577     | 0.989574          |\n",
      "PROGRESS: | 6         | 21       | 0.500000  | 1.021681     | 0.988705          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79950\n",
      "PROGRESS: Number of coefficients    : 79951\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.244163     | 0.990877          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.361242     | 0.990877          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.598400     | 0.990877          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.716479     | 0.990443          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.873583     | 0.988705          |\n",
      "PROGRESS: | 6         | 20       | 1.000000  | 0.987658     | 0.987837          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80098\n",
      "PROGRESS: Number of coefficients    : 80099\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.251168     | 0.992181          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.367245     | 0.992181          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.609405     | 0.992181          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.722483     | 0.992181          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.873582     | 0.990443          |\n",
      "PROGRESS: | 6         | 20       | 1.000000  | 0.986658     | 0.989574          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79658\n",
      "PROGRESS: Number of coefficients    : 79659\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.251166     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.363241     | 0.991312          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.605402     | 0.991312          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.729486     | 0.990877          |\n",
      "PROGRESS: | 5         | 17       | 1.000000  | 0.846562     | 0.990009          |\n",
      "PROGRESS: | 6         | 18       | 1.000000  | 0.918612     | 0.719374          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80441\n",
      "PROGRESS: Number of coefficients    : 80442\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.252168     | 0.990443          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.363242     | 0.990443          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.596398     | 0.990443          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.710475     | 0.990443          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.865577     | 0.989574          |\n",
      "PROGRESS: | 6         | 20       | 1.000000  | 0.979652     | 0.988271          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79909\n",
      "PROGRESS: Number of coefficients    : 79910\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.238157     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.360240     | 0.991312          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.589391     | 0.991312          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.706470     | 0.991312          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.855568     | 0.990009          |\n",
      "PROGRESS: | 6         | 20       | 1.000000  | 0.978650     | 0.989574          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79835\n",
      "PROGRESS: Number of coefficients    : 79836\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.258173     | 0.992618          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.367246     | 0.992618          |\n",
      "PROGRESS: Warning: Reached max step size.\n",
      "PROGRESS: | 3         | 13       | 25.000000 | 0.604403     | 0.992618          |\n",
      "PROGRESS: | 4         | 15       | 1.000000  | 0.721482     | 0.992184          |\n",
      "PROGRESS: | 5         | 18       | 5.000000  | 0.880588     | 0.990881          |\n",
      "PROGRESS: | 6         | 20       | 1.000000  | 0.995665     | 0.987842          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "{'f1_score': 0.8448687350835321, 'auc': 0.806875556875557, 'recall': 0.9725274725274725, 'precision': 0.7468354430379747, 'log_loss': 0.693074778753319, 'roc_curve': Columns:\n",
      "\tthreshold\tfloat\n",
      "\tfpr\tfloat\n",
      "\ttpr\tfloat\n",
      "\tp\tint\n",
      "\tn\tint\n",
      "\n",
      "Rows: 100001\n",
      "\n",
      "Data:\n",
      "+-----------+-----+-----+-----+----+\n",
      "| threshold | fpr | tpr |  p  | n  |\n",
      "+-----------+-----+-----+-----+----+\n",
      "|    0.0    | 1.0 | 1.0 | 182 | 74 |\n",
      "|   1e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   2e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   3e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   4e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   5e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   6e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   7e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   8e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   9e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "+-----------+-----+-----+-----+----+\n",
      "[100001 rows x 5 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns., 'confusion_matrix': Columns:\n",
      "\ttarget_label\tstr\n",
      "\tpredicted_label\tstr\n",
      "\tcount\tint\n",
      "\n",
      "Rows: 4\n",
      "\n",
      "Data:\n",
      "+--------------+-----------------+-------+\n",
      "| target_label | predicted_label | count |\n",
      "+--------------+-----------------+-------+\n",
      "|   negative   |     negative    |   14  |\n",
      "|   negative   |     positive    |   60  |\n",
      "|   positive   |     positive    |  177  |\n",
      "|   positive   |     negative    |   5   |\n",
      "+--------------+-----------------+-------+\n",
      "[4 rows x 3 columns]\n",
      ", 'accuracy': 0.74609375}\n",
      "31622776.6017\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79948\n",
      "PROGRESS: Number of coefficients    : 79949\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.246164     | 0.992618          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.360241     | 0.992618          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79969\n",
      "PROGRESS: Number of coefficients    : 79970\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.248166     | 0.992181          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.356239     | 0.992181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80034\n",
      "PROGRESS: Number of coefficients    : 80035\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.252169     | 0.991746          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.370250     | 0.991746          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79911\n",
      "PROGRESS: Number of coefficients    : 79912\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.246164     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.370246     | 0.991312          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79950\n",
      "PROGRESS: Number of coefficients    : 79951\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.264174     | 0.990877          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.395263     | 0.990877          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80098\n",
      "PROGRESS: Number of coefficients    : 80099\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.261175     | 0.992181          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.390260     | 0.992181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79658\n",
      "PROGRESS: Number of coefficients    : 79659\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.274184     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.399267     | 0.991312          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80441\n",
      "PROGRESS: Number of coefficients    : 80442\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.269172     | 0.990443          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.393254     | 0.990443          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79909\n",
      "PROGRESS: Number of coefficients    : 79910\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.252168     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.372250     | 0.991312          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79835\n",
      "PROGRESS: Number of coefficients    : 79836\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.273183     | 0.992618          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.391262     | 0.992618          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "{'f1_score': 0.8542199488491048, 'auc': 0.803088803088803, 'recall': 0.9175824175824175, 'precision': 0.7990430622009569, 'log_loss': 0.6931287724002287, 'roc_curve': Columns:\n",
      "\tthreshold\tfloat\n",
      "\tfpr\tfloat\n",
      "\ttpr\tfloat\n",
      "\tp\tint\n",
      "\tn\tint\n",
      "\n",
      "Rows: 100001\n",
      "\n",
      "Data:\n",
      "+-----------+-----+-----+-----+----+\n",
      "| threshold | fpr | tpr |  p  | n  |\n",
      "+-----------+-----+-----+-----+----+\n",
      "|    0.0    | 1.0 | 1.0 | 182 | 74 |\n",
      "|   1e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   2e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   3e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   4e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   5e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   6e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   7e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   8e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   9e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "+-----------+-----+-----+-----+----+\n",
      "[100001 rows x 5 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns., 'confusion_matrix': Columns:\n",
      "\ttarget_label\tstr\n",
      "\tpredicted_label\tstr\n",
      "\tcount\tint\n",
      "\n",
      "Rows: 4\n",
      "\n",
      "Data:\n",
      "+--------------+-----------------+-------+\n",
      "| target_label | predicted_label | count |\n",
      "+--------------+-----------------+-------+\n",
      "|   negative   |     negative    |   32  |\n",
      "|   negative   |     positive    |   42  |\n",
      "|   positive   |     negative    |   15  |\n",
      "|   positive   |     positive    |  167  |\n",
      "+--------------+-----------------+-------+\n",
      "[4 rows x 3 columns]\n",
      ", 'accuracy': 0.77734375}\n",
      "100000000.0\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79948\n",
      "PROGRESS: Number of coefficients    : 79949\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.266178     | 0.992618          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.376251     | 0.992618          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79969\n",
      "PROGRESS: Number of coefficients    : 79970\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.264175     | 0.992181          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.378249     | 0.992181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80034\n",
      "PROGRESS: Number of coefficients    : 80035\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.262176     | 0.991746          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.374250     | 0.991746          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79911\n",
      "PROGRESS: Number of coefficients    : 79912\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.262173     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.386258     | 0.991312          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79950\n",
      "PROGRESS: Number of coefficients    : 79951\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.256169     | 0.990877          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.374248     | 0.990877          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80098\n",
      "PROGRESS: Number of coefficients    : 80099\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.242165     | 0.992181          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.355237     | 0.992181          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79658\n",
      "PROGRESS: Number of coefficients    : 79659\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.268180     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.384258     | 0.991312          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80441\n",
      "PROGRESS: Number of coefficients    : 80442\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.249159     | 0.990443          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.377244     | 0.990443          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79909\n",
      "PROGRESS: Number of coefficients    : 79910\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.257171     | 0.991312          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.369246     | 0.991312          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79835\n",
      "PROGRESS: Number of coefficients    : 79836\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000000  | 0.261174     | 0.992618          |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.376252     | 0.992618          |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n",
      "{'f1_score': 0.8542199488491048, 'auc': 0.6921591921591922, 'recall': 0.9175824175824175, 'precision': 0.7990430622009569, 'log_loss': 0.6931413591115217, 'roc_curve': Columns:\n",
      "\tthreshold\tfloat\n",
      "\tfpr\tfloat\n",
      "\ttpr\tfloat\n",
      "\tp\tint\n",
      "\tn\tint\n",
      "\n",
      "Rows: 100001\n",
      "\n",
      "Data:\n",
      "+-----------+-----+-----+-----+----+\n",
      "| threshold | fpr | tpr |  p  | n  |\n",
      "+-----------+-----+-----+-----+----+\n",
      "|    0.0    | 1.0 | 1.0 | 182 | 74 |\n",
      "|   1e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   2e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   3e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   4e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   5e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   6e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   7e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   8e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   9e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "+-----------+-----+-----+-----+----+\n",
      "[100001 rows x 5 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns., 'confusion_matrix': Columns:\n",
      "\ttarget_label\tstr\n",
      "\tpredicted_label\tstr\n",
      "\tcount\tint\n",
      "\n",
      "Rows: 4\n",
      "\n",
      "Data:\n",
      "+--------------+-----------------+-------+\n",
      "| target_label | predicted_label | count |\n",
      "+--------------+-----------------+-------+\n",
      "|   negative   |     negative    |   32  |\n",
      "|   negative   |     positive    |   42  |\n",
      "|   positive   |     negative    |   15  |\n",
      "|   positive   |     positive    |  167  |\n",
      "+--------------+-----------------+-------+\n",
      "[4 rows x 3 columns]\n",
      ", 'accuracy': 0.77734375}\n",
      "316227766.017\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79948\n",
      "PROGRESS: Number of coefficients    : 79949\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Warning: Unusual termination criterion reached.\n",
      "Returning the best step found so far. This typically happens when the number of features is much larger than the number of training samples. Consider pruning features manually or increasing the regularization value.\n",
      "PROGRESS: Warning: Rounding errors prevent further progress. \n",
      "There may not be a step which satisfies the sufficient decrease and curvature conditions. \n",
      "Tolerances may be too small or dataset may be poorly scaled. This typically happens when the number of features is much larger than the number of training samples. Consider pruning features manually or increasing the regularization value.\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Terminated due to numerical difficulties in line search.\n",
      "PROGRESS: This model may not be ideal. To improve it, consider doing one of the following:\n",
      "(a) Increasing the regularization.\n",
      "(b) Standardizing the input data.\n",
      "(c) Removing highly correlated features.\n",
      "(d) Removing `inf` and `NaN` values in the training data.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79969\n",
      "PROGRESS: Number of coefficients    : 79970\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Warning: Unusual termination criterion reached.\n",
      "Returning the best step found so far. This typically happens when the number of features is much larger than the number of training samples. Consider pruning features manually or increasing the regularization value.\n",
      "PROGRESS: Warning: Rounding errors prevent further progress. \n",
      "There may not be a step which satisfies the sufficient decrease and curvature conditions. \n",
      "Tolerances may be too small or dataset may be poorly scaled. This typically happens when the number of features is much larger than the number of training samples. Consider pruning features manually or increasing the regularization value.\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Terminated due to numerical difficulties in line search.\n",
      "PROGRESS: This model may not be ideal. To improve it, consider doing one of the following:\n",
      "(a) Increasing the regularization.\n",
      "(b) Standardizing the input data.\n",
      "(c) Removing highly correlated features.\n",
      "(d) Removing `inf` and `NaN` values in the training data.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80034\n",
      "PROGRESS: Number of coefficients    : 80035\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Warning: Unusual termination criterion reached.\n",
      "Returning the best step found so far. This typically happens when the number of features is much larger than the number of training samples. Consider pruning features manually or increasing the regularization value.\n",
      "PROGRESS: Warning: Rounding errors prevent further progress. \n",
      "There may not be a step which satisfies the sufficient decrease and curvature conditions. \n",
      "Tolerances may be too small or dataset may be poorly scaled. This typically happens when the number of features is much larger than the number of training samples. Consider pruning features manually or increasing the regularization value.\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Terminated due to numerical difficulties in line search.\n",
      "PROGRESS: This model may not be ideal. To improve it, consider doing one of the following:\n",
      "(a) Increasing the regularization.\n",
      "(b) Standardizing the input data.\n",
      "(c) Removing highly correlated features.\n",
      "(d) Removing `inf` and `NaN` values in the training data.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79911\n",
      "PROGRESS: Number of coefficients    : 79912\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Warning: Unusual termination criterion reached.\n",
      "Returning the best step found so far. This typically happens when the number of features is much larger than the number of training samples. Consider pruning features manually or increasing the regularization value.\n",
      "PROGRESS: Warning: Rounding errors prevent further progress. \n",
      "There may not be a step which satisfies the sufficient decrease and curvature conditions. \n",
      "Tolerances may be too small or dataset may be poorly scaled. This typically happens when the number of features is much larger than the number of training samples. Consider pruning features manually or increasing the regularization value.\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Terminated due to numerical difficulties in line search.\n",
      "PROGRESS: This model may not be ideal. To improve it, consider doing one of the following:\n",
      "(a) Increasing the regularization.\n",
      "(b) Standardizing the input data.\n",
      "(c) Removing highly correlated features.\n",
      "(d) Removing `inf` and `NaN` values in the training data.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79950\n",
      "PROGRESS: Number of coefficients    : 79951\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Warning: Unusual termination criterion reached.\n",
      "Returning the best step found so far. This typically happens when the number of features is much larger than the number of training samples. Consider pruning features manually or increasing the regularization value.\n",
      "PROGRESS: Warning: Rounding errors prevent further progress. \n",
      "There may not be a step which satisfies the sufficient decrease and curvature conditions. \n",
      "Tolerances may be too small or dataset may be poorly scaled. This typically happens when the number of features is much larger than the number of training samples. Consider pruning features manually or increasing the regularization value.\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Terminated due to numerical difficulties in line search.\n",
      "PROGRESS: This model may not be ideal. To improve it, consider doing one of the following:\n",
      "(a) Increasing the regularization.\n",
      "(b) Standardizing the input data.\n",
      "(c) Removing highly correlated features.\n",
      "(d) Removing `inf` and `NaN` values in the training data.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80098\n",
      "PROGRESS: Number of coefficients    : 80099\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Warning: Unusual termination criterion reached.\n",
      "Returning the best step found so far. This typically happens when the number of features is much larger than the number of training samples. Consider pruning features manually or increasing the regularization value.\n",
      "PROGRESS: Warning: Rounding errors prevent further progress. \n",
      "There may not be a step which satisfies the sufficient decrease and curvature conditions. \n",
      "Tolerances may be too small or dataset may be poorly scaled. This typically happens when the number of features is much larger than the number of training samples. Consider pruning features manually or increasing the regularization value.\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Terminated due to numerical difficulties in line search.\n",
      "PROGRESS: This model may not be ideal. To improve it, consider doing one of the following:\n",
      "(a) Increasing the regularization.\n",
      "(b) Standardizing the input data.\n",
      "(c) Removing highly correlated features.\n",
      "(d) Removing `inf` and `NaN` values in the training data.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79658\n",
      "PROGRESS: Number of coefficients    : 79659\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Warning: Unusual termination criterion reached.\n",
      "Returning the best step found so far. This typically happens when the number of features is much larger than the number of training samples. Consider pruning features manually or increasing the regularization value.\n",
      "PROGRESS: Warning: Rounding errors prevent further progress. \n",
      "There may not be a step which satisfies the sufficient decrease and curvature conditions. \n",
      "Tolerances may be too small or dataset may be poorly scaled. This typically happens when the number of features is much larger than the number of training samples. Consider pruning features manually or increasing the regularization value.\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Terminated due to numerical difficulties in line search.\n",
      "PROGRESS: This model may not be ideal. To improve it, consider doing one of the following:\n",
      "(a) Increasing the regularization.\n",
      "(b) Standardizing the input data.\n",
      "(c) Removing highly correlated features.\n",
      "(d) Removing `inf` and `NaN` values in the training data.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 80441\n",
      "PROGRESS: Number of coefficients    : 80442\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Warning: Unusual termination criterion reached.\n",
      "Returning the best step found so far. This typically happens when the number of features is much larger than the number of training samples. Consider pruning features manually or increasing the regularization value.\n",
      "PROGRESS: Warning: Rounding errors prevent further progress. \n",
      "There may not be a step which satisfies the sufficient decrease and curvature conditions. \n",
      "Tolerances may be too small or dataset may be poorly scaled. This typically happens when the number of features is much larger than the number of training samples. Consider pruning features manually or increasing the regularization value.\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Terminated due to numerical difficulties in line search.\n",
      "PROGRESS: This model may not be ideal. To improve it, consider doing one of the following:\n",
      "(a) Increasing the regularization.\n",
      "(b) Standardizing the input data.\n",
      "(c) Removing highly correlated features.\n",
      "(d) Removing `inf` and `NaN` values in the training data.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2302\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79909\n",
      "PROGRESS: Number of coefficients    : 79910\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Warning: Unusual termination criterion reached.\n",
      "Returning the best step found so far. This typically happens when the number of features is much larger than the number of training samples. Consider pruning features manually or increasing the regularization value.\n",
      "PROGRESS: Warning: Rounding errors prevent further progress. \n",
      "There may not be a step which satisfies the sufficient decrease and curvature conditions. \n",
      "Tolerances may be too small or dataset may be poorly scaled. This typically happens when the number of features is much larger than the number of training samples. Consider pruning features manually or increasing the regularization value.\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Terminated due to numerical difficulties in line search.\n",
      "PROGRESS: This model may not be ideal. To improve it, consider doing one of the following:\n",
      "(a) Increasing the regularization.\n",
      "(b) Standardizing the input data.\n",
      "(c) Removing highly correlated features.\n",
      "(d) Removing `inf` and `NaN` values in the training data.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 2303\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 79835\n",
      "PROGRESS: Number of coefficients    : 79836\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: Warning: Unusual termination criterion reached.\n",
      "Returning the best step found so far. This typically happens when the number of features is much larger than the number of training samples. Consider pruning features manually or increasing the regularization value.\n",
      "PROGRESS: Warning: Rounding errors prevent further progress. \n",
      "There may not be a step which satisfies the sufficient decrease and curvature conditions. \n",
      "Tolerances may be too small or dataset may be poorly scaled. This typically happens when the number of features is much larger than the number of training samples. Consider pruning features manually or increasing the regularization value.\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+\n",
      "PROGRESS: TERMINATED: Terminated due to numerical difficulties in line search.\n",
      "PROGRESS: This model may not be ideal. To improve it, consider doing one of the following:\n",
      "(a) Increasing the regularization.\n",
      "(b) Standardizing the input data.\n",
      "(c) Removing highly correlated features.\n",
      "(d) Removing `inf` and `NaN` values in the training data.\n",
      "{'f1_score': 0.8310502283105022, 'auc': 0.5, 'recall': 1.0, 'precision': 0.7109375, 'log_loss': 0.6931471805599447, 'roc_curve': Columns:\n",
      "\tthreshold\tfloat\n",
      "\tfpr\tfloat\n",
      "\ttpr\tfloat\n",
      "\tp\tint\n",
      "\tn\tint\n",
      "\n",
      "Rows: 100001\n",
      "\n",
      "Data:\n",
      "+-----------+-----+-----+-----+----+\n",
      "| threshold | fpr | tpr |  p  | n  |\n",
      "+-----------+-----+-----+-----+----+\n",
      "|    0.0    | 1.0 | 1.0 | 182 | 74 |\n",
      "|   1e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   2e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   3e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   4e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   5e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   6e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   7e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   8e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "|   9e-05   | 1.0 | 1.0 | 182 | 74 |\n",
      "+-----------+-----+-----+-----+----+\n",
      "[100001 rows x 5 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns., 'confusion_matrix': Columns:\n",
      "\ttarget_label\tstr\n",
      "\tpredicted_label\tstr\n",
      "\tcount\tint\n",
      "\n",
      "Rows: 2\n",
      "\n",
      "Data:\n",
      "+--------------+-----------------+-------+\n",
      "| target_label | predicted_label | count |\n",
      "+--------------+-----------------+-------+\n",
      "|   negative   |     positive    |   74  |\n",
      "|   positive   |     positive    |  182  |\n",
      "+--------------+-----------------+-------+\n",
      "[2 rows x 3 columns]\n",
      ", 'accuracy': 0.7109375}\n",
      "1000000000.0\n"
     ]
    }
   ],
   "source": [
    "for i in np.logspace(3, 9, num=13):\n",
    "    RSS = k_fold_cross_validation(10,i,train_data_pos_neg, 'Sentiment')\n",
    "    print RSS\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and tuning the classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start with the p/n classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graphlab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-84d9cb7ed713>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraphlab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'full_train_2016.tsv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtweets_doc2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraphlab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'full_train_2016doc2vec.tsv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'1gram features'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_analytics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'2gram features'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_analytics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'graphlab' is not defined"
     ]
    }
   ],
   "source": [
    "tweets = graphlab.SFrame('full_train_2016.tsv')\n",
    "tweets_doc2vec = graphlab.SFrame('full_train_2016doc2vec.tsv')\n",
    "\n",
    "tweets['1gram features'] = gl.text_analytics.count_ngrams(tweets['Tweet'], 1)\n",
    "tweets['2gram features'] = gl.text_analytics.count_ngrams(tweets['Tweet'], 2)\n",
    "tweets['3gram features'] = gl.text_analytics.count_ngrams(tweets['Tweet'], 3)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = DeepTextAnalyzer(pos_neg_model_w2v)\n",
    "tweets['vectors_pos_neg'] = tweets['Tweet'].apply(lambda p: dt.txt2avg_vector(p, is_html=False))\n",
    "dt = DeepTextAnalyzer(pos_neutral_model_w2v)\n",
    "tweets['vectors_pos_neutral'] = tweets['Tweet'].apply(lambda p: dt.txt2avg_vector(p, is_html=False))\n",
    "dt = DeepTextAnalyzer(neutral_neg_model_w2v)\n",
    "tweets['vectors_neutral_neg'] = tweets['Tweet'].apply(lambda p: dt.txt2avg_vector(p, is_html=False))\n",
    "#for the one label classifiers\n",
    "dt = DeepTextAnalyzer(positive_nonpositive_model_w2v)\n",
    "tweets['vectors_pos_ornot'] = tweets['Tweet'].apply(lambda p: dt.txt2avg_vector(p, is_html=False))\n",
    "dt = DeepTextAnalyzer(negative_nonnegative_model_w2v)\n",
    "tweets['vectors_neg_ornot'] = tweets['Tweet'].apply(lambda p: dt.txt2avg_vector(p, is_html=False))\n",
    "dt = DeepTextAnalyzer(neutral_nonneutral_model_w2v)\n",
    "tweets['vectors_neutral_ornot'] = tweets['Tweet'].apply(lambda p: dt.txt2avg_vector(p, is_html=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[  6.41991734e-04   6.84577913e-04   1.21212285e-03  -1.03691558e-03': '-4.22232561e-02   3.59229334e-02   4.11115922e-02   3.14613571e-03'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['word_count'] = graphlab.text_analytics.count_words(tweets['Tweet'])\n",
    "tfidf = graphlab.text_analytics.tf_idf(tweets['word_count'])\n",
    "tweets['tfidf'] = tfidf\n",
    "tweets.head()\n",
    "tweets_doc2vec[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Will start building and tunning the Pos/Neg classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c61ed6227b35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtweets_pos_neg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sentiment'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'neutral'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtweets_neutral\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sentiment'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'neutral'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_data_pos_neg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data_pos_neg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets_pos_neg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtweets_pos_neutral\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sentiment'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'negative'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_data_pos_neutral\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data_pos_neutral\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets_pos_neutral\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tweets' is not defined"
     ]
    }
   ],
   "source": [
    "tweets_pos_neg = tweets[tweets['Sentiment'] != 'neutral']\n",
    "tweets_neutral = tweets[tweets['Sentiment'] == 'neutral']\n",
    "train_data_pos_neg,test_data_pos_neg = tweets_pos_neg.random_split(.8, seed=0)\n",
    "tweets_pos_neutral = tweets[tweets['Sentiment'] != 'negative']\n",
    "train_data_pos_neutral,test_data_pos_neutral = tweets_pos_neutral.random_split(.8, seed=0)\n",
    "tweets_neutral_neg = tweets[tweets['Sentiment'] != 'positive']\n",
    "train_data_neutral_neg,test_data_neutral_neg = tweets_neutral_neg.random_split(.8, seed=0)\n",
    "##dt = DeepTextAnalyzer(doc2vec_reviewsonly_dm)\n",
    "##tweets['vectors_doc2vec_reviewsonly_dm'] = tweets['Tweet'].apply(lambda p: dt.txt2avg_vector(p, is_html=False))\n",
    "##'split_sentence' 'split_sentence' \n",
    "tweets['vectors_doc2vec_reviewsonly_dm'] = tweets['Tweet'].apply(lambda p: doc2vec_reviewsonly_dm.infer_vector(split_sentence(p)))\n",
    "##tweets['vectors_doc2vec_reviewsonly_dm'] = tweets_doc2vec.head\n",
    "##tweets = tweets.append(tweets_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">TweetID</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Sentiment</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Tweet</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">1gram features</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">2gram features</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">264183816548130000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">ga by my hous hit<br>$NUM.$NUM i m go to ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'on': 1L, 'hit': 1L,<br>'hous': 1L, 'i': 1L,  ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'to chapel': 1L, 'num<br>num': 1L, 'chapel hill': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">264249301910310000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">iranian gener say israel<br>s iron dome can t deal ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 1L, 'deal': 1L,<br>'we': 1L, 'say': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'with their': 1L, 'dome<br>can': 1L, 'talk po': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">264105751826538000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">with j davlar $NUMth.<br>main rival are team ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'week': 1L, 'it': 1L,<br>'an': 1L, 'are': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'main rival': 1L, 'j<br>davlar': 1L, 'end to': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">264094586689953000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">talk about act s amp;<br>amp; sat s, decid   i ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'me': 1L, 'and': 1L,<br>'about': 2L, 'decid': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'act s': 1L, 'everyth<br>about': 1L, 'to go': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">254941790757601000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">they may have a superbowl<br>in dalla, but dalla a ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 1L, 'a': 2L,<br>'dalla': 2L, 'superbo ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'ain t': 1L, 'at user':<br>2L, 'have a': 1L, 'dalla ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">264169034155696000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">im bring the ne load of<br>candi tomorrow, i just ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'load': 1L, 'all': 1L,<br>'just': 1L, 'get': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'load of': 1L, 'ne<br>load': 1L, 'it doesn': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">263192091700654000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">appl software, retail<br>chief out in overhaul: ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'on': 1L, 'francisco':<br>1L, 'heads': 1L, 'san': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'inc ceo': 1L, 'overhaul<br>san': 1L, 'monday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">263398998675693000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">AT USER AT USER AT USER i<br>just watch it sridevi s ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'on': 1L, 'u': 1L,<br>'from': 1L, 'just': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'nums sun': 1L, 'sun<br>morn': 1L, 'at user': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">260200142420992000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">livewir nadal confirm for<br>mexican open in febru ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'me': 1L, 'play': 1L,<br>'nadal': 2L, 'mexican': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'mexican open': 1L,<br>'nadal is': 1L, 'febr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">264087629237202000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">AT USER i didnt want to<br>just pop up... but ye ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 1L, 'have': 1L,<br>'just': 1L, 'pop': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'tell her': 1L, 'to<br>just': 1L, 'just pop': ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "<table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">3gram features</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">vectors_pos_neg</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">vectors_pos_neutral</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">vectors_neutral_neg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'num num i': 1L, 'hous<br>hit num': 1L, 'hill on ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0342471636832,<br>-0.00631276750937, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0614859685302,<br>0.170688733459, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.00743709597737,<br>-0.00723362248391, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'end up find': 1L, 'talk<br>po that': 1L, 'may end ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.081562243402,<br>0.0449653379619, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0205346476287,<br>0.0112347928807, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.00614882539958,<br>-0.0422288924456, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'rival are team': 1L,<br>'po end to': 1L, 'are ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0830649361014,<br>0.0383101254702, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0281793419272,<br>0.0636716261506, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0255065504462,<br>-0.0196313019842, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'colleg ne me': 1L, 'sat<br>s decid': 1L, 'colleg ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.00941443629563,<br>0.0329728163779, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0297066103667,<br>0.0356755778193, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0356532298028,<br>0.00269617256708, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'dalla ain t': 1L, 'a<br>superbowl not': 1L, 'at ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0500536933541,<br>-0.0502088814974, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.124739453197,<br>0.0695337057114, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0195374079049,<br>0.0297130830586, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'tomorrow i just': 1L,<br>'of candi tomorrow': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0501487441361,<br>0.0374873019755, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0260556004941,<br>0.0388131812215, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0237936079502,<br>0.0295581556857, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'appl inc ceo': 1L,<br>'francisco appl inc': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0379747897387,<br>-0.0597186461091, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0991781353951,<br>0.00786241423339, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0370806977153,<br>0.0337579250336, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'sridevi s comeback':<br>1L, 'at user i': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0390961170197,<br>-0.0682726055384, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.247600421309,<br>0.21358782053, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0262866914272,<br>0.0637785792351, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'rafael nadal is': 1L,<br>'confirm for mexican': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0232565160841,<br>-0.0263509117067, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.036987580359,<br>0.0352749191225, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0336031988263,<br>-0.0205079820007, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'hill next wednesday':<br>1L, 'chapel hill next': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0550381690264,<br>-0.0229573007673, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0678718611598,<br>0.0464017465711, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.00362627091818,<br>0.00711045181379, ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "<table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">vectors_pos_ornot</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">vectors_neg_ornot</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">vectors_neutral_ornot</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">word_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.101378776133,<br>0.0298841483891, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.107362709939,<br>0.0156400781125, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.019663291052,<br>0.0531490631402, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'on': 1L, 'hit': 1L,<br>':)': 1L, 'hous': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0445179529488,<br>0.0330055840313, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0137776732445,<br>0.0295914765447, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0159151721746,<br>-0.0254893582314, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 1L, 'deal': 1L,<br>'we': 1L, 'say': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0149716874585,<br>0.00490152975544, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0111285755411,<br>-0.0402547456324, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0192081406713,<br>0.0530241504312, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'davlar': 1L, 'it': 1L,<br>'an': 1L, 'are': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0358334593475,<br>0.0334879718721, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.069250240922,<br>0.0393856465816, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0030781805981,<br>0.0070589222014, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'me': 1L, 'and': 1L,<br>'about': 2L, 'decid': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.119036376476,<br>0.0381285659969, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0140887675807,<br>0.0114530129358, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0239956267178,<br>0.10719768703, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 1L, 'a': 2L,<br>'superbowl.': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0697232410312,<br>0.080910153687, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0333307869732,<br>0.0145465582609, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0211657956243,<br>0.109776839614, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'load': 1L, 'all': 1L,<br>'tomorrow,': 1L, 'just': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.137864977121,<br>0.0433993898332, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0866493880749,<br>-0.0162165500224, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0226152595133,<br>0.0240177866071, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'software,': 1L, 'on':<br>1L, 'francisco': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.124932035804,<br>0.100208617747, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0196576211601,<br>0.0983927100897, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0218674205244,<br>0.325505137444, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'on': 1L, 'from': 1L,<br>'just': 1L, 'her': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0341028273106,<br>0.0649784579873, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0324186980724,<br>0.0216868873686, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0569956153631,<br>0.0922425314784, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'play': 1L, 'set': 1L,<br>'mexican': 1L, 'for': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.106770344079,<br>0.0454658530653, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.069984279573,<br>-0.0488183870912, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0255725402385,<br>0.130263000727, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 1L, 'want': 1L,<br>'just': 1L, 'pop': 1L, ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "<table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">tfidf</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">vectors_doc2vec_tweetsonl<br>y_dm ...</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'on':<br>1.3268970210288216, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0166323650628,<br>0.0130460672081, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and':<br>1.4566928163977748, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.00277050188743,<br>-0.0259603187442, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'davlar':<br>9.37568530456302, 'it': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.00768466852605,<br>0.00743510387838, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'me':<br>2.9605883453914252, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0301001947373,<br>0.00343579053879, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and':<br>1.4566928163977748, 'a': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0360737740993,<br>0.00264362315647, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'load':<br>6.542471960506805, 'a ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.00282242754474,<br>0.013861049898, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'software,':<br>7.98939094344313, 'on': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0242651179433,<br>-0.0449849031866, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'on':<br>1.3268970210288216, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0169136710465,<br>-0.000693836482242, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'play':<br>3.4224419702752362, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0130272116512,<br>0.0127924606204, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and':<br>1.4566928163977748, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.00623194966465,<br>-0.0164404921234, ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[10 rows x 15 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tTweetID\tint\n",
       "\tSentiment\tstr\n",
       "\tTweet\tstr\n",
       "\t1gram features\tdict\n",
       "\t2gram features\tdict\n",
       "\t3gram features\tdict\n",
       "\tvectors_pos_neg\tarray\n",
       "\tvectors_pos_neutral\tarray\n",
       "\tvectors_neutral_neg\tarray\n",
       "\tvectors_pos_ornot\tarray\n",
       "\tvectors_neg_ornot\tarray\n",
       "\tvectors_neutral_ornot\tarray\n",
       "\tword_count\tdict\n",
       "\ttfidf\tdict\n",
       "\tvectors_doc2vec_tweetsonly_dm\tarray\n",
       "\n",
       "Rows: 10\n",
       "\n",
       "Data:\n",
       "+--------------------+-----------+-------------------------------+\n",
       "|      TweetID       | Sentiment |             Tweet             |\n",
       "+--------------------+-----------+-------------------------------+\n",
       "| 264183816548130000 |  positive | ga by my hous hit $NUM.$NU... |\n",
       "| 264249301910310000 |  negative | iranian gener say israel s... |\n",
       "| 264105751826538000 |  positive | with j davlar $NUMth. main... |\n",
       "| 264094586689953000 |  negative | talk about act s amp; amp;... |\n",
       "| 254941790757601000 |  negative | they may have a superbowl ... |\n",
       "| 264169034155696000 |  neutral  | im bring the ne load of ca... |\n",
       "| 263192091700654000 |  neutral  | appl software, retail chie... |\n",
       "| 263398998675693000 |  positive | AT USER AT USER AT USER i ... |\n",
       "| 260200142420992000 |  neutral  | livewir nadal confirm for ... |\n",
       "| 264087629237202000 |  positive | AT USER i didnt want to ju... |\n",
       "+--------------------+-----------+-------------------------------+\n",
       "+-------------------------------+-------------------------------+\n",
       "|         1gram features        |         2gram features        |\n",
       "+-------------------------------+-------------------------------+\n",
       "| {'on': 1L, 'hit': 1L, 'hou... | {'to chapel': 1L, 'num num... |\n",
       "| {'and': 1L, 'deal': 1L, 'w... | {'with their': 1L, 'dome c... |\n",
       "| {'week': 1L, 'it': 1L, 'an... | {'main rival': 1L, 'j davl... |\n",
       "| {'me': 1L, 'and': 1L, 'abo... | {'act s': 1L, 'everyth abo... |\n",
       "| {'and': 1L, 'a': 2L, 'dall... | {'ain t': 1L, 'at user': 2... |\n",
       "| {'load': 1L, 'all': 1L, 'j... | {'load of': 1L, 'ne load':... |\n",
       "| {'on': 1L, 'francisco': 1L... | {'inc ceo': 1L, 'overhaul ... |\n",
       "| {'on': 1L, 'u': 1L, 'from'... | {'nums sun': 1L, 'sun morn... |\n",
       "| {'me': 1L, 'play': 1L, 'na... | {'mexican open': 1L, 'nada... |\n",
       "| {'and': 1L, 'have': 1L, 'j... | {'tell her': 1L, 'to just'... |\n",
       "+-------------------------------+-------------------------------+\n",
       "+-------------------------------+-------------------------------+\n",
       "|         3gram features        |        vectors_pos_neg        |\n",
       "+-------------------------------+-------------------------------+\n",
       "| {'num num i': 1L, 'hous hi... | [-0.0342471636832, -0.0063... |\n",
       "| {'end up find': 1L, 'talk ... | [0.081562243402, 0.0449653... |\n",
       "| {'rival are team': 1L, 'po... | [0.0830649361014, 0.038310... |\n",
       "| {'colleg ne me': 1L, 'sat ... | [0.00941443629563, 0.03297... |\n",
       "| {'dalla ain t': 1L, 'a sup... | [0.0500536933541, -0.05020... |\n",
       "| {'tomorrow i just': 1L, 'o... | [0.0501487441361, 0.037487... |\n",
       "| {'appl inc ceo': 1L, 'fran... | [0.0379747897387, -0.05971... |\n",
       "| {'sridevi s comeback': 1L,... | [0.0390961170197, -0.06827... |\n",
       "| {'rafael nadal is': 1L, 'c... | [0.0232565160841, -0.02635... |\n",
       "| {'hill next wednesday': 1L... | [0.0550381690264, -0.02295... |\n",
       "+-------------------------------+-------------------------------+\n",
       "+-------------------------------+-------------------------------+\n",
       "|      vectors_pos_neutral      |      vectors_neutral_neg      |\n",
       "+-------------------------------+-------------------------------+\n",
       "| [-0.0614859685302, 0.17068... | [-0.00743709597737, -0.007... |\n",
       "| [-0.0205346476287, 0.01123... | [-0.00614882539958, -0.042... |\n",
       "| [-0.0281793419272, 0.06367... | [-0.0255065504462, -0.0196... |\n",
       "| [-0.0297066103667, 0.03567... | [-0.0356532298028, 0.00269... |\n",
       "| [0.124739453197, 0.0695337... | [0.0195374079049, 0.029713... |\n",
       "| [0.0260556004941, 0.038813... | [-0.0237936079502, 0.02955... |\n",
       "| [0.0991781353951, 0.007862... | [0.0370806977153, 0.033757... |\n",
       "| [0.247600421309, 0.2135878... | [0.0262866914272, 0.063778... |\n",
       "| [0.036987580359, 0.0352749... | [0.0336031988263, -0.02050... |\n",
       "| [0.0678718611598, 0.046401... | [0.00362627091818, 0.00711... |\n",
       "+-------------------------------+-------------------------------+\n",
       "+-------------------------------+-------------------------------+\n",
       "|       vectors_pos_ornot       |       vectors_neg_ornot       |\n",
       "+-------------------------------+-------------------------------+\n",
       "| [-0.101378776133, 0.029884... | [-0.107362709939, 0.015640... |\n",
       "| [0.0445179529488, 0.033005... | [0.0137776732445, 0.029591... |\n",
       "| [0.0149716874585, 0.004901... | [0.0111285755411, -0.04025... |\n",
       "| [0.0358334593475, 0.033487... | [-0.069250240922, 0.039385... |\n",
       "| [0.119036376476, 0.0381285... | [-0.0140887675807, 0.01145... |\n",
       "| [0.0697232410312, 0.080910... | [-0.0333307869732, 0.01454... |\n",
       "| [0.137864977121, 0.0433993... | [-0.0866493880749, -0.0162... |\n",
       "| [0.124932035804, 0.1002086... | [0.0196576211601, 0.098392... |\n",
       "| [0.0341028273106, 0.064978... | [-0.0324186980724, 0.02168... |\n",
       "| [0.106770344079, 0.0454658... | [-0.069984279573, -0.04881... |\n",
       "+-------------------------------+-------------------------------+\n",
       "+-------------------------------+-------------------------------+\n",
       "|     vectors_neutral_ornot     |           word_count          |\n",
       "+-------------------------------+-------------------------------+\n",
       "| [-0.019663291052, 0.053149... | {'on': 1L, 'hit': 1L, ':)'... |\n",
       "| [-0.0159151721746, -0.0254... | {'and': 1L, 'deal': 1L, 'w... |\n",
       "| [0.0192081406713, 0.053024... | {'davlar': 1L, 'it': 1L, '... |\n",
       "| [-0.0030781805981, 0.00705... | {'me': 1L, 'and': 1L, 'abo... |\n",
       "| [-0.0239956267178, 0.10719... | {'and': 1L, 'a': 2L, 'supe... |\n",
       "| [0.0211657956243, 0.109776... | {'load': 1L, 'all': 1L, 't... |\n",
       "| [-0.0226152595133, 0.02401... | {'software,': 1L, 'on': 1L... |\n",
       "| [-0.0218674205244, 0.32550... | {'on': 1L, 'from': 1L, 'ju... |\n",
       "| [-0.0569956153631, 0.09224... | {'play': 1L, 'set': 1L, 'm... |\n",
       "| [-0.0255725402385, 0.13026... | {'and': 1L, 'want': 1L, 'j... |\n",
       "+-------------------------------+-------------------------------+\n",
       "+-------------------------------+-------------------------------+\n",
       "|             tfidf             | vectors_doc2vec_tweetsonly_dm |\n",
       "+-------------------------------+-------------------------------+\n",
       "| {'on': 1.3268970210288216,... | [0.0166323650628, 0.013046... |\n",
       "| {'and': 1.4566928163977748... | [0.00277050188743, -0.0259... |\n",
       "| {'davlar': 9.3756853045630... | [0.00768466852605, 0.00743... |\n",
       "| {'me': 2.9605883453914252,... | [0.0301001947373, 0.003435... |\n",
       "| {'and': 1.4566928163977748... | [0.0360737740993, 0.002643... |\n",
       "| {'load': 6.542471960506805... | [0.00282242754474, 0.01386... |\n",
       "| {'software,': 7.9893909434... | [0.0242651179433, -0.04498... |\n",
       "| {'on': 1.3268970210288216,... | [0.0169136710465, -0.00069... |\n",
       "| {'play': 3.422441970275236... | [0.0130272116512, 0.012792... |\n",
       "| {'and': 1.4566928163977748... | [-0.00623194966465, -0.016... |\n",
       "+-------------------------------+-------------------------------+\n",
       "[10 rows x 15 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature_set_pos_neg = ['tfidf','1gram features','2gram features','3gram features','vectors_pos_neg','vectors_pos_neutral','vectors_pos_ornot','vectors_neg_ornot']\n",
    "#model_pos_neg =graphlab.logistic_classifier.create(train_data_pos_neg,\n",
    "                                                     #target='Sentiment',\n",
    "                                                     #features=feature_set_pos_neg,\n",
    "                                                     #max_iterations=100,\n",
    "                                                     #l1_penalty=0,\n",
    "                                                     #l2_penalty=1000.0,\n",
    "                                                     #class_weights = 'auto',\n",
    "                                                     #validation_set=test_data_pos_neg)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Creating a validation set from 5 percent of training data. This may take a while.\n",
      "          You can set ``validation_set=None`` to disable validation tracking.\n",
      "\n",
      "PROGRESS: The following methods are available for this type of problem.\n",
      "PROGRESS: LogisticClassifier, SVMClassifier\n",
      "PROGRESS: The returned model will be chosen according to validation accuracy.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 7744\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 4\n",
      "PROGRESS: Number of unpacked features : 221952\n",
      "PROGRESS: Number of coefficients    : 221953\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy | Validation-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | 1         | 3        | 0.000129  | 0.237159     | 0.997030          | 0.693827            |\n",
      "PROGRESS: | 2         | 5        | 1.000000  | 0.351234     | 0.998321          | 0.693827            |\n",
      "PROGRESS: | 3         | 6        | 1.000000  | 0.415281     | 0.998967          | 0.698765            |\n",
      "PROGRESS: | 4         | 7        | 1.000000  | 0.484322     | 0.998838          | 0.696296            |\n",
      "PROGRESS: | 5         | 9        | 1.000000  | 0.595397     | 0.999225          | 0.696296            |\n",
      "PROGRESS: | 6         | 10       | 1.000000  | 0.660441     | 0.999096          | 0.696296            |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: SVM:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 7744\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 4\n",
      "PROGRESS: Number of unpacked features : 221952\n",
      "PROGRESS: Number of coefficients    : 221953\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy | Validation-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | 1         | 3        | 0.000129  | 0.634423     | 0.997030          | 0.693827            |\n",
      "PROGRESS: | 2         | 5        | 1.000000  | 0.760508     | 0.998450          | 0.691358            |\n",
      "PROGRESS: | 3         | 6        | 1.000000  | 0.807544     | 0.998838          | 0.691358            |\n",
      "PROGRESS: | 4         | 7        | 1.000000  | 0.856573     | 0.998063          | 0.693827            |\n",
      "PROGRESS: | 5         | 9        | 1.000000  | 0.933624     | 0.999096          | 0.691358            |\n",
      "PROGRESS: | 6         | 10       | 1.000000  | 0.980659     | 0.998580          | 0.691358            |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Model selection based on validation accuracy:\n",
      "PROGRESS: ---------------------------------------------\n",
      "PROGRESS: LogisticClassifier              : 0.698765\n",
      "PROGRESS: SVMClassifier                   : 0.691358\n",
      "PROGRESS: ---------------------------------------------\n",
      "PROGRESS: Selecting LogisticClassifier based on validation set performance.\n"
     ]
    }
   ],
   "source": [
    "#feature_set_pos_neutral = ['tfidf','1gram features','2gram features','3gram features','vectors_pos_neutral','vectors_pos_ornot','vectors_neutral_ornot']\n",
    "#feature_set_pos_neutral = ['tfidf','1gram features','2gram features','3gram features','vectors_pos_neutral','vectors_pos_ornot','vectors_neutral_ornot','vectors_doc2vec_tweetsonly_dm']\n",
    "#feature_set_pos_neutral = ['tfidf','1gram features','2gram features','3gram features','vectors_doc2vec_tweetsonly_dm']\n",
    "feature_set_pos_neutral = ['tfidf','1gram features','2gram features','3gram features']\n",
    "\n",
    "model_pos_neutral =graphlab.classifier.create(train_data_pos_neutral,\n",
    "                                                     target='Sentiment',\n",
    "                                                     features=feature_set_pos_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Creating a validation set from 5 percent of training data. This may take a while.\n",
      "          You can set ``validation_set=None`` to disable validation tracking.\n",
      "\n",
      "PROGRESS: The following methods are available for this type of problem.\n",
      "PROGRESS: LogisticClassifier, SVMClassifier\n",
      "PROGRESS: The returned model will be chosen according to validation accuracy.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 5162\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 4\n",
      "PROGRESS: Number of unpacked features : 163307\n",
      "PROGRESS: Number of coefficients    : 163308\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy | Validation-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000097  | 0.325217     | 0.984502          | 0.766798            |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.400271     | 0.999031          | 0.758893            |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.438296     | 0.999225          | 0.758893            |\n",
      "PROGRESS: | 4         | 8        | 1.000000  | 0.481322     | 0.999613          | 0.758893            |\n",
      "PROGRESS: | 5         | 9        | 1.000000  | 0.520353     | 0.999613          | 0.758893            |\n",
      "PROGRESS: | 6         | 10       | 1.000000  | 0.561377     | 0.999613          | 0.758893            |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: SVM:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 5162\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 4\n",
      "PROGRESS: Number of unpacked features : 163307\n",
      "PROGRESS: Number of coefficients    : 163308\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy | Validation-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | 1         | 3        | 0.000194  | 0.074053     | 0.984502          | 0.766798            |\n",
      "PROGRESS: | 2         | 5        | 1.000000  | 0.129090     | 0.999031          | 0.766798            |\n",
      "PROGRESS: | 3         | 6        | 1.000000  | 0.160107     | 0.999613          | 0.766798            |\n",
      "PROGRESS: | 4         | 7        | 1.000000  | 0.195132     | 0.999613          | 0.766798            |\n",
      "PROGRESS: | 5         | 8        | 1.000000  | 0.229156     | 0.999613          | 0.766798            |\n",
      "PROGRESS: | 6         | 13       | 2.000000  | 0.334228     | 0.999613          | 0.766798            |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Model selection based on validation accuracy:\n",
      "PROGRESS: ---------------------------------------------\n",
      "PROGRESS: LogisticClassifier              : 0.758893\n",
      "PROGRESS: SVMClassifier                   : 0.766798\n",
      "PROGRESS: ---------------------------------------------\n",
      "PROGRESS: Selecting SVMClassifier based on validation set performance.\n"
     ]
    }
   ],
   "source": [
    "#feature_set_neutral_neg = ['tfidf','1gram features','2gram features','3gram features','vectors_neutral_neg','vectors_neg_ornot','vectors_neutral_ornot']\n",
    "#feature_set_neutral_neg = ['tfidf','1gram features','2gram features','3gram features','vectors_neutral_neg','vectors_neg_ornot','vectors_neutral_ornot','vectors_doc2vec_tweetsonly_dm']\n",
    "feature_set_neutral_neg = ['tfidf','1gram features','2gram features','3gram features']\n",
    "#feature_set_neutral_neg = ['tfidf','1gram features','2gram features','3gram features','vectors_doc2vec_tweetsonly_dm']\n",
    "model_neutral_neg =graphlab.classifier.create(train_data_neutral_neg,\n",
    "                                                     target='Sentiment',\n",
    "                                                     features=feature_set_neutral_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets['PositiveorNot'] = tweets['Sentiment'].apply(lambda p: p=='positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">TweetID</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Sentiment</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Tweet</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">1gram features</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">2gram features</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">264183816548130000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">ga by my hous hit<br>$NUM.$NUM i m go to ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'on': 1L, 'hit': 1L,<br>'hous': 1L, 'i': 1L,  ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'to chapel': 1L, 'num<br>num': 1L, 'chapel hill': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">264249301910310000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">iranian gener say israel<br>s iron dome can t deal ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 1L, 'deal': 1L,<br>'we': 1L, 'say': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'with their': 1L, 'dome<br>can': 1L, 'talk po': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">264105751826538000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">with j davlar $NUMth.<br>main rival are team ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'week': 1L, 'it': 1L,<br>'an': 1L, 'are': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'main rival': 1L, 'j<br>davlar': 1L, 'end to': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">264094586689953000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">talk about act s amp;<br>amp; sat s, decid   i ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'me': 1L, 'and': 1L,<br>'about': 2L, 'decid': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'act s': 1L, 'everyth<br>about': 1L, 'to go': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">254941790757601000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">they may have a superbowl<br>in dalla, but dalla a ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 1L, 'a': 2L,<br>'dalla': 2L, 'superbo ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'ain t': 1L, 'at user':<br>2L, 'have a': 1L, 'dalla ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">264169034155696000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">im bring the ne load of<br>candi tomorrow, i just ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'load': 1L, 'all': 1L,<br>'just': 1L, 'get': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'load of': 1L, 'ne<br>load': 1L, 'it doesn': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">263192091700654000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">appl software, retail<br>chief out in overhaul: ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'on': 1L, 'francisco':<br>1L, 'heads': 1L, 'san': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'inc ceo': 1L, 'overhaul<br>san': 1L, 'monday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">263398998675693000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">AT USER AT USER AT USER i<br>just watch it sridevi s ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'on': 1L, 'u': 1L,<br>'from': 1L, 'just': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'nums sun': 1L, 'sun<br>morn': 1L, 'at user': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">260200142420992000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">livewir nadal confirm for<br>mexican open in febru ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'me': 1L, 'play': 1L,<br>'nadal': 2L, 'mexican': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'mexican open': 1L,<br>'nadal is': 1L, 'febr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">264087629237202000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">AT USER i didnt want to<br>just pop up... but ye ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 1L, 'have': 1L,<br>'just': 1L, 'pop': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'tell her': 1L, 'to<br>just': 1L, 'just pop': ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "<table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">3gram features</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">vectors_pos_neg</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">vectors_pos_neutral</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">vectors_neutral_neg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'num num i': 1L, 'hous<br>hit num': 1L, 'hill on ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0342471636832,<br>-0.00631276750937, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0614859685302,<br>0.170688733459, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.00743709597737,<br>-0.00723362248391, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'end up find': 1L, 'talk<br>po that': 1L, 'may end ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.081562243402,<br>0.0449653379619, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0205346476287,<br>0.0112347928807, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.00614882539958,<br>-0.0422288924456, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'rival are team': 1L,<br>'po end to': 1L, 'are ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0830649361014,<br>0.0383101254702, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0281793419272,<br>0.0636716261506, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0255065504462,<br>-0.0196313019842, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'colleg ne me': 1L, 'sat<br>s decid': 1L, 'colleg ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.00941443629563,<br>0.0329728163779, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0297066103667,<br>0.0356755778193, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0356532298028,<br>0.00269617256708, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'dalla ain t': 1L, 'a<br>superbowl not': 1L, 'at ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0500536933541,<br>-0.0502088814974, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.124739453197,<br>0.0695337057114, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0195374079049,<br>0.0297130830586, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'tomorrow i just': 1L,<br>'of candi tomorrow': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0501487441361,<br>0.0374873019755, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0260556004941,<br>0.0388131812215, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0237936079502,<br>0.0295581556857, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'appl inc ceo': 1L,<br>'francisco appl inc': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0379747897387,<br>-0.0597186461091, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0991781353951,<br>0.00786241423339, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0370806977153,<br>0.0337579250336, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'sridevi s comeback':<br>1L, 'at user i': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0390961170197,<br>-0.0682726055384, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.247600421309,<br>0.21358782053, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0262866914272,<br>0.0637785792351, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'rafael nadal is': 1L,<br>'confirm for mexican': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0232565160841,<br>-0.0263509117067, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.036987580359,<br>0.0352749191225, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0336031988263,<br>-0.0205079820007, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'hill next wednesday':<br>1L, 'chapel hill next': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0550381690264,<br>-0.0229573007673, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0678718611598,<br>0.0464017465711, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.00362627091818,<br>0.00711045181379, ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "<table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">vectors_pos_ornot</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">vectors_neg_ornot</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">vectors_neutral_ornot</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">word_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.101378776133,<br>0.0298841483891, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.107362709939,<br>0.0156400781125, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.019663291052,<br>0.0531490631402, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'on': 1L, 'hit': 1L,<br>':)': 1L, 'hous': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0445179529488,<br>0.0330055840313, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0137776732445,<br>0.0295914765447, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0159151721746,<br>-0.0254893582314, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 1L, 'deal': 1L,<br>'we': 1L, 'say': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0149716874585,<br>0.00490152975544, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0111285755411,<br>-0.0402547456324, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0192081406713,<br>0.0530241504312, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'davlar': 1L, 'it': 1L,<br>'an': 1L, 'are': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0358334593475,<br>0.0334879718721, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.069250240922,<br>0.0393856465816, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0030781805981,<br>0.0070589222014, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'me': 1L, 'and': 1L,<br>'about': 2L, 'decid': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.119036376476,<br>0.0381285659969, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0140887675807,<br>0.0114530129358, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0239956267178,<br>0.10719768703, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 1L, 'a': 2L,<br>'superbowl.': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0697232410312,<br>0.080910153687, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0333307869732,<br>0.0145465582609, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0211657956243,<br>0.109776839614, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'load': 1L, 'all': 1L,<br>'tomorrow,': 1L, 'just': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.137864977121,<br>0.0433993898332, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0866493880749,<br>-0.0162165500224, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0226152595133,<br>0.0240177866071, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'software,': 1L, 'on':<br>1L, 'francisco': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.124932035804,<br>0.100208617747, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0196576211601,<br>0.0983927100897, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0218674205244,<br>0.325505137444, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'on': 1L, 'from': 1L,<br>'just': 1L, 'her': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0341028273106,<br>0.0649784579873, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0324186980724,<br>0.0216868873686, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0569956153631,<br>0.0922425314784, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'play': 1L, 'set': 1L,<br>'mexican': 1L, 'for': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.106770344079,<br>0.0454658530653, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.069984279573,<br>-0.0488183870912, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0255725402385,<br>0.130263000727, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 1L, 'want': 1L,<br>'just': 1L, 'pop': 1L, ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "<table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">tfidf</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">PositiveorNot</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">NegativeorNot</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">NeutralorNot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'on':<br>1.3268970210288216, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and':<br>1.4566928163977748, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'davlar':<br>9.37568530456302, 'it': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'me':<br>2.9605883453914252, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and':<br>1.4566928163977748, 'a': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'load':<br>6.542471960506805, 'a ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'software,':<br>7.98939094344313, 'on': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'on':<br>1.3268970210288216, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'play':<br>3.4224419702752362, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and':<br>1.4566928163977748, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[10 rows x 17 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tTweetID\tint\n",
       "\tSentiment\tstr\n",
       "\tTweet\tstr\n",
       "\t1gram features\tdict\n",
       "\t2gram features\tdict\n",
       "\t3gram features\tdict\n",
       "\tvectors_pos_neg\tarray\n",
       "\tvectors_pos_neutral\tarray\n",
       "\tvectors_neutral_neg\tarray\n",
       "\tvectors_pos_ornot\tarray\n",
       "\tvectors_neg_ornot\tarray\n",
       "\tvectors_neutral_ornot\tarray\n",
       "\tword_count\tdict\n",
       "\ttfidf\tdict\n",
       "\tPositiveorNot\tint\n",
       "\tNegativeorNot\tint\n",
       "\tNeutralorNot\tint\n",
       "\n",
       "Rows: 10\n",
       "\n",
       "Data:\n",
       "+--------------------+-----------+-------------------------------+\n",
       "|      TweetID       | Sentiment |             Tweet             |\n",
       "+--------------------+-----------+-------------------------------+\n",
       "| 264183816548130000 |  positive | ga by my hous hit $NUM.$NU... |\n",
       "| 264249301910310000 |  negative | iranian gener say israel s... |\n",
       "| 264105751826538000 |  positive | with j davlar $NUMth. main... |\n",
       "| 264094586689953000 |  negative | talk about act s amp; amp;... |\n",
       "| 254941790757601000 |  negative | they may have a superbowl ... |\n",
       "| 264169034155696000 |  neutral  | im bring the ne load of ca... |\n",
       "| 263192091700654000 |  neutral  | appl software, retail chie... |\n",
       "| 263398998675693000 |  positive | AT USER AT USER AT USER i ... |\n",
       "| 260200142420992000 |  neutral  | livewir nadal confirm for ... |\n",
       "| 264087629237202000 |  positive | AT USER i didnt want to ju... |\n",
       "+--------------------+-----------+-------------------------------+\n",
       "+-------------------------------+-------------------------------+\n",
       "|         1gram features        |         2gram features        |\n",
       "+-------------------------------+-------------------------------+\n",
       "| {'on': 1L, 'hit': 1L, 'hou... | {'to chapel': 1L, 'num num... |\n",
       "| {'and': 1L, 'deal': 1L, 'w... | {'with their': 1L, 'dome c... |\n",
       "| {'week': 1L, 'it': 1L, 'an... | {'main rival': 1L, 'j davl... |\n",
       "| {'me': 1L, 'and': 1L, 'abo... | {'act s': 1L, 'everyth abo... |\n",
       "| {'and': 1L, 'a': 2L, 'dall... | {'ain t': 1L, 'at user': 2... |\n",
       "| {'load': 1L, 'all': 1L, 'j... | {'load of': 1L, 'ne load':... |\n",
       "| {'on': 1L, 'francisco': 1L... | {'inc ceo': 1L, 'overhaul ... |\n",
       "| {'on': 1L, 'u': 1L, 'from'... | {'nums sun': 1L, 'sun morn... |\n",
       "| {'me': 1L, 'play': 1L, 'na... | {'mexican open': 1L, 'nada... |\n",
       "| {'and': 1L, 'have': 1L, 'j... | {'tell her': 1L, 'to just'... |\n",
       "+-------------------------------+-------------------------------+\n",
       "+-------------------------------+-------------------------------+\n",
       "|         3gram features        |        vectors_pos_neg        |\n",
       "+-------------------------------+-------------------------------+\n",
       "| {'num num i': 1L, 'hous hi... | [-0.0342471636832, -0.0063... |\n",
       "| {'end up find': 1L, 'talk ... | [0.081562243402, 0.0449653... |\n",
       "| {'rival are team': 1L, 'po... | [0.0830649361014, 0.038310... |\n",
       "| {'colleg ne me': 1L, 'sat ... | [0.00941443629563, 0.03297... |\n",
       "| {'dalla ain t': 1L, 'a sup... | [0.0500536933541, -0.05020... |\n",
       "| {'tomorrow i just': 1L, 'o... | [0.0501487441361, 0.037487... |\n",
       "| {'appl inc ceo': 1L, 'fran... | [0.0379747897387, -0.05971... |\n",
       "| {'sridevi s comeback': 1L,... | [0.0390961170197, -0.06827... |\n",
       "| {'rafael nadal is': 1L, 'c... | [0.0232565160841, -0.02635... |\n",
       "| {'hill next wednesday': 1L... | [0.0550381690264, -0.02295... |\n",
       "+-------------------------------+-------------------------------+\n",
       "+-------------------------------+-------------------------------+\n",
       "|      vectors_pos_neutral      |      vectors_neutral_neg      |\n",
       "+-------------------------------+-------------------------------+\n",
       "| [-0.0614859685302, 0.17068... | [-0.00743709597737, -0.007... |\n",
       "| [-0.0205346476287, 0.01123... | [-0.00614882539958, -0.042... |\n",
       "| [-0.0281793419272, 0.06367... | [-0.0255065504462, -0.0196... |\n",
       "| [-0.0297066103667, 0.03567... | [-0.0356532298028, 0.00269... |\n",
       "| [0.124739453197, 0.0695337... | [0.0195374079049, 0.029713... |\n",
       "| [0.0260556004941, 0.038813... | [-0.0237936079502, 0.02955... |\n",
       "| [0.0991781353951, 0.007862... | [0.0370806977153, 0.033757... |\n",
       "| [0.247600421309, 0.2135878... | [0.0262866914272, 0.063778... |\n",
       "| [0.036987580359, 0.0352749... | [0.0336031988263, -0.02050... |\n",
       "| [0.0678718611598, 0.046401... | [0.00362627091818, 0.00711... |\n",
       "+-------------------------------+-------------------------------+\n",
       "+-------------------------------+-------------------------------+\n",
       "|       vectors_pos_ornot       |       vectors_neg_ornot       |\n",
       "+-------------------------------+-------------------------------+\n",
       "| [-0.101378776133, 0.029884... | [-0.107362709939, 0.015640... |\n",
       "| [0.0445179529488, 0.033005... | [0.0137776732445, 0.029591... |\n",
       "| [0.0149716874585, 0.004901... | [0.0111285755411, -0.04025... |\n",
       "| [0.0358334593475, 0.033487... | [-0.069250240922, 0.039385... |\n",
       "| [0.119036376476, 0.0381285... | [-0.0140887675807, 0.01145... |\n",
       "| [0.0697232410312, 0.080910... | [-0.0333307869732, 0.01454... |\n",
       "| [0.137864977121, 0.0433993... | [-0.0866493880749, -0.0162... |\n",
       "| [0.124932035804, 0.1002086... | [0.0196576211601, 0.098392... |\n",
       "| [0.0341028273106, 0.064978... | [-0.0324186980724, 0.02168... |\n",
       "| [0.106770344079, 0.0454658... | [-0.069984279573, -0.04881... |\n",
       "+-------------------------------+-------------------------------+\n",
       "+-------------------------------+-------------------------------+\n",
       "|     vectors_neutral_ornot     |           word_count          |\n",
       "+-------------------------------+-------------------------------+\n",
       "| [-0.019663291052, 0.053149... | {'on': 1L, 'hit': 1L, ':)'... |\n",
       "| [-0.0159151721746, -0.0254... | {'and': 1L, 'deal': 1L, 'w... |\n",
       "| [0.0192081406713, 0.053024... | {'davlar': 1L, 'it': 1L, '... |\n",
       "| [-0.0030781805981, 0.00705... | {'me': 1L, 'and': 1L, 'abo... |\n",
       "| [-0.0239956267178, 0.10719... | {'and': 1L, 'a': 2L, 'supe... |\n",
       "| [0.0211657956243, 0.109776... | {'load': 1L, 'all': 1L, 't... |\n",
       "| [-0.0226152595133, 0.02401... | {'software,': 1L, 'on': 1L... |\n",
       "| [-0.0218674205244, 0.32550... | {'on': 1L, 'from': 1L, 'ju... |\n",
       "| [-0.0569956153631, 0.09224... | {'play': 1L, 'set': 1L, 'm... |\n",
       "| [-0.0255725402385, 0.13026... | {'and': 1L, 'want': 1L, 'j... |\n",
       "+-------------------------------+-------------------------------+\n",
       "+-------------------------------+---------------+---------------+--------------+\n",
       "|             tfidf             | PositiveorNot | NegativeorNot | NeutralorNot |\n",
       "+-------------------------------+---------------+---------------+--------------+\n",
       "| {'on': 1.3268970210288216,... |       1       |       0       |      0       |\n",
       "| {'and': 1.4566928163977748... |       0       |       1       |      0       |\n",
       "| {'davlar': 9.3756853045630... |       1       |       0       |      0       |\n",
       "| {'me': 2.9605883453914252,... |       0       |       1       |      0       |\n",
       "| {'and': 1.4566928163977748... |       0       |       1       |      0       |\n",
       "| {'load': 6.542471960506805... |       0       |       0       |      1       |\n",
       "| {'software,': 7.9893909434... |       0       |       0       |      1       |\n",
       "| {'on': 1.3268970210288216,... |       1       |       0       |      0       |\n",
       "| {'play': 3.422441970275236... |       0       |       0       |      1       |\n",
       "| {'and': 1.4566928163977748... |       1       |       0       |      0       |\n",
       "+-------------------------------+---------------+---------------+--------------+\n",
       "[10 rows x 17 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets['NegativeorNot'] = tweets['Sentiment'].apply(lambda p: p=='negative')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_positive_ornot,test_data_positive_ornot = tweets.random_split(.8, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Creating a validation set from 5 percent of training data. This may take a while.\n",
      "          You can set ``validation_set=None`` to disable validation tracking.\n",
      "\n",
      "PROGRESS: The following methods are available for this type of problem.\n",
      "PROGRESS: LogisticClassifier, SVMClassifier\n",
      "PROGRESS: The returned model will be chosen according to validation accuracy.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 9022\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 256956\n",
      "PROGRESS: Number of coefficients    : 256957\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy | Validation-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | 1         | 5        | 0.000028  | 0.944632     | 0.773221          | 0.584034            |\n",
      "PROGRESS: | 2         | 7        | 1.000000  | 1.636092     | 0.996121          | 0.689076            |\n",
      "PROGRESS: | 3         | 8        | 1.000000  | 1.981324     | 0.997562          | 0.691176            |\n",
      "PROGRESS: | 4         | 9        | 1.000000  | 2.324550     | 0.999002          | 0.689076            |\n",
      "PROGRESS: | 5         | 10       | 1.000000  | 2.657778     | 0.999335          | 0.691176            |\n",
      "PROGRESS: | 6         | 11       | 1.000000  | 3.001008     | 0.999335          | 0.689076            |\n",
      "PROGRESS: | 10        | 15       | 1.000000  | 5.052370     | 0.999446          | 0.697479            |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: SVM:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 9022\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 256956\n",
      "PROGRESS: Number of coefficients    : 256957\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy | Validation-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000010  | 2.281519     | 0.773221          | 0.584034            |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 4.012676     | 0.995677          | 0.705882            |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 4.344895     | 0.997340          | 0.707983            |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 4.688127     | 0.998670          | 0.705882            |\n",
      "PROGRESS: | 5         | 11       | 1.000000  | 5.018347     | 0.998892          | 0.699580            |\n",
      "PROGRESS: | 6         | 12       | 1.000000  | 5.346563     | 0.999113          | 0.697479            |\n",
      "PROGRESS: | 10        | 16       | 1.000000  | 6.630422     | 0.999335          | 0.693277            |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Model selection based on validation accuracy:\n",
      "PROGRESS: ---------------------------------------------\n",
      "PROGRESS: LogisticClassifier              : 0.697479\n",
      "PROGRESS: SVMClassifier                   : 0.693277\n",
      "PROGRESS: ---------------------------------------------\n",
      "PROGRESS: Selecting LogisticClassifier based on validation set performance.\n"
     ]
    }
   ],
   "source": [
    "feature_set_positive_ornot = ['tfidf','1gram features','2gram features','3gram features','vectors_pos_neutral','vectors_neg_ornot']\n",
    "model_positive_ornot =graphlab.classifier.create(train_data_positive_ornot,\n",
    "                                                     target='PositiveorNot',\n",
    "                                                     features=feature_set_positive_ornot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create() got an unexpected keyword argument 'max_iterations'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-ba3a270d87f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m                                                      \u001b[0ml2_penalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                                                      \u001b[0mclass_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                                                      validation_set=test_data_positive_ornot)\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: create() got an unexpected keyword argument 'max_iterations'"
     ]
    }
   ],
   "source": [
    "feature_set_negative_ornot = ['tfidf','1gram features','2gram features','3gram features','vectors_neg_ornot']\n",
    "model_negative_ornot =graphlab.logistic_classifier.create(train_data_positive_ornot,\n",
    "                                                     target='NegativeorNot',\n",
    "                                                     features=feature_set_negative_ornot,\n",
    "                                                     max_iterations=100,\n",
    "                                                     l1_penalty=0,\n",
    "                                                     l2_penalty=100.0,\n",
    "                                                     class_weights = 'auto',\n",
    "                                                     validation_set=test_data_positive_ornot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 9498\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 267997\n",
      "PROGRESS: Number of coefficients    : 267998\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy | Validation-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000053  | 0.585394     | 0.993578          | 0.637826            |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.912611     | 0.997157          | 0.656087            |\n",
      "PROGRESS: | 3         | 9        | 0.500000  | 1.354907     | 0.996420          | 0.600000            |\n",
      "PROGRESS: | 4         | 12       | 0.500000  | 1.801204     | 0.998105          | 0.654783            |\n",
      "PROGRESS: | 5         | 13       | 0.500000  | 2.016349     | 0.998315          | 0.657391            |\n",
      "PROGRESS: | 6         | 14       | 0.500000  | 2.237495     | 0.998631          | 0.652609            |\n",
      "PROGRESS: | 10        | 18       | 0.500000  | 3.122086     | 0.998737          | 0.661739            |\n",
      "PROGRESS: | 11        | 19       | 0.500000  | 3.343235     | 0.998737          | 0.662174            |\n",
      "PROGRESS: | 15        | 28       | 1.000000  | 4.790194     | 0.998947          | 0.662174            |\n",
      "PROGRESS: | 20        | 35       | 0.500000  | 6.112080     | 0.998947          | 0.661304            |\n",
      "PROGRESS: | 25        | 41       | 1.000000  | 7.345900     | 0.998842          | 0.661739            |\n",
      "PROGRESS: | 30        | 54       | 0.500000  | 9.363246     | 0.998947          | 0.657826            |\n",
      "PROGRESS: | 35        | 63       | 1.000000  | 10.923287    | 0.998947          | 0.658261            |\n",
      "PROGRESS: | 40        | 70       | 1.000000  | 12.268183    | 0.998947          | 0.658261            |\n",
      "PROGRESS: | 45        | 78       | 1.000000  | 13.777188    | 0.998947          | 0.658261            |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: SUCCESS: Optimal solution found.\n",
      "PROGRESS:\n"
     ]
    }
   ],
   "source": [
    "feature_set_neutral_ornot = ['tfidf','1gram features','2gram features','3gram features','vectors_neutral_ornot']\n",
    "model_neutral_ornot =graphlab.logistic_classifier.create(train_data_positive_ornot,\n",
    "                                                     target='NeutralorNot',\n",
    "                                                     features=feature_set_neutral_ornot,\n",
    "                                                     max_iterations=100,\n",
    "                                                     l1_penalty=0,\n",
    "                                                     l2_penalty=6000.0,\n",
    "                                                     class_weights = 'auto',\n",
    "                                                     validation_set=test_data_positive_ornot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Creating a validation set from 5 percent of training data. This may take a while.\n",
      "          You can set ``validation_set=None`` to disable validation tracking.\n",
      "\n",
      "PROGRESS: SVM:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 9008\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 256438\n",
      "PROGRESS: Number of coefficients    : 256439\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy | Validation-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | 1         | 3        | 0.000111  | 0.318217     | 0.913521          | 0.581633            |\n",
      "PROGRESS: | 2         | 5        | 1.000000  | 0.621414     | 0.995671          | 0.642857            |\n",
      "PROGRESS: | 3         | 6        | 1.000000  | 0.810541     | 0.998224          | 0.644898            |\n",
      "PROGRESS: | 4         | 7        | 1.000000  | 1.031687     | 0.998890          | 0.644898            |\n",
      "PROGRESS: | 5         | 8        | 1.000000  | 1.225817     | 0.999001          | 0.644898            |\n",
      "PROGRESS: | 6         | 9        | 1.000000  | 1.426951     | 0.999001          | 0.644898            |\n",
      "PROGRESS: | 10        | 14       | 1.000000  | 2.345565     | 0.999112          | 0.648980            |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n"
     ]
    }
   ],
   "source": [
    "feature_set_neutral_ornot = ['tfidf','1gram features','2gram features','3gram features','vectors_neutral_ornot']\n",
    "model_neutral_ornot2 =graphlab.svm_classifier.create(train_data_positive_ornot,\n",
    "                                                     target='NeutralorNot',\n",
    "                                                     penalty = 5,\n",
    "                                                     features=feature_set_neutral_ornot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------------+-------+--------------------+\n",
      "|     name    |         index          | class |       value        |\n",
      "+-------------+------------------------+-------+--------------------+\n",
      "| (intercept) |          None          |   1   |  -0.416915693605   |\n",
      "|    tfidf    |          sat.          |   1   | -0.000402498372747 |\n",
      "|    tfidf    |           on           |   1   |  0.0013792924027   |\n",
      "|    tfidf    |          hill          |   1   | -0.00168018345928  |\n",
      "|    tfidf    |           ga           |   1   | -0.00276972369808  |\n",
      "|    tfidf    |           to           |   1   | 0.000334328684012  |\n",
      "|    tfidf    |           by           |   1   | -3.7489156981e-05  |\n",
      "|    tfidf    |           i            |   1   | -0.00382513275871  |\n",
      "|    tfidf    |         chapel         |   1   | -0.000885496207379 |\n",
      "|    tfidf    |           my           |   1   | -0.00412605603479  |\n",
      "|    tfidf    |          hit           |   1   |  -0.0013032830852  |\n",
      "|    tfidf    |           go           |   1   | 0.000987382486947  |\n",
      "|    tfidf    |          hous          |   1   |  0.00159045793424  |\n",
      "|    tfidf    |       $num.$num        |   1   |  0.00158052194333  |\n",
      "|    tfidf    |           :)           |   1   |  -0.0118104691026  |\n",
      "|    tfidf    |           m            |   1   | -0.00164587696369  |\n",
      "|    tfidf    |          out)          |   1   |  0.00149426995155  |\n",
      "|    tfidf    |          find          |   1   |  0.00265150699301  |\n",
      "|    tfidf    |          deal          |   1   | -0.000543976780407 |\n",
      "|    tfidf    |         gener          |   1   |  0.00159532384388  |\n",
      "|    tfidf    |          talk          |   1   |  0.00396687001667  |\n",
      "|    tfidf    |           t            |   1   | -0.00563902305359  |\n",
      "|    tfidf    |          can           |   1   | -0.00408655928675  |\n",
      "|    tfidf    |          dome          |   1   | -0.00679097340457  |\n",
      "|    tfidf    |         israel         |   1   | -0.00222857711494  |\n",
      "|    tfidf    |           po           |   1   |  -0.014547566314   |\n",
      "|    tfidf    |          say           |   1   |  0.00119049437407  |\n",
      "|    tfidf    |           s            |   1   | -0.00061403819476  |\n",
      "|    tfidf    |        iranian         |   1   | -0.00670065516718  |\n",
      "|    tfidf    |          with          |   1   | 0.000320771324027  |\n",
      "|    tfidf    |         missil         |   1   | -0.00628891473731  |\n",
      "|    tfidf    |           we           |   1   | -0.00110193257299  |\n",
      "|    tfidf    |         their          |   1   |  0.0030541704326   |\n",
      "|    tfidf    |          that          |   1   | -0.000640069802079 |\n",
      "|    tfidf    |          and           |   1   | -0.000787687318252 |\n",
      "|    tfidf    |           up           |   1   | -0.000578577387075 |\n",
      "|    tfidf    |          iron          |   1   |  0.00534976870179  |\n",
      "|    tfidf    |          may           |   1   | 9.67725929179e-05  |\n",
      "|    tfidf    |         (keep          |   1   |  0.00239701649169  |\n",
      "|    tfidf    |          end           |   1   |  0.00217126122394  |\n",
      "|    tfidf    |       tomorrow.        |   1   | -0.00128777827505  |\n",
      "|    tfidf    |           of           |   1   | 0.000155215088602  |\n",
      "|    tfidf    |          week          |   1   | -0.000941235375265 |\n",
      "|    tfidf    |           a            |   1   | -0.00238999792173  |\n",
      "|    tfidf    |           an           |   1   | -0.000685262002086 |\n",
      "|    tfidf    |           j            |   1   | -0.00153504158025  |\n",
      "|    tfidf    |           it           |   1   | -0.00505191750293  |\n",
      "|    tfidf    |         davlar         |   1   | -0.00582622223326  |\n",
      "|    tfidf    |        $numth.         |   1   | -0.00313447185664  |\n",
      "|    tfidf    |         rival          |   1   | -0.000451676357991 |\n",
      "|    tfidf    |         train          |   1   |  0.00535873030231  |\n",
      "|    tfidf    |          main          |   1   |  0.00107947720902  |\n",
      "|    tfidf    |          team          |   1   |  0.00279997904459  |\n",
      "|    tfidf    |          make          |   1   | -0.000920527478087 |\n",
      "|    tfidf    |          are           |   1   | 0.000184275274704  |\n",
      "|    tfidf    |        poland.         |   1   | -0.00582622223326  |\n",
      "|    tfidf    |          hope          |   1   | -0.00642391797857  |\n",
      "|    tfidf    |           me           |   1   | -0.00173422084156  |\n",
      "|    tfidf    |           ne           |   1   | -0.00423964566543  |\n",
      "|    tfidf    |        everyth         |   1   | -0.00409424701268  |\n",
      "|    tfidf    |        colleg,         |   1   | -0.00692772236552  |\n",
      "|    tfidf    |         appli          |   1   | 0.000247937797708  |\n",
      "|    tfidf    |          sat           |   1   |  0.0005608102932   |\n",
      "|    tfidf    |         about          |   1   | -0.000411305712195 |\n",
      "|    tfidf    |         colleg         |   1   |  0.00148872504455  |\n",
      "|    tfidf    |          act           |   1   | -0.00440313015094  |\n",
      "|    tfidf    |          amp;          |   1   | -0.00139958747193  |\n",
      "|    tfidf    |           s,           |   1   | -0.00838191046584  |\n",
      "|    tfidf    |         decid          |   1   | -0.00375206015201  |\n",
      "|    tfidf    |          out.          |   1   |  0.00317233654224  |\n",
      "|    tfidf    |          want          |   1   |  0.00113259403799  |\n",
      "|    tfidf    |          user          |   1   | -0.00072363699537  |\n",
      "|    tfidf    |         owner.         |   1   | -0.00808220779735  |\n",
      "|    tfidf    |       superbowl.       |   1   |  0.00260965850664  |\n",
      "|    tfidf    |          they          |   1   |  0.00116142428399  |\n",
      "|    tfidf    |           at           |   1   | 0.000866363306554  |\n",
      "|    tfidf    |          not           |   1   | -0.00294063240516  |\n",
      "|    tfidf    |          ain           |   1   | 0.000923572836346  |\n",
      "|    tfidf    |          have          |   1   | -0.000987135909737 |\n",
      "|    tfidf    |      quarterback       |   1   | -0.00808220779735  |\n",
      "|    tfidf    |          but           |   1   | -0.00111047229165  |\n",
      "|    tfidf    |       superbowl        |   1   |  0.00208621405402  |\n",
      "|    tfidf    |           in           |   1   |  0.00318373581829  |\n",
      "|    tfidf    |         dalla,         |   1   | -0.00808220779735  |\n",
      "|    tfidf    |         dalla          |   1   | -0.00106741415459  |\n",
      "|    tfidf    |         squich         |   1   |  0.0160837783356   |\n",
      "|    tfidf    |          get           |   1   | 9.82180527907e-05  |\n",
      "|    tfidf    |         doesn          |   1   | 0.000100881135958  |\n",
      "|    tfidf    |          the           |   1   |  0.00175774332388  |\n",
      "|    tfidf    |         candi          |   1   | -0.00112889333063  |\n",
      "|    tfidf    |           im           |   1   | -0.000882766537652 |\n",
      "|    tfidf    |         bring          |   1   | 0.000686612302302  |\n",
      "|    tfidf    |          load          |   1   | 0.000584160971946  |\n",
      "|    tfidf    |       tomorrow,        |   1   |  0.00601532684811  |\n",
      "|    tfidf    |          all           |   1   | -0.00561411789577  |\n",
      "|    tfidf    |          just          |   1   | -0.00157553012374  |\n",
      "|    tfidf    |        heads...        |   1   |  0.00737722863761  |\n",
      "|    tfidf    |          cook          |   1   | -0.00131918057284  |\n",
      "|    tfidf    |          ceo           |   1   | 0.000266972418362  |\n",
      "|    tfidf    |         monday         |   1   | 0.000122047622148  |\n",
      "|    tfidf    |          appl          |   1   | -0.000711626344808 |\n",
      "|    tfidf    |          url           |   1   |  0.00468334494149  |\n",
      "|    tfidf    |       software,        |   1   |  0.00861631198737  |\n",
      "|    tfidf    |          tim           |   1   | -0.000923306760597 |\n",
      "|    tfidf    |         retail         |   1   |  0.00256902850127  |\n",
      "|    tfidf    |         chief          |   1   |  0.00389196187867  |\n",
      "|    tfidf    |          out           |   1   | 0.000821738389534  |\n",
      "|    tfidf    |       overhaul:        |   1   |  0.00861631198737  |\n",
      "|    tfidf    |         replac         |   1   |  0.00216522846109  |\n",
      "|    tfidf    |          san           |   1   |  0.0040833328535   |\n",
      "|    tfidf    |       francisco        |   1   |  0.0105049923327   |\n",
      "|    tfidf    |          inc           |   1   |  0.00487212621191  |\n",
      "|    tfidf    |          sun           |   1   | -0.00105845775713  |\n",
      "|    tfidf    |        $nums??         |   1   | -0.00798756625488  |\n",
      "|    tfidf    |          from          |   1   |  0.00484893141828  |\n",
      "|    tfidf    |          nta           |   1   |  0.00341775938576  |\n",
      "|    tfidf    |          her           |   1   | -0.000688011127996 |\n",
      "|    tfidf    |        sridevi         |   1   | -0.00798756625488  |\n",
      "|    tfidf    |          morn          |   1   | -0.00665498838256  |\n",
      "|    tfidf    |         rememb         |   1   | -0.00196944667669  |\n",
      "|    tfidf    |         watch          |   1   | -0.00179272826266  |\n",
      "|    tfidf    |           ;)           |   1   | -0.00631302811787  |\n",
      "|    tfidf    |      comeback....      |   1   | -0.00798756625488  |\n",
      "|    tfidf    |           u            |   1   | -0.00193156089382  |\n",
      "|    tfidf    |    livewirathletics    |   1   |  0.0080993753653   |\n",
      "|    tfidf    |          play          |   1   |  0.00359449529076  |\n",
      "|    tfidf    |          set           |   1   |  0.00479268876141  |\n",
      "|    tfidf    |        livewir         |   1   |  0.0080993753653   |\n",
      "|    tfidf    |         nadal          |   1   |  0.00277973882205  |\n",
      "|    tfidf    |        mexican         |   1   |  0.00448002004238  |\n",
      "|    tfidf    |          open          |   1   |  0.00449226402044  |\n",
      "|    tfidf    |          for           |   1   | -0.00255203235532  |\n",
      "|    tfidf    |        confirm         |   1   |  0.00814509981527  |\n",
      "|    tfidf    |         me...          |   1   |  0.00874596730864  |\n",
      "|    tfidf    |       february:        |   1   |  0.00926507399483  |\n",
      "|    tfidf    |         rafael         |   1   |  0.00464065841567  |\n",
      "|    tfidf    |           is           |   1   | -0.00308802476116  |\n",
      "|    tfidf    |          ask           |   1   |  0.00103604902207  |\n",
      "|    tfidf    |          ill           |   1   |  -0.001478949703   |\n",
      "|    tfidf    |          pop           |   1   | -0.00502996690871  |\n",
      "|    tfidf    |         up...          |   1   | 0.000182309084814  |\n",
      "|    tfidf    |         didnt          |   1   |  0.00912491512725  |\n",
      "|    tfidf    |          she           |   1   | -0.000328862984833 |\n",
      "|    tfidf    |          tell          |   1   | -7.24925143228e-05 |\n",
      "|    tfidf    |          next          |   1   | -0.000142972636416 |\n",
      "|    tfidf    |          yep           |   1   | -0.00345775651325  |\n",
      "|    tfidf    |          you           |   1   | -0.00229456597047  |\n",
      "|    tfidf    |         should         |   1   | 3.66927395619e-06  |\n",
      "|    tfidf    |         come..         |   1   | -0.00693721529855  |\n",
      "|    tfidf    |       wednesday        |   1   | 0.000558449620303  |\n",
      "|    tfidf    |         grammi         |   1   |  0.00133756147384  |\n",
      "|    tfidf    |          sing          |   1   | 0.000126339940718  |\n",
      "|    tfidf    |         could          |   1   | -0.00187767160452  |\n",
      "|    tfidf    |          big           |   1   | -0.00287694213985  |\n",
      "|    tfidf    |          mayb          |   1   | -0.00223135303805  |\n",
      "|    tfidf    |          true          |   1   | -0.00225986135463  |\n",
      "|    tfidf    |         becom          |   1   |  0.00329618400107  |\n",
      "|    tfidf    |         novemb         |   1   | 0.000656873001688  |\n",
      "|    tfidf    |         hmmmm          |   1   |  0.0047631429178   |\n",
      "|    tfidf    |         releas         |   1   | -0.000367481396247 |\n",
      "|    tfidf    |           if           |   1   |  0.0023480643178   |\n",
      "|    tfidf    |          date          |   1   | -0.00205148089734  |\n",
      "|    tfidf    |           --           |   1   |  0.00221247825644  |\n",
      "|    tfidf    |         $num,          |   1   |  0.00452249820077  |\n",
      "|    tfidf    |          oct           |   1   |  0.00484857630881  |\n",
      "|    tfidf    |          irna          |   1   |  0.0075132228544   |\n",
      "|    tfidf    |        tehran,         |   1   |  0.00408347199188  |\n",
      "|    tfidf    |          line          |   1   |  0.00147481403048  |\n",
      "|    tfidf    |      secretary...      |   1   |  0.0075132228544   |\n",
      "|    tfidf    |          iran          |   1   | -0.00020065065987  |\n",
      "|    tfidf    |          mko           |   1   |  0.0075132228544   |\n",
      "|    tfidf    |       campaign:        |   1   |  0.0075132228544   |\n",
      "|    tfidf    |         global         |   1   |  0.00123071710843  |\n",
      "|    tfidf    |         delist         |   1   |  0.0075132228544   |\n",
      "|    tfidf    |          list          |   1   | 0.000309658891904  |\n",
      "|    tfidf    |       terrorist        |   1   | -0.00220426483869  |\n",
      "|    tfidf    |        tomorrow        |   1   | -0.00224417168554  |\n",
      "|    tfidf    |        weather         |   1   | -0.00169743802616  |\n",
      "|    tfidf    |        tonight         |   1   | -0.00195895195074  |\n",
      "|    tfidf    |       $num-$num        |   1   |  0.00275245898393  |\n",
      "|    tfidf    |         chanc          |   1   | -0.00162163334253  |\n",
      "|    tfidf    |          leyt          |   1   |  0.00748390300639  |\n",
      "|    tfidf    |         cebu,          |   1   |  0.00899433829257  |\n",
      "|    tfidf    |          over          |   1   |  0.00155077780273  |\n",
      "|    tfidf    |          rain          |   1   | -0.000491404563215 |\n",
      "|    tfidf    |        visayas;        |   1   |  0.00748390300639  |\n",
      "|    tfidf    |           e.           |   1   | 0.000761836368617  |\n",
      "|    tfidf    |         expect         |   1   |  0.00153804215612  |\n",
      "|    tfidf    |      light-moder       |   1   |  0.00748390300639  |\n",
      "|    tfidf    |         samar          |   1   |  0.00748390300639  |\n",
      "|    tfidf    |         bohol,         |   1   |  0.00748390300639  |\n",
      "|    tfidf    |        against         |   1   |  0.00328595934367  |\n",
      "|    tfidf    |      championship      |   1   |  0.00181966528345  |\n",
      "|    tfidf    |          nfc           |   1   |  0.00215032716396  |\n",
      "|    tfidf    |        rematch         |   1   | -0.00383966895007  |\n",
      "|    tfidf    |          one           |   1   | -7.00357437735e-05 |\n",
      "|    tfidf    |         ticket         |   1   | 0.000435476331547  |\n",
      "|    tfidf    |          left          |   1   |  0.00279537212688  |\n",
      "|    tfidf    |           ny           |   1   |  0.00156108289214  |\n",
      "|    tfidf    |          game          |   1   |  0.00130182767399  |\n",
      "|    tfidf    |         giant          |   1   |  0.00139120636898  |\n",
      "|    tfidf    |          don           |   1   | -0.00373255286724  |\n",
      "|    tfidf    |         again          |   1   | -0.00266256877893  |\n",
      "|    tfidf    |      tomorrow...       |   1   |  -0.0104434304279  |\n",
      "|    tfidf    |         store          |   1   |  0.00140105093891  |\n",
      "|    tfidf    |           so           |   1   | -0.00454456742167  |\n",
      "|    tfidf    |          went          |   1   | 0.000744262406152  |\n",
      "|    tfidf    |         days?          |   1   | -0.00671556645436  |\n",
      "|    tfidf    |          whi           |   1   | -0.00252876774995  |\n",
      "|    tfidf    |         these          |   1   | -0.00167579938028  |\n",
      "|    tfidf    |          $num          |   1   |  0.00155856178919  |\n",
      "|    tfidf    |         search         |   1   |  0.00475313382655  |\n",
      "|    tfidf    |          will          |   1   |  0.00106762870336  |\n",
      "|    tfidf    |         castl          |   1   | -0.00364889904227  |\n",
      "|    tfidf    |         cover          |   1   |  0.00255879965777  |\n",
      "|    tfidf    |         issue.         |   1   | -0.00227179823637  |\n",
      "|    tfidf    |         none.          |   1   | -0.00671556645436  |\n",
      "|    tfidf    |         river          |   1   |  0.00448402504839  |\n",
      "|    tfidf    |         austin         |   1   | -4.92934100413e-05 |\n",
      "|    tfidf    |          see           |   1   |  -0.0055005436545  |\n",
      "|    tfidf    |         rather         |   1   | -0.00265838252374  |\n",
      "|    tfidf    |         would          |   1   | 0.000610254855591  |\n",
      "|    tfidf    |          alot          |   1   |  0.00346809366768  |\n",
      "|    tfidf    |          heat          |   1   | -0.000469706041283 |\n",
      "|    tfidf    |          cost          |   1   |  0.00505580489267  |\n",
      "|    tfidf    |       more...and       |   1   |  0.0119470205371   |\n",
      "|    tfidf    |          plu           |   1   | -0.00799162436392  |\n",
      "|    tfidf    |       getafterit       |   1   | -0.00640575727271  |\n",
      "|    tfidf    |           u,           |   1   | -0.00632023881748  |\n",
      "|    tfidf    |         anyth          |   1   | -0.00202028075668  |\n",
      "|    tfidf    |         never          |   1   | -0.00284103415666  |\n",
      "|    tfidf    |         start          |   1   | -0.00266908085111  |\n",
      "|    tfidf    |          mean          |   1   | -0.00175619594794  |\n",
      "|    tfidf    |          goal          |   1   | -0.00341751109988  |\n",
      "|    tfidf    |          work          |   1   | 2.05710090509e-06  |\n",
      "|    tfidf    |          your          |   1   |  -0.0025043121769  |\n",
      "|    tfidf    |          now           |   1   | 0.000671444640454  |\n",
      "|    tfidf    |         dream          |   1   |  -0.004582071665   |\n",
      "|    tfidf    | tomorrow......tomorrow |   1   | -0.00640575727271  |\n",
      "|    tfidf    |      comes....if       |   1   | -0.00640575727271  |\n",
      "|    tfidf    |          me.           |   1   | -0.00202221811862  |\n",
      "|    tfidf    |          vick          |   1   | -0.00446377192063  |\n",
      "|    tfidf    |          need          |   1   |  0.00146940727414  |\n",
      "|    tfidf    |         soon.          |   1   | -0.000268084934673 |\n",
      "|    tfidf    |          had           |   1   | -0.000743655860433 |\n",
      "|    tfidf    |        upgrade.        |   1   |  -0.0038555454491  |\n",
      "|    tfidf    |        flacco,         |   1   |  -0.0057409710073  |\n",
      "|    tfidf    |          back          |   1   | -4.54440128789e-05 |\n",
      "|    tfidf    |        benched,        |   1   |  -0.0057409710073  |\n",
      "|    tfidf    |          jen           |   1   |  0.00052883833366  |\n",
      "|    tfidf    |        thought         |   1   |  0.0021841684727   |\n",
      "|    tfidf    |           wa           |   1   | -0.00193106092509  |\n",
      "|    tfidf    |       yesterday.       |   1   |  0.00149432595056  |\n",
      "|    tfidf    |          much          |   1   | -0.00430074361487  |\n",
      "|    tfidf    |          too           |   1   | -0.00505553098306  |\n",
      "|    tfidf    |          andi          |   1   |  0.00261386687481  |\n",
      "|    tfidf    |         littl          |   1   |  0.00144248937049  |\n",
      "|    tfidf    |        android         |   1   |  0.00159695463705  |\n",
      "|    tfidf    |          look          |   1   | -0.00501558430686  |\n",
      "|    tfidf    |           do           |   1   |  0.0025291573258   |\n",
      "|    tfidf    |          ...           |   1   |  0.00435736005969  |\n",
      "|    tfidf    |          oooh          |   1   | -0.00160738675656  |\n",
      "|    tfidf    |         nikon          |   1   | -0.00329945426346  |\n",
      "|    tfidf    |           ti           |   1   | -0.00232795202171  |\n",
      "|    tfidf    |           ..           |   1   | -0.00173478873284  |\n",
      "|    tfidf    |          lake          |   1   | -0.000611980358529 |\n",
      "|    tfidf    |        vega,...        |   1   |  0.00899206173926  |\n",
      "|    tfidf    |          vega          |   1   |  0.00290024170426  |\n",
      "|    tfidf    |           la           |   1   | 0.000399821582494  |\n",
      "|    tfidf    |         black          |   1   | -0.000262930654285 |\n",
      "|    tfidf    |          save          |   1   | -0.00221023396612  |\n",
      "|    tfidf    |          huge          |   1   | -0.00363147587754  |\n",
      "|    tfidf    |         aerial         |   1   |  0.00899206173926  |\n",
      "|    tfidf    |         strip,         |   1   |  0.00899206173926  |\n",
      "|    tfidf    |          view          |   1   |  0.00121818608268  |\n",
      "|    tfidf    |         city,          |   1   |  0.00510534202802  |\n",
      "|    tfidf    |         vega,          |   1   |  0.00899206173926  |\n",
      "|    tfidf    |         friday         |   1   | -0.00368702339097  |\n",
      "|    tfidf    |          pari          |   1   |  0.00423789760991  |\n",
      "|    tfidf    |          alon          |   1   | -0.00573582429397  |\n",
      "|    tfidf    |        keiffer         |   1   | -0.00670205542695  |\n",
      "|    tfidf    |      lies,$numst       |   1   | -0.00670205542695  |\n",
      "|    tfidf    |         jenel          |   1   | -0.00283241393243  |\n",
      "|    tfidf    |          said          |   1   |  0.00540595654151  |\n",
      "|    tfidf    |         amp;th         |   1   | -0.00670205542695  |\n",
      "|    tfidf    |        hosp.now        |   1   | -0.00670205542695  |\n",
      "|    tfidf    |       tomorrow?        |   1   |  0.0111305500957   |\n",
      "|    tfidf    |          clas          |   1   | -0.00940591151008  |\n",
      "|    tfidf    |         lmfao          |   1   | -0.00548748635635  |\n",
      "|    tfidf    |           as           |   1   | -0.00744258131652  |\n",
      "|    tfidf    |        nerves,         |   1   | -0.00975403277008  |\n",
      "|    tfidf    |           hi           |   1   | 8.57155882304e-06  |\n",
      "|    tfidf    |         tiger          |   1   |  0.00230472232934  |\n",
      "|    tfidf    |          got           |   1   | -0.00125446528919  |\n",
      "|    tfidf    |         royal          |   1   |  0.00903752681632  |\n",
      "|    tfidf    |          tag           |   1   |  0.0124843639644   |\n",
      "|    tfidf    |         avila          |   1   |  0.00824989091896  |\n",
      "|    tfidf    |          mon           |   1   | 0.000680647590069  |\n",
      "|    tfidf    |           c            |   1   | -0.00171428878824  |\n",
      "|    tfidf    |          guy           |   1   |  -0.0004359024522  |\n",
      "|    tfidf    |          kid           |   1   | -0.00186467065815  |\n",
      "|    tfidf    |          bill          |   1   |  0.00202805906171  |\n",
      "|    tfidf    |         murray         |   1   |  0.00257271458315  |\n",
      "|    tfidf    |        research        |   1   |  0.00450883642322  |\n",
      "|    tfidf    |       tenenbaum        |   1   |  0.00824989091896  |\n",
      "|    tfidf    |         sure,          |   1   | 0.000573806575176  |\n",
      "|    tfidf    |         think          |   1   | 0.000644159838002  |\n",
      "|    tfidf    |          boro          |   1   | -0.00158463132518  |\n",
      "|    tfidf    |    december/januari    |   1   |  -0.0081869433635  |\n",
      "|    tfidf    |          beat          |   1   | -0.00302029465117  |\n",
      "|    tfidf    |        swansea?        |   1   |  -0.0081869433635  |\n",
      "|    tfidf    |         volar          |   1   | -0.000940318955518 |\n",
      "|    tfidf    |         1959,          |   1   |  0.00672168894598  |\n",
      "|    tfidf    |        awards,         |   1   |  0.00292717620543  |\n",
      "|    tfidf    |          held          |   1   |  0.00148786541901  |\n",
      "|    tfidf    |         frank          |   1   |  0.00152592666747  |\n",
      "|    tfidf    |           4            |   1   | -0.00218241594077  |\n",
      "|    tfidf    |         first          |   1   | 0.000808060857307  |\n",
      "|    tfidf    |        sinatra         |   1   |  0.00793583780446  |\n",
      "|    tfidf    |        domenico        |   1   |  0.00672168894598  |\n",
      "|    tfidf    |        modugno         |   1   |  0.00672168894598  |\n",
      "|    tfidf    |       year,with        |   1   |  0.00672168894598  |\n",
      "|    tfidf    |         record         |   1   |  0.00122874323411  |\n",
      "|    tfidf    |          lee           |   1   |  0.00288511616319  |\n",
      "|    tfidf    |         peggi          |   1   |  0.00672168894598  |\n",
      "|    tfidf    |        jennett         |   1   | -0.00802874486054  |\n",
      "|    tfidf    |          omg           |   1   | -0.00792280581523  |\n",
      "|    tfidf    |           :d           |   1   |  -0.0110032612885  |\n",
      "|    tfidf    |         gonna          |   1   | -0.00243554590163  |\n",
      "|    tfidf    |         friend         |   1   | -0.00252145247545  |\n",
      "|    tfidf    |        lt;$num         |   1   | -0.00558470466475  |\n",
      "|    tfidf    |          did?          |   1   | -0.00680880759452  |\n",
      "|    tfidf    |          day           |   1   | -0.00473755906457  |\n",
      "|    tfidf    |         studi          |   1   | -2.55158227534e-05 |\n",
      "|    tfidf    |          del           |   1   | -0.00053873839998  |\n",
      "|    tfidf    |         twitit         |   1   | -0.00806378876047  |\n",
      "|    tfidf    |         mcfli          |   1   | -0.00257399766909  |\n",
      "|    tfidf    |          come          |   1   | -0.00165089883785  |\n",
      "|    tfidf    |         plata          |   1   | -0.00806378876047  |\n",
      "|    tfidf    |       argentina        |   1   | -0.00261658492651  |\n",
      "|    tfidf    |          thi           |   1   | -0.00212820610316  |\n",
      "|    tfidf    |          time          |   1   | -0.00328389129108  |\n",
      "|    tfidf    |          lol           |   1   | -0.00493470031904  |\n",
      "|    tfidf    |       sag...yea        |   1   | -0.00484219583969  |\n",
      "|    tfidf    |         thank          |   1   |  -0.0106685745364  |\n",
      "|    tfidf    |          venu          |   1   |  0.00469441942544  |\n",
      "|    tfidf    |          also          |   1   | -0.000919319034648 |\n",
      "|    tfidf    |          fair          |   1   | 3.52504461494e-05  |\n",
      "|    tfidf    |        rising.         |   1   | -0.00484219583969  |\n",
      "|    tfidf    |         aspect         |   1   | -0.00598004399474  |\n",
      "|    tfidf    |       anything.        |   1   | -0.00484219583969  |\n",
      "|    tfidf    |         wonder         |   1   | 0.000234451278626  |\n",
      "|    tfidf    |          btwn          |   1   | -0.00484219583969  |\n",
      "|    tfidf    |        sun/moon        |   1   | -0.00484219583969  |\n",
      "|    tfidf    |           w/           |   1   |  0.00286774224912  |\n",
      "|    tfidf    |          afc           |   1   |  0.00184112379441  |\n",
      "|    tfidf    |          td.           |   1   |  0.00748643109385  |\n",
      "|    tfidf    |         bronco         |   1   | -0.00118744676324  |\n",
      "|    tfidf    |         peyton         |   1   |  0.00013613823082  |\n",
      "|    tfidf    |         second         |   1   |  0.00178969385615  |\n",
      "|    tfidf    |          tie           |   1   |  0.00896200032636  |\n",
      "|    tfidf    |         month.         |   1   |  0.00356369203716  |\n",
      "|    tfidf    |         player         |   1   |  0.00139932774136  |\n",
      "|    tfidf    |         offens         |   1   |  0.00492482758928  |\n",
      "|    tfidf    |          man           |   1   | -0.00123286027077  |\n",
      "|    tfidf    |          name          |   1   | 0.000978778415678  |\n",
      "|    tfidf    |         $numth         |   1   | -0.000185353502993 |\n",
      "|    tfidf    |         bradi          |   1   |  0.00325783246202  |\n",
      "|    tfidf    |          such          |   1   | -0.00574220483197  |\n",
      "|    tfidf    |         honor,         |   1   |  0.00748643109385  |\n",
      "|    tfidf    |          tom           |   1   |  0.00748896957433  |\n",
      "|    tfidf    |           ?            |   1   |  0.00147523088137  |\n",
      "|    tfidf    |         decemb         |   1   |  0.0054390808547   |\n",
      "|    tfidf    |        kendrick        |   1   | 0.000710847810368  |\n",
      "|    tfidf    |         lamar          |   1   | -0.000776960162183 |\n",
      "|    tfidf    |         school         |   1   | -0.00308353176993  |\n",
      "|    tfidf    |         phone          |   1   | -0.00589359585423  |\n",
      "|    tfidf    |       league,...       |   1   |  0.00817930180856  |\n",
      "|    tfidf    |        premier         |   1   | -0.00296271605431  |\n",
      "|    tfidf    |         place          |   1   |  0.00273152918188  |\n",
      "|    tfidf    |           -            |   1   |  0.0035285391231   |\n",
      "|    tfidf    |        current         |   1   |  0.00256934329898  |\n",
      "|    tfidf    |         wigan          |   1   |  0.00513248559277  |\n",
      "|    tfidf    |          site          |   1   |  0.00585294915934  |\n",
      "|    tfidf    |          andr          |   1   |  0.00571521842034  |\n",
      "|    tfidf    |          warn          |   1   | 6.33999769481e-05  |\n",
      "|    tfidf    |         occupi         |   1   |  0.0107358109558   |\n",
      "|    tfidf    |         offici         |   1   |  0.00208754462346  |\n",
      "|    tfidf    |         might          |   1   |  0.00162816543897  |\n",
      "|    tfidf    |          coy           |   1   |  0.00817930180856  |\n",
      "|    tfidf    |     alldayb$numtch     |   1   |  0.0140323566277   |\n",
      "|    tfidf    |         night.         |   1   | -0.00111177347802  |\n",
      "|    tfidf    |        concert         |   1   | -0.00307503388713  |\n",
      "|    tfidf    |        gallagh         |   1   |  0.00793466073824  |\n",
      "|    tfidf    |        enough,         |   1   |  0.0140323566277   |\n",
      "|    tfidf    |          noel          |   1   |  0.00436119930381  |\n",
      "|    tfidf    |        raleigh         |   1   |  0.0102902507621   |\n",
      "|    tfidf    |       scoreboard       |   1   | -0.000748141849412 |\n",
      "|    tfidf    |       wednesday,       |   1   |  0.00252700716101  |\n",
      "|    tfidf    |       homegrown        |   1   |  -0.0100818565138  |\n",
      "|    tfidf    |         video          |   1   | 0.000486700117159  |\n",
      "|    tfidf    |          miss          |   1   |  0.00107600478314  |\n",
      "|    tfidf    |          most          |   1   | -0.00232202404319  |\n",
      "|    tfidf    |          sign          |   1   |  0.00377449289127  |\n",
      "|    tfidf    |          day:          |   1   |  0.0027725896743   |\n",
      "|    tfidf    |       throughout       |   1   |  -0.0106626033458  |\n",
      "|    tfidf    |         pretti         |   1   | -0.00536241586285  |\n",
      "|    tfidf    |         plenti         |   1   |  0.00112780716175  |\n",
      "|    tfidf    |          fire          |   1   |  0.00188883834723  |\n",
      "|    tfidf    |         room.          |   1   | -0.00773154057797  |\n",
      "|    tfidf    |         stage          |   1   | -0.00246187695058  |\n",
      "|    tfidf    |         paul.          |   1   | -0.00731371180791  |\n",
      "|    tfidf    |          room          |   1   |  0.00275446358448  |\n",
      "|    tfidf    |          st.           |   1   |  0.00726376190578  |\n",
      "|    tfidf    |          here          |   1   | 0.000710187335144  |\n",
      "|    tfidf    |        center.         |   1   | -0.00788234876646  |\n",
      "|    tfidf    |           re           |   1   | -0.00136648948116  |\n",
      "|    tfidf    |          row           |   1   | -0.00170819759829  |\n",
      "|    tfidf    |         promo          |   1   |  0.00635049848119  |\n",
      "|    tfidf    |           be           |   1   | -0.00169376300771  |\n",
      "|    tfidf    |         shout          |   1   |  0.00353536612481  |\n",
      "|    tfidf    |         still          |   1   |  0.00159841277772  |\n",
      "|    tfidf    |  boro,cardiff,leicest  |   1   |  0.00870275891769  |\n",
      "|    tfidf    |         there          |   1   |  0.0034947230252   |\n",
      "|    tfidf    |          down          |   1   |  0.00139979385994  |\n",
      "|    tfidf    |          etc           |   1   |  0.00389741381799  |\n",
      "|    tfidf    |         until          |   1   | -0.00205918953604  |\n",
      "|    tfidf    |         may.i          |   1   |  0.00870275891769  |\n",
      "|    tfidf    |       all-madden       |   1   |  0.00809455499616  |\n",
      "|    tfidf    |         raven          |   1   | 0.000144938324306  |\n",
      "|    tfidf    |         brown          |   1   | -0.00164177114747  |\n",
      "|    tfidf    |           hc           |   1   |  0.00881211055782  |\n",
      "|    tfidf    |           k.           |   1   |  0.00874076211825  |\n",
      "|    tfidf    |          fri           |   1   |  0.00729124055717  |\n",
      "|    tfidf    |      connectedcar      |   1   |  0.00809455499616  |\n",
      "|    tfidf    |      madden$num,       |   1   |  0.00809455499616  |\n",
      "|    tfidf    |        xfacalac        |   1   |  0.00809455499616  |\n",
      "|    tfidf    |           (            |   1   | 0.000422966592113  |\n",
      "|    tfidf    |           :            |   1   | 0.000312887108515  |\n",
      "|    tfidf    |        oversea         |   1   | -0.00661519593905  |\n",
      "|    tfidf    |         rosann         |   1   | -0.00661519593905  |\n",
      "|    tfidf    |         sunday         |   1   | -0.000975472790287 |\n",
      "|    tfidf    |        restock         |   1   | -0.00661519593905  |\n",
      "|    tfidf    |       nooooooooo       |   1   |  -0.0055088906139  |\n",
      "|    tfidf    |           ve           |   1   | -0.000147189203581 |\n",
      "|    tfidf    |          them          |   1   | -0.00310489547143  |\n",
      "|    tfidf    |         oriol          |   1   |  0.00945060893486  |\n",
      "|    tfidf    |          wake          |   1   | -0.000193014442552 |\n",
      "|    tfidf    |         gotta          |   1   |  0.00347677531898  |\n",
      "|    tfidf    |       stayhungri       |   1   |  0.00945060893486  |\n",
      "|    tfidf    |         goal.          |   1   |  0.0107286576744   |\n",
      "|    tfidf    |         deeper         |   1   |  0.00420883703651  |\n",
      "|    tfidf    |         field          |   1   | -0.00185349455594  |\n",
      "|    tfidf    |          far           |   1   |  0.00140863710179  |\n",
      "|    tfidf    |          than          |   1   | -0.00578174348326  |\n",
      "|    tfidf    |         billi          |   1   |  0.00492557209173  |\n",
      "|    tfidf    |         forget         |   1   | -0.00550417571149  |\n",
      "|    tfidf    |        cundiff         |   1   |  0.00450688024476  |\n",
      "|    tfidf    |           (;           |   1   | -0.00571399743314  |\n",
      "|    tfidf    |         dawn:          |   1   |  -0.0123823879266  |\n",
      "|    tfidf    |        movies.         |   1   | -0.00585168755319  |\n",
      "|    tfidf    |          part          |   1   | 0.000367904294708  |\n",
      "|    tfidf    |         $num.          |   1   | -0.000858060531152 |\n",
      "|    tfidf    |          head          |   1   |  0.00154021801215  |\n",
      "|    tfidf    |         break          |   1   | -0.00109209216047  |\n",
      "|    tfidf    |          it.           |   1   | -0.00681677334729  |\n",
      "|    tfidf    |         after          |   1   | 0.000346408030373  |\n",
      "|    tfidf    |          vow,          |   1   |   0.016355490179   |\n",
      "|    tfidf    |         tweet          |   1   |  0.00468441999128  |\n",
      "|    tfidf    |        pennycan        |   1   | -0.00875591294809  |\n",
      "|    tfidf    |        station         |   1   | -0.00399900891449  |\n",
      "|    tfidf    |          air           |   1   |  0.00278995570624  |\n",
      "|    tfidf    |          same          |   1   |  0.00135058131934  |\n",
      "|    tfidf    |        januari         |   1   |  0.00374032462181  |\n",
      "|    tfidf    |          been          |   1   | -0.00136762657265  |\n",
      "|    tfidf    |         episod         |   1   | -0.00288569819826  |\n",
      "|    tfidf    |          town          |   1   | -0.00566436459174  |\n",
      "|    tfidf    |         cougar         |   1   | -0.00608879325959  |\n",
      "|    tfidf    |           ha           |   1   | 0.000549584784552  |\n",
      "|    tfidf    |        jordan.         |   1   | -0.00810858572069  |\n",
      "|    tfidf    |          new           |   1   | 0.000438668132774  |\n",
      "|    tfidf    |          us,           |   1   |  0.00469612500902  |\n",
      "|    tfidf    |        minutes.        |   1   | -0.00135367242833  |\n",
      "|    tfidf    |           he           |   1   | -0.000315297544051 |\n",
      "|    tfidf    |       thursday.        |   1   | -0.00160847150693  |\n",
      "|    tfidf    |        exhibit         |   1   | 0.000623585426041  |\n",
      "|    tfidf    |        kentucki        |   1   |  -0.0016848807783  |\n",
      "|    tfidf    |          dure          |   1   |  0.00336082792758  |\n",
      "|    tfidf    |          alex          |   1   | -0.00037335022413  |\n",
      "|    tfidf    |       poythress        |   1   |  0.00889151539937  |\n",
      "|    tfidf    |        rebound         |   1   | -0.000285041858913 |\n",
      "|    tfidf    |         point          |   1   |  0.00132615391661  |\n",
      "|    tfidf    |         debut          |   1   |  0.00331784739584  |\n",
      "|    tfidf    |         night          |   1   |  -0.0014520902086  |\n",
      "|    tfidf    |      collection.       |   1   | -0.00784265648229  |\n",
      "|    tfidf    |         rampl          |   1   | -0.00784265648229  |\n",
      "|    tfidf    |           wu           |   1   | -0.00492438129843  |\n",
      "|    tfidf    |         heart          |   1   | -0.00362481743805  |\n",
      "|    tfidf    |        charlott        |   1   |  -0.0102479067707  |\n",
      "|    tfidf    |         jason          |   1   |  0.00211147334413  |\n",
      "|    tfidf    |         ray...         |   1   |  0.0102383232435   |\n",
      "|    tfidf    |         equip          |   1   |  0.0110556723629   |\n",
      "|    tfidf    |          live          |   1   |  0.00247140575412  |\n",
      "|    tfidf    |           pm           |   1   |  0.00124257983199  |\n",
      "|    tfidf    |          cst           |   1   |  0.00124176371263  |\n",
      "|    tfidf    |        contacte        |   1   |  0.0102383232435   |\n",
      "|    tfidf    |        night...        |   1   |  0.00414389362751  |\n",
      "|    tfidf    |    ensemble/faculti    |   1   | -0.00857491982193  |\n",
      "|    tfidf    |         combo          |   1   | -0.00817496460649  |\n",
      "|    tfidf    |          tcu           |   1   |  0.00289288892935  |\n",
      "|    tfidf    |          amaz          |   1   |  -0.0104765083323  |\n",
      "|    tfidf    |          jazz          |   1   |  0.00355573398325  |\n",
      "|    tfidf    |         white          |   1   |  0.00501604559914  |\n",
      "|    tfidf    |          jim           |   1   |  0.00369179661846  |\n",
      "|    tfidf    |         shank          |   1   |  0.00926137917214  |\n",
      "|    tfidf    |         swore          |   1   |  0.00926137917214  |\n",
      "|    tfidf    |         coulda         |   1   |  0.00857668267767  |\n",
      "|    tfidf    |          it?           |   1   |  0.00128980473859  |\n",
      "|    tfidf    |         inning         |   1   | -0.00629019775185  |\n",
      "|    tfidf    |          saw           |   1   |   -0.00227193534   |\n",
      "|    tfidf    |         nation         |   1   | 0.000108929521171  |\n",
      "|    tfidf    |        redskin         |   1   |  0.00645752193127  |\n",
      "|    tfidf    |        quarter         |   1   | -0.00246617376135  |\n",
      "|    tfidf    |          didn          |   1   | -8.69329094409e-05 |\n",
      "|    tfidf    |         mound.         |   1   |  0.00857668267767  |\n",
      "|    tfidf    |         feel,          |   1   |  0.00857668267767  |\n",
      "|    tfidf    |         intend         |   1   |  0.00666153425449  |\n",
      "|    tfidf    |        thursday        |   1   |  0.0023742470317   |\n",
      "|    tfidf    |        capello         |   1   |  0.00335427663448  |\n",
      "|    tfidf    |         to...          |   1   |  0.00320246715709  |\n",
      "|    tfidf    |         russia         |   1   |  0.00122674889521  |\n",
      "|    tfidf    |         fabio          |   1   | -0.000889080022323 |\n",
      "|    tfidf    |         match:         |   1   |  0.00103545293538  |\n",
      "|    tfidf    |         manag          |   1   | -0.000989077486131 |\n",
      "|    tfidf    |        weekend         |   1   | -0.00239121664549  |\n",
      "|    tfidf    |           ev           |   1   | -0.00903965284344  |\n",
      "|    tfidf    |        kick-off        |   1   | -0.00283122579209  |\n",
      "|    tfidf    |           bu           |   1   |  0.00307344968922  |\n",
      "|    tfidf    |          trip          |   1   |  0.00639245038502  |\n",
      "|    tfidf    |         servic         |   1   |  0.00129658425628  |\n",
      "|    tfidf    |          boy           |   1   | -0.00374694584338  |\n",
      "|    tfidf    |          girl          |   1   | -0.00107836405097  |\n",
      "|    tfidf    |          club          |   1   |  0.00341194475583  |\n",
      "|    tfidf    |          goin          |   1   |  0.00339552560088  |\n",
      "|    tfidf    |           ur           |   1   |  0.00115835560706  |\n",
      "|    tfidf    |         extra          |   1   |  -0.0043400061533  |\n",
      "|    tfidf    |          job           |   1   | 0.000800542292485  |\n",
      "|    tfidf    |          good          |   1   |  -0.0110533276221  |\n",
      "|    tfidf    |         there.         |   1   | -0.00142959493139  |\n",
      "|    tfidf    |          jan           |   1   | -0.000695918877233 |\n",
      "|    tfidf    |        alright         |   1   | -0.00675296758498  |\n",
      "|    tfidf    |           y            |   1   | -0.00342137442045  |\n",
      "|    tfidf    |         spurs,         |   1   | -0.00705636385581  |\n",
      "|    tfidf    |         parker         |   1   | -0.00283748346989  |\n",
      "|    tfidf    |         life.          |   1   | -0.00936999511695  |\n",
      "|    tfidf    |          men           |   1   |  -0.0015563981889  |\n",
      "|    tfidf    |         desper         |   1   |  0.00314605977329  |\n",
      "|    tfidf    |       (februari        |   1   | -0.00154297485127  |\n",
      "|    tfidf    |        $numth)         |   1   | -0.00297346422168  |\n",
      "|    tfidf    |         known          |   1   | 0.000846086274892  |\n",
      "|    tfidf    |          day.          |   1   | -0.00149206976798  |\n",
      "|    tfidf    |         invas          |   1   |  0.00944295825368  |\n",
      "|    tfidf    |          call          |   1   |  0.00394148708899  |\n",
      "|    tfidf    |          mani          |   1   | -0.00445063296799  |\n",
      "|    tfidf    |         peopl          |   1   | -0.00115155716274  |\n",
      "|    tfidf    |        similar         |   1   | -0.00210462821434  |\n",
      "|    tfidf    |       australia        |   1   |  0.00923299067783  |\n",
      "|    tfidf    |        circumst        |   1   |  0.00874483756414  |\n",
      "|    tfidf    |       australia.       |   1   |  0.00874483756414  |\n",
      "|    tfidf    |       (reuters)        |   1   |  0.00640617327379  |\n",
      "|    tfidf    |          web           |   1   | -6.27782831259e-05 |\n",
      "|    tfidf    |         earli          |   1   | 0.000574515409147  |\n",
      "|    tfidf    |          damn          |   1   |  -0.0122125958926  |\n",
      "|    tfidf    |        saturday        |   1   |  0.00232528713469  |\n",
      "|    tfidf    |          tho           |   1   |  0.0005021102281   |\n",
      "|    tfidf    |         nigga          |   1   | -0.00254747262498  |\n",
      "|    tfidf    |         trynna         |   1   |  0.00175727387224  |\n",
      "|    tfidf    |         harri          |   1   | -0.00268836875881  |\n",
      "|    tfidf    |         style          |   1   |  0.00306495354882  |\n",
      "|    tfidf    |       monday,...       |   1   |  0.00720622105062  |\n",
      "|    tfidf    |          leah          |   1   |  0.00720622105062  |\n",
      "|    tfidf    |         kailyn         |   1   |  0.00720622105062  |\n",
      "|    tfidf    |        chelsea,        |   1   |  0.00720622105062  |\n",
      "|    tfidf    |         third          |   1   |  0.00434990935067  |\n",
      "|    tfidf    |         post:          |   1   |  0.00894932608889  |\n",
      "|    tfidf    |         season         |   1   | -0.00107105304634  |\n",
      "|    tfidf    |          mtv           |   1   |  0.0015853239488   |\n",
      "|    tfidf    |          teen          |   1   |  -0.0039618358382  |\n",
      "|    tfidf    |         return         |   1   |  0.00258143415066  |\n",
      "|    tfidf    |         intens         |   1   | -0.000457941370675 |\n",
      "|    tfidf    |          mom           |   1   | -0.00128423207678  |\n",
      "|    tfidf    |        jenelle,        |   1   |  0.00720622105062  |\n",
      "|    tfidf    |          blog          |   1   |  0.00234253791586  |\n",
      "|    tfidf    |           2            |   1   | 0.000574526524565  |\n",
      "|    tfidf    |          davo          |   1   |  0.00680190605835  |\n",
      "|    tfidf    |          busi          |   1   | 0.000116641959372  |\n",
      "|    tfidf    |          meet          |   1   |  0.00189660172158  |\n",
      "|    tfidf    |         leader         |   1   |  0.00483361975713  |\n",
      "|    tfidf    |         annual         |   1   |  0.00276926034383  |\n",
      "|    tfidf    |         award          |   1   | 6.14085577031e-05  |\n",
      "|    tfidf    |        ceremoni        |   1   | -0.00075532865499  |\n",
      "|    tfidf    |       itvfashion       |   1   |  -0.0120373928717  |\n",
      "|    tfidf    |          kyle          |   1   |  0.00342245012989  |\n",
      "|    tfidf    |          fit           |   1   | -0.000304214479356 |\n",
      "|    tfidf    |         jeremi         |   1   |  0.00206835884336  |\n",
      "|    tfidf    |          joe           |   1   |  0.00300041665723  |\n",
      "|    tfidf    |          gone          |   1   |  0.00352654220182  |\n",
      "|    tfidf    |          obma          |   1   | -0.00530020599177  |\n",
      "|    tfidf    |        predict         |   1   |  0.00240316219937  |\n",
      "|    tfidf    |         2unit          |   1   | -0.00530020599177  |\n",
      "|    tfidf    |    color,party,age,    |   1   | -0.00530020599177  |\n",
      "|    tfidf    |          sex,          |   1   | -0.00607952631843  |\n",
      "|    tfidf    |         nevada         |   1   |  0.00228132807529  |\n",
      "|    tfidf    |         $numst         |   1   | -0.000383224896433 |\n",
      "|    tfidf    |         divid          |   1   | -0.00180847130311  |\n",
      "|    tfidf    |         latino         |   1   |  0.00253456802578  |\n",
      "|    tfidf    |        governor        |   1   |  0.00374608439137  |\n",
      "|    tfidf    |          vote          |   1   |  0.00170783957022  |\n",
      "|    tfidf    |         wealth         |   1   | -0.00530020599177  |\n",
      "|    tfidf    |        romney,         |   1   | -0.00530020599177  |\n",
      "|    tfidf    |          gop           |   1   | -0.00123793940864  |\n",
      "|    tfidf    |        sendoff         |   1   | -0.00773065992843  |\n",
      "|    tfidf    |         final          |   1   | 0.000287597698593  |\n",
      "|    tfidf    |         given          |   1   | -0.000722699011006 |\n",
      "|    tfidf    |        halle...        |   1   | -0.00773065992843  |\n",
      "|    tfidf    |          pay           |   1   |  0.00401362285158  |\n",
      "|    tfidf    |        houston:        |   1   | -0.00773065992843  |\n",
      "|    tfidf    |          move          |   1   |  0.00245564488426  |\n",
      "|    tfidf    |         tribut         |   1   | -0.00389619668709  |\n",
      "|    tfidf    |        whitney         |   1   | -0.00288808838722  |\n",
      "|    tfidf    |        houston         |   1   |  -0.0046912561179  |\n",
      "|    tfidf    |         (cst)          |   1   |  0.00957320046123  |\n",
      "|    tfidf    |          nov           |   1   |  0.00862694607042  |\n",
      "|    tfidf    |          tune          |   1   | -0.00181829622926  |\n",
      "|    tfidf    |        announc         |   1   |  0.00117741171562  |\n",
      "|    tfidf    |         talli          |   1   |  0.00907246302653  |\n",
      "|    tfidf    |         commun         |   1   |  0.00302360200979  |\n",
      "|    tfidf    |         $numam         |   1   | -0.00145173501325  |\n",
      "|    tfidf    |      $num/$numc:       |   1   | -0.00944192879684  |\n",
      "|    tfidf    |         photo          |   1   | -0.00231697890471  |\n",
      "|    tfidf    |        peterson        |   1   |  0.00523409709133  |\n",
      "|    tfidf    |         befor          |   1   |  0.00232984977956  |\n",
      "|    tfidf    |        untouch         |   1   |  0.00323317442128  |\n",
      "|    tfidf    |           no           |   1   | -0.00247226220401  |\n",
      "|    tfidf    |          more          |   1   |  -0.0017700743953  |\n",
      "|    tfidf    |          drew          |   1   |  0.00563458350616  |\n",
      "|    tfidf    |         check          |   1   |  0.0026422845623   |\n",
      "|    tfidf    |          movi          |   1   | -0.00416449793421  |\n",
      "|    tfidf    |          pump          |   1   |  -0.0019048861962  |\n",
      "|    tfidf    |        preview:        |   1   |  0.00335042928146  |\n",
      "|    tfidf    |         woman          |   1   | -0.000626646346998 |\n",
      "|    tfidf    |       georgetown       |   1   | -0.00222804273655  |\n",
      "|    tfidf    |         inject         |   1   | -0.00877449662057  |\n",
      "|    tfidf    |         found          |   1   | 0.000172690962574  |\n",
      "|    tfidf    |          piec          |   1   | -0.00369860563335  |\n",
      "|    tfidf    |       something.       |   1   |  0.00202583810136  |\n",
      "|    tfidf    |         mo...          |   1   |  0.00115088824976  |\n",
      "|    tfidf    |       amp;$num;        |   1   |  0.0036006081282   |\n",
      "|    tfidf    |        compani         |   1   | -0.00497783170716  |\n",
      "|    tfidf    |          bos           |   1   |  0.00695500314781  |\n",
      "|    tfidf    |        shakeup         |   1   |  0.00978941888287  |\n",
      "|    tfidf    |         mobil          |   1   | -0.00183633187928  |\n",
      "|    tfidf    |          inc.          |   1   |  0.00276527434294  |\n",
      "|    tfidf    |          push          |   1   |  0.00529784648489  |\n",
      "|    tfidf    |        softwar         |   1   | -0.000416431386773 |\n",
      "|    tfidf    |          out:          |   1   | -0.000980771561797 |\n",
      "|    tfidf    |       saturday.        |   1   | -0.00107669082441  |\n",
      "|    tfidf    |          bra           |   1   |  0.00692574694395  |\n",
      "|    tfidf    |          leia          |   1   |  0.00671016994782  |\n",
      "|    tfidf    |        princess        |   1   | -0.00175490267464  |\n",
      "|    tfidf    |          lose          |   1   | 0.000713610380657  |\n",
      "|    tfidf    |           ie           |   1   |  0.00373796960367  |\n",
      "|    tfidf    |         glock          |   1   |  0.00671016994782  |\n",
      "|    tfidf    |          some          |   1   | 0.000186887894856  |\n",
      "|    tfidf    |         phish          |   1   |  0.00671016994782  |\n",
      "|    tfidf    |          jort          |   1   |  0.00671016994782  |\n",
      "|    tfidf    |       amp;tank.        |   1   |  0.00671016994782  |\n",
      "|    tfidf    |          port          |   1   | 0.000718998580228  |\n",
      "|    tfidf    |        myself,         |   1   |  0.00502338365251  |\n",
      "|    tfidf    |          app           |   1   | -0.000915188695161 |\n",
      "|    tfidf    |         that.          |   1   |  0.00354497211414  |\n",
      "|    tfidf    |        bring...        |   1   |  0.00860082511855  |\n",
      "|    tfidf    |        monday,         |   1   |  0.00408452757642  |\n",
      "|    tfidf    |         nexus:         |   1   |  0.00860082511855  |\n",
      "|    tfidf    |         galaxi         |   1   | -0.00378024782191  |\n",
      "|    tfidf    |         camera         |   1   |  0.00252390271302  |\n",
      "|    tfidf    |        introduc        |   1   |  0.00166544157488  |\n",
      "|    tfidf    |         sphere         |   1   |  0.00860082511855  |\n",
      "|    tfidf    |       wednesday.       |   1   | -0.00226960287702  |\n",
      "|    tfidf    |         anoth          |   1   | -0.00563206291528  |\n",
      "|    tfidf    |       pumpkin...       |   1   |  0.0109633588837   |\n",
      "|    tfidf    |         nope,          |   1   |  0.0109633588837   |\n",
      "|    tfidf    |        camden,         |   1   |  0.0109633588837   |\n",
      "|    tfidf    |          eat           |   1   | -0.00299914652171  |\n",
      "|    tfidf    |         greek          |   1   | 0.000190709951074  |\n",
      "|    tfidf    |          food          |   1   |  0.00135943860341  |\n",
      "|    tfidf    |         $num:          |   1   |  0.00798973376617  |\n",
      "|    tfidf    |           ca           |   1   |  0.00362315898999  |\n",
      "|    tfidf    |          faq:          |   1   |  0.00778227751029  |\n",
      "|    tfidf    |         march          |   1   | -0.00193325273159  |\n",
      "|    tfidf    |       skeuomorph       |   1   |  0.00778227751029  |\n",
      "|    tfidf    |       francisco,       |   1   |  0.00778227751029  |\n",
      "|    tfidf    |         design         |   1   |  0.00063277472949  |\n",
      "|    tfidf    |         execut         |   1   |  0.00879534741942  |\n",
      "|    tfidf    |         appl:          |   1   |  0.00778227751029  |\n",
      "|    tfidf    |         behind         |   1   |  0.00189831968907  |\n",
      "|    tfidf    |          year          |   1   | -0.00242393300516  |\n",
      "|    tfidf    |        busiest         |   1   |  0.0115197890224   |\n",
      "|    tfidf    |          day;          |   1   |  0.00700442152029  |\n",
      "|    tfidf    |       halloween        |   1   |  0.0024447964675   |\n",
      "|    tfidf    |        sunday.         |   1   | -0.000617384360507 |\n",
      "|    tfidf    |          papa          |   1   |  0.00241632714247  |\n",
      "|    tfidf    |          bowl          |   1   | 0.000641610619214  |\n",
      "|    tfidf    |          fact          |   1   | 0.000409137603037  |\n",
      "|    tfidf    |          john          |   1   |  0.00166212339183  |\n",
      "|    tfidf    |         super          |   1   | -0.000829520956432 |\n",
      "|    tfidf    |          man.          |   1   | -0.00442728535087  |\n",
      "|    tfidf    |          omg.          |   1   |  0.00216690672411  |\n",
      "|    tfidf    |         ricki          |   1   |  0.0033804306834   |\n",
      "|    tfidf    |         martin         |   1   | 0.000379529544969  |\n",
      "|    tfidf    |          gay           |   1   |  0.00172375370808  |\n",
      "|    tfidf    |         cavali         |   1   | -0.00691235938242  |\n",
      "|    tfidf    |        clipper         |   1   | -0.00696705106604  |\n",
      "|    tfidf    |          wish          |   1   | -0.00624166503023  |\n",
      "|    tfidf    |         still.         |   1   | -0.00782983382618  |\n",
      "|    tfidf    |          were          |   1   |  0.00102090641341  |\n",
      "|    tfidf    |         happen         |   1   |  0.00213664470379  |\n",
      "|    tfidf    |         ne...          |   1   | -0.00676293149758  |\n",
      "|    tfidf    |       professor?       |   1   | -0.00676293149758  |\n",
      "|    tfidf    |        musician        |   1   |  0.00085318199562  |\n",
      "|    tfidf    |       gallagher:       |   1   | -0.00676293149758  |\n",
      "|    tfidf    |          nes.          |   1   | -0.00676293149758  |\n",
      "|    tfidf    |         yeah,          |   1   | 0.000124682350411  |\n",
      "|    tfidf    |         brian          |   1   |  0.00236257469714  |\n",
      "|    tfidf    |       longchamp        |   1   |  0.0084616211362   |\n",
      "|    tfidf    |          prix          |   1   |  0.0084616211362   |\n",
      "|    tfidf    |          vow           |   1   |  0.00396597509339  |\n",
      "|    tfidf    |       royallieu        |   1   |  0.0084616211362   |\n",
      "|    tfidf    |          like          |   1   |  0.00441238122405  |\n",
      "|    tfidf    |           de           |   1   |  0.00407928845867  |\n",
      "|    tfidf    |        midsumm         |   1   |  0.0084616211362   |\n",
      "|    tfidf    |          babi          |   1   | -0.00351813318576  |\n",
      "|    tfidf    |          asia          |   1   |  0.00640805432129  |\n",
      "|    tfidf    |          hell          |   1   |  -0.0036345450346  |\n",
      "|    tfidf    |           ,            |   1   | 4.35397494937e-05  |\n",
      "|    tfidf    |          yea           |   1   |  0.00808922136547  |\n",
      "|    tfidf    |         lmfaoo         |   1   |  -0.0111015267975  |\n",
      "|    tfidf    |        valentin        |   1   | -0.00428498520684  |\n",
      "|    tfidf    |      graduation?       |   1   |   0.012086797492   |\n",
      "|    tfidf    |         year,          |   1   | -0.000198750507799 |\n",
      "|    tfidf    |          then          |   1   | -0.000154315716572 |\n",
      "|    tfidf    |       already..        |   1   |  0.00110434747159  |\n",
      "|    tfidf    |        christma        |   1   |  0.00043630920229  |\n",
      "|    tfidf    |         soon,          |   1   |  0.00719412338524  |\n",
      "|    tfidf    |         mjtune         |   1   | -0.00871503946521  |\n",
      "|    tfidf    |          juli          |   1   | -0.000578020693782 |\n",
      "|    tfidf    |        wembley         |   1   | -0.00871503946521  |\n",
      "|    tfidf    |         (live          |   1   | -0.00871503946521  |\n",
      "|    tfidf    |        jackson         |   1   |  0.00556688171355  |\n",
      "|    tfidf    |        michael         |   1   |  0.00550179717142  |\n",
      "|    tfidf    |         l.o.v.         |   1   | -0.00871503946521  |\n",
      "|    tfidf    |        nowplay         |   1   |  0.00396358115187  |\n",
      "|    tfidf    |          life          |   1   | -0.00165200285247  |\n",
      "|    tfidf    |          spot          |   1   | -0.00297429296673  |\n",
      "|    tfidf    |           yo           |   1   |  0.00068588146274  |\n",
      "|    tfidf    |      seed...whil       |   1   | -0.00656228888596  |\n",
      "|    tfidf    |          near          |   1   |  0.00541354883243  |\n",
      "|    tfidf    |         laker          |   1   | -0.00935408408099  |\n",
      "|    tfidf    |        dick...w        |   1   | -0.00656228888596  |\n",
      "|    tfidf    |          gon           |   1   | -0.000758875510031 |\n",
      "|    tfidf    |         finish         |   1   | -6.10378433882e-05 |\n",
      "|    tfidf    |         sadat,         |   1   | -0.00723936449222  |\n",
      "|    tfidf    |          adt,          |   1   | -0.00670415613957  |\n",
      "|    tfidf    |        ghandi,         |   1   | -0.00670415613957  |\n",
      "|    tfidf    |         octob          |   1   |  0.00453683593615  |\n",
      "|    tfidf    |      principles..      |   1   | -0.00670415613957  |\n",
      "|    tfidf    |         them,          |   1   | -0.00222394414999  |\n",
      "|    tfidf    |          argu          |   1   | -0.00868670112215  |\n",
      "|    tfidf    |         them.          |   1   | -0.000592638012898 |\n",
      "|    tfidf    |         amber          |   1   | -0.00677649707802  |\n",
      "|    tfidf    |          news          |   1   |  0.00206439521803  |\n",
      "|    tfidf    |         reunit         |   1   | -0.00802664005408  |\n",
      "|    tfidf    |        barbara         |   1   | -0.00779696281273  |\n",
      "|    tfidf    |        myself.         |   1   |  0.00876399893376  |\n",
      "|    tfidf    |         combin         |   1   |  0.00242612775226  |\n",
      "|    tfidf    |         that,          |   1   | -0.000675599975467 |\n",
      "|    tfidf    |        februari        |   1   |  0.00487216262275  |\n",
      "|    tfidf    |        florida.        |   1   |  0.00165544686161  |\n",
      "|    tfidf    |          muse          |   1   | 0.000815425062833  |\n",
      "|    tfidf    |          two           |   1   |  0.0063465682136   |\n",
      "|    tfidf    |          rubi          |   1   |  0.00853495556683  |\n",
      "|    tfidf    |         realli         |   1   | -0.00511578223536  |\n",
      "|    tfidf    |         equal          |   1   |   -0.00249714773   |\n",
      "|    tfidf    |         prior,         |   1   |  0.00853495556683  |\n",
      "|    tfidf    |        friday.         |   1   |  0.00413337863994  |\n",
      "|    tfidf    |         welcom         |   1   | -0.000903269809473 |\n",
      "|    tfidf    |           )            |   1   | -0.00128612729528  |\n",
      "|    tfidf    |          nick          |   1   | -8.42466832582e-05 |\n",
      "|    tfidf    |         manila         |   1   |  -0.0137156162845  |\n",
      "|    tfidf    |        banner:         |   1   |  0.0164490285668   |\n",
      "|    tfidf    |       basketbal        |   1   |  0.00122003957744  |\n",
      "|    tfidf    |         rumbl          |   1   |  0.00647657482972  |\n",
      "|    tfidf    |          edit          |   1   |  0.00352206563841  |\n",
      "|    tfidf    |         battl          |   1   |  0.00243624876913  |\n",
      "|    tfidf    |  $numyearstaylorswift  |   1   | -0.00624759342591  |\n",
      "|    tfidf    |          acm           |   1   | -0.00624759342591  |\n",
      "|    tfidf    |         reign          |   1   | -0.00832311843573  |\n",
      "|    tfidf    |         album          |   1   | -0.000253153089682 |\n",
      "|    tfidf    |          red,          |   1   | -0.00674635331693  |\n",
      "|    tfidf    |       entertain        |   1   | -0.00208832197839  |\n",
      "|    tfidf    |          bmi           |   1   | -0.00624759342591  |\n",
      "|    tfidf    |           6            |   1   | 0.000658832603127  |\n",
      "|    tfidf    |   20millionswifties,   |   1   | -0.00624759342591  |\n",
      "|    tfidf    |         award,         |   1   | -0.00636126612341  |\n",
      "|    tfidf    |          4th           |   1   |  0.00211788163789  |\n",
      "|    tfidf    |         amas,          |   1   | -0.00624759342591  |\n",
      "|    tfidf    |         later          |   1   |   0.001839885959   |\n",
      "|    tfidf    |         sort).         |   1   |  0.00732730058169  |\n",
      "|    tfidf    |          (of           |   1   |  0.00732730058169  |\n",
      "|    tfidf    |         intern         |   1   |  0.00981908526153  |\n",
      "|    tfidf    |         human          |   1   |  0.00151640378197  |\n",
      "|    tfidf    |         smooch         |   1   |  0.00732730058169  |\n",
      "|    tfidf    |          kiss          |   1   |  0.00139870091924  |\n",
      "|    tfidf    |          nay?          |   1   |  0.00732730058169  |\n",
      "|    tfidf    |         month,         |   1   | 0.000169598989113  |\n",
      "|    tfidf    |         ca...          |   1   |  0.00783190895783  |\n",
      "|    tfidf    |         fido:          |   1   |  0.00732730058169  |\n",
      "|    tfidf    |           or           |   1   |  0.00368489990691  |\n",
      "|    tfidf    |         happi          |   1   |  -0.0138930358126  |\n",
      "|    tfidf    |        birthday        |   1   | -0.00775981571613  |\n",
      "|    tfidf    |        midnight        |   1   | -0.000344777259272 |\n",
      "|    tfidf    |          east          |   1   |  0.00271861065389  |\n",
      "|    tfidf    |         which          |   1   | -0.000314226840255 |\n",
      "|    tfidf    |         coast          |   1   | 0.000294514355616  |\n",
      "|    tfidf    |         map...         |   1   |  -0.0104693593839  |\n",
      "|    tfidf    |      error-ridden      |   1   | -0.00436388633377  |\n",
      "|    tfidf    |         apolog         |   1   | -0.00168595346363  |\n",
      "|    tfidf    |          app:          |   1   |  0.0111672430218   |\n",
      "|    tfidf    |          map           |   1   |  0.00343228015912  |\n",
      "|    tfidf    |          nba           |   1   |  0.00222511300875  |\n",
      "|    tfidf    |       playersnew       |   1   |  0.0087836245576   |\n",
      "|    tfidf    |         monta          |   1   |  0.00820376715658  |\n",
      "|    tfidf    |          elli          |   1   |  0.00875837136271  |\n",
      "|    tfidf    |           /            |   1   | -0.00364963876176  |\n",
      "|    tfidf    |          five          |   1   |  0.00150949004956  |\n",
      "|    tfidf    |         provid         |   1   |  0.00195454637684  |\n",
      "|    tfidf    |      $num-of-$num      |   1   |  0.00948484170289  |\n",
      "|    tfidf    |         shoot          |   1   | -0.000681289430416 |\n",
      "|    tfidf    |          care          |   1   | -0.00310036891201  |\n",
      "|    tfidf    |          take          |   1   | -0.000905342564591 |\n",
      "|    tfidf    |        school,         |   1   |  0.0015154464053   |\n",
      "|    tfidf    |        videos,         |   1   |  0.0094164573003   |\n",
      "|    tfidf    |         teach          |   1   |  0.00232491046108  |\n",
      "|    tfidf    |        itself.         |   1   | -0.000172400905956 |\n",
      "|    tfidf    |         spend          |   1   | -0.000770348965236 |\n",
      "|    tfidf    |          own           |   1   | 0.000852981669218  |\n",
      "|    tfidf    |         larri          |   1   |  0.00596497085215  |\n",
      "|    tfidf    |          bird          |   1   |  0.00261523707518  |\n",
      "|    tfidf    |          rest          |   1   | 0.000105739231444  |\n",
      "|    tfidf    |         th...          |   1   | -0.00132790721083  |\n",
      "|    tfidf    |         locker         |   1   |  0.00442894896874  |\n",
      "|    tfidf    |         amp;t          |   1   |   0.002155171199   |\n",
      "|    tfidf    |         offer          |   1   | 0.000988862983838  |\n",
      "|    tfidf    |         storag         |   1   |  0.0125333729751   |\n",
      "|    tfidf    |           io           |   1   |  0.00311360024884  |\n",
      "|    tfidf    |           s.           |   1   | -0.00254972381528  |\n",
      "|    tfidf    |      celebration.      |   1   | -0.000293389839475 |\n",
      "|    tfidf    |           dr           |   1   | -0.00380501313713  |\n",
      "|    tfidf    |      anniversari       |   1   | -0.00158817459767  |\n",
      "|    tfidf    |          king          |   1   | -0.00251381659986  |\n",
      "|    tfidf    |          our           |   1   | -0.00179661031317  |\n",
      "|    tfidf    |         shane          |   1   |  0.0114379452426   |\n",
      "|    tfidf    |          told          |   1   |  0.00185955726914  |\n",
      "|    tfidf    |      $numth-star       |   1   |  0.0114379452426   |\n",
      "|    tfidf    |         couldn         |   1   | -0.00460767202341  |\n",
      "|    tfidf    |        categori        |   1   |  0.00669248579885  |\n",
      "|    tfidf    |          skip          |   1   | -0.00281912108815  |\n",
      "|    tfidf    |          love          |   1   |  -0.010636988941   |\n",
      "|    tfidf    |         everi          |   1   |  0.0012246895911   |\n",
      "|    tfidf    |          give          |   1   | -0.00182700749913  |\n",
      "|    tfidf    |          half          |   1   |  0.00440891685119  |\n",
      "|    tfidf    |       everithing       |   1   | -0.00552187299006  |\n",
      "|    tfidf    |         besid          |   1   | -0.000955917595961 |\n",
      "|    tfidf    |           el           |   1   | -0.00117939800447  |\n",
      "|    tfidf    |         soaps.         |   1   | -0.00552187299006  |\n",
      "|    tfidf    |          off           |   1   | -0.00337370304718  |\n",
      "|    tfidf    |          lot           |   1   | 0.000767778183764  |\n",
      "|    tfidf    |          home          |   1   |  0.00159829983591  |\n",
      "|    tfidf    |          ray           |   1   |  0.0013819523821   |\n",
      "|    tfidf    |          onli          |   1   | 0.000890897501388  |\n",
      "|    tfidf    |          lewi          |   1   | -0.00379697594211  |\n",
      "|    tfidf    |        either.         |   1   | -0.00147096587446  |\n",
      "|    tfidf    |        jersey.         |   1   |  -0.0092457073125  |\n",
      "|    tfidf    |          york          |   1   |  0.00535386422561  |\n",
      "|    tfidf    |        giants:         |   1   |  0.0119388042512   |\n",
      "|    tfidf    |         $numnd         |   1   | 0.000522163002433  |\n",
      "|    tfidf    |      game-by-gam       |   1   |  0.0119388042512   |\n",
      "|    tfidf    |          doe           |   1   | 0.000102957075784  |\n",
      "|    tfidf    |         sound          |   1   | -0.00106778399629  |\n",
      "|    tfidf    |         queen          |   1   |  0.00416633584504  |\n",
      "|    tfidf    |          rock          |   1   | -0.00390341646036  |\n",
      "|    tfidf    |        visit...        |   1   |  0.0009474462788   |\n",
      "|    tfidf    |          seem          |   1   | 7.55354757589e-05  |\n",
      "|    tfidf    |          nash          |   1   |  0.00681082537986  |\n",
      "|    tfidf    |         along          |   1   | -0.00215174882851  |\n",
      "|    tfidf    |          done          |   1   | -0.00388664232039  |\n",
      "|    tfidf    |       thornton.        |   1   |  0.00943498630409  |\n",
      "|    tfidf    |  biggestdayoftheyear   |   1   | -0.00748161139684  |\n",
      "|    tfidf    |          ever          |   1   | -0.00522457309742  |\n",
      "|    tfidf    |         remind         |   1   | 0.000244233777933  |\n",
      "|    tfidf    |        forget?         |   1   | -0.00748161139684  |\n",
      "|    tfidf    |          text          |   1   |  0.00304555072015  |\n",
      "|    tfidf    |       tomorrow..       |   1   | -0.00605889925513  |\n",
      "|    tfidf    |          hade          |   1   | -0.00790831084116  |\n",
      "|    tfidf    |          :-)           |   1   |  -0.0046249411641  |\n",
      "|    tfidf    |          hero          |   1   | -0.00239116069174  |\n",
      "|    tfidf    |         entitl         |   1   | -0.000783458809876 |\n",
      "|    tfidf    |         olympu         |   1   |  0.00517555731853  |\n",
      "|    tfidf    |          wow           |   1   |  -0.0025127284003  |\n",
      "|    tfidf    |          book          |   1   |  0.00177580742793  |\n",
      "|    tfidf    |         perci          |   1   | 0.000699907225144  |\n",
      "|    tfidf    |          fan           |   1   | -0.00144778743149  |\n",
      "|    tfidf    |         africa         |   1   |  0.00322973805146  |\n",
      "|    tfidf    |          ahli          |   1   | -0.00758951859158  |\n",
      "|    tfidf    |         leagu          |   1   |  0.00328369345314  |\n",
      "|    tfidf    |          leg           |   1   |  0.00125456046834  |\n",
      "|    tfidf    |        without         |   1   |  -0.004153183155   |\n",
      "|    tfidf    |           al           |   1   | -0.00493433559842  |\n",
      "|    tfidf    |         esper          |   1   | -0.00758951859158  |\n",
      "|    tfidf    |          star          |   1   | 0.000560540707514  |\n",
      "|    tfidf    |        champion        |   1   |  0.00449297688981  |\n",
      "|    tfidf    |        youssef         |   1   | -0.00758951859158  |\n",
      "|    tfidf    |         msakni         |   1   | -0.00758951859158  |\n",
      "|    tfidf    |         legend         |   1   | -0.00555954186089  |\n",
      "|    tfidf    |        madonna.        |   1   | 0.000982373436362  |\n",
      "|    tfidf    |          sex           |   1   |   0.004441803339   |\n",
      "|    tfidf    |         canio          |   1   | -0.00780934013139  |\n",
      "|    tfidf    |         aston          |   1   |  0.00298681331495  |\n",
      "|    tfidf    |          sun:          |   1   |  0.00732818736351  |\n",
      "|    tfidf    |         villa          |   1   | 0.000410695169728  |\n",
      "|    tfidf    |           di           |   1   | 0.000592664355544  |\n",
      "|    tfidf    |         paolo          |   1   | -0.00780934013139  |\n",
      "|    tfidf    |          song          |   1   | -0.00181373864227  |\n",
      "|    tfidf    |          high          |   1   |  0.00114186803377  |\n",
      "|    tfidf    |         heard          |   1   | -0.000734636376295 |\n",
      "|    tfidf    |           ^^           |   1   | -0.00362483020539  |\n",
      "|    tfidf    |         haven          |   1   | -0.000436787503983 |\n",
      "|    tfidf    |       obviously,       |   1   | -0.00683002646374  |\n",
      "|    tfidf    |          even          |   1   | -0.00249032086281  |\n",
      "|    tfidf    |        ep,that         |   1   | -0.00683002646374  |\n",
      "|    tfidf    |           am           |   1   | 0.000279839497921  |\n",
      "|    tfidf    |         well,          |   1   | -0.000670507863853 |\n",
      "|    tfidf    |          1st           |   1   |  0.00564161737202  |\n",
      "|    tfidf    |        classic         |   1   | 0.000381774294696  |\n",
      "|    tfidf    |           yr           |   1   |  0.00247331613509  |\n",
      "|    tfidf    |        marsali         |   1   |  -0.0065086715916  |\n",
      "|    tfidf    |         fact:          |   1   | -0.00961559921021  |\n",
      "|    tfidf    |         artist         |   1   |  0.00136080253445  |\n",
      "|    tfidf    |        histori         |   1   | -0.00569579484634  |\n",
      "|    tfidf    |          both          |   1   | -0.00136965526013  |\n",
      "|    tfidf    |         repeat         |   1   | -0.000568670955875 |\n",
      "|    tfidf    |         thing          |   1   | -0.00275549957874  |\n",
      "|    tfidf    |         david          |   1   | -0.000965884296848 |\n",
      "|    tfidf    |       letterman        |   1   |  0.00261288580748  |\n",
      "|    tfidf    |        tonight.        |   1   | -0.00479742129313  |\n",
      "|    tfidf    |        audienc         |   1   | -0.00804068066327  |\n",
      "|    tfidf    |          seen          |   1   | -0.00221454889788  |\n",
      "|    tfidf    |         panel?         |   1   |  0.00919595610369  |\n",
      "|    tfidf    |         court          |   1   |  0.00207708681698  |\n",
      "|    tfidf    |          gov.          |   1   |  0.00408298546542  |\n",
      "|    tfidf    |         member         |   1   |  0.00327786833275  |\n",
      "|    tfidf    |          pick          |   1   |  0.00329071531834  |\n",
      "|    tfidf    |        indiana         |   1   |  0.0073966351032   |\n",
      "|    tfidf    |         daniel         |   1   | -0.000449372125085 |\n",
      "|    tfidf    |         suprem         |   1   |  0.0104165313435   |\n",
      "|    tfidf    |         $numrd         |   1   | -0.00018620649577  |\n",
      "|    tfidf    |         mitch          |   1   | 0.000465815561194  |\n",
      "|    tfidf    |         femal          |   1   | -0.00189584363511  |\n",
      "|    tfidf    |          a.m.          |   1   |  0.00316936525451  |\n",
      "|    tfidf    |         dress          |   1   | -0.000646686270653 |\n",
      "|    tfidf    |        two-ton         |   1   | -0.00649810797878  |\n",
      "|    tfidf    |         uniqu          |   1   | -0.00942829289977  |\n",
      "|    tfidf    |          grey          |   1   |  -0.0083670565339  |\n",
      "|    tfidf    |         yellow         |   1   | -0.00953828964185  |\n",
      "|    tfidf    |          gold          |   1   | 0.000328131679504  |\n",
      "|    tfidf    |          wed           |   1   | -0.00228479747003  |\n",
      "|    tfidf    |         puzzl          |   1   |  0.00534638910207  |\n",
      "|    tfidf    |         gu...          |   1   |  0.00722372202306  |\n",
      "|    tfidf    |         islam          |   1   |  0.00817593369224  |\n",
      "|    tfidf    |        command         |   1   |  0.00326798104214  |\n",
      "|    tfidf    |         senior         |   1   | 0.000235378316301  |\n",
      "|    tfidf    |        revolut         |   1   |  0.00115012162037  |\n",
      "|    tfidf    |          armi          |   1   | 8.33197764306e-05  |\n",
      "|    tfidf    |        harakah:        |   1   |  0.00722372202306  |\n",
      "|    tfidf    |        monitor         |   1   |  0.00822071640272  |\n",
      "|    tfidf    |          ship          |   1   | 0.000610811297084  |\n",
      "|    tfidf    |         gulf:          |   1   |  0.00760510259104  |\n",
      "|    tfidf    |          rang          |   1   |  0.00071630007018  |\n",
      "|    tfidf    |          long          |   1   | -0.00114165609843  |\n",
      "|    tfidf    |          edg           |   1   | -0.00514355965606  |\n",
      "+-------------+------------------------+-------+--------------------+\n",
      "[267998 rows x 4 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coefficients = model_neutral_ornot['coefficients'] \n",
    "\n",
    "coefficients.print_rows(num_rows=1000, num_columns=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Creating a validation set from 5 percent of training data. This may take a while.\n",
      "          You can set ``validation_set=None`` to disable validation tracking.\n",
      "\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 8992\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 5\n",
      "PROGRESS: Number of unpacked features : 255233\n",
      "PROGRESS: Number of coefficients    : 255234\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy | Validation-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | 1         | 3        | 0.000111  | 0.356241     | 0.919373          | 0.606719            |\n",
      "PROGRESS: | 2         | 6        | 0.500000  | 0.795532     | 0.994440          | 0.632411            |\n",
      "PROGRESS: | 3         | 7        | 0.500000  | 1.003671     | 0.996330          | 0.634387            |\n",
      "PROGRESS: | 4         | 8        | 0.500000  | 1.223818     | 0.997776          | 0.638340            |\n",
      "PROGRESS: | 5         | 9        | 0.500000  | 1.428954     | 0.998665          | 0.642292            |\n",
      "PROGRESS: | 6         | 10       | 0.500000  | 1.647099     | 0.998999          | 0.642292            |\n",
      "PROGRESS: | 10        | 14       | 0.500000  | 2.524683     | 0.999110          | 0.650198            |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n"
     ]
    }
   ],
   "source": [
    "feature_set_neutral_ornot = ['tfidf','1gram features','2gram features','3gram features','vectors_neutral_ornot']\n",
    "model_neutral_ornot3 =graphlab.sentiment_analysis.create(train_data_positive_ornot,\n",
    "                                                     target='NeutralorNot',\n",
    "                                                     features=feature_set_neutral_ornot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class                         : LogisticClassifier\n",
       "\n",
       "Schema\n",
       "------\n",
       "Number of coefficients        : 255234\n",
       "Number of examples            : 8992\n",
       "Number of classes             : 2\n",
       "Number of feature columns     : 5\n",
       "Number of unpacked features   : 255233\n",
       "\n",
       "Hyperparameters\n",
       "---------------\n",
       "L1 penalty                    : 0.0\n",
       "L2 penalty                    : 0.2\n",
       "\n",
       "Training Summary\n",
       "----------------\n",
       "Solver                        : auto\n",
       "Solver iterations             : 10\n",
       "Solver status                 : TERMINATED: Iteration limit reached.\n",
       "Training time (sec)           : 2.6177\n",
       "\n",
       "Settings\n",
       "--------\n",
       "Log-likelihood                : 15.0696\n",
       "\n",
       "Highest Positive Coefficients\n",
       "-----------------------------\n",
       "3gram features[someon see jurass]: 2.7662\n",
       "3gram features[be po all]     : 2.484\n",
       "3gram features[imaaa be po]   : 2.484\n",
       "3gram features[still go to]   : 2.3836\n",
       "3gram features[you still go]  : 2.3836\n",
       "\n",
       "Lowest Negative Coefficients\n",
       "----------------------------\n",
       "1gram features[wohooo]        : -1.9288\n",
       "2gram features[wohooo rt]     : -1.9288\n",
       "3gram features[wohooo rt at]  : -1.9288\n",
       "1gram features[yaaayi]        : -1.5458\n",
       "2gram features[yaaayi d]      : -1.5458"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_neutral_ornot3['classifier']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Creating the Arbitrer classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graphlab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cd310f127c05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#feature_set_pos_neg_neutral = ['tfidf','1gram features','2gram features','3gram features']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#feature_set_pos_neg_neutral = ['tfidf','1gram features','2gram features','3gram features','vectors_doc2vec_tweetsonly_dm']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m model_pos_neg_neutral =graphlab.classifier.create(train_data_positive_ornot,\n\u001b[0m\u001b[0;32m      6\u001b[0m                                                      \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Sentiment'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                                                      features=feature_set_pos_neg_neutral)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'graphlab' is not defined"
     ]
    }
   ],
   "source": [
    "#feature_set_pos_neg_neutral = ['tfidf','1gram features','2gram features','3gram features','vectors_neg_ornot','vectors_pos_neg','vectors_pos_neutral','vectors_neutral_neg','vectors_pos_ornot']\n",
    "feature_set_pos_neg_neutral = ['tfidf','1gram features','2gram features','3gram features','vectors_neg_ornot','vectors_pos_neg','vectors_pos_neutral','vectors_neutral_neg','vectors_pos_ornot','vectors_doc2vec_tweetsonly_dm']\n",
    "#feature_set_pos_neg_neutral = ['tfidf','1gram features','2gram features','3gram features']\n",
    "#feature_set_pos_neg_neutral = ['tfidf','1gram features','2gram features','3gram features','vectors_doc2vec_tweetsonly_dm']\n",
    "model_pos_neg_neutral =graphlab.classifier.create(train_data_positive_ornot,\n",
    "                                                     target='Sentiment',\n",
    "                                                     features=feature_set_pos_neg_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to oaabde01@louisville.edu and will expire on September 22, 2017.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: C:\\Users\\OMARAB~1\\AppData\\Local\\Temp\\graphlab_server_1482636821.log.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\OmarAbdelwahab\\local_test_sms_preprocessed.tsv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\OmarAbdelwahab\\local_test_sms_preprocessed.tsv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.036491 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.036491 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[long,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\OmarAbdelwahab\\local_test_sms_preprocessed.tsv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\OmarAbdelwahab\\local_test_sms_preprocessed.tsv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 2069 lines in 0.018499 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 2069 lines in 0.018499 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">ID</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Sentiment</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">10936</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">ye i am go from school<br>have class till $NUM can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">11051</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">can u tape the match for<br>me? i\\u$NUMl rush over ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">10966</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">too mani peopl at my hous<br>my rel are here a po ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">11211</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">yea i have spoken to him<br>liao. inde he is ne over ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">11350</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">haha... i want to see. e<br>macdonald here cheaper. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">10539</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">i\\u$NUMm go down now<br>liao\\u$NUMc with my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">10544</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">ya\\u$NUMc ok for me...<br>erm can let me know the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">11463</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">he told u i\\u$NUMm consid<br>liao mah. i duno\\u$NU ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">11062</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">will you be po on sunday<br>night? i just met ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">11035</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">tom\\u$NUMc u think it is<br>a relative\\u$NUM hous or ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[10 rows x 3 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tID\tint\n",
       "\tSentiment\tstr\n",
       "\tTweet\tstr\n",
       "\n",
       "Rows: 10\n",
       "\n",
       "Data:\n",
       "+-------+-----------+-------------------------------+\n",
       "|   ID  | Sentiment |             Tweet             |\n",
       "+-------+-----------+-------------------------------+\n",
       "| 10936 |  neutral  | ye i am go from school hav... |\n",
       "| 11051 |  neutral  | can u tape the match for m... |\n",
       "| 10966 |  neutral  | too mani peopl at my hous ... |\n",
       "| 11211 |  negative | yea i have spoken to him l... |\n",
       "| 11350 |  positive | haha... i want to see. e m... |\n",
       "| 10539 |  neutral  | i\\u$NUMm go down now liao\\... |\n",
       "| 10544 |  neutral  | ya\\u$NUMc ok for me... erm... |\n",
       "| 11463 |  negative | he told u i\\u$NUMm consid ... |\n",
       "| 11062 |  positive | will you be po on sunday n... |\n",
       "| 11035 |  neutral  | tom\\u$NUMc u think it is a... |\n",
       "+-------+-----------+-------------------------------+\n",
       "[10 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphlab\n",
    "test_tweets = graphlab.SFrame('local_test_sms_preprocessed.tsv')\n",
    "#test_tweets['1gram features'] = gl.text_analytics.count_ngrams(test_tweets['Tweet'], 1)\n",
    "#test_tweets['2gram features'] = gl.text_analytics.count_ngrams(test_tweets['Tweet'], 2)\n",
    "#test_tweets['3gram features'] = gl.text_analytics.count_ngrams(test_tweets['Tweet'], 3)\n",
    "test_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = DeepTextAnalyzer(pos_neg_model_w2v)\n",
    "test_tweets['vectors_pos_neg'] = test_tweets['Tweet'].apply(lambda p: dt.txt2avg_vector(p, is_html=False))\n",
    "dt = DeepTextAnalyzer(pos_neutral_model_w2v)\n",
    "test_tweets['vectors_pos_neutral'] = test_tweets['Tweet'].apply(lambda p: dt.txt2avg_vector(p, is_html=False))\n",
    "dt = DeepTextAnalyzer(neutral_neg_model_w2v)\n",
    "test_tweets['vectors_neutral_neg'] = test_tweets['Tweet'].apply(lambda p: dt.txt2avg_vector(p, is_html=False))\n",
    "#for the one label classifiers\n",
    "dt = DeepTextAnalyzer(positive_nonpositive_model_w2v)\n",
    "test_tweets['vectors_pos_ornot'] = test_tweets['Tweet'].apply(lambda p: dt.txt2avg_vector(p, is_html=False))\n",
    "dt = DeepTextAnalyzer(negative_nonnegative_model_w2v)\n",
    "test_tweets['vectors_neg_ornot'] = test_tweets['Tweet'].apply(lambda p: dt.txt2avg_vector(p, is_html=False))\n",
    "dt = DeepTextAnalyzer(neutral_nonneutral_model_w2v)\n",
    "test_tweets['vectors_neutral_ornot'] = test_tweets['Tweet'].apply(lambda p: dt.txt2avg_vector(p, is_html=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tweets['word_count'] = graphlab.text_analytics.count_words(test_tweets['Tweet'])\n",
    "tfidf = graphlab.text_analytics.tf_idf(test_tweets['word_count'])\n",
    "test_tweets['tfidf'] = tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tweets['pos_neg'] = model_pos_neg_svm.predict(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">ID</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Sentiment</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Tweet</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">1gram features</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">2gram features</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">10936</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">ye i am go from school<br>have class till $NUM can ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'am': 1L, 'num': 2L,<br>'at': 2L, 'have': 3L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'till num': 1L, 'mum<br>at': 1L, 'school have': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">11051</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">can u tape the match for<br>me? i\\u$NUMl rush over ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'me': 1L, 'rush': 1L,<br>'for': 1L, 'i': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'tape the': 1L, 'for<br>me': 1L, 'straight ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">10966</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">too mani peopl at my hous<br>my rel are here a po ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'are': 1L, 'a': 1L,<br>'china': 1L, 'hous': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'a po': 1L, 'besid the':<br>1L, 'are here': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">11211</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">yea i have spoken to him<br>liao. inde he is ne over ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'dat': 1L, 'yea': 1L,<br>'to': 1L, 'i': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'liao inde': 1L, 'him<br>liao': 1L, 'to him': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">11350</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">haha... i want to see. e<br>macdonald here cheaper. ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'e': 1L, 'i': 1L,<br>'haha': 1L, 'here': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'to see': 1L, 'haha i':<br>1L, 'want to': 1L, 'yum ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">10539</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">i\\u$NUMm go down now<br>liao\\u$NUMc with my ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'is': 2L, 'some': 1L,<br>'it': 1L, 'one': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'with my': 1L, 'numc<br>with': 1L, 'go around': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">10544</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">ya\\u$NUMc ok for me...<br>erm can let me know the ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'btw': 1L, 'is': 1L,<br>'times': 1L, 'for': 2L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'know the': 1L, 'numc<br>pay': 1L, 'numc per': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">11463</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">he told u i\\u$NUMm consid<br>liao mah. i duno\\u$NU ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'shop': 2L, 'right': 1L,<br>'dont': 1L, 'rest': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'out on': 1L, 's n': 1L,<br>'i dont': 1L, 'at num': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">11062</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">will you be po on sunday<br>night? i just met ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'play': 1L, 'just': 1L,<br>'it': 1L, 'chit': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'gather at': 1L, 'we<br>suggest': 1L, 'suggest ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">11035</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">tom\\u$NUMc u think it is<br>a relative\\u$NUM hous or ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'a': 1L, 'hous': 1L,<br>'is': 1L, 'numc': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'tom u': 1L, 'relative<br>u': 1L, 'a relative': ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "<table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">3gram features</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">vectors_pos_neg</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">vectors_pos_neutral</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">vectors_neutral_neg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'me at law': 1L, 'have<br>quit a': 1L, 'so we ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.035168338567,<br>0.00918153766543, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.00176338967867,<br>0.0283480659127, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0446561165154,<br>-0.0203523822129, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'me i u': 1L, 'tape the<br>match': 1L, 'straight ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0655306726694,<br>-0.0232573132962, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0400044322014,<br>0.188965007663, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0909014493227,<br>-0.0196762103587, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'peopl at my': 1L, 'my<br>hous my': 1L, 'besid the ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0238313488662,<br>-0.0928379893303, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0222482308745,<br>-0.082340657711, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.00548681616783,<br>-0.065991550684, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'to him liao': 1L, 'liao<br>inde he': 1L, 'inde he ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0501853302121,<br>0.0484661832452, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0661410614848,<br>0.0560490302742, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.00637151673436,<br>0.00969193037599, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'i want to': 1L, 'haha i<br>want': 1L, 'cheaper yum ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0554741024971,<br>0.0320442803204, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0445970743895,<br>-0.0969626754522, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0370185188949,<br>0.0234570447356, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'now liao u': 1L, 'with<br>my fren': 1L, 'go around ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.00725097814575,<br>0.0381912887096, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0788305178285,<br>0.0385205931962, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0726149454713,<br>-0.0875139832497, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'n location btw': 1L,<br>'the student is': 1L, 'u ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0402469374239,<br>0.00610201340169, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0535011850297,<br>0.0883706212044, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0691907554865,<br>-0.035958673805, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'prob i b': 1L, 'right<br>ok we': 1L, 'out on ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0140185765922,<br>0.00165138393641, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0074925548397,<br>0.0124946022406, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0226946771145,<br>-0.0185796283185, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'blah hope you': 1L,<br>'just met sebastian': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.000795624218881,<br>0.0187017787248, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0157801378518,<br>0.0595314726233, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0254035256803,<br>-0.000890672905371, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'tom u numc': 1L, 'a<br>relative u': 1L, 'u numc ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.159741789103,<br>-0.00901857297868, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.031462777406,<br>0.169567242265, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.120544791222,<br>-0.0883922800422, ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "<table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">vectors_pos_ornot</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">vectors_neg_ornot</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">vectors_neutral_ornot</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">word_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0177610144019,<br>-0.0272081196308, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0603038035333,<br>0.0327579751611, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.019504180178,<br>0.0110903726891, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'am': 1L, 'at': 2L,<br>'have': 3L, 'go': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0103400424123,<br>0.0560394711792, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0545724071562,<br>0.122312910855, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.132495060563,<br>0.118412099779, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'rush': 1L, 'over': 1L,<br>'for': 1L, 'straight': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0485919564962,<br>-0.0729125663638, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.00382145587355,<br>-0.00589061435312, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0173671506345,<br>0.0257357731462, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'are': 1L, 'a': 1L,<br>'china': 1L, 'hous': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0202737376094,<br>0.0294752325863, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0573082603514,<br>0.00301070511341, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0127499764785,<br>0.0314986221492, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'over': 1L, 'yea': 1L,<br>'to': 1L, 'i': 1L, 'is': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.227087289095,<br>-0.0895695909858, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.091937661171,<br>-0.0471357218921, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0447770729661,<br>0.105817034841, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'e': 1L, 'i': 1L,<br>'haha...': 1L, 'here': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.00991722289473,<br>-0.0898912623525, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.100801028311,<br>0.0298384632915, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0344792045653,<br>0.0650407224894, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'liao\\\\u$numc': 1L,<br>'is': 2L, 'some': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0295303631574,<br>0.0499828793108, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0110074914992,<br>0.0410201698542, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0079679004848,<br>0.110682018101, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'me...': 1L, 'is': 1L,<br>'n': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0503706298769,<br>-0.0279206950217, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0686047747731,<br>0.0160716176033, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.00229666987434,<br>0.0522280037403, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'shop': 2L, 'dont': 1L,<br>'$num.$num.': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0349769555032,<br>0.0188401527703, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.0667972266674,<br>0.0128983575851, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0362698584795,<br>0.0560548789799, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'just': 1L, 'play': 1L,<br>'chit': 1L, 'blah': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[-0.112383715808,<br>-0.032089073211, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.00156550353859,<br>0.135991349816, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[0.0973015502095,<br>0.155104324222, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'a': 1L, 'restauratn?':<br>1L, 'relative\\\\u$num': ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "<table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">tfidf</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">pos_neg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'am':<br>3.6835769591641156, ' ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'rush':<br>6.025382765311443, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'are':<br>2.9434727955163993, 'a': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'over':<br>5.149914027957543, 'y ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'e': 2.2782344030735304,<br>'i': 1.0015022444651664, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'liao\\\\u$numc':<br>4.801607333689327, 'is': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'me...':<br>5.149914027957543, 'is': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'shop':<br>8.831889705754685, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'just':<br>3.0197001609042835, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'a': 1.9410885389428434,<br>'restauratn?': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[10 rows x 15 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tID\tint\n",
       "\tSentiment\tstr\n",
       "\tTweet\tstr\n",
       "\t1gram features\tdict\n",
       "\t2gram features\tdict\n",
       "\t3gram features\tdict\n",
       "\tvectors_pos_neg\tarray\n",
       "\tvectors_pos_neutral\tarray\n",
       "\tvectors_neutral_neg\tarray\n",
       "\tvectors_pos_ornot\tarray\n",
       "\tvectors_neg_ornot\tarray\n",
       "\tvectors_neutral_ornot\tarray\n",
       "\tword_count\tdict\n",
       "\ttfidf\tdict\n",
       "\tpos_neg\tstr\n",
       "\n",
       "Rows: 10\n",
       "\n",
       "Data:\n",
       "+-------+-----------+-------------------------------+\n",
       "|   ID  | Sentiment |             Tweet             |\n",
       "+-------+-----------+-------------------------------+\n",
       "| 10936 |  neutral  | ye i am go from school hav... |\n",
       "| 11051 |  neutral  | can u tape the match for m... |\n",
       "| 10966 |  neutral  | too mani peopl at my hous ... |\n",
       "| 11211 |  negative | yea i have spoken to him l... |\n",
       "| 11350 |  positive | haha... i want to see. e m... |\n",
       "| 10539 |  neutral  | i\\u$NUMm go down now liao\\... |\n",
       "| 10544 |  neutral  | ya\\u$NUMc ok for me... erm... |\n",
       "| 11463 |  negative | he told u i\\u$NUMm consid ... |\n",
       "| 11062 |  positive | will you be po on sunday n... |\n",
       "| 11035 |  neutral  | tom\\u$NUMc u think it is a... |\n",
       "+-------+-----------+-------------------------------+\n",
       "+-------------------------------+-------------------------------+\n",
       "|         1gram features        |         2gram features        |\n",
       "+-------------------------------+-------------------------------+\n",
       "| {'am': 1L, 'num': 2L, 'at'... | {'till num': 1L, 'mum at':... |\n",
       "| {'me': 1L, 'rush': 1L, 'fo... | {'tape the': 1L, 'for me':... |\n",
       "| {'are': 1L, 'a': 1L, 'chin... | {'a po': 1L, 'besid the': ... |\n",
       "| {'dat': 1L, 'yea': 1L, 'to... | {'liao inde': 1L, 'him lia... |\n",
       "| {'e': 1L, 'i': 1L, 'haha':... | {'to see': 1L, 'haha i': 1... |\n",
       "| {'is': 2L, 'some': 1L, 'it... | {'with my': 1L, 'numc with... |\n",
       "| {'btw': 1L, 'is': 1L, 'tim... | {'know the': 1L, 'numc pay... |\n",
       "| {'shop': 2L, 'right': 1L, ... | {'out on': 1L, 's n': 1L, ... |\n",
       "| {'play': 1L, 'just': 1L, '... | {'gather at': 1L, 'we sugg... |\n",
       "| {'a': 1L, 'hous': 1L, 'is'... | {'tom u': 1L, 'relative u'... |\n",
       "+-------------------------------+-------------------------------+\n",
       "+-------------------------------+-------------------------------+\n",
       "|         3gram features        |        vectors_pos_neg        |\n",
       "+-------------------------------+-------------------------------+\n",
       "| {'me at law': 1L, 'have qu... | [-0.035168338567, 0.009181... |\n",
       "| {'me i u': 1L, 'tape the m... | [-0.0655306726694, -0.0232... |\n",
       "| {'peopl at my': 1L, 'my ho... | [-0.0238313488662, -0.0928... |\n",
       "| {'to him liao': 1L, 'liao ... | [0.0501853302121, 0.048466... |\n",
       "| {'i want to': 1L, 'haha i ... | [0.0554741024971, 0.032044... |\n",
       "| {'now liao u': 1L, 'with m... | [-0.00725097814575, 0.0381... |\n",
       "| {'n location btw': 1L, 'th... | [-0.0402469374239, 0.00610... |\n",
       "| {'prob i b': 1L, 'right ok... | [0.0140185765922, 0.001651... |\n",
       "| {'blah hope you': 1L, 'jus... | [0.000795624218881, 0.0187... |\n",
       "| {'tom u numc': 1L, 'a rela... | [-0.159741789103, -0.00901... |\n",
       "+-------------------------------+-------------------------------+\n",
       "+-------------------------------+-------------------------------+\n",
       "|      vectors_pos_neutral      |      vectors_neutral_neg      |\n",
       "+-------------------------------+-------------------------------+\n",
       "| [-0.00176338967867, 0.0283... | [-0.0446561165154, -0.0203... |\n",
       "| [-0.0400044322014, 0.18896... | [-0.0909014493227, -0.0196... |\n",
       "| [0.0222482308745, -0.08234... | [0.00548681616783, -0.0659... |\n",
       "| [-0.0661410614848, 0.05604... | [0.00637151673436, 0.00969... |\n",
       "| [-0.0445970743895, -0.0969... | [-0.0370185188949, 0.02345... |\n",
       "| [-0.0788305178285, 0.03852... | [-0.0726149454713, -0.0875... |\n",
       "| [0.0535011850297, 0.088370... | [-0.0691907554865, -0.0359... |\n",
       "| [-0.0074925548397, 0.01249... | [-0.0226946771145, -0.0185... |\n",
       "| [-0.0157801378518, 0.05953... | [0.0254035256803, -0.00089... |\n",
       "| [0.031462777406, 0.1695672... | [-0.120544791222, -0.08839... |\n",
       "+-------------------------------+-------------------------------+\n",
       "+-------------------------------+-------------------------------+\n",
       "|       vectors_pos_ornot       |       vectors_neg_ornot       |\n",
       "+-------------------------------+-------------------------------+\n",
       "| [0.0177610144019, -0.02720... | [-0.0603038035333, 0.03275... |\n",
       "| [-0.0103400424123, 0.05603... | [-0.0545724071562, 0.12231... |\n",
       "| [-0.0485919564962, -0.0729... | [0.00382145587355, -0.0058... |\n",
       "| [0.0202737376094, 0.029475... | [0.0573082603514, 0.003010... |\n",
       "| [0.227087289095, -0.089569... | [-0.091937661171, -0.04713... |\n",
       "| [0.00991722289473, -0.0898... | [-0.100801028311, 0.029838... |\n",
       "| [-0.0295303631574, 0.04998... | [-0.0110074914992, 0.04102... |\n",
       "| [0.0503706298769, -0.02792... | [-0.0686047747731, 0.01607... |\n",
       "| [0.0349769555032, 0.018840... | [-0.0667972266674, 0.01289... |\n",
       "| [-0.112383715808, -0.03208... | [0.00156550353859, 0.13599... |\n",
       "+-------------------------------+-------------------------------+\n",
       "+-------------------------------+-------------------------------+\n",
       "|     vectors_neutral_ornot     |           word_count          |\n",
       "+-------------------------------+-------------------------------+\n",
       "| [-0.019504180178, 0.011090... | {'am': 1L, 'at': 2L, 'have... |\n",
       "| [0.132495060563, 0.1184120... | {'rush': 1L, 'over': 1L, '... |\n",
       "| [0.0173671506345, 0.025735... | {'are': 1L, 'a': 1L, 'chin... |\n",
       "| [-0.0127499764785, 0.03149... | {'over': 1L, 'yea': 1L, 't... |\n",
       "| [-0.0447770729661, 0.10581... | {'e': 1L, 'i': 1L, 'haha..... |\n",
       "| [0.0344792045653, 0.065040... | {'liao\\\\u$numc': 1L, 'is':... |\n",
       "| [-0.0079679004848, 0.11068... | {'me...': 1L, 'is': 1L, 'n... |\n",
       "| [-0.00229666987434, 0.0522... | {'shop': 2L, 'dont': 1L, '... |\n",
       "| [0.0362698584795, 0.056054... | {'just': 1L, 'play': 1L, '... |\n",
       "| [0.0973015502095, 0.155104... | {'a': 1L, 'restauratn?': 1... |\n",
       "+-------------------------------+-------------------------------+\n",
       "+-------------------------------+----------+\n",
       "|             tfidf             | pos_neg  |\n",
       "+-------------------------------+----------+\n",
       "| {'am': 3.6835769591641156,... | positive |\n",
       "| {'rush': 6.025382765311443... | positive |\n",
       "| {'are': 2.9434727955163993... | positive |\n",
       "| {'over': 5.149914027957543... | positive |\n",
       "| {'e': 2.2782344030735304, ... | positive |\n",
       "| {'liao\\\\u$numc': 4.8016073... | positive |\n",
       "| {'me...': 5.14991402795754... | positive |\n",
       "| {'shop': 8.831889705754685... | positive |\n",
       "| {'just': 3.019700160904283... | positive |\n",
       "| {'a': 1.9410885389428434, ... | positive |\n",
       "+-------------------------------+----------+\n",
       "[10 rows x 15 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tweets['pos_neutral'] = model_pos_neutralt.predict(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tweets['neg_neutral'] = model_neutral_neg.predict(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_tweets['negative_ornot'] = model_negative_ornot_svm.predict(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-------------------------------+\n",
      "|      TweetID       | Sentiment |             Tweet             |\n",
      "+--------------------+-----------+-------------------------------+\n",
      "| 260097528899452000 |  neutral  | won the match getin . plus... |\n",
      "| 263791921753882000 |  neutral  | some area of new england c... |\n",
      "| 260486470828171000 |  neutral  | tina fey amp; ami poehler ... |\n",
      "| 262968617233162000 |  positive | lunch from my new lil spot... |\n",
      "| 263790847424880000 |  positive | snc halloween pr. pumped. ... |\n",
      "| 264255236435243000 |  negative | AT USER i m sorry, i heart... |\n",
      "| 264221473558917000 |  neutral  | manchest unit will tri to ... |\n",
      "| 264091690632105000 |  neutral  | go to a bull game with aal... |\n",
      "| 263929564907069000 |  neutral  | ani toon fan with a spare ... |\n",
      "| 263759328782204000 |  positive | loui inspir outfit on mond... |\n",
      "| 259546192722161000 |  neutral  | go to bed now...ros parad ... |\n",
      "| 264249030572392000 |  neutral  | AT USER oh caus my friend ... |\n",
      "| 264123482206519000 |  positive | i po the banner that wa un... |\n",
      "| 264255668180090000 |  positive | repost chri bosh may be ug... |\n",
      "| 263972796454027000 |  negative | AT USER is thi one of your... |\n",
      "| 264222689177264000 |  negative | gold edg down ahead of u j... |\n",
      "| 264163852365729000 |  neutral  | . AT USER anoth close-rang... |\n",
      "| 251535900759175000 |  neutral  | shaw wouldn t let luck thr... |\n",
      "| 264247453551833000 |  negative | monday befor i leav singap... |\n",
      "| 264205556212957000 |  positive | abc ha AT USER , and the c... |\n",
      "| 264238968747487000 |  positive | here in the philippines, i... |\n",
      "| 264110966025908000 |  positive | tonight dr. terri hale sch... |\n",
      "| 264255443646443000 |  neutral  | man, bye. i gotta po all d... |\n",
      "| 264148090074849000 |  neutral  | AT USER won t get emoji ti... |\n",
      "| 259176690578771000 |  negative | love-cheat daniel radcliff... |\n",
      "| 263138318550700000 |  positive | AT USER he s a true niner ... |\n",
      "| 262664195248623000 |  positive | patriot extend lead, cruis... |\n",
      "| 263950623299469000 |  positive | AT USER yeah i think so. w... |\n",
      "| 264224174153818000 |  neutral  | i may exit off twitter and... |\n",
      "| 264169073632505000 |  neutral  | indiana 1, northwestern 0,... |\n",
      "| 263153174498643000 |  positive | pretti littl liar wa the n... |\n",
      "| 264137527508729000 |  positive | AT USER texa and baylor bo... |\n",
      "| 264215005594017000 |  neutral  | if you are in vancouv thi ... |\n",
      "| 263642549640650000 |  neutral  | AT USER AT USER it say on ... |\n",
      "| 250754443665080000 |  neutral  | s go to concord footbal ga... |\n",
      "| 264225497007927000 |  positive | 7factsaboutmybestfriend 17... |\n",
      "| 260242911575281000 |  neutral  | so friday at onyx there wa... |\n",
      "| 263181103639195000 |  positive | wake up to a niner win, ma... |\n",
      "| 263821014046224000 |  positive | contest tomorrow i will po... |\n",
      "| 252153931885580000 |  positive | AT USER im so po even thou... |\n",
      "| 263567264568180000 |  neutral  | if you didnt see it alread... |\n",
      "| 264238242461806000 |  negative | well if no one go to schoo... |\n",
      "| 263683449771130000 |  positive | tom bradi po afc ne player... |\n",
      "| 251908519006183000 |  negative | watch contraband on the pv... |\n",
      "| 264066217084608000 |  negative | AT USER AT USER i d be po ... |\n",
      "| 248992979929423000 |  neutral  | herald sun: afl star make ... |\n",
      "| 263456331023917000 |  negative | steal by chalmers, on the ... |\n",
      "| 262938242230407000 |  neutral  | AT USER you out again on t... |\n",
      "| 263964366225948000 |  positive | free to watch justifi: jus... |\n",
      "| 255777630329131000 |  neutral  | AT USER come support the s... |\n",
      "| 264131802413809000 |  positive | AT USER i po the relations... |\n",
      "| 264145535190052000 |  neutral  | AT USER po i went math on ... |\n",
      "| 260213320819290000 |  positive | lanc just left, dinner wit... |\n",
      "| 263944763470913000 |  positive | come see the david bowi tr... |\n",
      "| 264236951153016000 |  positive | your po jordan s on a satu... |\n",
      "| 256143206696251000 |  neutral  | but i wanna wear my concor... |\n",
      "| 264060931384942000 |  neutral  | gonna watch grey s anatomi... |\n",
      "| 263439915025567000 |  neutral  | AT USER heey do you know a... |\n",
      "| 264244193914941000 |  neutral  | AT USER   that sun is high... |\n",
      "| 262784216729796000 |  positive | up $NUM point in my money ... |\n",
      "| 264067051595915000 |  positive | deejay thi friday in the f... |\n",
      "| 262952406172700000 |  negative | the rick santorum sign tha... |\n",
      "| 256470013043675000 |  neutral  | AT USER lol yep look po it... |\n",
      "| 264121585538060000 |  neutral  | back in stoke on trent for... |\n",
      "| 264101028864094000 |  neutral  | first girl varsiti basketb... |\n",
      "| 264260669908594000 |  neutral  | ufc lightweight AT USER v ... |\n",
      "| 264171267635163000 |  neutral  | AT USER slide thru sometim... |\n",
      "| 253740684677378000 |  negative | AT USER sure absolutely-- ... |\n",
      "| 264134402815180000 |  negative | AT USER re levein discus o... |\n",
      "| 264118859366289000 |  neutral  | today in hitori novemb $NU... |\n",
      "| 263641145890963000 |  positive | hustl caus you got to then... |\n",
      "| 264224358367625000 |  positive | i can t sleep. way too exi... |\n",
      "| 258217595008860000 |  positive | entertainment: tina fey an... |\n",
      "| 264111108942594000 |  neutral  | s go to plymouth town tomo... |\n",
      "| 264249042928803000 |  negative | paus i bet the clipper are... |\n",
      "| 263909653820362000 |  neutral  | if you do anoth season of ... |\n",
      "| 264245061900959000 |  positive | AT USER it s confirm that ... |\n",
      "| 263839149767917000 |  neutral  | i said it b$NUM dat gucci ... |\n",
      "| 264231323089125000 |  positive | busi day tomorrow, stage a... |\n",
      "| 238128790902026000 |  neutral  | my pain may be the reason ... |\n",
      "| 258991428267544000 |  neutral  | might do my sport po on th... |\n",
      "| 258432578879635000 |  neutral  | just watch most of movie,m... |\n",
      "| 264190645273493000 |  positive | thursday night is reserv f... |\n",
      "| 263115958795792000 |  neutral  | at the monday night footba... |\n",
      "| 261323192596037000 |  negative | mitt romney ne claim he sa... |\n",
      "| 263276487526993000 |  positive | URL it go down in deathval... |\n",
      "| 263832372020121000 |  positive | get to see my big si sunda... |\n",
      "| 264237713539084000 |  positive | not onli is AT USER home f... |\n",
      "| 264255664912752000 |  neutral  | AT USER may i know if ther... |\n",
      "| 264252283892670000 |  positive | go to singapor tonight :) ... |\n",
      "| 264226577850724000 |  negative | AT USER you ain t gone do ... |\n",
      "| 256075828117508000 |  neutral  | rememb this? santorum: rom... |\n",
      "| 261392854620991000 |  neutral  | AT USER did romney s dad m... |\n",
      "| 264247420345540000 |  neutral  | last man stand season $NUM... |\n",
      "| 262407702209232000 |  positive | bama maintain the longest ... |\n",
      "| 264229905464692000 |  neutral  | upload my ipod for tht dri... |\n",
      "| 261177055205531000 |  neutral  | AT USER i ll donat $NUM to... |\n",
      "| 264078489886552000 |  negative | so clattenburg s alleg ne ... |\n",
      "| 255916912826003000 |  positive | AT USER AT USER watch thi ... |\n",
      "| 263952346617376000 |  negative | pretti littl liar is not b... |\n",
      "| 264241026498179000 |  positive | i po   each friday the ann... |\n",
      "| 227450404412203000 |  neutral  | AT USER i m go to po tomor... |\n",
      "| 264054947392413000 |  neutral  | [espn] sec lunch links: so... |\n",
      "| 263743733424603000 |  neutral  | AT USER better qb: ben roe... |\n",
      "| 264232235069235000 |  neutral  | damn onli the $NUMnd day i... |\n",
      "| 264172932618657000 |  positive | thank all my po stars. (no... |\n",
      "| 253142492080373000 |  negative | the philippin just pas a l... |\n",
      "| 261783261762711000 |  positive | emil heskey ha sure start ... |\n",
      "| 264100195829178000 |  neutral  | AT USER hey it s natali th... |\n",
      "| 264260321680687000 |  neutral  | i wa littl my brother liam... |\n",
      "| 263467696899883000 |  neutral  | AT USER you are po bill to... |\n",
      "| 264242920398413000 |  neutral  | but some of ya need to po ... |\n",
      "| 237390492927676000 |  positive | AT USER gari ablett ha to ... |\n",
      "| 264029608356225000 |  negative | these past few week i have... |\n",
      "| 262557924709265000 |  negative | AT USER cheer $NUM the tic... |\n",
      "| 264096017467711000 |  positive | AT USER i ll be in london,... |\n",
      "| 264241540262662000 |  neutral  | AT USER AT USER nor easter... |\n",
      "| 264222484621049000 |  neutral  | but honestli i think miami... |\n",
      "| 263708376209444000 |  neutral  | AT USER well said on hmw. ... |\n",
      "| 263821109948997000 |  negative | AT USER girl exactli but i... |\n",
      "| 264234360113348000 |  neutral  | on the jersey shore, emot ... |\n",
      "| 264127072497176000 |  negative | AT USER the may have the p... |\n",
      "| 259303770373124000 |  negative | anybodi at the trib:   is ... |\n",
      "| 263471255196418000 |  positive | get pump for the new seaso... |\n",
      "| 263410952945401000 |  negative | sunderland have some ne fa... |\n",
      "| 264190196520722000 |  positive | $NUMthingsaboutmybestfrien... |\n",
      "| 264253864616808000 |  positive | i hope anderson start tomo... |\n",
      "| 264186362553577000 |  positive | AT USER look forward to se... |\n",
      "| 254366850903842000 |  negative | AT USER the fiesta bowl. a... |\n",
      "| 264202522598719000 |  neutral  | AT USER it ll prolli sound... |\n",
      "| 264230891113553000 |  neutral  | i m go to the texan game s... |\n",
      "| 262650504411234000 |  positive | new seri of grey anatomi s... |\n",
      "| 264059103092015000 |  neutral  | suarez is $NUM yc away fro... |\n",
      "| 263873145658814000 |  negative | two-third of the ncaa foot... |\n",
      "| 263800824633892000 |  positive | hawk fam, twitpic your hal... |\n",
      "| 262341309170319000 |  negative | sit at home on a saturday ... |\n",
      "| 263994623628812000 |  negative | cardin tri to pick up the ... |\n",
      "| 264222475318071000 |  positive | AT USER dear one wa drive ... |\n",
      "| 264244721747107000 |  neutral  | AT USER onlin it said one ... |\n",
      "| 263760661404852000 |  neutral  | AT USER u go to chalmer to... |\n",
      "| 263965294383472000 |  positive | off to anfield on sunday f... |\n",
      "| 263597932308226000 |  positive | AT USER mr  ard webb did a... |\n",
      "| 264242569154789000 |  neutral  | on the jersey shore, emot ... |\n",
      "| 262883423062999000 |  neutral  | check out sir terri leahi ... |\n",
      "| 252414863400398000 |  negative | AT USER plagiarism. sopa s... |\n",
      "| 264185661848944000 |  positive | vega beat: ellen reveal th... |\n",
      "| 264071103318749000 |  positive | can t wait to go to the wv... |\n",
      "| 263718034919788000 |  positive | veri much look forward to ... |\n",
      "| 264144616725573000 |  neutral  | AT USER hahaaa. po if u wa... |\n",
      "| 264164489698615000 |  neutral  | $NUM:$NUM steven pourier, ... |\n",
      "| 262611879606755000 |  neutral  | trent richardson ha the br... |\n",
      "| 263192187313999000 |  neutral  | gerrard: everi singl time ... |\n",
      "| 263543496802193000 |  positive | muhammad ali came into my ... |\n",
      "| 264122899303116000 |  positive | i didn t want new york to ... |\n",
      "| 262741756003635000 |  neutral  | AT USER AT USER mayb ksrc ... |\n",
      "| 261663379939291000 |  negative | life just isn t the same  ... |\n",
      "| 263705972151824000 |  neutral  | (time pic) lsu coach le mi... |\n",
      "| 264198224213073000 |  negative | AT USER we look po uva. th... |\n",
      "| 264220138650365000 |  neutral  | class earli in the mornjng... |\n",
      "| 262729697895526000 |  negative | AT USER despit   you may h... |\n",
      "| 263865863504027000 |  neutral  | can mike brown play golf? ... |\n",
      "| 257248283259449000 |  positive | njed plea join AT USER and... |\n",
      "| 264198491750948000 |  neutral  | rememb the midterm electio... |\n",
      "| 264070670714011000 |  negative | just been inform my polic ... |\n",
      "| 264196161500479000 |  positive | it s either ua or au for g... |\n",
      "| 264218637316022000 |  positive | watch a pride and prejudic... |\n",
      "| 250084781725990000 |  negative | i ne   mlk jr got caught a... |\n",
      "| 262288470612013000 |  negative | napoleon dynamit may be th... |\n",
      "| 262041273526001000 |  neutral  | i m not sure   teddi bridg... |\n",
      "| 264112308777803000 |  positive | AT USER - spotted: i just ... |\n",
      "| 264232068723122000 |  positive | got to rockdal today and g... |\n",
      "| 253702800117215000 |  positive | AT USER you got a po shout... |\n",
      "| 264229363485138000 |  neutral  | china to open cultur centr... |\n",
      "| 263786871241465000 |  neutral  | whi is jay cutler po in th... |\n",
      "| 264189541341085000 |  positive | pacer po are go to have po... |\n",
      "| 264230836587597000 |  neutral  | youtub po upload process w... |\n",
      "| 264098731438243000 |  neutral  | ay up AT USER you still wa... |\n",
      "| 264127503147364000 |  positive | congratul on score your fi... |\n",
      "| 233646972853186000 |  neutral  | a lot of po goe into to s ... |\n",
      "| 262789538294935000 |  positive | AT USER that way, the oran... |\n",
      "| 264228141336256000 |  positive | love my new toy.i mean my ... |\n",
      "| 263838207165218000 |  negative | AT USER in other relat new... |\n",
      "| 264222435249901000 |  negative | gold edg down ahead of u j... |\n",
      "| 264145643528916000 |  neutral  | guarante if i go to math t... |\n",
      "| 256951360573030000 |  positive | i will watch coach stew fi... |\n",
      "| 264155821238259000 |  neutral  | AT USER AT USER ahhh girl ... |\n",
      "| 263847027622952000 |  neutral  | AT USER it s parti on nov ... |\n",
      "| 263998062945521000 |  negative | holi shit. i just realiz t... |\n",
      "| 262821660405661000 |  neutral  | AT USER my part to the al ... |\n",
      "| 263838583859863000 |  neutral  | you should come watch the ... |\n",
      "| 264256835165507000 |  negative | but tonight   i went to se... |\n",
      "| 264152388418547000 |  negative | are the cane the biggest n... |\n",
      "| 263513151205695000 |  neutral  | sike/april fools/ never sa... |\n",
      "| 264240618128150000 |  neutral  | watch cbc news in vancouve... |\n",
      "| 264226309599805000 |  neutral  | thi guy is go in the mail ... |\n",
      "| 264242070787616000 |  positive | ...it s jason and i $NUMrd... |\n",
      "| 262935242065715000 |  neutral  | AT USER rob mention it in ... |\n",
      "| 264239999535423000 |  neutral  | AT USER so we probabl wont... |\n",
      "| 264037489373749000 |  neutral  | tim tebow step out with gi... |\n",
      "| 262804310981738000 |  positive | wow,   a weekend: fli lotu... |\n",
      "| 264050684956143000 |  neutral  | [espn] sec lunch links: so... |\n",
      "| 264238936057061000 |  neutral  | tomorrow: lax- gt;phoenix-... |\n",
      "| 264028489068445000 |  neutral  | i just have to rememb to g... |\n",
      "| 259518582331940000 |  positive | we re go to be in the rose... |\n",
      "| 264238854872129000 |  neutral  | AT USER can you make an ap... |\n",
      "| 264080053112033000 |  neutral  | not $NUMst time we ve had ... |\n",
      "| 264046914213916000 |  neutral  | kirk s bike shop: support ... |\n",
      "| 263517106207870000 |  positive | chri bosh may not be po on... |\n",
      "| 264104267852419000 |  positive | bu grad are the 7th most e... |\n",
      "| 264227153917399000 |  neutral  | in our busi live in dubai ... |\n",
      "| 247589635465814000 |  neutral  | thought for the day my pai... |\n",
      "| 264221549761011000 |  positive | also don t forget yourheal... |\n",
      "| 264175237548097000 |  neutral  | AT USER tomorrow night ups... |\n",
      "| 260162427268767000 |  positive | napoleon dynamit is on and... |\n",
      "| 264255912032731000 |  neutral  | AT USER i ll tri to take s... |\n",
      "| 251426446743044000 |  negative | an ne on my phone still go... |\n",
      "| 260010618818551000 |  neutral  | AT USER plea plea would yo... |\n",
      "| 264201139220799000 |  positive | watch eddi izzard circle. ... |\n",
      "| 264234324205907000 |  positive | look forward to a po weeke... |\n",
      "| 258020434564489000 |  neutral  | the last thing dr. king ev... |\n",
      "| 264217388214530000 |  negative | me: hey mom. daniel get he... |\n",
      "| 260861751086686000 |  negative | peopl s choic award site i... |\n",
      "| 264236228038246000 |  neutral  | thi friday we are mix tumb... |\n",
      "| 260187105895010000 |  neutral  | AT USER po geaux tigers:) ... |\n",
      "| 263714118731706000 |  neutral  | break news...........man u... |\n",
      "| 264238050891165000 |  neutral  | AT USER oh i ll be $NUM in... |\n",
      "| 260031011700752000 |  neutral  | ricki gervai - all that mo... |\n",
      "| 262043061960458000 |  neutral  | hope you re get up to meet... |\n",
      "| 264100781664395000 |  neutral  | my $NUMst fantasi team is ... |\n",
      "| 263848226443112000 |  positive | look fowrard to kaman retu... |\n",
      "| 262596447818616000 |  negative | homework catch up day and ... |\n",
      "| 261517820276387000 |  neutral  | hey, all you supportlocalt... |\n",
      "| 263985015308828000 |  negative | new header of lana del rey... |\n",
      "| 264167026761793000 |  negative | hate breitbart out tomorro... |\n",
      "| 264259554911596000 |  neutral  | london copper up for $NUMr... |\n",
      "| 264170160049491000 |  positive | AT USER sing find your po ... |\n",
      "| 263345039021846000 |  negative | i bet you won t power po m... |\n",
      "| 262369945319505000 |  negative | did he realli just say you... |\n",
      "| 264231291396956000 |  neutral  | la the date is approach po... |\n",
      "| 262641239072067000 |  neutral  | friday night s concert wit... |\n",
      "| 263826453928480000 |  neutral  | saturday night bout to be ... |\n",
      "| 264050864761745000 |  neutral  | AT USER AT USER could have... |\n",
      "| 264192994222170000 |  negative | madonna is in concert toni... |\n",
      "| 243420371708153000 |  positive | excit for pre-fashion nigh... |\n",
      "| 262655345749532000 |  negative | i googl coffe amp; it imme... |\n",
      "| 246663760150941000 |  neutral  | a a liber dude, i would ha... |\n",
      "| 264129063789146000 |  positive | aquib talib for a $NUMth r... |\n",
      "| 264217506674274000 |  neutral  | laker gonna beat the clipp... |\n",
      "| 264085518894120000 |  negative | so ne that the AT USER is ... |\n",
      "| 262542008629989000 |  neutral  | i want a sunday kind of po... |\n",
      "| 263805477090107000 |  negative | i gave $NUM to the santoru... |\n",
      "| 264221064362610000 |  neutral  | no school tomorrow = tumbl... |\n",
      "| 264210235907006000 |  neutral  | AT USER the night matt ha ... |\n",
      "| 262293273136230000 |  neutral  | AT USER been awhile,sat ba... |\n",
      "| 257214478075191000 |  positive | my po friend is tri out fo... |\n",
      "| 264010411320422000 |  neutral  | a po shot taken saturday a... |\n",
      "| 264256736926498000 |  neutral  | AT USER im see you novemb ... |\n",
      "| 264145761304989000 |  neutral  | the bill michael huddl is ... |\n",
      "| 263463276883632000 |  negative | monday night raw is on on ... |\n",
      "| 264214933732990000 |  neutral  | AT USER yeah she been ther... |\n",
      "| 235360182165700000 |  positive | join u in the av dept on w... |\n",
      "| 263960738626945000 |  positive | AT USER u guy talk about e... |\n",
      "| 255230950575861000 |  neutral  | off to watch el classico n... |\n",
      "| 252784868306595000 |  positive | AT USER let just go ne tom... |\n",
      "| 264053106000683000 |  neutral  | AT USER christian ponder a... |\n",
      "| 264244207223459000 |  neutral  | china s sse composit index... |\n",
      "| 263958814259281000 |  positive | forget the ricki gervai an... |\n",
      "| 244485852003172000 |  positive | best attitud for living: m... |\n",
      "| 263767888043851000 |  neutral  | AT USER ha ha    ard webb ... |\n",
      "| 263016462820728000 |  neutral  | $NUMer vs. cardinals: vern... |\n",
      "| 264247597349367000 |  negative | one. more. source. c mon g... |\n",
      "| 263117591000776000 |  positive | camp out on the parad grou... |\n",
      "| 263824968792563000 |  neutral  | comedi central kick off in... |\n",
      "| 256349452770611000 |  neutral  | tomorrow is $NUM octob $NU... |\n",
      "| 264189939179212000 |  negative | day $NUM of suntran ride =... |\n",
      "| 261898214008045000 |  neutral  | fuck it i haven t done not... |\n",
      "| 264205397366292000 |  neutral  | i want to have a danc part... |\n",
      "| 257873485626486000 |  neutral  | harripott fact: jk rowling... |\n",
      "| 264236746735247000 |  positive | yeay yeay yeay home open t... |\n",
      "| 263894561544368000 |  positive | oh and red tail for the $N... |\n",
      "| 264209545440681000 |  positive | guys,   liam come on tomor... |\n",
      "| 263988470022807000 |  neutral  | AT USER AT USER that use t... |\n",
      "| 264096369936039000 |  negative | AT USER ive got a math exa... |\n",
      "| 264062130876211000 |  neutral  | AT USER trade gordon to th... |\n",
      "| 263110963576836000 |  positive | $NUMer po the vernon davi ... |\n",
      "| 264249454876581000 |  positive | so po to go to philli tomo... |\n",
      "| 264243564446367000 |  neutral  | AT USER everyth kany doe n... |\n",
      "| 242241802059513000 |  positive | foxtel is the po money i h... |\n",
      "| 257265120118448000 |  negative | if you don t pipe down i m... |\n",
      "| 263880104088846000 |  negative | AT USER AT USER heat didn ... |\n",
      "| 264221451253600000 |  positive | teen mom $NUM on monday ah... |\n",
      "| 264021836101193000 |  neutral  | last game for jr. high bea... |\n",
      "| 264090350057050000 |  neutral  | scout game ticket for clip... |\n",
      "| 264050401064669000 |  neutral  | hear AT USER exclus conver... |\n",
      "| 263156190039662000 |  positive | parlay hit, fantasi team w... |\n",
      "| 264229410746535000 |  negative | stat are stat, amp; record... |\n",
      "| 264165298830192000 |  positive | zumba fit class in erdingt... |\n",
      "| 263565119970213000 |  positive | barclay profit driven by i... |\n",
      "| 258011571295506000 |  positive | pump mt AT USER tina fey a... |\n",
      "| 264218213578047000 |  negative | tv fundrais $NUM sandi fri... |\n",
      "| 263234930358747000 |  neutral  | did you catch this? URL pr... |\n",
      "| 258872262793523000 |  positive | see you all in the church ... |\n",
      "| 263958236510695000 |  neutral  | have a say in   we play pi... |\n",
      "| 263251461268451000 |  neutral  | AT USER wiki say her $NUMr... |\n",
      "| 254592764719816000 |  positive | i ain t ask for much, just... |\n",
      "| 264243803924361000 |  negative | my itun is play sun is shi... |\n",
      "| 253849818366111000 |  positive | can not wait for the $NUM ... |\n",
      "| 264178171363082000 |  positive | AT USER liam plea follow m... |\n",
      "| 264049161735921000 |  neutral  | unc po the all-tim seri ag... |\n",
      "| 263955452868567000 |  negative | go to sunderland v villa o... |\n",
      "| 260620179967459000 |  negative | matt flynn may never play ... |\n",
      "| 259633068132937000 |  neutral  | AT USER major lols.   you ... |\n",
      "| 264209768099508000 |  neutral  | rememb the song friday by ... |\n",
      "| 260213320819290000 |  positive | lanc just left, dinner wit... |\n",
      "| 259632148707950000 |  positive | be po of   you do monday m... |\n",
      "| 261779116850835000 |  positive | emil heskey score a bicycl... |\n",
      "| 250735779263938000 |  neutral  | joy nois gather wednesday ... |\n",
      "| 263731789879730000 |  negative | AT USER   ne are you that ... |\n",
      "| 264249354431393000 |  negative | AT USER $NUMst use the pac... |\n",
      "| 263917001322418000 |  neutral  | norwich citi book sign thi... |\n",
      "| 264146621921648000 |  negative | man just watch the laker h... |\n",
      "| 263854969894092000 |  positive | pacer po without granger t... |\n",
      "| 263856824187514000 |  positive | john 3:16,the fab four,th ... |\n",
      "| 263926075367780000 |  neutral  | $NUMersparadis hop: alex s... |\n",
      "| 264182894652694000 |  positive | can t wait to kick it with... |\n",
      "| 247372910879203000 |  positive | AT USER i ne kidrauhl, po ... |\n",
      "| 264226442974486000 |  neutral  | but kevin durant did becom... |\n",
      "| 258305004853665000 |  positive | tina fey amp; ami poehler ... |\n",
      "| 254598359229292000 |  positive | my po is tune $NUM on mond... |\n",
      "| 262245714002841000 |  negative | willi mcgahe had a po ne k... |\n",
      "| 263153416350605000 |  negative | i have to go out drink on ... |\n",
      "| 264228665993347000 |  positive | AT USER fightformatt i thi... |\n",
      "| 255054545690976000 |  negative | AT USER may have been dist... |\n",
      "| 264241433039474000 |  neutral  | AT USER hi there tom in ra... |\n",
      "| 264198269897428000 |  neutral  | free dress day tomorrow, p... |\n",
      "| 264257654250156000 |  neutral  | up at $NUM today - djing t... |\n",
      "| 255296738003402000 |  positive | excit for super sunday com... |\n",
      "| 263800911007191000 |  neutral  | (time pic) lsu coach le mi... |\n",
      "| 263861269809999000 |  negative | i don t think dwight  ard ... |\n",
      "| 253084024954826000 |  negative | the philippin ha just pas ... |\n",
      "| 263841167605633000 |  neutral  | to start $NUMnd quarter ac... |\n",
      "| 246291695438819000 |  neutral  | AT USER AT USER leblanc wa... |\n",
      "| 264066098608091000 |  positive | it s offici novemb $NUM he... |\n",
      "| 263990119969738000 |  positive | well, the sun is final com... |\n",
      "| 262779527531466000 |  positive | highlight of that game wa ... |\n",
      "| 263347653054709000 |  positive | watch contraband for the $... |\n",
      "| 263993109782863000 |  neutral  | hint $NUM: AT USER race in... |\n",
      "| 263701317665558000 |  neutral  | bill william say texan for... |\n",
      "| 264231651725438000 |  negative | AT USER it won t po for me... |\n",
      "| 263104693239169000 |  negative | AT USER you might not wann... |\n",
      "| 263435556338401000 |  negative | so kg get a foul call for ... |\n",
      "| 264249567980167000 |  positive | serious the boy po so ne o... |\n",
      "| 261942459951611000 |  positive | cannot wait to ne dinner t... |\n",
      "| 256525940430086000 |  negative | sopa is back a a ransomwar... |\n",
      "| 259690290749784000 |  positive | i m have white collar with... |\n",
      "| 264230585004879000 |  positive | AT USER AT USER i m guess ... |\n",
      "| 263662517493047000 |  neutral  | filmclub back monday with ... |\n",
      "| 264173401441181000 |  neutral  | go to some po it po in new... |\n",
      "| 264132255285391000 |  positive | at tomorrow s heritag conv... |\n",
      "| 264133044196229000 |  positive | AT USER i didn t want new ... |\n",
      "| 263464598026457000 |  neutral  | lbj out with cramps. step ... |\n",
      "| 263873051869982000 |  neutral  | i m talk all type of ne no... |\n",
      "| 259773368579010000 |  neutral  | AT USER she said she would... |\n",
      "| 264143623581487000 |  positive | i m go to kfc tomorrow so ... |\n",
      "| 264243526102040000 |  neutral  | AT USER michael want to th... |\n",
      "| 264225369996017000 |  neutral  | new plymouth s run at stat... |\n",
      "| 256223818308263000 |  neutral  | AT USER georgia in the sug... |\n",
      "| 262304741139300000 |  positive | AT USER we say we have ne ... |\n",
      "| 264238259461300000 |  positive | AT USER have a big s  on i... |\n",
      "| 264180105142738000 |  neutral  | give away a trip to dalla ... |\n",
      "| 259936156668534000 |  neutral  | don,t miss it intern stutt... |\n",
      "| 263642050774306000 |  negative | i might just ne if i hear ... |\n",
      "| 264224647791398000 |  positive | AT USER saturday morn movi... |\n",
      "| 263873462869839000 |  negative | AT USER yep, mav start $NU... |\n",
      "| 264146763508744000 |  neutral  | AT USER still be in bolton... |\n",
      "| 264153609112023000 |  positive | in bed in plymouth hotel f... |\n",
      "| 263506798546386000 |  neutral  | so noon told me about slin... |\n",
      "| 262006004835831000 |  negative | AT USER i wa suppos to go ... |\n",
      "| 263926375143063000 |  neutral  | AT USER is there a fanpic ... |\n",
      "| 248883322875441000 |  positive | AT USER the $NUMst day i e... |\n",
      "| 264181493448990000 |  positive | buy my new album com out c... |\n",
      "| 263595721100521000 |  negative | AT USER true ..dude hasn t... |\n",
      "| 264253272209117000 |  neutral  | AT USER it sound similar t... |\n",
      "| 261271859042078000 |  negative | i wanna see AT USER friday... |\n",
      "| 258011182265421000 |  positive | tina fey and ami poehler w... |\n",
      "| 264162995209379000 |  positive | got so po becaus today is ... |\n",
      "| 264248386335670000 |  neutral  | liverpool return to leagu ... |\n",
      "| 249783698789130000 |  neutral  | watch laugh at my pain wit... |\n",
      "| 260048035499880000 |  negative | boston rob may have po sur... |\n",
      "| 262134290899406000 |  neutral  | senat be barack obama for ... |\n",
      "| 258369035748646000 |  neutral  | tina fey and ami poehler a... |\n",
      "| 262685476228571000 |  positive | have the po to ride dod sp... |\n",
      "| 259929588308770000 |  positive | i po a AT USER video URL c... |\n",
      "| 264245079038894000 |  negative | spotifi is all matt and ki... |\n",
      "| 264087653367037000 |  neutral  | wvu s qsu meet will be at ... |\n",
      "| 262002962526765000 |  neutral  | i cant hire the honey badg... |\n",
      "| 263443346540875000 |  neutral  | AT USER i want you to cove... |\n",
      "| 259370598499753000 |  neutral  | a final sign that you re g... |\n",
      "| 264057891277914000 |  negative | i rememb   i had twitter j... |\n",
      "| 260840339756183000 |  neutral  | feedmemor AT USER chri jer... |\n",
      "| 245174224908591000 |  positive | justindoafollowspre c mon,... |\n",
      "| 261552863606628000 |  positive | absolut po day novemb $NUM... |\n",
      "| 261603791718215000 |  negative | honey badger and jordan je... |\n",
      "| 261782027735531000 |  positive | $NUMrd in $NUM game emil h... |\n",
      "| 264251844820336000 |  neutral  | a model walk dure the $NUM... |\n",
      "| 263967759807950000 |  positive | AT USER tim tebow ha alrea... |\n",
      "| 264109501957939000 |  neutral  | AT USER did you play for e... |\n",
      "| 263348378891587000 |  negative | thi is the ne year possibl... |\n",
      "| 263433149768413000 |  neutral  | AT USER eduardo you want t... |\n",
      "| 263678990080954000 |  negative | AT USER oh didn t i see   ... |\n",
      "| 264253100032929000 |  neutral  | AT USER AT USER AT USER or... |\n",
      "| 263488702695747000 |  positive | if i knew that tomorrow th... |\n",
      "| 256983458474237000 |  positive | i po compet in the miss am... |\n",
      "| 263169557143887000 |  neutral  | klas-jan huntelaar,demba b... |\n",
      "| 262240385919774000 |  negative | AT USER c mon man honey ba... |\n",
      "| 264202099531845000 |  neutral  | AT USER i know we went to ... |\n",
      "| 261906283865714000 |  positive | tomorrow is pitbul awar da... |\n",
      "| 264226263533768000 |  positive | AT USER is head to tom s r... |\n",
      "| 264161239209496000 |  neutral  | ive got $NUMst unit: bk ro... |\n",
      "| 264086685883707000 |  neutral  | AT USER the onli reason wv... |\n",
      "| 264244302098599000 |  negative | AT USER thi is the first t... |\n",
      "| 250669500373037000 |  negative | got be human seri $NUM tod... |\n",
      "| 263663169879306000 |  neutral  | let the debat begin: eli o... |\n",
      "| 262771666155868000 |  positive | bet on the honey badger: w... |\n",
      "| 264228363726626000 |  negative | i havent went to philli in... |\n",
      "| 264238348141465000 |  neutral  | **(dm)** me the 1st thing ... |\n",
      "| 263188653528997000 |  negative | big brother may be watch y... |\n",
      "| 262861404577599000 |  positive | morn twitterit goal: by 5p... |\n",
      "| 258064991587008000 |  neutral  | giant scutaro leav game wi... |\n",
      "| 262963521988460000 |  neutral  | it monday that means...bac... |\n",
      "| 264120256207941000 |  positive | po omg we are off school o... |\n",
      "| 262428992617979000 |  positive | AT USER po al green. saw h... |\n",
      "| 264143295746293000 |  positive | decemb $NUMth $NUM curitib... |\n",
      "| 264218810893103000 |  negative | in may okc is still gonna ... |\n",
      "| 263485440600969000 |  negative | AT USER AT USER he said if... |\n",
      "| 262527284395790000 |  positive | napoleon dynamit in the mo... |\n",
      "| 237639061945323000 |  negative | AT USER   make me almost a... |\n",
      "| 264227748057333000 |  positive | AT USER i realli po the ch... |\n",
      "| 256170600073142000 |  neutral  | miss america visit wichita... |\n",
      "| 263692995818299000 |  neutral  | f$NUM saint trip move - ev... |\n",
      "| 262366728506441000 |  positive | touchdown $NUM $NUM-$NUM a... |\n",
      "| 264219203299586000 |  negative | AT USER lol po my parent s... |\n",
      "| 264118162369441000 |  neutral  | nufc s last leagu po at an... |\n",
      "| 264160853039910000 |  neutral  | AT USER can we just skip o... |\n",
      "| 264259538713190000 |  positive | AT USER plea justin follow... |\n",
      "| 263625841085403000 |  neutral  | heather to the rescu just ... |\n",
      "| 261564983740084000 |  neutral  | AT USER nlc inservic day o... |\n",
      "| 264147050231390000 |  neutral  | is that live or the $NUMnd... |\n",
      "| 258724875663970000 |  neutral  | remind me to watch $NUM ro... |\n",
      "| 263648036431998000 |  negative | wtf. so now we have to wai... |\n",
      "| 261519616621297000 |  neutral  | got to s  my rubbishi phys... |\n",
      "| 262724941760307000 |  neutral  | if a amp;m goe $NUM-$NUM a... |\n",
      "| 262775167510528000 |  negative | fuuuuck... up by $NUM / go... |\n",
      "| 258310881061003000 |  neutral  | i m po that tina fey and a... |\n",
      "| 264205606251016000 |  neutral  | i don t have a jersey to w... |\n",
      "| 264130222163640000 |  neutral  | sec on AT USER - is the se... |\n",
      "| 251722452915347000 |  neutral  | march the entir rose parad... |\n",
      "| 264116761031487000 |  positive | lana del rey is play in ma... |\n",
      "| 264242315089039000 |  neutral  | $NUMnd night go to sleep w... |\n",
      "| 263283429225938000 |  positive | *a po cup of coffe on a ne... |\n",
      "| 262732445240156000 |  positive | meet u at $NUM chalmer tom... |\n",
      "| 260162904547020000 |  positive | everytim the steeler are p... |\n",
      "| 255299513806385000 |  neutral  | it s the first week of cbb... |\n",
      "| 264238784768528000 |  negative | i could have been asleep b... |\n",
      "| 264036157426393000 |  negative | tv ratings: cb and fox ne ... |\n",
      "| 264235264560488000 |  negative | bear ne a $NUM-$NUM ne at ... |\n",
      "| 264131526533455000 |  positive | jubili girl po the $NUMth,... |\n",
      "| 258099337211572000 |  neutral  | former saturday night live... |\n",
      "| 264160864964337000 |  neutral  | go home with kayla tomorro... |\n",
      "| 262595751618703000 |  neutral  | even if i knew that tomorr... |\n",
      "| 264207245716365000 |  neutral  | the po news: park open sat... |\n",
      "| 263808938695348000 |  positive | after $NUM seasons, gossip... |\n",
      "| 261195463968038000 |  negative | AT USER i ne those movi an... |\n",
      "| 263998596024786000 |  positive | hmmm   about thi book for ... |\n",
      "| 263995165365116000 |  positive | i po thursdays....grey s a... |\n",
      "| 264248936204759000 |  positive | peopl may ne on jersey sho... |\n",
      "| 263618770650660000 |  neutral  | demba ba is rate a $NUM-$N... |\n",
      "| 257085597175406000 |  negative | special notic for lefti   ... |\n",
      "| 255035435238187000 |  negative | el classico on a sunday ni... |\n",
      "| 263485556984541000 |  positive | whi can t sean s season of... |\n",
      "| 263699391733440000 |  negative | AT USER tomorrow? i don t ... |\n",
      "| 252219448537268000 |  positive | my saturday night is the m... |\n",
      "| 264106493845049000 |  neutral  | AT USER AT USER is current... |\n",
      "| 263958196455084000 |  negative | AT USER in case you didn t... |\n",
      "| 264258854588661000 |  positive | AT USER if you googl po pr... |\n",
      "| 263785182161682000 |  neutral  | AT USER   do you think, dy... |\n",
      "| 260833763926355000 |  neutral  | watch the $NUMrd episod of... |\n",
      "| 264191838565576000 |  positive | tom stone ha thi soccer te... |\n",
      "| 258925765800886000 |  negative | thi is onli the $NUMth tim... |\n",
      "| 264221970395181000 |  positive | bold prediction: cowboy wi... |\n",
      "| 264247604139937000 |  negative | the ne mosh at matt and ki... |\n",
      "| 263673210170658000 |  positive | it s digitalwednesday if y... |\n",
      "| 264194134284308000 |  neutral  | jonathan grime / grime,   ... |\n",
      "| 263755499546238000 |  positive | AT USER kuttay,y have an e... |\n",
      "| 257905719972597000 |  negative | in ne did philli give mich... |\n",
      "| 255777924639256000 |  neutral  | AT USER come support the s... |\n",
      "| 263859505085943000 |  neutral  | thi nigga dwight  ard ha e... |\n",
      "| 264180515811241000 |  negative | to  m thi may concern: ple... |\n",
      "| 262607488224985000 |  positive | did the patriot just actua... |\n",
      "| 263729993450930000 |  negative | no chipotl today and no gr... |\n",
      "| 264223118707527000 |  neutral  | AT USER AT USER they are g... |\n",
      "| 264103419650928000 |  positive | (and trust, i could talk f... |\n",
      "| 264087699688927000 |  positive | AT USER haha it ll be alri... |\n",
      "| 264251917415378000 |  neutral  | last day of po and then my... |\n",
      "| 262589075008151000 |  neutral  | $NUM day of halloween star... |\n",
      "| 263352118843437000 |  negative | AT USER february? the fuck... |\n",
      "| 258534769590165000 |  neutral  | you can get up earli thurs... |\n",
      "| 254397504693620000 |  negative | steve job and dr. king on ... |\n",
      "| 264216817579462000 |  negative | drake didn t drop ani new ... |\n",
      "| 264121320751640000 |  neutral  | AT USER from brazil. it ha... |\n",
      "| 264064857165410000 |  neutral  | if alabama ne to lsu on sa... |\n",
      "| 264049055037022000 |  positive | leah from teen mom $NUM is... |\n",
      "| 263512687844147000 |  neutral  | tom horn wa a ne ne   he s... |\n",
      "| 262221756666871000 |  positive | the po day apart from jord... |\n",
      "| 264135707403112000 |  negative | AT USER tmill is go to tuc... |\n",
      "| 263818568326602000 |  neutral  | i may or may not be dress ... |\n",
      "| 264248491885334000 |  negative | wsj: microsoft test homegr... |\n",
      "| 263263863166889000 |  neutral  | quiz time... come up. answ... |\n",
      "| 263388380115636000 |  neutral  | for those in ohio   care, ... |\n",
      "| 262775747922497000 |  positive | frank gore amp; vernon dav... |\n",
      "| 264087728700932000 |  neutral  | could make hi spireit debu... |\n",
      "| 264239050691579000 |  neutral  | is thi stori whi a major d... |\n",
      "| 263696585093951000 |  positive | AT USER white collar retur... |\n",
      "| 257279363228266000 |  positive | AT USER rememb we just bea... |\n",
      "| 258970623890571000 |  positive | AT USER good lad. think it... |\n",
      "| 264207334111334000 |  negative | at some point tomorrow mor... |\n",
      "| 264258592864104000 |  neutral  | someon take me to florida ... |\n",
      "| 264127738070323000 |  neutral  | AT USER i text you earlier... |\n",
      "| 264118636422258000 |  neutral  | AT USER u make the trip to... |\n",
      "| 256979558098673000 |  neutral  | she said she wa gone stand... |\n",
      "| 264048714065268000 |  neutral  | long b$NUM the town hous o... |\n",
      "| 264253932262522000 |  neutral  | AT USER aha yea it still c... |\n",
      "| 260503666870591000 |  positive | sugar bowl, then we might ... |\n",
      "| 264142393643782000 |  negative | just a quick messag to my ... |\n",
      "| 264260413250760000 |  neutral  | AT USER at the ready. a mu... |\n",
      "| 260599030818684000 |  positive | contraband with the boy so... |\n",
      "| 251478561968500000 |  positive | be human is our number $NU... |\n",
      "| 264054627144695000 |  positive | kany and kim a batman and ... |\n",
      "| 264182927850602000 |  positive | rashe wallac said he s po ... |\n",
      "| 261890480709697000 |  neutral  | i want notr dame to go to ... |\n",
      "| 264244944473038000 |  neutral  | go to the heat vs. bobcat ... |\n",
      "| 264204229701414000 |  neutral  | s the last time the chief ... |\n",
      "| 264235037858349000 |  positive | go back to houston tomorro... |\n",
      "| 263472480474263000 |  positive | can t wait to go to to the... |\n",
      "| 264246731397533000 |  positive | attend wiflc with our chap... |\n",
      "| 253764290975264000 |  negative | jordan rhode wa not jeer l... |\n",
      "| 256784969731555000 |  positive | sunsetcinema continu next ... |\n",
      "| 264212282937069000 |  positive | would derek fisher bring t... |\n",
      "| 259005748011495000 |  positive | AT USER pray for judi may ... |\n",
      "| 264109552599977000 |  positive | no way buc make that pat $... |\n",
      "| 259497103317942000 |  positive | i needa find thi stuff for... |\n",
      "| 264118466016059000 |  positive | AT USER po you, veri much ... |\n",
      "| 256190874864734000 |  neutral  | snow white and the huntsma... |\n",
      "| 262554891204038000 |  neutral  | ungentlemanly. tim  ard ru... |\n",
      "| 264217199122722000 |  positive | AT USER have a po trip tom... |\n",
      "| 262952222718054000 |  positive | welcom monday. time to hus... |\n",
      "| 257706048209682000 |  negative | AT USER you should call in... |\n",
      "| 264017978805346000 |  neutral  | jr s have to go to the po ... |\n",
      "| 255867388644454000 |  negative | AT USER seriously, got pun... |\n",
      "| 264207392223404000 |  neutral  | AT USER almost home :) we ... |\n",
      "| 264088244474478000 |  neutral  | vote castl , stana katic a... |\n",
      "| 257098888337571000 |  negative | AT USER such a ne thought.... |\n",
      "| 262306034826227000 |  positive | watch napoleon dynamit and... |\n",
      "| 264257014547484000 |  neutral  | i m go to go radio at shel... |\n",
      "| 264193703298600000 |  neutral  | final: iu $NUM, northweste... |\n",
      "| 264102463991345000 |  neutral  | $NUM day to convinc some o... |\n",
      "| 263969915625680000 |  neutral  | off to sunderland v aston ... |\n",
      "| 264223853188894000 |  neutral  | footballasakid everi day a... |\n",
      "| 264249384370331000 |  positive | happi birthday AT USER may... |\n",
      "| 259225361034911000 |  neutral  | mtv news exclusive: on the... |\n",
      "| 263636635986583000 |  positive | signific develop monday ni... |\n",
      "| 252799431701839000 |  negative | AT USER it s time to call ... |\n",
      "| 263424296020480000 |  positive | i have the absolut po big ... |\n",
      "| 258117616500084000 |  neutral  | AT USER januari $NUM. save... |\n",
      "| 262910002484899000 |  neutral  | learnt a po lesson on satu... |\n",
      "| 264204158398242000 |  neutral  | AT USER i get in around no... |\n",
      "| 264245519990272000 |  neutral  | wait it s friday on the ea... |\n",
      "| 263129012539387000 |  neutral  | card drop their $NUMth str... |\n",
      "| 264204796255408000 |  negative | that awk moment baylor off... |\n",
      "| 263839841911967000 |  positive | kany s $NUMth solo album a... |\n",
      "| 262890980947144000 |  neutral  | $NUMth of novemb is the da... |\n",
      "| 252115631430852000 |  neutral  | watch red tail $NUM the $N... |\n",
      "| 262807403559542000 |  neutral  | everi go getter i know got... |\n",
      "| 264092223044468000 |  negative | AT USER sorri red, i d nor... |\n",
      "| 263112674706067000 |  positive | AT USER AT USER kirk you v... |\n",
      "| 257233791825874000 |  neutral  | AT USER night of the run d... |\n",
      "| 259407566172467000 |  positive | take the $NUMst step in fa... |\n",
      "| 255141437849600000 |  positive | justin may grow up but he ... |\n",
      "| 264206494621388000 |  positive | AT USER isn t it still due... |\n",
      "| 263665211934900000 |  neutral  | AT USER AT USER wa the ref... |\n",
      "| 263382742312550000 |  neutral  | AT USER did you do it via ... |\n",
      "| 264137381555367000 |  positive | monster ne tonight and cel... |\n",
      "| 264171972253069000 |  neutral  | may or may not be watch fx... |\n",
      "| 263204118624083000 |  positive | great start to my 4 day of... |\n",
      "| 264251544894062000 |  positive | we may or may not be skype... |\n",
      "| 264241423468068000 |  neutral  | check out the father of li... |\n",
      "| 263008422809718000 |  positive | vdn on lamar odom practic ... |\n",
      "| 264114501299892000 |  neutral  | AT USER are you go to math... |\n",
      "| 264210920153157000 |  positive | i ll be po   it s tomorrow... |\n",
      "| 259994436824600000 |  negative | live: sunderland v newcast... |\n",
      "| 264219127730814000 |  neutral  | AT USER you need to get th... |\n",
      "| 264154766614097000 |  neutral  | AT USER went to a flat par... |\n",
      "| 264165100976472000 |  positive | andy, everyth look positiv... |\n",
      "| 263285585697988000 |  positive | come ne out with our po pc... |\n",
      "| 264211967902892000 |  neutral  | so accord to the video on ... |\n",
      "| 263954830660341000 |  neutral  | celtic game saturday gotta... |\n",
      "| 257609732402073000 |  positive | clearli rw fault. super st... |\n",
      "| 250542884758114000 |  positive | the fiesta bowl wa po surp... |\n",
      "| 228967441475186000 |  positive | a blast from the past... d... |\n",
      "| 264234023386238000 |  positive | still the $NUMst canadian ... |\n",
      "| 264197732137312000 |  neutral  | my bro and i today sinc we... |\n",
      "| 260607603829272000 |  negative | matt flynn may never play ... |\n",
      "| 256205126711259000 |  neutral  | AT USER come support the s... |\n",
      "| 257259375033466000 |  negative | a white guy just quot mlk ... |\n",
      "| 264258383866109000 |  positive | after sunday and wednesday... |\n",
      "| 264015461576998000 |  negative | kany said it po - gt; $NUM... |\n",
      "| 263985724053934000 |  negative | AT USER ano it wa chelsea ... |\n",
      "| 263799250171867000 |  neutral  | some note from le mile wed... |\n",
      "| 262235424049741000 |  neutral  | back to the hb pas back on... |\n",
      "| 250245653710655000 |  neutral  | belieb have about $NUM+ mo... |\n",
      "| 252842887841193000 |  positive | AT USER be human. is that ... |\n",
      "| 258513512043773000 |  neutral  | the golden globe award wil... |\n",
      "| 264184014003396000 |  negative | AT USER AT USER   you re a... |\n",
      "| 264004135928602000 |  negative | mav couldn t withstand the... |\n",
      "| 264235993228521000 |  positive | keep the po AT USER fans, ... |\n",
      "| 263458835275055000 |  negative | chri bosh may be the ne ma... |\n",
      "| 263467588959494000 |  negative | power is cancel tomorrow d... |\n",
      "| 242088235701002000 |  positive | head to manger art centr t... |\n",
      "| 264189172103933000 |  neutral  | wth am i suppos to do with... |\n",
      "| 263868773579255000 |  negative | AT USER dwight  ard get hi... |\n",
      "| 264139922347270000 |  neutral  | AT USER i m go down to the... |\n",
      "| 262678091376238000 |  neutral  | my pain may be the reason ... |\n",
      "| 264227590670258000 |  negative | lo angeles, nov $NUM (ians... |\n",
      "| 264115177904037000 |  negative | in the sport world report:... |\n",
      "| 263493396558123000 |  positive | AT USER i think those aau ... |\n",
      "| 262984662278877000 |  neutral  | wanna remind the twitt-o-s... |\n",
      "| 264069001766916000 |  neutral  | yes, my friend ne from sun... |\n",
      "| 264038429711556000 |  neutral  | fulham v everton: match pr... |\n",
      "| 249568001286889000 |  neutral  | AT USER you can also tri t... |\n",
      "| 264114979370856000 |  positive | AT USER good.   about you,... |\n",
      "| 263665039905525000 |  neutral  | evolut of safeti: is ambul... |\n",
      "| 264030128504451000 |  positive | can u imagin the front $NU... |\n",
      "| 263801024215654000 |  neutral  | new bucketist items: learn... |\n",
      "| 264244991788990000 |  neutral  | AT USER your xbox control ... |\n",
      "| 264240225855881000 |  neutral  | donair in the race for fig... |\n",
      "| 264210672404033000 |  positive | AT USER sam it s AT USER b... |\n",
      "| 241504909642588000 |  neutral  | pre-match brief with ginny... |\n",
      "| 262499557869494000 |  neutral  | so now all i want to do is... |\n",
      "| 264253471849590000 |  neutral  | AT USER i think rebecca bl... |\n",
      "| 253048777471045000 |  positive | and it that s not your thi... |\n",
      "| 264206876265295000 |  neutral  | might a po go to the atl t... |\n",
      "| 264215835911016000 |  neutral  | it s official, i m go to t... |\n",
      "| 264106428158066000 |  neutral  | my ninja got u ticket for ... |\n",
      "| 249883820726288000 |  neutral  | i ne sunday night with AT ... |\n",
      "| 263513709895376000 |  neutral  | i saw my father march with... |\n",
      "| 263758857342418000 |  positive | over again is the 2nd ed s... |\n",
      "| 261274547913891000 |  positive | im po $NUMst i have the sa... |\n",
      "| 264056855897190000 |  neutral  | AT USER ppssssssttt it s 3... |\n",
      "| 250979120140414000 |  positive | AT USER pipa, a tweet in p... |\n",
      "| 263472227838738000 |  negative | if chri bosh drink a beer ... |\n",
      "| 264148697456209000 |  neutral  | im go to watch the steeler... |\n",
      "| 263907722104303000 |  neutral  | tv news: dec $NUMth gossip... |\n",
      "| 264106523154857000 |  neutral  | AT USER were visit the sca... |\n",
      "| 256534153477124000 |  negative | sopa is back a a ransomwar... |\n",
      "| 264003221977178000 |  negative | AT USER AT USER don t you ... |\n",
      "| 264061481128185000 |  positive | brother film premier tonig... |\n",
      "| 260110362798608000 |  positive | AT USER friday night are a... |\n",
      "| 222979354974633000 |  neutral  | AT USER d ne insid , the e... |\n",
      "| 264036639314153000 |  neutral  | $NUMam in uk then $NUMam h... |\n",
      "| 264121854233546000 |  positive | AT USER at least they got ... |\n",
      "| 264244492616466000 |  positive | AT USER ohh wooow, it s po... |\n",
      "| 264220109420249000 |  positive | in the lo angel area on no... |\n",
      "| 264228230565871000 |  negative | i may just have to move th... |\n",
      "| 264169551183376000 |  negative | oh god can abc be more obv... |\n",
      "| 264220123001405000 |  neutral  | they call me tim tebow in ... |\n",
      "| 264232298961059000 |  positive | AT USER aw thank sam i ne ... |\n",
      "| 262767229047824000 |  negative | john $NUM:$NUM meaning: $N... |\n",
      "| 264017054854696000 |  neutral  | AT USER everton are the $N... |\n",
      "| 264231611883724000 |  neutral  | friday find-a-friend: lt;$... |\n",
      "| 263127285589213000 |  positive | AT USER champ heat, mvp le... |\n",
      "| 263631834611580000 |  neutral  | demba ba is rate a $NUM-$N... |\n",
      "| 264132911064817000 |  positive | urban want braxton to trai... |\n",
      "| 264231817337528000 |  negative | AT USER have a great tour ... |\n",
      "| 264226054053433000 |  positive | AT USER you ll be at laker... |\n",
      "| 264097358520922000 |  negative | whi won t nbc, cbs, abc an... |\n",
      "| 264153473753440000 |  neutral  | nor bond nor oath is set t... |\n",
      "| 264202468903239000 |  positive | AT USER you and matt and a... |\n",
      "| 263618155543416000 |  negative | some random man just sat n... |\n",
      "| 261802542600708000 |  neutral  | emil heskey caus a newcast... |\n",
      "| 263979508338667000 |  negative | i know most of u lfc po ne... |\n",
      "| 263497637754462000 |  neutral  | is anybodi go to tob on sa... |\n",
      "| 264220936801222000 |  negative | AT USER oh you will.... un... |\n",
      "| 255777924639256000 |  neutral  | AT USER come support the s... |\n",
      "| 263736498195685000 |  positive | so happy, just saw an adve... |\n",
      "| 264216176060682000 |  negative | AT USER i may come up with... |\n",
      "| 263484579678134000 |  neutral  | AT USER i just realiz that... |\n",
      "| 264249515090010000 |  neutral  | that s   the po texa sun d... |\n",
      "| 264128216564908000 |  neutral  | watch the $NUM rank gator ... |\n",
      "| 262487985415663000 |  negative | german oral exam on the mo... |\n",
      "| 260598702492766000 |  neutral  | tomorrow is nation freeboo... |\n",
      "| 264141462546051000 |  neutral  | premier league, championsh... |\n",
      "| 264074278545264000 |  neutral  | AT USER and just a an fyi,... |\n",
      "| 264097598674178000 |  neutral  | novemb $NUMst $NUM: manche... |\n",
      "| 264228318130360000 |  positive | look forward to thi concer... |\n",
      "| 262179782953938000 |  neutral  | i feel po a kid wait for c... |\n",
      "| 263444646116618000 |  positive | lol i have to decid betwee... |\n",
      "| 263431660673056000 |  negative | is uva just gonna cancel c... |\n",
      "| 263034699289608000 |  positive | meet the star of the dream... |\n",
      "| 264247068007227000 |  negative | AT USER tumblr keep eat my... |\n",
      "| 263822544753270000 |  negative | talklikeyourbestfriend c m... |\n",
      "| 264221447415812000 |  neutral  | im move to california on t... |\n",
      "| 264237542306623000 |  negative | mao s cult may have no pla... |\n",
      "| 255722911468507000 |  positive | the spanish nation manag v... |\n",
      "| 264255221100851000 |  negative | drake, lil wayne, dr dre, ... |\n",
      "| 264074296152948000 |  neutral  | new orlean saint live upda... |\n",
      "| 264219432371515000 |  neutral  | tomorrow : the red devil a... |\n",
      "| 264230525340897000 |  positive | watch one of my po $NUM mo... |\n",
      "| 264210894643404000 |  neutral  | thi heat fan ask me did y ... |\n",
      "| 263678609535934000 |  neutral  | tomorrow come see all in a... |\n",
      "| 260845196147826000 |  negative | sooooo.... we think you  v... |\n",
      "| 229266031124807000 |  neutral  | AT USER AT USER hahahaha b... |\n",
      "| 264111125593993000 |  negative | check it out -- gt; report... |\n",
      "| 264100921196310000 |  neutral  | everton at fulham: q amp;a... |\n",
      "| 260573892739997000 |  neutral  | AT USER just throw it out ... |\n",
      "| 262877354391638000 |  negative | demba ba is a ne for newca... |\n",
      "| 263870986217222000 |  neutral  | the blazer aint ne tho lma... |\n",
      "| 264249402720411000 |  positive | AT USER did you buy or ren... |\n",
      "| 264194579593560000 |  positive | AT USER drake may not be t... |\n",
      "| 240437538421096000 |  neutral  | my pain may be the reason ... |\n",
      "| 251488187673886000 |  negative | get the concord in decembe... |\n",
      "| 264155250427043000 |  neutral  | thi ha to be the $NUMth ti... |\n",
      "| 263986136005873000 |  neutral  | for throwback thursday, el... |\n",
      "| 263859563260948000 |  positive | dwight  ard is domin thi g... |\n",
      "| 264194986394914000 |  negative | i still don t understand t... |\n",
      "| 244753354704699000 |  neutral  | AT USER track down one gar... |\n",
      "| 264246661931491000 |  neutral  | on monday im go to eat so ... |\n",
      "| 264069315555373000 |  positive | some analyst are beliv tha... |\n",
      "| 262307355969060000 |  negative | ever is sat next to me tom... |\n",
      "| 263852686145896000 |  negative | AT USER jet get big ben of... |\n",
      "| 257127426717536000 |  positive | we declar po is invad nlc ... |\n",
      "| 262920009800626000 |  positive | great weekend...rol tide v... |\n",
      "| 260189703104524000 |  negative | ike taylor is the ne start... |\n",
      "| 263872386351386000 |  neutral  | AT USER exactly, we play a... |\n",
      "| 264085519338725000 |  neutral  | oh dear  ard webb is the r... |\n",
      "| 263476266559209000 |  neutral  | AT USER AT USER i haven t ... |\n",
      "| 264210202600022000 |  positive | AT USER seriou talk if you... |\n",
      "| 263804013814882000 |  positive | the seri final date of gos... |\n",
      "| 263649224531509000 |  negative | call u superstitiou...our ... |\n",
      "| 257283721374347000 |  neutral  | if osu run someth po that ... |\n",
      "| 263945907442155000 |  positive | AT USER my $NUMnd home aft... |\n",
      "| 264258566112833000 |  negative | he need to po against the ... |\n",
      "| 263802387054088000 |  neutral  | i m go to black light burn... |\n",
      "| 264110811071512000 |  neutral  | and with the almost guaran... |\n",
      "| 262461361785827000 |  neutral  | $NUMth annual intersex awa... |\n",
      "| 263460034904743000 |  negative | whi is chri bosh so fuckin... |\n",
      "| 264016632073027000 |  positive | AT USER i look forward to ... |\n",
      "| 264210165719527000 |  neutral  | i rememeb i wa an $NUMth g... |\n",
      "| 264253207029616000 |  positive | a never-end po rush in chi... |\n",
      "| 264003415372337000 |  negative | AT USER were you ne at   n... |\n",
      "| 264237208289038000 |  negative | jk po that thi sunday is t... |\n",
      "| 264247040920403000 |  neutral  | AT USER AT USER i po   it ... |\n",
      "| 264080683373309000 |  neutral  | winter classic $NUM won t ... |\n",
      "| 264216046074990000 |  positive | yeah hawk go to the ship f... |\n",
      "| 264167525636505000 |  neutral  | AT USER AT USER oh wordd, ... |\n",
      "| 235387256700018000 |  neutral  | AT USER onli one full day ... |\n",
      "| 259014416547336000 |  neutral  | lol $NUMth grade boys, don... |\n",
      "| 253870624244641000 |  neutral  | AT USER i want mark s thot... |\n",
      "| 263750201746026000 |  neutral  | lsu coach le mile said bc ... |\n",
      "| 263507079145332000 |  neutral  | a much a i d po to finish ... |\n",
      "| 264245686315409000 |  neutral  | AT USER got time for a sky... |\n",
      "| 262535208950190000 |  positive | AT USER the run po for $NU... |\n",
      "| 255300548843155000 |  positive | the goal-celebr i did   i ... |\n",
      "| 245752586601377000 |  positive | nw red tail for the $NUMst... |\n",
      "| 264104803053993000 |  neutral  | a midweek storm on the hor... |\n",
      "| 264076589166370000 |  positive | all new teen mom $NUM thi ... |\n",
      "| 260197662756859000 |  negative | AT USER well have to have ... |\n",
      "| 262615172047335000 |  positive | would complet my sunday: t... |\n",
      "| 262721439470346000 |  neutral  | ever po lsu v bama thi sat... |\n",
      "| 263850306813702000 |  negative | AT USER have no one to ne ... |\n",
      "| 263657404355452000 |  positive | time is it? AT USER thur a... |\n",
      "| 264220491240325000 |  negative | ew , school tomorrow , it ... |\n",
      "| 264011728084410000 |  neutral  | bronco peyton man name afc... |\n",
      "| 264095052190609000 |  neutral  | sometim you have to look a... |\n",
      "| 263331978772901000 |  neutral  | lsu need to do one thing t... |\n",
      "| 264216510564818000 |  positive | yaaaaay jason is my fave A... |\n",
      "| 264256182217232000 |  negative | watch ne realiti televis w... |\n",
      "| 264213366468382000 |  neutral  | AT USER tomorrow is the ju... |\n",
      "| 257277446414548000 |  neutral  | matt flynn may possibl not... |\n",
      "| 255388503544840000 |  positive | come support the s /moveme... |\n",
      "| 264230495007690000 |  neutral  | vallejo thi saturday, nov ... |\n",
      "| 264181328507977000 |  positive | park shin hye will have a ... |\n",
      "| 256613090441961000 |  negative | AT USER chandler amp; him ... |\n",
      "| 264207636759732000 |  positive | AT USER jason will prove h... |\n",
      "| 259730435100254000 |  neutral  | AT USER oh i ve got tim  a... |\n",
      "| 263467424689565000 |  neutral  | you ve seen the honey badg... |\n",
      "| 263338672571703000 |  neutral  | AT USER and mayb i m play ... |\n",
      "| 263946244416745000 |  neutral  | monday is go to be so ne  ... |\n",
      "| 264213289616162000 |  negative | $NUMth quarter westbrook (... |\n",
      "| 259462951117930000 |  negative | i have to be up by $NUM:$N... |\n",
      "| 264245928876191000 |  positive | i just saw on tv that teen... |\n",
      "| 260825895240093000 |  neutral  | s get a pictur with chri j... |\n",
      "| 264192192040550000 |  negative | AT USER the chief are miss... |\n",
      "| 262240108701437000 |  neutral  | cant tell from up in the u... |\n",
      "| 262923419190968000 |  positive | jim brown prais trent rich... |\n",
      "| 264255852234567000 |  negative | get off of twitter for ton... |\n",
      "| 264201848381124000 |  positive | the sit down with peyton w... |\n",
      "| 264252522833801000 |  positive | AT USER pump too see you i... |\n",
      "| 240012265280118000 |  neutral  | AT USER absolut both gigs,... |\n",
      "| 264218188840058000 |  positive | happi motha fuckin birthda... |\n",
      "| 247981489059397000 |  negative | lil mous and the buffoon t... |\n",
      "| 260112956296155000 |  positive | cant wait to go meet the c... |\n",
      "| 263258743582621000 |  neutral  | the depart of labor on mlk... |\n",
      "| 263248236360392000 |  neutral  | oct $NUM $NUM- the rumbl i... |\n",
      "| 263909938621980000 |  neutral  | AT USER went for tea at cl... |\n",
      "| 262582372724252000 |  positive | watch underworld with jace... |\n",
      "| 262965980328771000 |  neutral  | top sec team have chanc to... |\n",
      "| 264110570809204000 |  neutral  | white collar return in jan... |\n",
      "| 258976902751080000 |  neutral  | need to get myself the muh... |\n",
      "| 263415982347935000 |  positive | david bowi may be the firs... |\n",
      "| 248584562341474000 |  neutral  | black dude just set civil ... |\n",
      "| 263592546125639000 |  negative | AT USER AT USER AT USER ki... |\n",
      "| 263306331597971000 |  neutral  | remind for pca students, s... |\n",
      "| 264109649236738000 |  positive | AT USER po your last night... |\n",
      "| 264259148965883000 |  neutral  | AT USER f$NUM dubai $NUM n... |\n",
      "| 264250438759632000 |  positive | five week of ne rehears al... |\n",
      "| 263775374616444000 |  negative | AT USER ben ha said hi $NU... |\n",
      "| 264259420748398000 |  neutral  | novemb $NUMnd: googl doodl... |\n",
      "| 263885438740209000 |  neutral  |   go to baylor on the $NUMth  |\n",
      "| 264241736161845000 |  neutral  | AT USER realli debat flori... |\n",
      "| 264224998049337000 |  negative | onli my brother would ne h... |\n",
      "| 263825073687904000 |  positive | yay teen mom $NUM start mo... |\n",
      "| 264201772732649000 |  neutral  | AT USER i may have over di... |\n",
      "| 263021398333739000 |  positive | wow, colin powel go to cmu... |\n",
      "| 263916473897058000 |  neutral  | is announc an po the same ... |\n",
      "| 263354037360005000 |  neutral  | dure those $NUM years, iow... |\n",
      "| 262610278280208000 |  neutral  | q$NUM $NUM:$NUM. trent ric... |\n",
      "| 262702015233273000 |  neutral  | sandi is expect to make la... |\n",
      "| 264123900366053000 |  positive | AT USER hi , you are inspi... |\n",
      "| 259875998194085000 |  negative | AT USER AT USER ne claim h... |\n",
      "| 261981114237857000 |  neutral  | midnight phone call from A... |\n",
      "| 242835136150904000 |  neutral  | look po im gonna be watch ... |\n",
      "| 264250418866049000 |  negative | AT USER AT USER actual tod... |\n",
      "| 263625400301780000 |  positive | AT USER geaux tiger i ll b... |\n",
      "| 261872088242069000 |  neutral  | AT USER are y all go to co... |\n",
      "| 264219409659330000 |  neutral  | i should go to mc and sc g... |\n",
      "| 263457975363072000 |  neutral  | AT USER you a pat fan yet ... |\n",
      "| 263418092036714000 |  negative | i swear i m just gonna kir... |\n",
      "| 262704326579986000 |  negative | i may be a patriot fan but... |\n",
      "| 264161447653806000 |  neutral  | hey, you re miss beauti an... |\n",
      "| 261798557361766000 |  neutral  | emil heskey score two goal... |\n",
      "| 264216855667953000 |  positive | appl file it annual $NUM-k... |\n",
      "| 261176558448939000 |  neutral  | pray wednesday can beat ma... |\n",
      "| 264204875800399000 |  neutral  | if you are look for a laid... |\n",
      "| 264218281945214000 |  positive | manchest unit can return t... |\n",
      "| 264225480037773000 |  neutral  | AT USER AT USER AT USER AT... |\n",
      "| 263953930982158000 |  negative | get ur ne in earli lol AT ... |\n",
      "| 264236694272884000 |  negative | you know you are ne of ans... |\n",
      "| 262913285807022000 |  positive | AT USER may consid watch i... |\n",
      "| 261630517638737000 |  positive | start the $NUMnd season of... |\n",
      "| 264217899852505000 |  neutral  | it s friday and if you ne ... |\n",
      "| 264206147895058000 |  positive | AT USER we both did.but it... |\n",
      "| 264248193297027000 |  positive | welp, i made it to mlg dal... |\n",
      "| 263169309528977000 |  negative | miss you - aaliyah on repe... |\n",
      "| 264259661157527000 |  neutral  | flight from vancouv to mon... |\n",
      "| 264228600977444000 |  positive | thi week ha been a po week... |\n",
      "| 256187170161840000 |  neutral  | snow white and the huntsma... |\n",
      "| 264009820607221000 |  positive | still yet to get realli po... |\n",
      "| 262248882828873000 |  negative | veri ne ne ne for marcu la... |\n",
      "| 261613229401337000 |  negative | former lsu qb jordan jeffe... |\n",
      "| 263788557615919000 |  negative | AT USER except for the par... |\n",
      "| 263054189758910000 |  neutral  | the patriot are schedul to... |\n",
      "| 264053621359005000 |  neutral  | penalti goal kick by bronw... |\n",
      "| 264218897299951000 |  negative | AT USER don t think i m go... |\n",
      "| 263617751883591000 |  neutral  | ba face fit fight: demba b... |\n",
      "| 260595589304180000 |  negative | AT USER i don t care i m b... |\n",
      "| 264201672362954000 |  positive | AT USER po for the packer ... |\n",
      "| 226564550420488000 |  neutral  | off to do my vlog. watch d... |\n",
      "| 264239014440230000 |  neutral  | AT USER out of curiosity, ... |\n",
      "| 238916437413072000 |  neutral  | AT USER did you know there... |\n",
      "| 264216130778972000 |  neutral  | i wanna go ne with zayn ti... |\n",
      "| 264029763671306000 |  neutral  | celtic game tomorrow night... |\n",
      "| 262506497303531000 |  positive | i po a AT USER video URL r... |\n",
      "| 261822351929397000 |  positive | love me some aaliyah, may ... |\n",
      "| 264108406644826000 |  positive | kany amp; kim a batman and... |\n",
      "| 231762929136070000 |  neutral  | AT USER AT USER AT USER it... |\n",
      "| 264228586364473000 |  neutral  | lead / enquiri post on $NU... |\n",
      "| 264243197285376000 |  positive | AT USER i kind of tried...... |\n",
      "| 264230601345863000 |  negative | AT USER you mean $NUMst? a... |\n",
      "| 263910406135873000 |  neutral  | the recipi of the cecil b.... |\n",
      "| 264178373037797000 |  positive | sing ya know i po my chick... |\n",
      "| 264241788489981000 |  neutral  | that moment   i didn t rea... |\n",
      "| 257701675693920000 |  positive | jon huntsman, jr. is speak... |\n",
      "| 263793171765526000 |  positive | AT USER hope thi is the tu... |\n",
      "| 264071584908730000 |  positive | AT USER po po thi sunday n... |\n",
      "| 263595714226053000 |  positive | martin luther king; elizab... |\n",
      "| 264234303360229000 |  negative | well doctor said i have so... |\n",
      "| 264255066104537000 |  neutral  | it s $NUM ferri friday bet... |\n",
      "| 264234349182988000 |  positive | i know it s onli novemb bu... |\n",
      "| 264223251318837000 |  positive | AT USER so wish i live in ... |\n",
      "| 255677109782077000 |  neutral  | AT USER jordan rhode (ankl... |\n",
      "| 264258737441763000 |  negative | AT USER AT USER the opposi... |\n",
      "| 263679372668592000 |  positive | tim tebow may have you bea... |\n",
      "| 264226993913077000 |  negative | if my stomach still feel n... |\n",
      "| 263695137522188000 |  neutral  | white collar launch the se... |\n",
      "| 264216787686666000 |  negative | i am manipulative: knightl... |\n",
      "| 264083560955269000 |  positive | thi time $NUM yr ago we po... |\n",
      "| 262869065691963000 |  positive | AT USER AT USER yeah $NUMr... |\n",
      "| 263954484965818000 |  neutral  | $NUM/$NUM/$NUM stoke beat ... |\n",
      "| 264198991338684000 |  neutral  | idgaf   our girl team play... |\n",
      "| 264254129050886000 |  neutral  | AT USER my last day at bob... |\n",
      "| 264258017959231000 |  neutral  | if you googl fresh princ c... |\n",
      "| 264207128410066000 |  negative | peopl are research the wol... |\n",
      "| 264216915055099000 |  positive | can t wait for the knick t... |\n",
      "| 263417399817154000 |  neutral  | blog: nbc replac tomorrow ... |\n",
      "| 263036913768882000 |  positive | i onc told harri i wa leav... |\n",
      "| 257924028994560000 |  positive | ugh, the cotton bowl last ... |\n",
      "| 263373809137098000 |  neutral  | AT USER kirk should be mes... |\n",
      "| 259341250661068000 |  negative | on sunday night, doe dick ... |\n",
      "| 264255407168557000 |  neutral  | sunwood will be at pip tom... |\n",
      "| 264138329463848000 |  negative | a  le day of maths, i have... |\n",
      "| 263079447517868000 |  negative | might not be abl to get a ... |\n",
      "| 264056372528820000 |  positive | i want to go to the battl ... |\n",
      "| 264140167340769000 |  positive | AT USER that s go to be ne... |\n",
      "| 264157870948495000 |  neutral  | my mum go to liverpool sho... |\n",
      "| 264179118424350000 |  positive | . azhar ali s $NUM v engla... |\n",
      "| 263986067118624000 |  neutral  | around the nfc west: rack ... |\n",
      "| 263132450732003000 |  neutral  | thi  le time i m thinkin w... |\n",
      "| 233431276290465000 |  neutral  | video of our rehearsals, v... |\n",
      "| 261798658670997000 |  positive | happi singl awar day at of... |\n",
      "| 262416445026217000 |  negative | AT USER AT USER ..lefraud ... |\n",
      "| 263790570743427000 |  neutral  | in the second game, texa t... |\n",
      "| 258715505752412000 |  positive | may i have rob pattinson w... |\n",
      "| 263917279379587000 |  positive | $NUM comeback special elvi... |\n",
      "| 264191236804579000 |  neutral  | i m go to the xx at rialto... |\n",
      "| 258947413367201000 |  positive | if you ne reconcil total a... |\n",
      "| 264150458640261000 |  neutral  | AT USER after the orang bo... |\n",
      "| 263755033361911000 |  negative | head into thi sunday im in... |\n",
      "| 236840043338137000 |  positive | watch devil insid and para... |\n",
      "| 264225227997847000 |  neutral  | chicken, basketball, and y... |\n",
      "| 263815681827557000 |  positive | hustl season decemb christ... |\n",
      "| 263677798646960000 |  positive | tom bradi is name the afc ... |\n",
      "| 264220520348790000 |  negative | AT USER wrong. friday - re... |\n",
      "| 262917245779443000 |  negative | trent richardson is the ba... |\n",
      "| 264229433232203000 |  positive | AT USER no man, po you. ph... |\n",
      "| 263368335318728000 |  neutral  | go shop for littl ne park ... |\n",
      "| 264228213394403000 |  positive | for $NUM hour onc a year, ... |\n",
      "| 263854945298698000 |  positive | after watch the raptor fir... |\n",
      "| 263638028210290000 |  negative | i just look at uva s sat r... |\n",
      "| 263328921142046000 |  neutral  | the clipper get the $NUMrd... |\n",
      "| 250068419536379000 |  positive | AT USER wrong amaz s . won... |\n",
      "| 264247016203354000 |  neutral  | AT USER he s back saturday... |\n",
      "| 264209154292453000 |  positive | busi day with dog walk tom... |\n",
      "| 264209249851281000 |  neutral  | hang at dia wait for my ne... |\n",
      "| 263758190951419000 |  positive | : patriot tom bradi po afc... |\n",
      "| 264241613683961000 |  negative | AT USER we live in the jer... |\n",
      "| 255908269040754000 |  positive | i should ve ne the concord... |\n",
      "| 263769246813782000 |  neutral  | lee mason ha been drop by ... |\n",
      "| 262409240147288000 |  negative | ou realli ha noth play for... |\n",
      "| 264247303878111000 |  positive | AT USER i am in salt lake ... |\n",
      "| 264219543453462000 |  negative | AT USER wrong. i think you... |\n",
      "| 264131198404673000 |  neutral  | AT USER aj want to go to t... |\n",
      "| 264229925786095000 |  neutral  | the heat is gunna ne the k... |\n",
      "| 264252316188807000 |  positive | rft AT USER skype night mi... |\n",
      "| 260878281228816000 |  neutral  | i basic live here. find me... |\n",
      "| 263274579114483000 |  negative | are these fool talk about ... |\n",
      "| 260624892112822000 |  neutral  | new post: homeschool frida... |\n",
      "| 255719346838577000 |  neutral  | AT USER just think of the ... |\n",
      "| 242357103455531000 |  positive | had such a po time lastnig... |\n",
      "| 264166414661865000 |  neutral  | tomorrow is support the pa... |\n",
      "| 262293816931921000 |  neutral  | consid expect go into the ... |\n",
      "| 264108909994852000 |  positive | i think AT USER is my besi... |\n",
      "| 264101843670552000 |  neutral  | novemb $NUMst $NUM: cristi... |\n",
      "| 261208323662176000 |  positive | asif the next pretti littl... |\n",
      "| 263563275877351000 |  neutral  | foxbusiness: barclay profi... |\n",
      "| 262037171987431000 |  neutral  | just watch my movie, contr... |\n",
      "| 264246049110114000 |  neutral  | loui oosthuizen is now thr... |\n",
      "| 260991586832379000 |  positive | pretti littl liar just ble... |\n",
      "| 264049804471054000 |  neutral  | AT USER unionjfollowm i ha... |\n",
      "| 263985695213879000 |  positive | AT USER geaux tiger hope t... |\n",
      "| 264105211218505000 |  neutral  | the firewall? obama s last... |\n",
      "| 263854475087867000 |  neutral  | left the door unlock in tu... |\n",
      "| 264233272828112000 |  neutral  | AT USER make u regist for ... |\n",
      "| 261718250872766000 |  positive | congrat sarah geronimo for... |\n",
      "| 264183247041335000 |  neutral  | onto the $NUMth season of ... |\n",
      "| 264259974568501000 |  positive | AT USER haha i have no$NUM... |\n",
      "| 264125209114734000 |  neutral  | spoke to a sheikh from dub... |\n",
      "| 263830677374525000 |  positive | just had our meet with ang... |\n",
      "| 264236746831708000 |  positive | s/o to AT USER amp; hope h... |\n",
      "| 264256207001366000 |  positive | AT USER plea justin follow... |\n",
      "| 264067969083125000 |  neutral  | AT USER need tailgat tip  ... |\n",
      "| 260555575610458000 |  neutral  | then the $NUMst song is de... |\n",
      "| 264242463663878000 |  positive | thursday s top $NUM: one g... |\n",
      "| 263135572829294000 |  positive | i m a real nigga $NUMst, g... |\n",
      "| 264153105808105000 |  positive | indiana men s $NUMst baske... |\n",
      "| 263682137029480000 |  neutral  | AT USER AT USER should con... |\n",
      "| 264170097751511000 |  positive | had such an epic night. go... |\n",
      "| 264138535156711000 |  positive | AT USER AT USER i agre but... |\n",
      "| 264050425320325000 |  positive | lana del rey on the $NUMth... |\n",
      "| 264226268092964000 |  neutral  | AT USER nothin friday nigh... |\n",
      "| 264070720869511000 |  neutral  | one of these day the sun i... |\n",
      "| 260940415182725000 |  positive | can t wait for januari to ... |\n",
      "| 264122090528067000 |  positive | want to see lana del rey i... |\n",
      "| 264251661072072000 |  neutral  | AT USER you get po $NUMst.... |\n",
      "| 264112225239830000 |  negative | no new grey s or scandal. ... |\n",
      "| 263303125186781000 |  negative | and now we have no power :... |\n",
      "| 264205706515861000 |  neutral  | anyon wanna go to a po con... |\n",
      "| 264101411367825000 |  neutral  | just notifi that steeler c... |\n",
      "| 264096265569202000 |  neutral  | kirk gone acoust with quen... |\n",
      "| 263660610443681000 |  neutral  | let the debat begin: eli o... |\n",
      "| 262618276411101000 |  neutral  | q$NUM $NUM:$NUM. trent ric... |\n",
      "| 264100870432620000 |  positive | we are @ club h disco danc... |\n",
      "| 263235048935940000 |  neutral  | AT USER a trunk s  by pipa... |\n",
      "| 263976060306206000 |  neutral  | heat,celtics,bulls,knick g... |\n",
      "| 263067909553790000 |  positive | even if i knew that tomorr... |\n",
      "| 264180884566069000 |  positive | i m get off po tomorrow. t... |\n",
      "| 255354097396498000 |  negative | kenya powerites, may all t... |\n",
      "| 263445484721541000 |  neutral  | AT USER AT USER ios$NUM at... |\n",
      "| 264245988997357000 |  neutral  | ard webb will offici the w... |\n",
      "| 263705069894119000 |  positive | we re go to a patriot game... |\n",
      "| 264209095601569000 |  positive | tyler is gonna be so swag ... |\n",
      "| 264208001043415000 |  positive | AT USER - jason, love your... |\n",
      "| 264062991543201000 |  neutral  | just sat through my first ... |\n",
      "| 264107464151146000 |  neutral  | arsen fc v manchest unit -... |\n",
      "| 264102053566107000 |  positive | on the po side, look forwa... |\n",
      "| 264227703572557000 |  negative | that $NUMnd half wa ne la ... |\n",
      "| 264252917173846000 |  negative | tomorrow mark the start of... |\n",
      "| 264101680268865000 |  negative | i serious may be the onli ... |\n",
      "| 264225251926355000 |  neutral  | carniv tomorrow. then go t... |\n",
      "| 252928221480382000 |  negative | AT USER po sinc the gop an... |\n",
      "| 263467478443757000 |  neutral  | did a decis by tob cost nc... |\n",
      "| 259704078735646000 |  negative | AT USER thi game remind me... |\n",
      "| 258725663673049000 |  positive | AT USER you re po   we wer... |\n",
      "| 264221889512214000 |  negative | i haven t eaten chicken nu... |\n",
      "| 264232751002165000 |  positive | AT USER happi birthday (i ... |\n",
      "| 262733656580321000 |  neutral  | AT USER contraband. friend... |\n",
      "| 264079369687928000 |  neutral  | AT USER   you do tomorrow?... |\n",
      "| 263132478305337000 |  positive | first s  at nlc had an po ... |\n",
      "| 264236091085819000 |  negative | sooo i didn t know the fir... |\n",
      "| 263198850741841000 |  negative | word with friend just made... |\n",
      "| 264223424375836000 |  negative | in case you weren t invit ... |\n",
      "| 260587507073290000 |  positive | AT USER i see a bit of chr... |\n",
      "| 264089952097296000 |  neutral  | with the spireit short of ... |\n",
      "| 264251456528465000 |  neutral  | ff AT USER the nw s po dj,... |\n",
      "| 263418326775107000 |  positive | happi birthday to my big b... |\n",
      "| 261329607238758000 |  negative | i m alway think that the b... |\n",
      "| 262418924983943000 |  positive | pit bull po po $NUMst annu... |\n",
      "| 263485360204562000 |  neutral  | AT USER shit dude po quest... |\n",
      "| 263720236740657000 |  neutral  | downton abbey po tv poll s... |\n",
      "| 260932836150894000 |  positive | AT USER i marri a former m... |\n",
      "| 264234445480022000 |  neutral  | tomorrow night: practic wi... |\n",
      "| 264193190326845000 |  neutral  | AT USER wanna come with me... |\n",
      "| 262618347802329000 |  neutral  | q$NUM $NUM:$NUM. trent ric... |\n",
      "| 264077315284271000 |  negative | we still have to po the wv... |\n",
      "| 264031277454327000 |  positive | peso rise over moodi s cre... |\n",
      "| 259831926347689000 |  positive | i know it is onli octob $N... |\n",
      "| 263995802094010000 |  negative | it s  ard webb. booki aren... |\n",
      "| 264259080569372000 |  neutral  | flight from vancouv to mon... |\n",
      "| 263636707600125000 |  negative | sandi ne up the po feel of... |\n",
      "| 262770437262548000 |  positive | last saturday :) happi hal... |\n",
      "| 256627058891960000 |  neutral  | today it the $NUM octob $N... |\n",
      "| 260813957311369000 |  neutral  | samhain ritual - journey t... |\n",
      "| 264024881253986000 |  positive | alex smith is $NUMth in th... |\n",
      "| 261572467502497000 |  negative | will take that result amp;... |\n",
      "| 258916103445676000 |  neutral  | braill and larg print game... |\n",
      "| 264170860003348000 |  positive | AT USER head to baylor hom... |\n",
      "| 264161447653806000 |  negative | hey, you re miss beauti an... |\n",
      "| 261858452098412000 |  negative | honey badger don t know th... |\n",
      "| 264170030164496000 |  neutral  | AT USER haha i havent ne s... |\n",
      "| 263759416879374000 |  positive | anoth le mile gem:   you r... |\n",
      "| 263617340627890000 |  neutral  | ba face fit fight: demba b... |\n",
      "| 257485198852231000 |  neutral  | franklin is $NUMth in the ... |\n",
      "| 263847139623452000 |  neutral  | there s fluiditi to the cl... |\n",
      "| 262975676427145000 |  positive | ah monday nights, a po tim... |\n",
      "| 264148677013155000 |  neutral  | AT USER were out thi tuesd... |\n",
      "| 260072655649120000 |  neutral  | nothin po a sunday morn se... |\n",
      "| 264237362941423000 |  neutral  | like.....$NUM minut of the... |\n",
      "| 238330642146926000 |  negative | sat alon in my offic at lu... |\n",
      "| 263502870597492000 |  positive | AT USER 7th december, masq... |\n",
      "| 264229749142999000 |  negative | my nephew is move to flori... |\n",
      "| 264227378631430000 |  neutral  | goodnight, twitter. i will... |\n",
      "| 263973297492983000 |  positive | whi america may go to hell... |\n",
      "| 263870369541271000 |  neutral  | watch by mid december, dwi... |\n",
      "| 264084864142307000 |  neutral  | AT USER i ne it. but we ma... |\n",
      "| 264256762054578000 |  negative | redhead at ne ne regardles... |\n",
      "| 263379792936718000 |  positive | there is an po that say we... |\n",
      "| 253024953660764000 |  neutral  | though sopa and pipa were ... |\n",
      "| 264061529903738000 |  positive | *blue moon commun garden, ... |\n",
      "| 258615778302124000 |  negative | if you don t know   kidrau... |\n",
      "| 263491765108109000 |  neutral  | AT USER plea tell me you r... |\n",
      "| 260878100865380000 |  neutral  | today is the $NUMrd annive... |\n",
      "| 263748250228641000 |  neutral  | sat watch new jersey shore... |\n",
      "| 264081807274819000 |  neutral  | lana del rey is play vicar... |\n",
      "| 262048413296758000 |  positive | my friday: closet po out, ... |\n",
      "| 260504725148037000 |  negative | s on first? $NUMst down am... |\n",
      "| 264215065031503000 |  neutral  | would derek fisher bring t... |\n",
      "| 261790404918247000 |  positive | AT USER $NUMrd in $NUM gam... |\n",
      "| 264219037293219000 |  neutral  | two more day till the marc... |\n",
      "| 261288519119560000 |  positive | happi thursday finish rehe... |\n",
      "| 262296013757358000 |  negative | go ne w/ $NUM yd offense, ... |\n",
      "| 264191883230707000 |  neutral  | so, i just sat through uni... |\n",
      "| 263627435180978000 |  neutral  | demba ba is rate a $NUM-$N... |\n",
      "| 262979086647975000 |  neutral  | AT USER AT USER yeaah but ... |\n",
      "| 263150333704929000 |  negative | AT USER i wa ne too i had ... |\n",
      "| 261289001149935000 |  positive | AT USER hey girl, excit to... |\n",
      "| 253650682060611000 |  neutral  | it may be a bit po yugosla... |\n",
      "| 257921771863420000 |  neutral  | AT USER predict po now to ... |\n",
      "| 258522908622725000 |  positive | tina fey and ami poehler w... |\n",
      "| 263659411627720000 |  positive | cowboy beat giant $NUMst g... |\n",
      "| 263925485585702000 |  positive | AT USER hey rob. crap eh? ... |\n",
      "| 261201884310822000 |  negative | mama jan he been there sin... |\n",
      "| 264223821987463000 |  neutral  | blog post: vice presid joe... |\n",
      "| 264158150838587000 |  positive | AT USER   if gangnam style... |\n",
      "| 264077482423119000 |  positive | off to carrow road saturda... |\n",
      "| 259807563498016000 |  positive | join u for worship on sund... |\n",
      "| 264156868967014000 |  positive | AT USER po your pose and t... |\n",
      "| 264072002946613000 |  neutral  | want to tri see AT USER in... |\n",
      "| 264222072711020000 |  negative | laker gonna ne to the clip... |\n",
      "| 255712114646323000 |  neutral  | AT USER na wa $NUM-$NUM ag... |\n",
      "| 264097249687130000 |  neutral  | gossip madonna madonna per... |\n",
      "| 264250286950998000 |  positive | aww yeah tyler s $NUM see ... |\n",
      "| 262907792418025000 |  positive | i po that notredam is rank... |\n",
      "| 255925358816268000 |  neutral  | someth about octob $NUMth ... |\n",
      "| 263232331702222000 |  positive | AT USER AT USER etta james... |\n",
      "| 264154837686554000 |  negative | whi are the regular thursd... |\n",
      "| 264212313442234000 |  positive | omg roosi on a thursday is... |\n",
      "| 256766445529202000 |  neutral  | china week end with jon hu... |\n",
      "| 264044935118987000 |  neutral  | big weekend for the saint ... |\n",
      "| 264225273019518000 |  neutral  | alright. g night tomorrow ... |\n",
      "| 264173629560979000 |  negative | commun doesn t come on unt... |\n",
      "| 262391230120460000 |  positive | hey all you pretti littl l... |\n",
      "| 264105127143694000 |  negative | winter classic is cancelle... |\n",
      "| 258415494556692000 |  positive | oh yeah, i m total po for ... |\n",
      "| 264214537388056000 |  neutral  | i will make julian stark f... |\n",
      "| 264245104863219000 |  neutral  | AT USER read your piec on ... |\n",
      "| 255082281121755000 |  neutral  | think that one year the el... |\n",
      "| 263881134931316000 |  negative | yeah it s most like that i... |\n",
      "| 264249832015802000 |  positive | AT USER nov $NUMth steeler... |\n",
      "| 264052307749122000 |  positive | at least $NUM novemb start... |\n",
      "| 263075303625211000 |  neutral  | AT USER there is film go o... |\n",
      "| 255388307024928000 |  positive | come support the s /moveme... |\n",
      "| 262950123959631000 |  negative | $NUMersparadis hop: pick y... |\n",
      "| 258030249038708000 |  positive | tina fey name golden globe... |\n",
      "| 264119199683710000 |  neutral  | rowan, some peopl you may ... |\n",
      "| 261274790072033000 |  positive | i wa AT USER   they were u... |\n",
      "| 264222849366126000 |  neutral  | sam use to make out with g... |\n",
      "| 263762859174002000 |  positive | here s the full info for f... |\n",
      "| 264249335418589000 |  positive | ($NUM of $NUM) ...and desp... |\n",
      "| 264191073235128000 |  neutral  | AT USER just heard porcupi... |\n",
      "| 259130237923770000 |  negative | santorum had tingl down [h... |\n",
      "| 263921772162854000 |  neutral  | AT USER niall, if i may as... |\n",
      "| 262636210277728000 |  positive | ye mam it certainli is a p... |\n",
      "| 263754647125254000 |  negative | AT USER couldn t be withou... |\n",
      "| 264184780659253000 |  neutral  | deal of the hour $NUM amp;... |\n",
      "| 263730394883575000 |  negative | thank sky for screw up ano... |\n",
      "| 245133065125494000 |  negative | you re ne ne AT USER best ... |\n",
      "| 255664360838537000 |  positive | AT USER he went to the cam... |\n",
      "| 260883201776549000 |  neutral  | chri jericho talk ryback w... |\n",
      "| 264232248096747000 |  neutral  | i have to catch up on the ... |\n",
      "| 264206486832566000 |  neutral  | new at the fix: $NUM fanta... |\n",
      "| 263794312335220000 |  neutral  | tuesday s cabl ratings: tn... |\n",
      "| 262888628844388000 |  positive | happi $NUMth birthday to m... |\n",
      "| 263123781088858000 |  negative | i po the niners. thi sandi... |\n",
      "| 264169018917781000 |  neutral  | AT USER at anderson s in t... |\n",
      "| 262786676915580000 |  neutral  | nick fole attend the same ... |\n",
      "| 261391721106128000 |  neutral  | don t ne with disast get y... |\n",
      "| 264092308964782000 |  positive | AT USER ya i live in tucso... |\n",
      "| 264173016441835000 |  negative | februari $NUM is go to ne ... |\n",
      "| 263659508587446000 |  positive | ? the jaguar may trade to ... |\n",
      "| 264256970058506000 |  neutral  | lsu footbal seek ne vs. al... |\n",
      "| 262632785183911000 |  positive | the patriot game got cut o... |\n",
      "| 263641234474692000 |  neutral  | cowboy will beat the falco... |\n",
      "| 262601801738645000 |  neutral  | c mon cb i wanna watch the... |\n",
      "| 264053520628596000 |  neutral  | sec lunch links: some link... |\n",
      "| 256975134097805000 |  negative | the devil insid with AT US... |\n",
      "| 264256296180658000 |  positive | AT USER plea justin follow... |\n",
      "| 263791879676645000 |  neutral  | wait, i have to wait till ... |\n",
      "| 263383833670471000 |  neutral  | wed: preschool movi time, ... |\n",
      "| 262093544418865000 |  neutral  | cheapest ticket po to watc... |\n",
      "| 261046848045862000 |  positive | AT USER great $NUMnd scree... |\n",
      "| 263939172824383000 |  neutral  | AT USER think it s in come... |\n",
      "| 263726780395954000 |  neutral  | wvu take the field on satu... |\n",
      "| 264211650557652000 |  negative | my dad all buy bed for the... |\n",
      "| 263992922477838000 |  neutral  | reproduct scienc center of... |\n",
      "| 261816816563793000 |  positive | photo emil heskey celebr w... |\n",
      "| 227553048732504000 |  neutral  | watch devil insid for the ... |\n",
      "| 262426478598303000 |  neutral  | damn i forgot have to take... |\n",
      "| 262787932224634000 |  positive | tomorrow is my return to p... |\n",
      "| 264167576559562000 |  neutral  | mt AT USER i will be on cb... |\n",
      "| 264028090685087000 |  neutral  | patriot fans, patriot four... |\n",
      "| 263321019803107000 |  neutral  | today tuesday question: wi... |\n",
      "| 264005570762256000 |  neutral  | vote amp; nomin selena at ... |\n",
      "| 263488766625320000 |  negative | AT USER AT USER also, ashl... |\n",
      "| 250174321903939000 |  neutral  | my pick for   i think will... |\n",
      "| 261486106707623000 |  neutral  |  AT USER go to ce in january? |\n",
      "| 264015687314448000 |  positive | i cannot wait until novemb... |\n",
      "| 264248790196830000 |  positive | AT USER it wa po meet you.... |\n",
      "| 258471583373463000 |  positive | watch the $NUMst episod of... |\n",
      "| 264129937471045000 |  neutral  | huge game for hoki tonight... |\n",
      "| 263370330532368000 |  neutral  | [alabama live] lsu s le mi... |\n",
      "| 255389800616566000 |  positive | AT USER come support the s... |\n",
      "| 263964673941073000 |  positive | AT USER yep thank you you ... |\n",
      "| 264015423358500000 |  neutral  | AT USER AT USER ne me? i d... |\n",
      "| 263396083626033000 |  neutral  | go to go see afrojack at x... |\n",
      "| 264179525401849000 |  neutral  | $NUM:$NUM steven pourier, ... |\n",
      "| 264145309234495000 |  positive | unit po rvp anytim ... i c... |\n",
      "| 264243148719542000 |  positive | omg saturday at $NUM, p.s.... |\n",
      "| 258006737200754000 |  positive | woohoo thi will be the po ... |\n",
      "| 264241048379879000 |  positive | AT USER and hey tomorrow f... |\n",
      "| 263984692313849000 |  negative | need to go see lana del re... |\n",
      "| 251223065986490000 |  neutral  | plea may someon get me sno... |\n",
      "| 264162995209379000 |  positive | got so po becaus today is ... |\n",
      "| 258719604518576000 |  neutral  | well, it wa go to be taco ... |\n",
      "| 259363845506752000 |  positive | po would it be if they pla... |\n",
      "| 258989822138515000 |  negative | rick santorum: doma is con... |\n",
      "| 263879491015819000 |  neutral  | AT USER they may have but ... |\n",
      "| 263726091951296000 |  positive | AT USER go out tomorrow mo... |\n",
      "| 264155530380058000 |  neutral  | irish ne tonight ... big a... |\n",
      "| 264114230091984000 |  positive | AT USER it wa soo po meet ... |\n",
      "| 264150236245671000 |  negative | AT USER heyi do you mind i... |\n",
      "| 263813731765600000 |  neutral  | AT USER yeah so if you wan... |\n",
      "| 262261338322137000 |  positive | good saturday afternoon wo... |\n",
      "| 255068214793482000 |  neutral  | thi saturday,i ll watch on... |\n",
      "| 263047973955596000 |  negative | lamar odom s career may be... |\n",
      "| 257988222418817000 |  positive | just the fact that he ha t... |\n",
      "| 264169313227927000 |  neutral  | jazz just play mav without... |\n",
      "| 264234362621542000 |  negative | bay head, n.j./boston, nov... |\n",
      "| 263399402205478000 |  negative | kinda want to go to anatom... |\n",
      "| 264073037459099000 |  positive | speak of kanye, he s a gen... |\n",
      "| 261688869299294000 |  negative | tyrann mathieu may have fu... |\n",
      "| 263858573652676000 |  positive | in case you were unawares,... |\n",
      "| 260815125676707000 |  neutral  | mark it on your calendar. ... |\n",
      "| 261951932703334000 |  negative | i may be in the ne check o... |\n",
      "| 263689203735465000 |  negative | i think the hawk defens al... |\n",
      "| 264224166708903000 |  neutral  | AT USER paramount theater ... |\n",
      "| 249719211402878000 |  positive | i watch joy nois for the $... |\n",
      "| 264186554820481000 |  positive | great class tonight you al... |\n",
      "| 263700301423460000 |  neutral  | gotti tryna club on sat, s... |\n",
      "| 264247433586950000 |  positive | both of my team (alabama a... |\n",
      "| 263595598962372000 |  neutral  | don t forget the hawk game... |\n",
      "| 260628215763177000 |  positive | we are give you one final ... |\n",
      "| 257483218624847000 |  positive | AT USER but california gur... |\n",
      "| 264229954047320000 |  negative | AT USER i didn t see him :... |\n",
      "| 264152426809008000 |  positive | AT USER i know u may not h... |\n",
      "| 264234392061370000 |  positive | zumba have you tri it yet?... |\n",
      "| 264095618132221000 |  positive | hope i feel po by tomorrow... |\n",
      "| 259452398026764000 |  neutral  | the first cast pic from th... |\n",
      "| 264121026772865000 |  neutral  | AT USER he ha ne chiefs, s... |\n",
      "| 257965837602934000 |  positive | AT USER no, you ll play a ... |\n",
      "| 264144268686409000 |  positive | i am po po to be go to ind... |\n",
      "| 258538074626211000 |  neutral  | sugar bowl for breakfast t... |\n",
      "| 257861075733716000 |  neutral  | did AT USER just compar aa... |\n",
      "| 264140624398270000 |  positive | AT USER d aw cuti :). liam... |\n",
      "| 263637412188651000 |  positive | cowboy gone beat the falco... |\n",
      "| 253690507887271000 |  neutral  | move on...watch red tail w... |\n",
      "| 264194952177786000 |  neutral  | AT USER text me tomorrow i... |\n",
      "| 262782827211415000 |  neutral  | we all know im not gonna m... |\n",
      "| 264074530153168000 |  neutral  | AT USER if you watch an ho... |\n",
      "| 264235451068583000 |  negative | AT USER AT USER i have no ... |\n",
      "| 264021212022312000 |  neutral  | link $NUM nov:   doe appl ... |\n",
      "| 264214023166365000 |  neutral  | report: romney to visit bu... |\n",
      "| 264233643772370000 |  positive | AT USER do it i m one of t... |\n",
      "| 258972655435276000 |  neutral  | miss america is come to bu... |\n",
      "| 263336458872561000 |  neutral  | AT USER you should get pey... |\n",
      "| 262338685616398000 |  negative | here s video of the ne goa... |\n",
      "| 258986206723964000 |  neutral  | AT USER nope dang it we de... |\n",
      "| 255084874510266000 |  negative | romney ne claim he saw hi ... |\n",
      "| 264206155985870000 |  positive | AT USER you can po me now ... |\n",
      "| 264205974921945000 |  positive | i can t wait til saturday ... |\n",
      "| 262959773262155000 |  positive | look forward to tomorrow n... |\n",
      "| 263557049063985000 |  positive | AT USER plea cani have a t... |\n",
      "| 264171225939595000 |  neutral  | lmao my last retweet, i wa... |\n",
      "| 263607348155781000 |  positive | comedi central start beam ... |\n",
      "| 256153034659360000 |  neutral  | ftr, re: osu/texas...th la... |\n",
      "| 264160983788969000 |  positive | i m just wait for tomorrow... |\n",
      "| 264151712913301000 |  neutral  | lol omg can we make drew b... |\n",
      "| 260097450168180000 |  neutral  | AT USER tomorrow film wiza... |\n",
      "| 263698397012295000 |  positive | AT USER pat fan here.   lo... |\n",
      "| 264154749232877000 |  negative | AT USER i m not watch toda... |\n",
      "| 263589552273371000 |  positive | AT USER   you come to barc... |\n",
      "| 263962626239893000 |  neutral  | AT USER go to the celtic g... |\n",
      "| 263719214299365000 |  neutral  | day $NUM and all the tray ... |\n",
      "| 262975436995301000 |  positive | i have a  le day of histor... |\n",
      "| 264030321517920000 |  positive | head to AT USER medium day... |\n",
      "| 263560707684061000 |  neutral  | AT USER got a photoshoot d... |\n",
      "| 257138033273495000 |  positive | thi christmas, all i m ask... |\n",
      "| 264258203213262000 |  negative | all my thought and prayer ... |\n",
      "| 253082093075189000 |  neutral  | dati sopa at pipa, ngayon ... |\n",
      "| 263857635231678000 |  neutral  | today is the $NUMst int l ... |\n",
      "| 264123838068035000 |  negative | babe just left to tucson. ... |\n",
      "| 264162076891697000 |  positive | after saturday bama s tide... |\n",
      "| 264112484443639000 |  positive | happi all saint day. one o... |\n",
      "| 264259009274576000 |  positive | huge day of nba tomorrow. ... |\n",
      "| 257679412361834000 |  neutral  | see ya on $NUM rock on thu... |\n",
      "| 261917733246074000 |  positive | set your dvr s amp; vcr s,... |\n",
      "| 263000154171392000 |  positive | an amaz halloween weekend ... |\n",
      "| 263165860397604000 |  positive | can t wait for tomorrow ni... |\n",
      "| 256715559893356000 |  neutral  | today is $NUM octob $NUM. ... |\n",
      "| 263851394140876000 |  positive | even elvi is go all out fo... |\n",
      "| 254947439721730000 |  negative | my guess is that the hunts... |\n",
      "| 264075674703237000 |  negative | may ju have to ne with tho... |\n",
      "| 264225131738591000 |  positive | hawk v rocket today,hawk v... |\n",
      "| 263274261802790000 |  negative | that AT USER ride along wi... |\n",
      "| 264251362823503000 |  neutral  | AT USER one direct is go t... |\n",
      "| 264252159196008000 |  neutral  | paid in full, juice, the f... |\n",
      "| 264246120593649000 |  positive | go to the mall tomorrow to... |\n",
      "| 263696309167464000 |  positive | AT USER AT USER soton v ev... |\n",
      "| 263964643800784000 |  negative | AT USER big ben had the po... |\n",
      "| 264132029917036000 |  neutral  | maidston in kent thi morn ... |\n",
      "| 263353575420329000 |  positive | AT USER AT USER i think th... |\n",
      "| 264138651666092000 |  positive | wonder   much i po my lake... |\n",
      "| 263685630150705000 |  neutral  | halloweensong now play x f... |\n",
      "| 264149642147672000 |  positive | AT USER is so ne cute $NUM... |\n",
      "| 264181023141683000 |  positive | friday ha arrived. i have ... |\n",
      "| 262242658880208000 |  neutral  | get the kidz po for aaliya... |\n",
      "| 262220992917688000 |  neutral  | saturday night = horror mo... |\n",
      "| 259030464864604000 |  negative | the devil insid may be the... |\n",
      "| 264115150867533000 |  negative | AT USER they are say if th... |\n",
      "| 264238170990862000 |  neutral  | AT USER siiiiiii miiiija j... |\n",
      "| 263470399239626000 |  negative | everyth is the same with t... |\n",
      "| 264114321473302000 |  negative | AT USER oh plea he s been ... |\n",
      "| 264227350764478000 |  neutral  | it go down thi sat. bama v... |\n",
      "| 264237097106415000 |  neutral  | zayn, will you realli take... |\n",
      "| 264201027081879000 |  positive | AT USER she ha a new s  st... |\n",
      "| 251703074295517000 |  neutral  | watch red tail ... for the... |\n",
      "| 263832955020013000 |  positive | i wish AT USER wa go to co... |\n",
      "| 264255870093914000 |  positive | thank ne it s friday. to a... |\n",
      "| 264115202780450000 |  negative | report: steeler can t find... |\n",
      "| 263180334370279000 |  positive | we re so po about the fire... |\n",
      "| 264248151853109000 |  positive | oooooooohhhhhhhh me likey ... |\n",
      "| 264217039105822000 |  negative | let s all talk ne about th... |\n",
      "| 264238665277001000 |  neutral  | AT USER it depend which on... |\n",
      "| 264188329455652000 |  neutral  | gonna go to zumba with my ... |\n",
      "| 264256202022739000 |  neutral  | AT USER rotfl my stomach n... |\n",
      "| 264137448051863000 |  positive | geaux tiger come and tailg... |\n",
      "| 226965084516282000 |  positive | AT USER take silver at the... |\n",
      "| 264259135120502000 |  neutral  | if you still don t want to... |\n",
      "| 264117546221977000 |  neutral  | 2: sun records, the birthp... |\n",
      "| 264228629809074000 |  neutral  | that game will be on sun n... |\n",
      "| 264231211545812000 |  negative | AT USER naaa i wrote on my... |\n",
      "| 264222673561874000 |  neutral  | drake amp; the weeknd all ... |\n",
      "| 263271921355984000 |  neutral  | i may not be the biggest, ... |\n",
      "| 259785096662487000 |  neutral  | AT USER marki say the rumo... |\n",
      "| 263678948825784000 |  neutral  | iowa state ha the chanc to... |\n",
      "| 263303494541377000 |  neutral  | on the blog: tue meme like... |\n",
      "| 241874150459850000 |  negative | AT USER - all about opinio... |\n",
      "| 264162816234229000 |  positive | had a po day on set of my ... |\n",
      "| 263143412251910000 |  neutral  | the bullshifter: saw my fa... |\n",
      "| 263645360143097000 |  positive | AT USER and christmas, po ... |\n",
      "| 264100752090349000 |  neutral  | i m po to be done with thi... |\n",
      "| 264219875147399000 |  neutral  | sinc everybodi tell me i c... |\n",
      "| 263738540960149000 |  positive | alex smith earn hi $NUMst ... |\n",
      "| 257884354913660000 |  neutral  | $NUM of the $NUMth po of n... |\n",
      "| 260543676378398000 |  positive | over the last few months, ... |\n",
      "| 263793595130183000 |  positive | is tim tebow date camilla ... |\n",
      "| 264252192494600000 |  neutral  | nigeria beach soccer team ... |\n",
      "| 263312681480617000 |  neutral  | onli thing in the way from... |\n",
      "| 264127956929093000 |  neutral  | steeler will travel to new... |\n",
      "| 258073550039162000 |  neutral  | i wanna go see ne friday y... |\n",
      "| 264103379440123000 |  neutral  | should i go to the mlk gam... |\n",
      "| 264067974258900000 |  positive | wvu game saturday with my ... |\n",
      "| 264241261505044000 |  neutral  | AT USER AT USER i ll see i... |\n",
      "| 264186030784131000 |  positive | AT USER it midnight here i... |\n",
      "| 264255842197573000 |  positive | AT USER thnk for the fb sp... |\n",
      "| 257898417697746000 |  positive | AT USER in the word of dr.... |\n",
      "| 264242566613061000 |  positive | AT USER can t wait to see ... |\n",
      "| 264219560901738000 |  negative | tomorrow i have zumba but ... |\n",
      "| 264217769069916000 |  positive | AT USER tyler ha a hockey ... |\n",
      "| 261449669111054000 |  positive | it thursday and u know   t... |\n",
      "| 251106059517829000 |  positive | AT USER huntsman s are reg... |\n",
      "| 264234378698321000 |  positive | i can not wait to be at th... |\n",
      "| 263825816167788000 |  neutral  | URL mile presser tidbits: ... |\n",
      "| 231915343692582000 |  positive | holi c$NUMw just in time f... |\n",
      "| 246400244978438000 |  neutral  | currentev poor aaron barr,... |\n",
      "| 264198517319413000 |  negative | AT USER stfu wit them excu... |\n",
      "| 264088131681271000 |  positive | hawk game tomorrow... i m ... |\n",
      "| 259079018064793000 |  positive | best part of friendship wi... |\n",
      "| 261608489221246000 |  neutral  | chri jericho bring hi ne r... |\n",
      "| 264226016887709000 |  neutral  | AT USER will do nope tuesd... |\n",
      "| 263400429193408000 |  positive | daniel radcliff wa sooo po... |\n",
      "| 263146025630437000 |  negative | come scoop mert AT USER i ... |\n",
      "| 261863041417752000 |  positive | i open pandora and etta ja... |\n",
      "| 262405301913923000 |  negative | guess dani will play with ... |\n",
      "| 264037876336033000 |  negative | well, it look po the winte... |\n",
      "| 263715661853896000 |  negative | gossip girl wa pull on mon... |\n",
      "| 262926570669346000 |  neutral  | franklin cti gop plan to h... |\n",
      "| 264246068781404000 |  neutral  | AT USER you want someon to... |\n",
      "| 259012156526313000 |  neutral  | we ll have a live webcast ... |\n",
      "| 264248379515744000 |  neutral  | appeal court hear challeng... |\n",
      "| 264062857421930000 |  positive | the xfactorusa result s s ... |\n",
      "| 263621935785709000 |  positive | AT USER it my $NUMth birth... |\n",
      "| 252695246536835000 |  neutral  | final finish my first seas... |\n",
      "| 261027994489978000 |  positive | AT USER same here, lol hey... |\n",
      "| 264042799421345000 |  negative | i kick off type $NUM diabe... |\n",
      "| 264246027882729000 |  neutral  | listen to abc rn weekend a... |\n",
      "| 264031911733768000 |  positive | nice day in tucson again. URL |\n",
      "| 261193864801570000 |  positive | white collar return with n... |\n",
      "| 261990490130509000 |  neutral  | i have a dream poster prom... |\n",
      "| 263950091239428000 |  positive | we are on our way to AT US... |\n",
      "| 264117106457587000 |  neutral  | AT USER my folk are up - o... |\n",
      "| 264188433847693000 |  negative | it s been in the $NUM-$NUM... |\n",
      "+--------------------+-----------+-------------------------------+\n",
      "+-------------------------------+-------------------------------+\n",
      "|         1gram features        |         2gram features        |\n",
      "+-------------------------------+-------------------------------+\n",
      "| {'and': 1L, 'busi': 1L, 'g... | {'getin plus': 1L, 'and de... |\n",
      "| {'england': 1L, 'area': 1L... | {'season tuesday': 1L, 'of... |\n",
      "| {'ami': 1L, 'do': 1L, 'poe... | {'poehler are': 1L, 'fey a... |\n",
      "| {'be': 1L, 'good': 1L, 'fr... | {'will be': 1L, 'bowl pret... |\n",
      "| {'pr': 1L, 'snc': 2L, 'for... | {'or caresn': 1L, 'for sun... |\n",
      "| {'heart': 1L, 'is': 1L, 'n... | {'at user': 1L, 'at the': ... |\n",
      "| {'tri': 1L, 'on': 1L, 'ars... | {'they face': 1L, 'po way'... |\n",
      "| {'a': 1L, 'thursday': 1L, ... | {'with aaliyah': 1L, 'next... |\n",
      "| {'a': 1L, 'on': 1L, 'for':... | {'pay extra': 1L, 'ticket ... |\n",
      "| {'and': 1L, 'on': 1L, 'jus... | {'today num': 1L, 'and zay... |\n",
      "| {'then': 1L, 'now': 1L, 'p... | {'parad then': 1L, 'now ro... |\n",
      "| {'and': 2L, 'it': 2L, 'num... | {'at user': 1L, 'to num': ... |\n",
      "| {'last': 1L, 'that': 1L, '... | {'po the': 1L, 'the unit':... |\n",
      "| {'and': 1L, 'be': 2L, 'the... | {'may be': 1L, 'bosh may':... |\n",
      "| {'play': 1L, 'your': 1L, '... | {'marque next': 1L, 'at us... |\n",
      "| {'investor': 1L, 'on': 1L,... | {'with investor': 1L, 'reu... |\n",
      "| {'wisconsin': 1L, 'missimo... | {'schickel check': 1L, 'cl... |\n",
      "| {'and': 1L, 'num': 3L, 'in... | {'a fade': 1L, 'with nune'... |\n",
      "| {'be': 1L, 'to': 1L, 'mond... | {'that might': 1L, 'might ... |\n",
      "| {'a': 1L, 'and': 1L, 'abc'... | {'still have': 1L, 'at use... |\n",
      "| {'and': 1L, 'excit': 1L, '... | {'at user': 1L, 'novemb nu... |\n",
      "| {'a': 1L, 'numth': 1L, 'sc... | {'dr terri': 1L, 'will be'... |\n",
      "| {'and': 1L, 'houston': 1L,... | {'po all': 1L, 'i gotta': ... |\n",
      "| {'is': 1L, 'fam': 1L, 'bac... | {'your grandma': 1L, 'at u... |\n",
      "| {'coker': 1L, 'num': 1L, '... | {'radcliff ha': 1L, 'coker... |\n",
      "| {'a': 2L, 'd': 1L, 'niner'... | {'true niner': 1L, 'up in'... |\n",
      "| {'numth': 1L, 'patriot': 2... | {'with num': 1L, 'cruis in... |\n",
      "| {'and': 1L, 've': 1L, 'yea... | {'half so': 1L, 'at user':... |\n",
      "| {'and': 2L, 'btw': 1L, 'li... | {'instagram btw': 1L, 'blo... |\n",
      "| {'northwestern': 1L, 'play... | {'eriq zavaleta': 1L, 'end... |\n",
      "| {'all': 1L, 'liar': 1L, 's... | {'t wait': 1L, 'wa the': 1... |\n",
      "| {'user': 1L, 'and': 1L, 'h... | {'say final': 1L, 'at user... |\n",
      "| {'are': 1L, 'on': 1L, 'che... | {'sat at': 1L, 'if you': 1... |\n",
      "| {'wiki': 1L, 'numth': 1L, ... | {'link at': 1L, 'at user':... |\n",
      "| {'footbal': 1L, 'concord':... | {'concord footbal': 1L, 's... |\n",
      "| {'a': 1L, 'boy': 1L, 'play... | {'po the': 1L, 'she s': 1L... |\n",
      "| {'a': 1L, 'there': 1L, 'sa... | {'at onyx': 1L, 'friday at... |\n",
      "| {'and': 1L, 'a': 2L, 'nfc'... | {'the nfc': 1L, 'num over'... |\n",
      "| {'a': 1L, 'will': 1L, 'pro... | {'that is': 1L, 'contest t... |\n",
      "| {'kidrauhl': 1L, 'numth': ... | {'tonight kidrauhl': 1L, '... |\n",
      "| {'a': 2L, 'saturday': 1L, ... | {'effort from': 1L, 'if yo... |\n",
      "| {'then': 1L, 'guess': 1L, ... | {'won t': 1L, 'well if': 1... |\n",
      "| {'week': 1L, 'via': 1L, 'f... | {'week for': 1L, 'for numn... |\n",
      "| {'on': 1L, 'pvr': 1L, 'to'... | {'pvr amp': 1L, 'contraban... |\n",
      "| {'packer': 1L, 'be': 2L, '... | {'at user': 2L, 'would be'... |\n",
      "| {'and': 1L, 'afl': 1L, 'sw... | {'u their': 1L, 'dane swan... |\n",
      "| {'and': 1L, 'on': 1L, 'cel... | {'that is': 1L, 'hi numrd'... |\n",
      "| {'again': 1L, 'for': 1L, '... | {'round num': 1L, 'chalmer... |\n",
      "| {'givens': 1L, 'numth': 1L... | {'raylan givens': 1L, 'a p... |\n",
      "| {'high': 1L, 'at': 2L, 'fr... | {'at user': 1L, 'park dale... |\n",
      "| {'family': 1L, 'costume': ... | {'relationship u': 1L, 'at... |\n",
      "| {'and': 1L, 'cba': 1L, 'ju... | {'at user': 1L, 'wed but':... |\n",
      "| {'and': 1L, 'just': 1L, 'd... | {'napoleon dynamit': 1L, '... |\n",
      "| {'m': 1L, 'dancers': 1L, '... | {'s i': 1L, '4 amp': 1L, '... |\n",
      "| {'and': 1L, 'on': 1L, 'po'... | {'cannot wait': 1L, 's on'... |\n",
      "| {'don': 1L, 'concord': 1L,... | {'tomorrow though': 1L, 'w... |\n",
      "| {'anatomi': 1L, 'and': 1L,... | {'grey s': 1L, 'today and'... |\n",
      "| {'dont': 1L, 'is': 1L, 'it... | {'at user': 1L, 'me so': 1... |\n",
      "| {'counti': 1L, 'fair': 1L,... | {'buckin it': 1L, 'at user... |\n",
      "| {'and': 1L, 'tomorrow': 1L... | {'up num': 1L, 'my money':... |\n",
      "| {'mish': 1L, 'it': 1L, 'in... | {'the first': 1L, 'tune fr... |\n",
      "| {'schedul': 1L, 'sign': 1L... | {'that wa': 1L, 'due to': ... |\n",
      "| {'and': 1L, 'el': 1L, 'it'... | {'at user': 1L, 'yep look'... |\n",
      "| {'on': 1L, 'for': 1L, 'in'... | {'back in': 1L, 'numnd tim... |\n",
      "| {'then': 1L, 'footbal': 1L... | {'varsiti basketbal': 1L, ... |\n",
      "| {'and': 1L, 'on': 2L, 'v':... | {'num card': 1L, 'at user'... |\n",
      "| {'anyway': 1L, 'at': 1L, '... | {'abc run': 1L, 'weekend i... |\n",
      "| {'and': 1L, 'absolutely': ... | {'wa not': 1L, 'at user': ... |\n",
      "| {'gone': 1L, 'is': 1L, 'at... | {'keep chang': 1L, 'at use... |\n",
      "| {'and': 1L, 'on': 1L, 'hot... | {'sang and': 1L, 'and url'... |\n",
      "| {'back': 1L, 'then': 1L, '... | {'everyday po': 1L, 'you g... |\n",
      "| {'a': 1L, 'christmas': 1L,... | {'m po': 1L, 'kid at': 1L,... |\n",
      "| {'and': 1L, 'golden': 1L, ... | {'jan num': 1L, 'laugh ash... |\n",
      "| {'town': 1L, 's': 1L, 'to'... | {'go to': 1L, 'to plymouth... |\n",
      "| {'clipper': 1L, 'paus': 1L... | {'the clipper': 1L, 'clipp... |\n",
      "| {'on': 1L, 'live': 1L, 'br... | {'if you': 1L, 'at user': ... |\n",
      "| {'delux': 1L, 'at': 1L, 'n... | {'delux version': 1L, 'at ... |\n",
      "| {'just': 1L, 'it': 1L, 'nu... | {'num sinc': 1L, 'same dat... |\n",
      "| {'and': 1L, 'it': 1L, 'at'... | {'night in': 1L, 'tomorrow... |\n",
      "| {'be': 2L, 'ch': 1L, 'pain... | {'may be': 1L, 'my pain': ... |\n",
      "| {'cbb': 1L, 'do': 1L, 'now... | {'po on': 1L, 'tomorrow cb... |\n",
      "| {'just': 1L, 'in': 2L, 'it... | {'20 min': 1L, 'of movie':... |\n",
      "| {'and': 1L, 'on': 1L, 'com... | {'nbc fx': 1L, 'and tonigh... |\n",
      "| {'and': 1L, 'footbal': 1L,... | {'game cardin': 1L, 'v nin... |\n",
      "| {'king': 1L, 'about': 1L, ... | {'king jr': 1L, 'won t': 1... |\n",
      "| {'tiger': 1L, 'url': 1L, '... | {'go down': 1L, 'at user':... |\n",
      "| {'and': 1L, 'get': 1L, 'pa... | {'at user': 1L, 'to see': ... |\n",
      "| {'and': 1L, 'is': 1L, 'num... | {'la i': 1L, 'at user': 1L... |\n",
      "| {'a': 1L, 'still': 1L, 'to... | {'he leav': 1L, 'meet tige... |\n",
      "| {'excit': 1L, 'for': 1L, '... | {'board tomorrow': 1L, 'sk... |\n",
      "| {'gone': 2L, 'we': 1L, 'do... | {'ain t': 1L, 'we gone': 1... |\n",
      "| {'and': 1L, 'healthcar': 1... | {'santorum romney': 1L, 'r... |\n",
      "| {'dad': 1L, 'king': 1L, 'm... | {'or no': 1L, 'at user': 1... |\n",
      "| {'on': 1L, 'via': 1L, 'las... | {'elect theme': 1L, 'with ... |\n",
      "| {'streak': 1L, 'unbeaten':... | {'activ unbeaten': 1L, 'ti... |\n",
      "| {'back': 1L, 'for': 1L, 'i... | {'my ipod': 1L, 'back to':... |\n",
      "| {'into': 1L, 'num': 1L, 'a... | {'at user': 2L, 'donat num... |\n",
      "| {'and': 1L, 'it': 1L, 'all... | {'career terry': 1L, 'so c... |\n",
      "| {'duvernay': 1L, 'from': 1... | {'at user': 2L, 'new film'... |\n",
      "| {'numth': 1L, 'liar': 1L, ... | {'the numth': 1L, 'back un... |\n",
      "| {'and': 1L, 'be': 1L, 'ann... | {'state alabama': 1L, 'ne ... |\n",
      "| {'devil': 1L, 'okay': 1L, ... | {'if that': 1L, 'at user':... |\n",
      "| {'a': 1L, 'on': 1L, 'diffe... | {'a thursday': 1L, 'you on... |\n",
      "| {'and': 1L, 'toni': 1L, 'r... | {'roethlisberg or': 1L, 'a... |\n",
      "| {'and': 1L, 'the': 2L, 'on... | {'in the': 1L, 'onli the':... |\n",
      "| {'madonna': 1L, 'all': 1L,... | {'po stars': 1L, 'no madon... |\n",
      "| {'a': 1L, 'pas': 1L, 'actu... | {'law ne': 1L, 'online url... |\n",
      "| {'a': 1L, 'stat': 1L, 'sur... | {'it continue': 1L, 'conti... |\n",
      "| {'cb': 1L, 'mind': 1L, 'it... | {'at user': 1L, 'monday or... |\n",
      "| {'and': 1L, 'bookofquot': ... | {'and i': 1L, 'i wa': 1L, ... |\n",
      "| {'wiki': 1L, 'shakespear':... | {'po bill': 1L, 'shakespea... |\n",
      "| {'and': 1L, 'just': 1L, 's... | {'them on': 1L, 'snippet a... |\n",
      "| {'and': 1L, 'gari': 1L, 'm... | {'at user': 1L, 'po and': ... |\n",
      "| {'past': 1L, 'thursday': 1... | {'s n': 1L, 'haven t': 1L,... |\n",
      "| {'a': 1L, 'ugli': 1L, 'loo... | {'at user': 1L, 'gervai ug... |\n",
      "| {'and': 1L, 'within': 1L, ... | {'and imagin': 1L, 'at use... |\n",
      "| {'and': 1L, 'then': 1L, 'a... | {'may chang': 1L, 'it may'... |\n",
      "| {'be': 1L, 'honestli': 1L,... | {'may be': 1L, 'i think': ... |\n",
      "| {'qtr': 1L, 'through': 1L,... | {'at user': 1L, 'through t... |\n",
      "| {'rva': 1L, 'becaus': 1L, ... | {'at user': 1L, 'novemb nu... |\n",
      "| {'outweigh': 1L, 'on': 1L,... | {'the peopl': 1L, 'cost of... |\n",
      "| {'they': 1L, 'to': 1L, 'ma... | {'user the': 1L, 'at user'... |\n",
      "| {'taylor': 1L, 've': 1L, '... | {'at the': 1L, 'find it': ... |\n",
      "| {'for': 1L, 'get': 1L, 'se... | {'the new': 1L, 'for the':... |\n",
      "| {'all': 1L, 'scorer': 1L, ... | {'with num': 1L, 'all were... |\n",
      "| {'and': 1L, 'numth': 1L, '... | {'po with': 1L, 'num iv': ... |\n",
      "| {'i': 1L, 's': 1L, 'agains... | {'against chelsea': 1L, 't... |\n",
      "| {'see': 1L, 'at': 1L, 'a':... | {'front row': 1L, 'you in'... |\n",
      "| {'and': 1L, 'rank': 1L, 's... | {'user the': 1L, 'numth in... |\n",
      "| {'sound': 1L, 'on': 1L, 'o... | {'po the': 1L, 'sound po':... |\n",
      "| {'texan': 1L, 'i': 1L, 'm'... | {'game sunday': 1L, 'the t... |\n",
      "| {'anatomi': 1L, 'on': 1L, ... | {'the return': 1L, 'new se... |\n",
      "| {'is': 1L, 'num': 1L, 'at'... | {'if he': 1L, 'the ne': 1L... |\n",
      "| {'a': 1L, 'huge': 1L, 'hea... | {'ncaa footbal': 1L, 'race... |\n",
      "| {'a': 1L, 'twitpic': 1L, '... | {'of ticket': 1L, 'at user... |\n",
      "| {'anatomi': 1L, 'on': 1L, ... | {'absolut nothing': 1L, 'g... |\n",
      "| {'tri': 1L, 'on': 1L, 'ari... | {'cardin are': 1L, 'left u... |\n",
      "| {'from': 1L, 'wa': 2L, 'us... | {'po to': 1L, 'one wa': 1L... |\n",
      "| {'it': 1L, 'one': 1L, 'dir... | {'at user': 1L, 'numam sit... |\n",
      "| {'chalmer': 1L, 'to': 1L, ... | {'at user': 1L, 'u go': 1L... |\n",
      "| {'excit': 1L, 'on': 1L, 'o... | {'at user': 1L, 'on sunday... |\n",
      "| {'webb': 1L, 'penalti': 1L... | {'match num': 1L, 'at user... |\n",
      "| {'outweigh': 1L, 'on': 1L,... | {'the peopl': 1L, 'boston ... |\n",
      "| {'saturday': 1L, 'sir': 1L... | {'whi he': 1L, 'check out'... |\n",
      "| {'on': 1L, 'plagiarism': 1... | {'you on': 1L, 'at user': ... |\n",
      "| {'reveal': 1L, 'featur': 1... | {'closet tuesday': 1L, 'of... |\n",
      "| {'on': 1L, 'tcu': 1L, 't':... | {'t wait': 1L, 'vs tcu': 1... |\n",
      "| {'and': 1L, 'celtic': 1L, ... | {'the celtic': 1L, 'firewo... |\n",
      "| {'gonn': 1L, 'hahaaa': 1L,... | {'at user': 1L, 'night bit... |\n",
      "| {'free': 1L, 'olc': 1L, 's... | {'pourier jr': 1L, 'shot f... |\n",
      "| {'a': 1L, 'on': 1L, 'brown... | {'the brown': 1L, 'brown o... |\n",
      "| {'it': 1L, 'num': 2L, 'ard... | {'long ball': 1L, 'everi s... |\n",
      "| {'ali': 1L, 'into': 1L, 't... | {'thi tuesday': 1L, 'tuesd... |\n",
      "| {'is': 1L, 'it': 1L, 'num'... | {'lt num': 1L, 'ellen num'... |\n",
      "| {'a': 1L, 'then': 1L, 'we'... | {'user mayb': 1L, 'at user... |\n",
      "| {'on': 1L, 'life': 1L, 'ni... | {'just isn': 1L, 'liar on'... |\n",
      "| {'mile': 1L, 'on': 1L, 'co... | {'matchup with': 1L, 'alab... |\n",
      "| {'is': 1L, 'it': 1L, 'nove... | {'at user': 1L, 'uva there... |\n",
      "| {'mornjng': 1L, 'earli': 1... | {'do get': 1L, 'see my': 1... |\n",
      "| {'a': 1L, 'despit': 1L, 'a... | {'an interview': 1L, 'i ac... |\n",
      "| {'golf': 1L, 'play': 1L, '... | {'hit up': 1L, 'w ty': 1L,... |\n",
      "| {'and': 1L, 'tuesday': 1L,... | {'thi tuesday': 1L, 'to ha... |\n",
      "| {'wisconsin': 1L, 'for': 1... | {'recall just': 1L, 'remem... |\n",
      "| {'a': 2L, 'chalmer': 1L, '... | {'a home': 2L, 'hat from':... |\n",
      "| {'it': 1L, 'auburn': 1L, '... | {'may be': 1L, 'be the': 1... |\n",
      "| {'prejudic': 1L, 'and': 2L... | {'then the': 1L, 'tonight ... |\n",
      "| {'the': 1L, 'apewalkin': 1... | {'white peopl': 1L, 'reaso... |\n",
      "| {'be': 1L, 'most': 1L, 'ma... | {'may be': 1L, 'dynamit ma... |\n",
      "| {'we': 1L, 'sure': 1L, 'i'... | {'find out': 1L, 'miami we... |\n",
      "| {'spotted': 1L, 'numth': 1... | {'po the': 1L, 'temporari ... |\n",
      "| {'and': 1L, 'houston': 1L,... | {'thank god': 1L, 'today a... |\n",
      "| {'a': 2L, 'monday': 1L, 'g... | {'night well': 1L, 'user y... |\n",
      "| {'a': 1L, 'nepal': 1L, 'in... | {'china to': 1L, 'nepal ka... |\n",
      "| {'and': 1L, 'qtr': 1L, 'es... | {'jay cutler': 1L, 'whi is... |\n",
      "| {'on': 1L, 'pacer': 1L, 'p... | {'po on': 1L, 'to have': 1... |\n",
      "| {'and': 1L, 'on': 1L, 'goo... | {'with option': 1L, 'on th... |\n",
      "| {'sunderland': 1L, 'num': ... | {'of ticket': 1L, 'at user... |\n",
      "| {'on': 1L, 'swansea': 1L, ... | {'exactli po': 1L, 'congra... |\n",
      "| {'movi': 1L, 'into': 1L, '... | {'our url': 1L, 'watch joy... |\n",
      "| {'be': 1L, 'is': 1L, 'oran... | {'at user': 1L, 'of me': 1... |\n",
      "| {'and': 1L, 'love': 1L, 'g... | {'my ipod': 1L, 'phone cal... |\n",
      "| {'there': 1L, 'no': 1L, 'r... | {'park tomorrow': 1L, 'rel... |\n",
      "| {'investor': 1L, 'on': 1L,... | {'with investor': 1L, 'reu... |\n",
      "| {'le': 1L, 'i': 1L, 'ill':... | {'go to': 1L, 'the le': 1L... |\n",
      "| {'coach': 1L, 'just': 1L, ... | {'at thi': 1L, 'bowl speec... |\n",
      "| {'at': 2L, 'in': 1L, 'go':... | {'wa not': 1L, 'at user': ... |\n",
      "| {'on': 1L, 'get': 1L, 'par... | {'on nov': 1L, 'nov numth'... |\n",
      "| {'on': 1L, 'liar': 1L, 'ju... | {'just realiz': 1L, 'liar ... |\n",
      "| {'shoot': 1L, 'the': 2L, '... | {'will be': 1L, 'joint wil... |\n",
      "| {'on': 1L, 'devil': 1L, 'r... | {'at user': 1L, 'the movi'... |\n",
      "| {'thought': 1L, 'about': 1... | {'madonna at': 1L, 'becaus... |\n",
      "| {'cane': 1L, 'deal': 1L, '... | {'not po': 1L, 'biggest ne... |\n",
      "| {'on': 1L, 'liar': 1L, 'pr... | {'never say': 1L, 'except ... |\n",
      "| {'cbc': 1L, 'on': 1L, 'ele... | {'network the': 1L, 'tomor... |\n",
      "| {'georgia': 1L, 'dept': 1L... | {'go in': 1L, 'in the': 1L... |\n",
      "| {'and': 2L, 'jason': 1L, '... | {'the happiest': 1L, 'and ... |\n",
      "| {'numth': 1L, 'to': 1L, 'r... | {'user rob': 1L, 'it in': ... |\n",
      "| {'see': 1L, 'at': 1L, 'go'... | {'probabl wont': 1L, 'at u... |\n",
      "| {'florida': 1L, 'bell': 1L... | {'tim tebow': 1L, 'tebow s... |\n",
      "| {'a': 1L, 'screw': 1L, 'ev... | {'morning screw': 1L, 'lot... |\n",
      "| {'a': 1L, 'on': 1L, 'diffe... | {'a thursday': 1L, 'you on... |\n",
      "| {'houston': 1L, 'gt': 3L, ... | {'long day': 1L, 'of flyin... |\n",
      "| {'anatomi': 1L, 'and': 1L,... | {'vampir diari': 1L, 'grey... |\n",
      "| {'be': 2L, 'we': 1L, 'obed... | {'will be': 1L, 'the other... |\n",
      "| {'user': 1L, 'an': 1L, 'at... | {'you make': 1L, 'at user'... |\n",
      "| {'all': 1L, 've': 2L, 'lfc... | {'on brendon': 1L, 'w can'... |\n",
      "| {'shop': 2L, 'numth': 1L, ... | {'po and': 1L, 'numth url'... |\n",
      "| {'a': 1L, 'be': 1L, 'eyes'... | {'he ha': 1L, 'chri bosh':... |\n",
      "| {'and': 1L, 'employ': 1L, ... | {'after harvard': 1L, 'mos... |\n",
      "| {'just': 1L, 'in': 2L, 'ou... | {'the peopl': 1L, 'peopl s... |\n",
      "| {'be': 2L, 'pain': 2L, 'fo... | {'reason for': 2L, 'somebo... |\n",
      "| {'do': 1L, 'don': 1L, 'wea... | {'at user': 1L, 'do zumba'... |\n",
      "| {'model': 1L, 'plu': 1L, '... | {'upstat anderson': 1L, 'a... |\n",
      "| {'and': 1L, 'on': 1L, 'sch... | {'on and': 1L, 'dynamit is... |\n",
      "| {'and': 1L, 'all': 1L, 'is... | {'at user': 1L, 'all i': 1... |\n",
      "| {'it': 1L, 'an': 1L, 'stil... | {'huntsman for': 1L, 'for ... |\n",
      "| {'we': 1L, 'coz': 1L, 'cho... | {'at user': 1L, 'lt num': ... |\n",
      "| {'me': 1L, 'eddi': 1L, 'an... | {'it s': 1L, 'i think': 1L... |\n",
      "| {'and': 1L, 'a': 2L, 'look... | {'po the': 1L, 'snow in': ... |\n",
      "| {'house': 1L, 'into': 1L, ... | {'integr into': 1L, 'be in... |\n",
      "| {'rang': 1L, 'sam': 1L, 'f... | {'she drives': 1L, 'mom da... |\n",
      "| {'choic': 1L, 'c': 1L, 'sl... | {'sooooo slow': 1L, 'want ... |\n",
      "| {'we': 1L, 'ah': 1L, 'twit... | {'ah well': 1L, 'are mix':... |\n",
      "| {'and': 2L, 'then': 1L, 'e... | {'that is': 1L, 'at user':... |\n",
      "| {'webb': 1L, 'on': 2L, 'ar... | {'after new': 1L, 'place a... |\n",
      "| {'be': 1L, 'february': 1L,... | {'oh i': 1L, 'be num': 1L,... |\n",
      "| {'all': 2L, 'd': 1L, 'that... | {'gervai all': 1L, 'd spen... |\n",
      "| {'ali': 1L, 'am': 1L, 'muh... | {'at the': 1L, 'our woman'... |\n",
      "| {'dwyan': 1L, 'fantasi': 1... | {'dwight ard': 1L, 'derric... |\n",
      "| {'return': 1L, 'look': 1L,... | {'bobcat mavnation': 1L, '... |\n",
      "| {'and': 1L, 'up': 1L, 'not... | {'the big': 1L, 'catch up'... |\n",
      "| {'choic': 1L, 'all': 1L, '... | {'for hma': 1L, 'num numpm... |\n",
      "| {'rey': 1L, 'her': 1L, 'la... | {'may chang': 1L, 'rey her... |\n",
      "| {'and': 1L, 'tomorrow': 1L... | {'oshkosh appleton': 1L, '... |\n",
      "| {'and': 1L, 'copper': 1L, ... | {'for numrd': 1L, 'copper ... |\n",
      "| {'at': 1L, 'in': 1L, 'your... | {'at user': 1L, 'sing find... |\n",
      "| {'bodi': 1L, 'power': 1L, ... | {'won t': 1L, 'at user': 1... |\n",
      "| {'and': 1L, 'all': 1L, 'ju... | {'day but': 1L, 'would pla... |\n",
      "| {'tickets': 1L, 'on': 1L, ... | {'angel be': 1L, 'num th':... |\n",
      "| {'concert': 1L, 'url': 1L,... | {'night s': 1L, 'concert w... |\n",
      "| {'be': 1L, 'wizard': 1L, '... | {'to be': 1L, 'live num': ... |\n",
      "| {'ran': 1L, 'it': 1L, 'at'... | {'at user': 2L, 'indy ne':... |\n",
      "| {'all': 1L, 'concert': 2L,... | {'of them': 1L, 'in concer... |\n",
      "| {'excit': 1L, 'pre': 1L, '... | {'of many': 1L, 'first of'... |\n",
      "| {'wiki': 1L, 'response': 1... | {'googl coffe': 1L, 'i don... |\n",
      "| {'a': 3L, 'be': 1L, 'consi... | {'a po': 1L, 'serious cons... |\n",
      "| {'a': 1L, 'numth': 1L, 'go... | {'by the': 1L, 'is larceny... |\n",
      "| {'clipper': 1L, 'beat': 1L... | {'the clipper': 1L, 'gonna... |\n",
      "| {'billion': 1L, 'classic':... | {'up num': 1L, 'at user': ... |\n",
      "| {'a': 1L, 'kind': 1L, 'i':... | {'etta jame': 1L, 'want a'... |\n",
      "| {'campaign': 1L, 'is': 1L,... | {'parti peopl': 1L, 'campa... |\n",
      "| {'school': 1L, 'le': 1L, '... | {'night till': 1L, 'the le... |\n",
      "| {'and': 1L, 'a': 1L, 'we':... | {'user the': 1L, 'we are':... |\n",
      "| {'el': 1L, 'doing': 1L, 'i... | {'at user': 1L, 'user been... |\n",
      "| {'tri': 1L, 'parad': 1L, '... | {'at user': 1L, 'lt num': ... |\n",
      "| {'a': 1L, 'mike': 1L, 'dev... | {'yellow rock': 1L, 'den y... |\n",
      "| {'and': 1L, 'me': 1L, 'nov... | {'at user': 1L, 'novemb nu... |\n",
      "| {'on': 2L, 'about': 1L, 'a... | {'at num': 1L, 'on the': 1... |\n",
      "| {'teen': 1L, 'a': 1L, 'fro... | {'numrd season': 1L, 'nump... |\n",
      "| {'yeah': 1L, 'georgia': 1L... | {'back in': 1L, 'i think':... |\n",
      "| {'all': 1L, 'av': 1L, 'age... | {'on wed': 1L, 'joy nois':... |\n",
      "| {'and': 1L, 'at': 1L, 'in'... | {'numth qt': 1L, 'user u':... |\n",
      "| {'even': 1L, 'el': 1L, 're... | {'nufc too': 1L, 'even rew... |\n",
      "| {'again': 1L, 'we': 1L, 'j... | {'our pirat': 1L, 'let tey... |\n",
      "| {'user': 1L, 'steel': 2L, ... | {'wa hang': 1L, 'out with'... |\n",
      "| {'a': 1L, 'index': 1L, 'in... | {'po indicator': 1L, 'a po... |\n",
      "| {'do': 1L, 'anim': 2L, 'fo... | {'forget the': 1L, 'vid to... |\n",
      "| {'be': 2L, 'pain': 1L, 'fo... | {'may be': 1L, 'the url': ... |\n",
      "| {'webb': 1L, 'bugger': 1L,... | {'cheeki bugger': 1L, 'at ... |\n",
      "| {'and': 1L, 'franc': 1L, '... | {'davi and': 1L, 'between ... |\n",
      "| {'c': 1L, 'google': 1L, 'j... | {'source c': 1L, 'just one... |\n",
      "| {'and': 1L, 'on': 1L, 'saf... | {'is safe': 1L, 'make sure... |\n",
      "| {'and': 1L, 'earli': 1L, '... | {'comedi central': 1L, 'of... |\n",
      "| {'ago': 1L, 'on': 2L, 'pre... | {'wizard of': 1L, 'ago on'... |\n",
      "| {'it': 1L, 'num': 2L, 'in'... | {'in downtown': 1L, 'know ... |\n",
      "| {'it': 1L, 'see': 1L, 'num... | {'haven t': 1L, 'can do': ... |\n",
      "| {'a': 1L, 'do': 1L, 'have'... | {'it at': 1L, 'have a': 1L... |\n",
      "| {'and': 1L, 'birthday': 1L... | {'jk rowling': 1L, 'birthd... |\n",
      "| {'and': 1L, 'the': 1L, 'al... | {'all there': 1L, 'at user... |\n",
      "| {'and': 1L, 'good': 1L, 'f... | {'kinda orta': 1L, 'for th... |\n",
      "| {'it': 2L, 'at': 1L, 'foll... | {'tomorrow plea': 1L, 'at ... |\n",
      "| {'over': 1L, 'afternoons':... | {'at user': 2L, 'on tuesda... |\n",
      "| {'wont': 1L, 'ive': 1L, 'p... | {'at user': 1L, 'exam tomo... |\n",
      "| {'gordon': 1L, 'cuban': 1L... | {'at user': 1L, 'kaman jae... |\n",
      "| {'caus': 1L, 'good': 1L, '... | {'po the': 1L, 'at user': ... |\n",
      "| {'and': 1L, 'wifff': 1L, '... | {'go to': 1L, 'at user': 3... |\n",
      "| {'a': 1L, 'from': 1L, 'doe... | {'black to': 1L, 'user eve... |\n",
      "| {'love': 1L, 'money': 1L, ... | {'foxtel is': 1L, 'on tues... |\n",
      "| {'mucha': 1L, 'all': 1L, '... | {'all year': 1L, 'm gonna'... |\n",
      "| {'year': 1L, 'at': 2L, 'in... | {'at user': 2L, 'their num... |\n",
      "| {'num': 1L, 'im': 1L, 'at'... | {'i could': 1L, 'at user':... |\n",
      "| {'a': 1L, 'numth': 2L, 'la... | {'for jr': 1L, 'numth a': ... |\n",
      "| {'clipper': 1L, 'and': 1L,... | {'of decemb': 1L, 'ticket ... |\n",
      "| {'on': 3L, 'all': 1L, 'con... | {'wbz num': 1L, 'hear at':... |\n",
      "| {'and': 1L, 'week': 1L, 'g... | {'ever and': 1L, 'and it':... |\n",
      "| {'be': 1L, 'say': 1L, 'bro... | {'up bron': 1L, 'record ca... |\n",
      "| {'load': 1L, 'everi': 1L, ... | {'danc steps': 1L, 'fit cl... |\n",
      "| {'rewards': 1L, 'banking':... | {'banking london': 1L, 're... |\n",
      "| {'golden': 1L, 'globe': 1L... | {'at user': 1L, 'an po': 1... |\n",
      "| {'people': 1L, 'opposit': ... | {'sandi friday': 1L, 'dure... |\n",
      "| {'a': 2L, 'on': 1L, 'marle... | {'you catch': 1L, 'a doubl... |\n",
      "| {'and': 1L, 'all': 1L, 'in... | {'church tomorrow': 1L, 'n... |\n",
      "| {'deal': 1L, 'play': 1L, '... | {'gossip girl': 1L, 'until... |\n",
      "| {'a': 1L, 'wiki': 1L, 'num... | {'at user': 3L, 'her numrd... |\n",
      "| {'gt': 3L, 'feedback': 1L,... | {'much just': 1L, 'atleast... |\n",
      "| {'and': 1L, 'is': 2L, 'it'... | {'and glove': 1L, 'today k... |\n",
      "| {'for': 1L, 'can': 1L, 'se... | {'wives truebitch': 1L, 't... |\n",
      "| {'me': 1L, 'on': 1L, 'c': ... | {'liam plea': 1L, 'at user... |\n",
      "| {'and': 1L, 'boy': 1L, 'al... | {'to num': 1L, 'uva by': 1... |\n",
      "| {'a': 1L, 'on': 1L, 'hold'... | {'a po': 1L, 'sunderland v... |\n",
      "| {'matt': 1L, 'nfl': 1L, 'f... | {'seattl seahawk': 1L, 'bl... |\n",
      "| {'be': 1L, 'major': 1L, 'f... | {'may be': 1L, 'later x': ... |\n",
      "| {'song': 1L, 'friday': 1L,... | {'the song': 1L, 'rememb t... |\n",
      "| {'and': 1L, 'just': 1L, 'd... | {'napoleon dynamit': 1L, '... |\n",
      "| {'king': 1L, 'luther': 1L,... | {'king jr': 1L, 'jr url': ... |\n",
      "| {'a': 1L, 'newcastl': 1L, ... | {'the why': 1L, 'score a':... |\n",
      "| {'and': 1L, 'night': 1L, '... | {'num pm': 1L, 'sanctuary ... |\n",
      "| {'end': 1L, 'that': 1L, 'd... | {'that downton': 1L, 'ne a... |\n",
      "| {'em': 1L, 'even': 1L, 'us... | {'user numst': 1L, 'at use... |\n",
      "| {'and': 1L, 'citi': 1L, 'u... | {'at user': 1L, 'and ex': ... |\n",
      "| {'and': 1L, 'from': 1L, 'j... | {'the laker': 1L, 'destroy... |\n",
      "| {'numth': 1L, 'night': 1L,... | {'po without': 1L, 'night ... |\n",
      "| {'all': 1L, 'four': 1L, 'n... | {'for the': 1L, 'four th':... |\n",
      "| {'week': 1L, 'and': 1L, 'a... | {'news br': 1L, 'numerspar... |\n",
      "| {'a': 2L, 'these': 1L, 'on... | {'a po': 1L, 't wait': 1L,... |\n",
      "| {'and': 1L, 'kidrauhl': 1L... | {'at user': 1L, 'tomorrow ... |\n",
      "| {'durant': 1L, 'becom': 1L... | {'numnd youngest': 1L, 'yo... |\n",
      "| {'ami': 1L, 'golden': 1L, ... | {'news today': 1L, 'poehle... |\n",
      "| {'is': 1L, 'num': 1L, 'at'... | {'at user': 1L, 'is tune':... |\n",
      "| {'a': 1L, 'late': 1L, 'mcg... | {'a po': 1L, 'fiesta bowl'... |\n",
      "| {'a': 1L, 'on': 1L, 'i': 2... | {'go out': 1L, 'firstworld... |\n",
      "| {'be': 1L, 'matt': 1L, 'fi... | {'at user': 1L, 'be inconv... |\n",
      "| {'fifa': 1L, 'el': 1L, 'my... | {'at user': 1L, 'user may'... |\n",
      "| {'and': 1L, 'just': 1L, 'c... | {'chip not': 1L, 'at user'... |\n",
      "| {'or': 1L, 'tomorrow': 1L,... | {'tomorrow po': 1L, 'lsu o... |\n",
      "| {'num': 2L, 'at': 2L, 'tom... | {'shoot then': 1L, 'till n... |\n",
      "| {'el': 1L, 'lock': 1L, 'it... | {'marseil psg': 1L, 'num u... |\n",
      "| {'mile': 1L, 'le': 1L, 'ma... | {'say he': 1L, 'coach le':... |\n",
      "| {'and': 1L, 'dwight': 1L, ... | {'alreadi ne': 1L, 'dwight... |\n",
      "| {'a': 1L, 'pas': 1L, 'just... | {'law ne': 1L, 'pas a': 1L... |\n",
      "| {'lamar': 1L, 'quarter': 1... | {'start numnd': 1L, 'actio... |\n",
      "| {'and': 1L, 'golden': 1L, ... | {'with it': 1L, 'user lebl... |\n",
      "| {'user': 2L, 'the': 1L, 'n... | {'kendal i': 1L, 'it s': 1... |\n",
      "| {'and': 1L, 'is': 1L, 'bac... | {'come up': 1L, 'work burn... |\n",
      "| {'a': 1L, 'numth': 1L, 'of... | {'of that': 1L, 'a john': ... |\n",
      "| {'for': 1L, 'numrd': 1L, '... | {'time mark': 1L, 'the man... |\n",
      "| {'numth': 1L, 'race': 2L, ... | {'num at': 1L, 'hint num':... |\n",
      "| {'a': 1L, 'bill': 1L, 'hi'... | {'forc hi': 1L, 'hi former... |\n",
      "| {'becaus': 1L, 'tumblr': 1... | {'at user': 1L, 'm honestl... |\n",
      "| {'a': 1L, 'anatomi': 1L, '... | {'user you': 1L, 'at user'... |\n",
      "| {'chalmer': 1L, 'foul': 1L... | {'for not': 1L, 'of mani':... |\n",
      "| {'album': 2L, 'again': 1L,... | {'po so': 1L, 's a': 1L, '... |\n",
      "| {'box': 1L, 'and': 2L, 'st... | {'ne dinner': 1L, 'muhamma... |\n",
      "| {'a': 2L, 'after': 1L, 'pi... | {'ne in': 1L, 'url infosec... |\n",
      "| {'i': 2L, 'january': 1L, '... | {'withdrawal i': 1L, 'back... |\n",
      "| {'and': 1L, 'on': 1L, 'gue... | {'and nick': 1L, 'po the':... |\n",
      "| {'thril': 1L, 'a': 1L, 'so... | {'son url': 1L, 'bowi s': ... |\n",
      "| {'a': 1L, 'newcastl': 1L, ... | {'some po': 1L, 'at user':... |\n",
      "| {'we': 1L, 'last': 1L, 'ur... | {'po the': 1L, 'convoc we'... |\n",
      "| {'aw': 1L, 'it': 1L, 'at':... | {'wnbc aw': 1L, 'at user':... |\n",
      "| {'cramps': 1L, 'numth': 1L... | {'chalmers c': 1L, 'ball o... |\n",
      "| {'be': 1L, 'all': 1L, 'lma... | {'may be': 1L, 'the celtic... |\n",
      "| {'be': 1L, 'said': 1L, 'wo... | {'she would': 1L, 'at user... |\n",
      "| {'becaus': 1L, 'kfc': 1L, ... | {'age he': 1L, 'be there':... |\n",
      "| {'michael': 1L, 'dc': 1L, ... | {'at user': 1L, 'dc game':... |\n",
      "| {'tonight': 1L, 'by': 1L, ... | {'new plymouth': 1L, 'q fi... |\n",
      "| {'the': 1L, 'georgia': 1L,... | {'watch saturday': 1L, 'su... |\n",
      "| {'king': 1L, 'luther': 1L,... | {'to perspect': 1L, 'at us... |\n",
      "| {'a': 2L, 'then': 1L, 'for... | {'back in': 1L, 'at user':... |\n",
      "| {'a': 1L, 'on': 1L, 'dalla... | {'page enter': 1L, 'at use... |\n",
      "| {'awar': 1L, 'don': 1L, 'w... | {'miss it': 1L, 'num octob... |\n",
      "| {'just': 1L, 'is': 1L, 'ha... | {'devil s': 1L, 'wa born':... |\n",
      "| {'love': 1L, 'movi': 1L, '... | {'tom amp': 1L, 'at user':... |\n",
      "| {'golden': 1L, 'num': 5L, ... | {'up num': 1L, 'at user': ... |\n",
      "| {'then': 1L, 'be': 1L, 'i'... | {'i think': 1L, 'at user':... |\n",
      "| {'and': 1L, 'night': 1L, '... | {'at user': 2L, 'night of'... |\n",
      "| {'and': 1L, 'on': 1L, 'abo... | {'ve been': 1L, 'on monday... |\n",
      "| {'and': 1L, 'some': 1L, 'n... | {'i could': 1L, 'at user':... |\n",
      "| {'a': 1L, 'on': 1L, 'for':... | {'anfield for': 1L, 'a fan... |\n",
      "| {'in': 1L, 'one': 1L, 'at'... | {'user the': 1L, 'at user'... |\n",
      "| {'nwc': 1L, 'it': 1L, 'nov... | {'in wshh': 1L, 'novemb nu... |\n",
      "| {'and': 1L, 'just': 1L, 's... | {'it in': 1L, 'smith just'... |\n",
      "| {'sound': 1L, 'them': 1L, ... | {'user it': 1L, 'to them':... |\n",
      "| {'so': 1L, 'wanna': 1L, 'i... | {'with al': 1L, 'miss my':... |\n",
      "| {'and': 2L, 'golden': 1L, ... | {'an po': 1L, 'januari and... |\n",
      "| {'and': 1L, 'is': 1L, 'bec... | {'at user': 1L, 'that it':... |\n",
      "| {'on': 1L, 'liverpool': 1L... | {'anfield ne': 1L, 'to lea... |\n",
      "| {'on': 1L, 'pain': 1L, 'th... | {'night url': 1L, 'my pain... |\n",
      "| {'and': 1L, 'rhap': 1L, 's... | {'rob may': 1L, 'rhap fan'... |\n",
      "| {'be': 1L, 'after': 1L, 'm... | {'in march': 1L, 'rick san... |\n",
      "| {'ami': 1L, 'and': 1L, 'ti... | {'poehler are': 1L, 'to ho... |\n",
      "| {'num': 1L, 'parad': 1L, '... | {'po to': 1L, 'to ride': 1... |\n",
      "| {'a': 1L, 'rumbl': 1L, 'ap... | {'num ppv': 1L, 'at user':... |\n",
      "| {'and': 1L, 'all': 1L, 'ma... | {'suggest david': 1L, 'dav... |\n",
      "| {'tonight': 1L, 'num': 1L,... | {'time again': 1L, 'their ... |\n",
      "| {'a': 1L, 'be': 2L, 'hire'... | {'now but': 1L, 'to mow': ... |\n",
      "| {'just': 1L, 'al': 1L, 'at... | {'s stay': 1L, 'saturday j... |\n",
      "| {'a': 1L, 'on': 2L, 'num':... | {'is that': 1L, 'toofar ur... |\n",
      "| {'even': 1L, 'on': 1L, 'fo... | {'a footbal': 1L, 'the cow... |\n",
      "| {'on': 1L, 'ryback': 1L, '... | {'should beat': 1L, 'user ... |\n",
      "| {'kidrauhl': 1L, 'c': 1L, ... | {'s trend': 1L, 'so let': ... |\n",
      "| {'absolut': 2L, 'c': 1L, '... | {'not wait': 1L, 'reveal p... |\n",
      "| {'and': 1L, 'both': 1L, 'f... | {'for pot': 1L, 'jordan je... |\n",
      "| {'newcastl': 1L, 'score': ... | {'open for': 1L, 'newcastl... |\n",
      "| {'a': 2L, 'numth': 1L, 'sa... | {'in paris': 1L, 'the numt... |\n",
      "| {'and': 1L, 'over': 1L, 'n... | {'at user': 1L, 'the nfl':... |\n",
      "| {'play': 1L, 'for': 1L, 'd... | {'swansea yesterday': 1L, ... |\n",
      "| {'and': 2L, 'have': 1L, 'i... | {'ne year': 1L, 'singl one... |\n",
      "| {'mlk': 1L, 'thursday': 1L... | {'or thursday': 1L, 'go to... |\n",
      "| {'numth': 1L, 'didn': 1L, ... | {'bradi is': 1L, 'at user'... |\n",
      "| {'num': 1L, 'at': 3L, 'in'... | {'at user': 3L, 'the stree... |\n",
      "| {'a': 1L, 'king': 1L, 'pla... | {'that tomorrow': 1L, 'if ... |\n",
      "| {'teen': 1L, 'tomorrow': 1... | {'t wait': 1L, 'tomorrow p... |\n",
      "| {'and': 1L, 'torress': 1L,... | {'fernando llorent': 1L, '... |\n",
      "| {'a': 1L, 'c': 1L, 'don': ... | {'just don': 1L, 'at user'... |\n",
      "| {'and': 1L, 'we': 1L, 'sur... | {'he got': 1L, 'at user': ... |\n",
      "| {'awar': 1L, 'u': 1L, 'thi... | {'u chang': 1L, 'have abou... |\n",
      "| {'respond': 1L, 'feed': 1L... | {'at user': 1L, 'is head':... |\n",
      "| {'em': 1L, 'ive': 1L, 'wat... | {'jerebko drummond': 1L, '... |\n",
      "| {'and': 1L, 'just': 1L, 'g... | {'user the': 1L, 'becaus w... |\n",
      "| {'count': 1L, 'a': 1L, 'di... | {'long time': 1L, 'first t... |\n",
      "| {'be': 1L, 'num': 1L, 'hum... | {'today may': 1L, 'that to... |\n",
      "| {'begin': 1L, 'be': 1L, 'g... | {'will be': 1L, 'swirling ... |\n",
      "| {'on': 1L, 'mathieu': 1L, ... | {'s lsu': 1L, 'file photo'... |\n",
      "| {'philli': 1L, 'back': 1L,... | {'went to': 1L, 'mom go': ... |\n",
      "| {'direction': 1L, 'one': 1... | {'the boy': 1L, 'me the': ... |\n",
      "| {'and': 1L, 'be': 1L, 'nor... | {'may be': 1L, 'surveil is... |\n",
      "| {'and': 1L, 'it': 1L, 'see... | {'5pm friday': 1L, 'by 5pm... |\n",
      "| {'scutaro': 2L, 'hip': 1L,... | {'game with': 1L, 'injury ... |\n",
      "| {'and': 1L, 'afrojack': 1L... | {'afrojack tonight': 1L, '... |\n",
      "| {'and': 1L, 'over': 1L, 'a... | {'i could': 1L, 'we ne': 1... |\n",
      "| {'concert': 1L, 'h': 1L, '... | {'july h': 1L, 'still got'... |\n",
      "| {'brazil': 1L, 'numth': 1L... | {'ha url': 1L, 'and last':... |\n",
      "| {'b': 1L, 'may': 1L, 'is':... | {'in may': 1L, 'still gonn... |\n",
      "| {'lmfaoooo': 1L, 'at': 2L,... | {'at user': 2L, 'mothafuck... |\n",
      "| {'start': 1L, 'of': 1L, 'l... | {'to start': 1L, 'of my': ... |\n",
      "| {'even': 1L, 'me': 1L, 'al... | {'at user': 2L, 'is that':... |\n",
      "| {'serv': 1L, 'the': 2L, 'a... | {'parfait and': 1L, 'po th... |\n",
      "| {'about': 1L, 'her': 1L, '... | {'her passi': 1L, 'wednesd... |\n",
      "| {'move': 1L, 'southampton'... | {'te url': 1L, 's januari'... |\n",
      "| {'begin': 1L, 'numnd': 1L,... | {'kinda ridiculous': 1L, '... |\n",
      "| {'and': 1L, 'el': 2L, 'at'... | {'she said': 1L, 'at user'... |\n",
      "| {'lfc': 1L, 'emot': 1L, 'a... | {'boro lfc': 1L, 'with the... |\n",
      "| {'and': 2L, 'play': 1L, 'j... | {'go to': 1L, 'at user': 1... |\n",
      "| {'see': 1L, 'num': 1L, 'at... | {'at user': 1L, 'lt num': ... |\n",
      "| {'do': 1L, 'heather': 1L, ... | {'numnetwork oldham': 1L, ... |\n",
      "| {'a': 1L, 'do': 1L, 'and':... | {'at user': 1L, 'teacher a... |\n",
      "| {'a': 1L, 'daboswinneyprob... | {'or the': 1L, 'crowd url'... |\n",
      "| {'me': 1L, 'on': 2L, 'to':... | {'rock onlin': 1L, 'to wat... |\n",
      "| {'wtf': 1L, 'we': 1L, 'epi... | {'to wait': 1L, 'wait til'... |\n",
      "| {'the': 2L, 'onli': 1L, 't... | {'tomorrow the': 1L, 'my r... |\n",
      "| {'and': 1L, 'a': 1L, 'actu... | {'if a': 1L, 'up in': 1L, ... |\n",
      "| {'num': 1L, 'at': 1L, 'in'... | {'davi in': 1L, 'at user':... |\n",
      "| {'and': 1L, 'golden': 1L, ... | {'fey and': 1L, 'globes am... |\n",
      "| {'a': 3L, 'don': 1L, 'shir... | {'i think': 1L, 'to wear':... |\n",
      "| {'on': 1L, 'url': 1L, 'is'... | {'night url': 1L, 'at user... |\n",
      "| {'school': 1L, 'valley': 1... | {'the dutchman': 1L, 'vall... |\n",
      "| {'reveal': 1L, 'play': 1L,... | {'at user': 1L, 'go po': 1... |\n",
      "| {'on': 1L, 'url': 1L, 'sky... | {'to sleep': 1L, 'at user'... |\n",
      "| {'some': 1L, 'everytim': 1... | {'cup of': 1L, 'morn amp':... |\n",
      "| {'a': 1L, 'chalmer': 1L, '... | {'to have': 1L, 'at num': ... |\n",
      "| {'night': 1L, 'play': 1L, ... | {'taylor introduc': 1L, 'g... |\n",
      "| {'cbb': 1L, 'week': 1L, 'n... | {'week of': 1L, 'cbb seaso... |\n",
      "| {'be': 1L, 'to': 1L, 'i': ... | {'by eleven': 1L, 'i could... |\n",
      "| {'impact': 1L, 'both': 1L,... | {'a night': 1L, 'effects u... |\n",
      "| {'a': 1L, 'on': 1L, 'ucla'... | {'angel gobear': 1L, 'ne a... |\n",
      "| {'and': 1L, 'trainer': 1L,... | {'ha num': 1L, 'jubili gir... |\n",
      "| {'ami': 1L, 'golden': 1L, ... | {'fey ami': 1L, 'poehler a... |\n",
      "| {'town': 1L, 'a': 2L, 'sma... | {'go home': 1L, 'wisconsin... |\n",
      "| {'even': 1L, 'king': 1L, '... | {'to pieces': 1L, 'pieces ... |\n",
      "| {'a': 1L, 'url': 1L, 'park... | {'m the': 1L, 'po news': 1... |\n",
      "| {'love': 1L, 'xoxox': 1L, ... | {'with num': 1L, 'to say':... |\n",
      "| {'and': 2L, 'numth': 1L, '... | {'i think': 1L, 'all those... |\n",
      "| {'and': 1L, 'on': 1L, 'abo... | {'birthday and': 1L, 'marc... |\n",
      "| {'and': 1L, 'on': 1L, 'thu... | {'closer to': 1L, 'grey s'... |\n",
      "| {'on': 1L, 'honestli': 1L,... | {'jersey shore': 1L, 'may ... |\n",
      "| {'a': 2L, 'on': 1L, 'liver... | {'newcatl unit': 1L, 'rate... |\n",
      "| {'tri': 2L, 'on': 1L, 'for... | {'po s': 1L, 'lefti tri': ... |\n",
      "| {'a': 1L, 'el': 1L, 'for':... | {'po for': 1L, 'sunday nig... |\n",
      "| {'thebachelor': 1L, 'excit... | {'start monday': 1L, 's se... |\n",
      "| {'onli': 1L, 'don': 1L, 'c... | {'t think': 1L, 'at user':... |\n",
      "| {'look': 1L, 'movi': 1L, '... | {'is po': 1L, 'the movi': ... |\n",
      "| {'tuesday': 1L, 'num': 1L,... | {'week trial': 1L, 'at use... |\n",
      "| {'anatomi': 1L, 'on': 1L, ... | {'didn t': 1L, 'grey anato... |\n",
      "| {'costum': 1L, 'that': 1L,... | {'m the': 1L, 'if you': 1L... |\n",
      "| {'and': 3L, 'do': 1L, 'rou... | {'dynasti league': 1L, 'tw... |\n",
      "| {'and': 1L, 'the': 2L, 'br... | {'tune ha': 1L, 'shore and... |\n",
      "| {'soccer': 1L, 'stone': 1L... | {'the big': 1L, 'wreckemte... |\n",
      "| {'numth': 1L, 'onli': 1L, ... | {'wear my': 1L, 'onli the'... |\n",
      "| {'bold': 1L, 'cowboy': 1L,... | {'will beat': 1L, 'sunday ... |\n",
      "| {'and': 1L, 'at': 2L, 'tom... | {'at user': 1L, 'time watc... |\n",
      "| {'respond': 1L, 'be': 1L, ... | {'devil s': 1L, 'one of': ... |\n",
      "| {'claim': 1L, 'nfl': 1L, '... | {'wa claim': 1L, 'by the':... |\n",
      "| {'and': 3L, 'all': 1L, 'al... | {'at user': 1L, 'think abo... |\n",
      "| {'give': 1L, 'philli': 1L,... | {'would be': 1L, 'for a': ... |\n",
      "| {'high': 1L, 'at': 2L, 'fr... | {'at user': 1L, 'user come... |\n",
      "| {'stat': 1L, 'nigga': 1L, ... | {'dwight ard': 1L, 'ha end... |\n",
      "| {'el': 1L, 'about': 1L, 'l... | {'get your': 1L, 'you talk... |\n",
      "| {'on': 1L, 'patriot': 1L, ... | {'patriot just': 1L, 'down... |\n",
      "| {'anatomi': 1L, 'me': 1L, ... | {'tomorrow obvious': 1L, '... |\n",
      "| {'brown': 1L, 'mike': 1L, ... | {'lakers mike': 1L, 'go in... |\n",
      "| {'and': 1L, 'singls': 1L, ... | {'trust i': 1L, 'i could':... |\n",
      "| {'on': 1L, 'we': 2L, 'll':... | {'po we': 1L, 'away hahaha... |\n",
      "| {'and': 2L, 'then': 1L, 'l... | {'see my': 1L, 'po and': 1... |\n",
      "| {'holla': 1L, 'poker': 1L,... | {'halloween start': 1L, 'd... |\n",
      "| {'and': 2L, 'all': 1L, 'nb... | {'community park': 1L, 'at... |\n",
      "| {'earli': 1L, 'the': 1L, '... | {'you url': 1L, 'if you': ... |\n",
      "| {'and': 1L, 'on': 1L, 'c':... | {'same level': 1L, 'and dr... |\n",
      "| {'drop': 1L, 'october': 1L... | {'didn t': 1L, 'cri thi': ... |\n",
      "| {'brazil': 1L, 'differ': 1... | {'at user': 1L, 'word for'... |\n",
      "| {'via': 1L, 'an': 1L, 'lsu... | {'an sec': 1L, 'to lsu': 1... |\n",
      "| {'and': 1L, 'the': 1L, 'is... | {'numrd child': 1L, 're nu... |\n",
      "| {'a': 3L, 'mlk': 1L, 'ariz... | {'s a': 1L, 's whi': 1L, '... |\n",
      "| {'on': 1L, 'nffc': 1L, 'fr... | {'the table': 1L, 'numth i... |\n",
      "| {'a': 1L, 'on': 1L, 'and':... | {'the numth': 1L, 'tmill i... |\n",
      "| {'a': 1L, 'be': 1L, 'i': 2... | {'can get': 1L, 'ne food':... |\n",
      "| {'homegrown': 1L, 'recentl... | {'design with': 1L, 'veri ... |\n",
      "| {'at': 1L, 'onli': 1L, 'ol... | {'page onli': 1L, 'u must'... |\n",
      "| {'buckey': 1L, 'loss': 1L,... | {'numth in': 1L, 'bc po': ... |\n",
      "| {'frank': 1L, 'make': 1L, ... | {'make po': 1L, 'po tomorr... |\n",
      "| {'user': 1L, 'on': 1L, 'sp... | {'hi spireit': 1L, 'at use... |\n",
      "| {'earli': 1L, 'a': 1L, 'ma... | {'voting url': 1L, 'stori ... |\n",
      "| {'on': 1L, 'tuesday': 1L, ... | {'tuesday januari': 1L, 't... |\n",
      "| {'and': 1L, 'we': 1L, 'in'... | {'sugar bowl': 1L, 'bowl a... |\n",
      "| {'el': 1L, 'it': 1L, 'at':... | {'numth in': 1L, 'at user'... |\n",
      "| {'and': 1L, 'march': 1L, '... | {'and po': 1L, 'can the': ... |\n",
      "| {'me': 1L, 'i': 1L, 'sun':... | {'the sun': 1L, 'live in':... |\n",
      "| {'on': 1L, 'run': 1L, 'rel... | {'i text': 1L, 'xc relay':... |\n",
      "| {'and': 1L, 'tinchi': 1L, ... | {'at user': 1L, 'templ are... |\n",
      "| {'gone': 1L, 'said': 1L, '... | {'in line': 1L, 'gone stan... |\n",
      "| {'town': 1L, 'numth': 1L, ... | {'at user': 1L, 'mlk get':... |\n",
      "| {'and': 2L, 'aha': 1L, 'ye... | {'at user': 1L, 'user aha'... |\n",
      "| {'be': 1L, 'play': 1L, 'ri... | {'sugar bowl': 1L, 'be the... |\n",
      "| {'and': 1L, 'el': 1L, 'jus... | {'my tvdukfamili': 1L, 'an... |\n",
      "| {'it': 2L, 'say': 1L, 'num... | {'the ready': 1L, 'ready a... |\n",
      "| {'boy': 1L, 'monday': 1L, ... | {'at user': 2L, 'contraban... |\n",
      "| {'a': 1L, 'be': 1L, 'don':... | {'it happi': 1L, 'num job'... |\n",
      "| {'week': 1L, 'be': 1L, 'al... | {'may be': 1L, 'and kim': ... |\n",
      "| {'play': 1L, 'said': 1L, '... | {'msg against': 1L, 'po to... |\n",
      "| {'saturday': 1L, 'the': 1L... | {'to go': 1L, 'nation cham... |\n",
      "| {'vs': 1L, 'heat': 1L, 'to... | {'heat vs': 1L, 'the heat'... |\n",
      "| {'a': 1L, 'last': 1L, 'num... | {'score a': 1L, 'a po': 1L... |\n",
      "| {'excit': 1L, 'houston': 1... | {'back to': 1L, 'tomorrow ... |\n",
      "| {'a': 1L, 'numth': 1L, 'we... | {'get a': 1L, 't wait': 1L... |\n",
      "| {'chapter': 1L, 'on': 1L, ... | {'item for': 1L, 'chapter ... |\n",
      "| {'last': 1L, 'wa': 1L, 'no... | {'the sun': 1L, 'rhode wa'... |\n",
      "| {'portray': 1L, 'a': 1L, '... | {'a po': 1L, 'continu next... |\n",
      "| {'a': 2L, 'be': 1L, 'may':... | {'derek fisher': 1L, 'woul... |\n",
      "| {'and': 1L, 'love': 1L, 'n... | {'at user': 1L, 'may the':... |\n",
      "| {'move': 1L, 'it': 1L, 'pa... | {'move up': 1L, 'dominik p... |\n",
      "| {'be': 2L, 'wssu': 1L, 'st... | {'with pipa': 1L, 'you kno... |\n",
      "| {'and': 1L, 'do': 1L, 'foo... | {'at user': 1L, 'and po': ... |\n",
      "| {'and': 1L, 'sander': 1L, ... | {'director rupert': 1L, 'h... |\n",
      "| {'wasn': 1L, 'it': 1L, 'ye... | {'sterl sent': 1L, 'goal i... |\n",
      "| {'and': 1L, 'is': 2L, 'it'... | {'po lt': 1L, 'see it': 1L... |\n",
      "| {'on': 1L, 'work': 1L, 'lo... | {'welcom monday': 1L, 'hus... |\n",
      "| {'matt': 2L, 'to': 1L, 'i'... | {'to hm': 1L, 'call in': 1... |\n",
      "| {'ralli': 1L, 'jr': 1L, 'p... | {'jr s': 1L, 's have': 1L,... |\n",
      "| {'and': 1L, 'numth': 1L, '... | {'seriously got': 1L, 'use... |\n",
      "| {'we': 1L, 'nin': 1L, 're'... | {'dubai na': 1L, 'almost h... |\n",
      "| {'choic': 1L, 'tomorrow': ... | {'choic awards': 1L, 'day ... |\n",
      "| {'and': 1L, 'eo': 1L, 'cha... | {'at user': 1L, 'depend on... |\n",
      "| {'and': 1L, 'do': 1L, 'i':... | {'saturday ever': 1L, 'dyn... |\n",
      "| {'shelter': 1L, 'to': 1L, ... | {'nov num': 1L, 'mi nov': ... |\n",
      "| {'tournament': 1L, 'iu': 1... | {'the b': 2L, 'at num': 1L... |\n",
      "| {'me': 1L, 'rey': 1L, 'lan... | {'one to': 1L, 'some one':... |\n",
      "| {'villa': 1L, 'aston': 1L,... | {'to sunderland': 1L, 'sun... |\n",
      "| {'saturday': 2L, 'school':... | {'at school': 1L, 'everi d... |\n",
      "| {'on': 1L, 'can': 1L, 'may... | {'t wait': 1L, 'at user': ... |\n",
      "| {'exclusive': 1L, 'added':... | {'f word': 2L, 'to mtv': 1... |\n",
      "| {'and': 1L, 'on': 1L, 'dev... | {'qb alex': 1L, 'same page... |\n",
      "| {'cbb': 1L, 'monday': 1L, ... | {'monday is': 1L, 'call in... |\n",
      "| {'saturday': 1L, 'on': 1L,... | {'agre to': 1L, 'num he': ... |\n",
      "| {'golden': 1L, 'globe': 1L... | {'date get': 1L, 'at user'... |\n",
      "| {'and': 1L, 'taxi': 1L, 'i... | {'you mean': 1L, 'town he'... |\n",
      "| {'and': 1L, 'all': 1L, 'al... | {'at user': 1L, 'i get': 1... |\n",
      "| {'on': 2L, 'itun': 1L, 'wh... | {'littl thing': 1L, 'the e... |\n",
      "| {'numth': 1L, 'to': 1L, 'n... | {'their numth': 1L, 'you s... |\n",
      "| {'money': 1L, 'doesn': 1L,... | {'tech but': 1L, 'a ne': 1... |\n",
      "| {'and': 1L, 'be': 1L, 'it'... | {'brighter than': 1L, 'num... |\n",
      "| {'ass': 1L, 'control': 1L,... | {'po for': 1L, 'all relev'... |\n",
      "| {'watch': 1L, 'tail': 1L, ... | {'tail num': 1L, 'numst ti... |\n",
      "| {'everi': 1L, 'got': 1L, '... | {'hustl from': 1L, 'of dec... |\n",
      "| {'and': 1L, 'lock': 1L, 'e... | {'at user': 1L, 'd normal'... |\n",
      "| {'and': 1L, 'numth': 1L, '... | {'po girl': 1L, 've been':... |\n",
      "| {'run': 1L, 'for': 1L, 'of... | {'for huntsman': 1L, 'at u... |\n",
      "| {'king': 1L, 'luther': 1L,... | {'king jr': 1L, 'take the'... |\n",
      "| {'kidrauhl': 1L, 'be': 3L,... | {'and make': 1L, 'other po... |\n",
      "| {'and': 1L, 'ah': 1L, 'it'... | {'my drake': 1L, 'at user'... |\n",
      "| {'webb': 1L, 'on': 1L, 'ut... | {'i take': 1L, 'a option':... |\n",
      "| {'igot': 1L, 'via': 1L, 'i... | {'at user': 1L, 'do it': 1... |\n",
      "| {'gahhhhden': 1L, 'and': 1... | {'tonight and': 1L, 'the g... |\n",
      "| {'a': 1L, 'be': 2L, 'about... | {'not be': 1L, 'movi about... |\n",
      "| {'and': 1L, 'al': 1L, 'jd'... | {'a jd': 1L, 'foot up': 1L... |\n",
      "| {'be': 1L, 'mademynight': ... | {'skype with': 1L, 'not be... |\n",
      "| {'and': 1L, 'it': 1L, 'say... | {'nump pt': 1L, 'it air': ... |\n",
      "| {'feel': 1L, 'it': 2L, 'se... | {'there so': 1L, 'tomorrow... |\n",
      "| {'to': 1L, 'xx': 1L, 'are'... | {'user are': 1L, 'are you'... |\n",
      "| {'be': 1L, 'about': 1L, 'j... | {'is po': 1L, 'so i': 1L, ... |\n",
      "| {'glamour': 1L, 'el': 1L, ... | {'or the': 1L, 'sunderland... |\n",
      "| {'wear': 1L, 'dwight': 1L,... | {'need to': 1L, 'dwight ar... |\n",
      "| {'drunken': 1L, 'a': 1L, '... | {'at user': 1L, 'went to':... |\n",
      "| {'and': 1L, 'set': 1L, 'ju... | {'andy everyth': 1L, 'the ... |\n",
      "| {'and': 1L, 'see': 1L, 'nu... | {'form num': 1L, 'job sear... |\n",
      "| {'30th': 1L, 'aliens': 1L,... | {'po backflip': 1L, 'harri... |\n",
      "| {'me': 1L, 'celtic': 1L, '... | {'someon to': 1L, 'go with... |\n",
      "| {'and': 1L, 'about': 1L, '... | {'ne storm': 1L, 'rw fault... |\n",
      "| {'fiesta': 1L, 'surpris': ... | {'fiesta bowl': 1L, 'to ho... |\n",
      "| {'a': 2L, 'golden': 1L, 'f... | {'s should': 1L, 'a golden... |\n",
      "| {'a': 1L, 'the': 2L, 'play... | {'club to': 1L, 'chang tha... |\n",
      "| {'and': 1L, 'in': 1L, 'bro... | {'sinc we': 1L, 'at user':... |\n",
      "| {'matt': 1L, 'via': 1L, 'm... | {'seattl seahawk': 1L, 'bl... |\n",
      "| {'high': 1L, 'at': 2L, 'fr... | {'at user': 1L, 'user come... |\n",
      "| {'conference': 1L, 'a': 2L... | {'now ne': 1L, 'jr on': 1L... |\n",
      "| {'and': 1L, 'cfc': 1L, 'br... | {'game i': 1L, 'season d':... |\n",
      "| {'again': 1L, 'gt': 1L, 'l... | {'gt numst': 1L, 'po gt': ... |\n",
      "| {'and': 1L, 'lfc': 1L, 'an... | {'night can': 1L, 'at user... |\n",
      "| {'mile': 1L, 'on': 1L, 'an... | {'williford and': 1L, 'fro... |\n",
      "| {'back': 2L, 'heard': 1L, ... | {'to stay': 1L, 'to think'... |\n",
      "| {'kidrauhl': 1L, 'belieb':... | {'belieb have': 1L, 'to tw... |\n",
      "| {'is': 2L, 'am': 1L, 'it':... | {'a guest': 1L, 'at user':... |\n",
      "| {'and': 1L, 'golden': 1L, ... | {'fey and': 1L, 'by tina':... |\n",
      "| {'and': 1L, 'numth': 1L, '... | {'the numth': 1L, 'at user... |\n",
      "| {'a': 2L, 'withstand': 1L,... | {'by the': 1L, 'numrd quar... |\n",
      "| {'begin': 1L, 'for': 1L, '... | {'at user': 1L, 'the final... |\n",
      "| {'be': 1L, 'may': 1L, 'ne'... | {'may be': 1L, 'chri bosh'... |\n",
      "| {'d': 1L, 'power': 1L, 'ne... | {'with it': 1L, 'd with': ... |\n",
      "| {'manger': 1L, 'art': 1L, ... | {'you on': 1L, 'solo piece... |\n",
      "| {'do': 1L, 'no': 1L, 'i': ... | {'night with': 1L, 'with m... |\n",
      "| {'a': 1L, 'loss': 1L, 'dwi... | {'at user': 1L, 'ard get':... |\n",
      "| {'be': 1L, 'down': 2L, 'ha... | {'there so': 1L, 'at user'... |\n",
      "| {'a': 1L, 'chaplin': 1L, '... | {'may be': 1L, 'my pain': ... |\n",
      "| {'a': 1L, 'knightley': 1L,... | {'charact a': 1L, 'not po'... |\n",
      "| {'fli': 1L, 'hotel': 1L, '... | {'steeler can': 1L, 'y url... |\n",
      "| {'coach': 1L, 'than': 1L, ... | {'more po': 1L, 'i think':... |\n",
      "| {'and': 1L, 'at': 1L, 'dyn... | {'the big': 1L, 'at user':... |\n",
      "| {'versu': 1L, 'manchest': ... | {'with five': 1L, 'chelsea... |\n",
      "| {'sta': 1L, 'for': 1L, 'vi... | {'includ player': 1L, 'v e... |\n",
      "| {'tri': 1L, 'numth': 1L, '... | {'or the': 1L, 'at user': ... |\n",
      "| {'about': 1L, 'good': 1L, ... | {'po for': 1L, 'at user': ... |\n",
      "| {'the': 1L, 'earn': 1L, '1... | {'safeti the': 1L, 'evolut... |\n",
      "| {'right': 1L, 'middle': 1L... | {'be hapo': 1L, 'on the': ... |\n",
      "| {'and': 1L, 'devil': 1L, '... | {'lyric to': 1L, 'to hous'... |\n",
      "| {'control': 1L, 'be': 2L, ... | {'assist lb': 1L, 'may be'... |\n",
      "| {'houston': 1L, 'year': 1L... | {'beat jorg': 1L, 'nail it... |\n",
      "| {'a': 1L, 'her': 1L, 'sam'... | {'a po': 1L, 'at user': 2L... |\n",
      "| {'pre': 1L, 'ablett': 1L, ... | {'see tomorrow': 1L, 'are ... |\n",
      "| {'and': 1L, 'all': 1L, 'hu... | {'do is': 1L, 'all i': 1L,... |\n",
      "| {'her': 1L, 'i': 1L, 'craw... | {'went crawl': 1L, 'i thin... |\n",
      "| {'and': 2L, 'heart': 1L, '... | {'it that': 1L, 's not': 1... |\n",
      "| {'a': 1L, 'atl': 1L, 'po':... | {'a po': 1L, 'might a': 1L... |\n",
      "| {'concert': 1L, 'i': 1L, '... | {'go to': 1L, 'the justin'... |\n",
      "| {'monday': 1L, 'for': 1L, ... | {'ninja got': 1L, 'for the... |\n",
      "| {'and': 1L, 'bestprimetime... | {'ne sunday': 1L, 'night w... |\n",
      "| {'and': 1L, 'king': 1L, 'l... | {'through the': 1L, 'king ... |\n",
      "| {'and': 1L, 'via': 1L, 'ed... | {'at user': 1L, 'ed song':... |\n",
      "| {'just': 1L, 'is': 1L, 'mi... | {'the same': 1L, 'birthday... |\n",
      "| {'and': 1L, 'rey': 1L, 'do... | {'at user': 1L, 'outsid to... |\n",
      "| {'a': 1L, 'family': 1L, 'm... | {'family may': 1L, 'may he... |\n",
      "| {'a': 1L, 'neck': 1L, 'dri... | {'won t': 1L, 'chri bosh':... |\n",
      "| {'and': 1L, 'giant': 1L, '... | {'to watch': 1L, 'im go': ... |\n",
      "| {'carri': 1L, 'sex': 1L, '... | {'spin off': 1L, 'gossip g... |\n",
      "| {'scarborough': 1L, 'on': ... | {'scarborough in': 1L, 'at... |\n",
      "| {'a': 2L, 'after': 1L, 'ur... | {'ransomwar virus': 1L, 'n... |\n",
      "| {'and': 1L, 'cpfc': 1L, 'd... | {'at user': 2L, 'bolton wi... |\n",
      "| {'a': 1L, 'be': 1L, 'satur... | {'a po': 1L, 'away saturda... |\n",
      "| {'all': 1L, 'in': 1L, 'mob... | {'are all': 1L, 'about mob... |\n",
      "| {'and': 1L, 'numth': 1L, '... | {'the exorcis': 1L, '4 see... |\n",
      "| {'then': 1L, 'but': 1L, 'p... | {'nov num': 1L, 'same date... |\n",
      "| {'a': 1L, 'feb': 1L, 'for'... | {'a po': 1L, 'feb numrd': ... |\n",
      "| {'google': 1L, 'ohh': 1L, ... | {'to sleep': 1L, 'at user'... |\n",
      "| {'angel': 1L, 'is': 1L, 'n... | {'num thi': 1L, 'do not': ... |\n",
      "| {'to': 2L, 'may': 1L, 'for... | {'po for': 1L, 'for the': ... |\n",
      "| {'taylor': 1L, 'give': 1L,... | {'they keep': 1L, 'abc be'... |\n",
      "| {'me': 1L, 'numth': 1L, 't... | {'tim tebow': 1L, 'the num... |\n",
      "| {'a': 1L, 'me': 1L, 'too':... | {'at user': 1L, 'text me':... |\n",
      "| {'and': 3L, 'porn': 1L, 'i... | {'numrd and': 1L, 'the guy... |\n",
      "| {'and': 1L, 'feederclub': ... | {'long ball': 1L, 'user ev... |\n",
      "| {'a': 1L, 'and': 1L, 'all'... | {'all he': 1L, 'friend lt'... |\n",
      "| {'coach': 1L, 'rivers': 1L... | {'at user': 1L, 'the year'... |\n",
      "| {'a': 2L, 'on': 1L, 'liver... | {'rate a': 1L, 'nurs a': 1... |\n",
      "| {'be': 1L, 'is': 1L, 'say'... | {'a po': 1L, 'the off': 1L... |\n",
      "| {'year': 1L, 'see': 1L, 'a... | {'at user': 1L, 'for a': 1... |\n",
      "| {'clipper': 1L, 'be': 2L, ... | {'laker v': 1L, 'at user':... |\n",
      "| {'benghazi': 1L, 'and': 1L... | {'last tuesday': 1L, 't nb... |\n",
      "| {'and': 1L, 'set': 1L, 'is... | {'may the': 1L, 'and po': ... |\n",
      "| {'and': 2L, 'matt': 1L, 'a... | {'alex no': 1L, 'at user':... |\n",
      "| {'and': 1L, 'me': 2L, 'goo... | {'i had': 1L, 'had skin': ... |\n",
      "| {'and': 1L, 'a': 1L, 'emil... | {'hi overhead': 1L, 'newca... |\n",
      "| {'lfc': 1L, 'back': 1L, 'n... | {'him but': 1L, 'front in'... |\n",
      "| {'on': 1L, 'to': 1L, 'is':... | {'anybodi go': 1L, 'is any... |\n",
      "| {'a': 1L, 'matt': 1L, 'may... | {'have to': 1L, 'at user':... |\n",
      "| {'high': 1L, 'at': 2L, 'fr... | {'at user': 1L, 'user come... |\n",
      "| {'and': 1L, 'just': 1L, 'a... | {'marathon thing': 1L, 'an... |\n",
      "| {'a': 1L, 'may': 1L, 'i': ... | {'come up': 1L, 'ne my': 1... |\n",
      "| {'just': 1L, 'is': 1L, 'se... | {'just realiz': 1L, 'at us... |\n",
      "| {'your': 1L, 'that': 1L, '... | {'doe to': 1L, 'that s': 1... |\n",
      "| {'a': 1L, 'swim': 1L, 'on'... | {'take on': 1L, 'num at': ... |\n",
      "| {'and': 1L, 'german': 1L, ... | {'on the': 2L, 'school and... |\n",
      "| {'real': 1L, 'awar': 1L, '... | {'so i': 1L, 'know it': 1L... |\n",
      "| {'ve': 1L, 'premier': 1L, ... | {'trebl togeth': 1L, 'thi ... |\n",
      "| {'and': 1L, 'just': 1L, 'a... | {'of real': 1L, 'from drew... |\n",
      "| {'portsmouth': 1L, 'old': ... | {'yr num': 1L, 'at the': 1... |\n",
      "| {'numth': 1L, 'all': 1L, '... | {'all there': 1L, 'to see'... |\n",
      "| {'kenda': 1L, 'feel': 1L, ... | {'part i': 1L, 'plea kenda... |\n",
      "| {'is': 1L, 'bama': 1L, 'ha... | {'between go': 1L, 'go on'... |\n",
      "| {'already': 1L, 'just': 1L... | {'already jesu': 1L, 'clas... |\n",
      "| {'and': 2L, 'factori': 1L,... | {'sugar bowl': 1L, 'bowl l... |\n",
      "| {'user': 1L, 'tho': 1L, 't... | {'everyth tomorrow': 1L, '... |\n",
      "| {'anoth': 1L, 'all': 1L, '... | {'the time': 1L, 'time at'... |\n",
      "| {'numth': 1L, 'on': 1L, 'm... | {'california on': 1L, 'on ... |\n",
      "| {'earli': 1L, 'a': 1L, 'in... | {'ne increa': 1L, 'on the'... |\n",
      "| {'el': 1L, 'set': 1L, 'cla... | {'vicent del': 1L, 's el':... |\n",
      "| {'all': 1L, 'at': 1L, 'in'... | {'at user': 1L, 'mac all':... |\n",
      "| {'orlean': 1L, 'about': 1L... | {'night url': 1L, 'press c... |\n",
      "| {'ggmu': 1L, 'on': 1L, 'de... | {'unit against': 1L, 'arse... |\n",
      "| {'and': 1L, 'heart': 1L, '... | {'time nev': 1L, 'one of':... |\n",
      "| {'and': 1L, 'me': 1L, 'all... | {'me did': 1L, 'heat fan':... |\n",
      "| {'a': 1L, 'all': 1L, 'cent... | {'call num': 1L, 'free but... |\n",
      "| {'choic': 1L, 'we': 1L, 't... | {'the peopl': 1L, 'site do... |\n",
      "| {'just': 1L, 'at': 2L, 'bu... | {'buke togeth': 1L, 'at us... |\n",
      "| {'gt': 1L, 'url': 1L, 'sun... | {'steeler can': 1L, 'out g... |\n",
      "| {'and': 1L, 'on': 1L, 'we'... | {'cottag confidenti': 1L, ... |\n",
      "| {'wa': 1L, 'on': 1L, 'onli... | {'includ 3rd': 1L, 'the on... |\n",
      "| {'limp': 1L, 'trip': 1L, '... | {'yesterday the': 1L, 'pro... |\n",
      "| {'aint': 1L, 'blazer': 1L,... | {'blazer aint': 1L, 'the b... |\n",
      "| {'on': 1L, 'all': 1L, 'buy... | {'or rent': 1L, 'will disc... |\n",
      "| {'me': 1L, 'be': 1L, 'all'... | {'y all': 1L, 'amp meeee':... |\n",
      "| {'be': 2L, 'pain': 2L, 'fo... | {'may be': 1L, 'my pain': ... |\n",
      "| {'a': 2L, 'on': 1L, 'and':... | {'camaro into': 1L, 'get t... |\n",
      "| {'week': 1L, 'on': 1L, 'gr... | {'grown up': 1L, 'up ha': ... |\n",
      "| {'and': 1L, 'do': 1L, 'bea... | {'huge icons': 1L, 'throwb... |\n",
      "| {'numth': 1L, 'le': 1L, 'q... | {'with num': 1L, 'thi game... |\n",
      "| {'don': 1L, 'behind': 1L, ... | {'ne behind': 1L, 'motor b... |\n",
      "| {'and': 1L, 'gari': 1L, 'o... | {'at user': 1L, 'the brown... |\n",
      "| {'and': 1L, 'me': 1L, 'so'... | {'a po': 1L, 'ally im': 1L... |\n",
      "| {'a': 1L, 'beliv': 1L, 'ro... | {'a po': 1L, 'sunday night... |\n",
      "| {'scored': 1L, 'me': 1L, '... | {'beware if': 1L, 'is sat'... |\n",
      "| {'on': 1L, 'we': 1L, 'off'... | {'at user': 3L, 'changes i... |\n",
      "| {'a': 1L, 'be': 1L, 'we': ... | {'po we': 1L, 'declar po':... |\n",
      "| {'play': 1L, 'chilli': 1L,... | {'play with': 1L, 'great w... |\n",
      "| {'the': 2L, 'all': 1L, 'fi... | {'must po': 1L, 'nfl he': ... |\n",
      "| {'and': 2L, 'again': 1L, '... | {'we play': 1L, 'and satur... |\n",
      "| {'webb': 1L, 'on': 1L, 'ol... | {'for the': 1L, 'arsen gam... |\n",
      "| {'pat': 1L, 'that': 1L, 'i... | {'the pond': 1L, 'at user'... |\n",
      "| {'and': 1L, 'ocdsupertoast... | {'if you': 1L, 'at user': ... |\n",
      "| {'over': 1L, 'it': 2L, 'sa... | {'seri final': 1L, 'gossip... |\n",
      "| {'wiki': 1L, 'numth': 1L, ... | {'of that': 1L, 'call u': ... |\n",
      "| {'year': 1L, 'osu': 1L, 'p... | {'sugar bowl': 1L, 'coupl ... |\n",
      "| {'england': 1L, 'is': 1L, ... | {'at user': 1L, 'wa califo... |\n",
      "| {'clipper': 1L, 'on': 1L, ... | {'the clipper': 1L, 'will ... |\n",
      "| {'kingdom': 1L, 'feb': 1L,... | {'burn at': 1L, 'kingdom f... |\n",
      "| {'and': 1L, 'almost': 1L, ... | {'rounder back': 1L, 'swap... |\n",
      "| {'awar': 1L, 'via': 1L, 'a... | {'around the': 1L, 'at use... |\n",
      "| {'fuckin': 1L, 'may': 1L, ... | {'bosh so': 1L, 'chri bosh... |\n",
      "| {'and': 1L, 'franc': 1L, '... | {'at user': 1L, 'with the'... |\n",
      "| {'rememeb': 1L, 'be': 1L, ... | {'amp in': 1L, 's wow': 1L... |\n",
      "| {'a': 1L, 'rush': 1L, 'nev... | {'may also': 1L, 'the bigg... |\n",
      "| {'town': 1L, 'a': 1L, 'fro... | {'at ne': 1L, 'at user': 1... |\n",
      "| {'season': 1L, 'don': 1L, ... | {'sunday is': 1L, 'po that... |\n",
      "| {'numth': 1L, 'everi': 1L,... | {'grade cry': 1L, 'the num... |\n",
      "| {'on': 1L, 'num': 2L, 'win... | {'winter classic': 2L, 'th... |\n",
      "| {'yeah': 1L, 'year': 1L, '... | {'s conquer': 1L, 'for the... |\n",
      "| {'and': 1L, 'gone': 1L, 'b... | {'at user': 2L, 'on the': ... |\n",
      "| {'thur': 1L, 'one': 1L, 'n... | {'onli one': 1L, 'at user'... |\n",
      "| {'just': 1L, 'grade': 1L, ... | {'jackin it': 1L, 'of wave... |\n",
      "| {'last': 1L, 'nite': 1L, '... | {'want mark': 1L, 'user i'... |\n",
      "| {'mile': 1L, 'on': 1L, 'le... | {'matchup with': 1L, 'alab... |\n",
      "| {'num': 1L, 'at': 1L, 'tom... | {'tails i': 1L, 'after num... |\n",
      "| {'pre': 1L, 'a': 2L, 'satu... | {'tomorrow want': 1L, 'gam... |\n",
      "| {'and': 1L, 'it': 1L, 'gle... | {'user the': 1L, 'at user'... |\n",
      "| {'el': 1L, 'last': 1L, 'go... | {'goal in': 1L, 'my po': 1... |\n",
      "| {'for': 1L, 'tail': 1L, 't... | {'nw red': 1L, 'numst time... |\n",
      "| {'impact': 1L, 'on': 1L, '... | {'impact to': 1L, 'be awar... |\n",
      "| {'teen': 1L, 'all': 1L, 't... | {'all new': 1L, 'monday i'... |\n",
      "| {'a': 1L, 'begin': 1L, 'ta... | {'ne stand': 1L, 'to have'... |\n",
      "| {'and': 1L, 'patriot': 1L,... | {'bumblebees steelers': 1L... |\n",
      "| {'saturday': 1L, 'champion... | {'go to': 1L, 'lsu v': 1L,... |\n",
      "| {'toronto': 1L, 'numth': 1... | {'one to': 1L, 'at user': ... |\n",
      "| {'code': 1L, 'central': 1L... | {'num with': 1L, 'biggerst... |\n",
      "| {'a': 1L, 'school': 1L, 't... | {'ne make': 1L, 'tomorrow ... |\n",
      "| {'it': 1L, 'month': 1L, 'p... | {'tom bradi': 1L, 'second ... |\n",
      "| {'and': 2L, 'have': 1L, 'h... | {'time i': 1L, 'for the': ... |\n",
      "| {'and': 1L, 'numth': 1L, '... | {'need to': 1L, 'it in': 1... |\n",
      "| {'and': 1L, 'numth': 1L, '... | {'is my': 1L, 'num xfactor... |\n",
      "| {'georgia': 1L, 'televis':... | {'he leav': 1L, 'that redu... |\n",
      "| {'justin': 1L, 'consert': ... | {'the justin': 1L, 'user t... |\n",
      "| {'is': 1L, 'second': 1L, '... | {'the second': 1L, 'wa the... |\n",
      "| {'high': 1L, 'at': 2L, 'su... | {'at the': 1L, '26th frm':... |\n",
      "| {'houston': 1L, 'concert':... | {'concert pub': 1L, 'at co... |\n",
      "| {'a': 1L, 'on': 1L, 'march... | {'park shin': 1L, 'o num':... |\n",
      "| {'concord': 1L, 'greys': 1... | {'at user': 1L, 'him were'... |\n",
      "| {'sell': 1L, 'again': 1L, ... | {'sell out': 1L, 'record t... |\n",
      "| {'a': 1L, 've': 1L, 'point... | {'oh i': 1L, 'at user': 1L... |\n",
      "| {'honey': 1L, 'now': 1L, '... | {'now get': 1L, 'pic url':... |\n",
      "| {'and': 1L, 'wiki': 1L, 'a... | {'devil s': 1L, 'at user':... |\n",
      "| {'be': 1L, 'school': 1L, '... | {'monday is': 1L, 'gcse st... |\n",
      "| {'numth': 1L, 'thunderup':... | {'badger is': 1L, 'is go':... |\n",
      "| {'is': 1L, 'it': 1L, 'num'... | {'thingy the': 1L, 'for th... |\n",
      "| {'just': 1L, 'is': 1L, 'it... | {'t wait': 1L, 'see it': 1... |\n",
      "| {'a': 1L, 'on': 1L, 'frida... | {'get a': 1L, 'pictur with... |\n",
      "| {'are': 1L, 'play': 1L, 't... | {'user the': 1L, 'at user'... |\n",
      "| {'a': 1L, 'be': 1L, 'from'... | {'may be': 1L, 'fb game': ... |\n",
      "| {'back': 1L, 'brown': 1L, ... | {'brown prais': 1L, 'run b... |\n",
      "| {'be': 1L, 'twitter': 2L, ... | {'thi weekend': 1L, 'off o... |\n",
      "| {'on': 2L, 'wa': 1L, 'open... | {'tune in': 1L, 'h open': ... |\n",
      "| {'numth': 1L, 'on': 1L, 'u... | {'you in': 1L, 'in tucson'... |\n",
      "| {'is': 1L, 'see': 1L, 'num... | {'if you': 1L, 'at user': ... |\n",
      "| {'love': 1L, 'dupe': 1L, '... | {'my wife': 1L, 'at user':... |\n",
      "| {'and': 1L, 'king': 1L, 's... | {'po hi': 1L, 'num years':... |\n",
      "| {'on': 1L, 'human': 1L, 'c... | {'cant wait': 1L, 'of be':... |\n",
      "| {'depart': 1L, 'is': 1L, '... | {'numpm even': 1L, 'to num... |\n",
      "| {'kinshasa': 1L, 'and': 1L... | {'ali and': 1L, 'take plac... |\n",
      "| {'on': 1L, 'use': 1L, 'for... | {'morn with': 1L, 'at user... |\n",
      "| {'a': 1L, 'underworld': 1L... | {'a po': 1L, 'underworld w... |\n",
      "| {'and': 2L, 'capit': 1L, '... | {'sun holiday': 1L, '1 gam... |\n",
      "| {'me': 1L, 'gt': 1L, 'retu... | {'january are': 1L, 'are y... |\n",
      "| {'myself': 1L, 'from': 1L,... | {'from at': 1L, 'muhammad ... |\n",
      "| {'a': 1L, 'be': 1L, 'photo... | {'may be': 1L, 'look po': ... |\n",
      "| {'set': 1L, 'just': 1L, 'c... | {'yrs dr': 1L, 'grave wish... |\n",
      "| {'a': 1L, 'numth': 1L, 'fr... | {'sexual ne': 1L, 'at user... |\n",
      "| {'oper': 1L, 'bring': 1L, ... | {'will be': 1L, 'christma ... |\n",
      "| {'about': 1L, 'your': 1L, ... | {'citi nesav': 1L, 'swanse... |\n",
      "| {'and': 1L, 'dubai': 1L, '... | {'user at': 1L, 'and direc... |\n",
      "| {'a': 1L, 'week': 1L, 'las... | {'cast travel': 1L, 'jbsa ... |\n",
      "| {'and': 1L, 'ha': 1L, 'sai... | {'said hi': 1L, 'ben ha': ... |\n",
      "| {'odyssea': 1L, 'greek': 1... | {'odyssea elytis': 1L, 'no... |\n",
      "| {'numth': 1L, 'to': 1L, 'b... | {'the numth': 1L, 'go to':... |\n",
      "| {'a': 1L, 'and': 1L, 'sour... | {'florida need': 1L, 'at u... |\n",
      "| {'onli': 1L, 'would': 1L, ... | {'ne hi': 1L, 'the numnd':... |\n",
      "| {'teen': 1L, 'onli': 1L, '... | {'start monday': 1L, 'the ... |\n",
      "| {'the': 1L, 'over': 1L, 'g... | {'at user': 1L, 'did the':... |\n",
      "| {'a': 1L, 'mlk': 1L, 'name... | {'po in': 1L, 'powel go': ... |\n",
      "| {'a': 1L, 'golden': 1L, 'a... | {'an po': 1L, 'golden glob... |\n",
      "| {'on': 1L, 'plu': 1L, 'le'... | {'the le': 1L, 'numth on':... |\n",
      "| {'rush': 1L, 'yard': 1L, '... | {'amp num': 1L, 'po to': 1... |\n",
      "| {'landfal': 1L, 'monday': ... | {'sometim monday': 1L, 'al... |\n",
      "| {'brazil': 1L, 'the': 1L, ... | {'are inspir': 1L, 'inspir... |\n",
      "| {'king': 1L, 'luther': 1L,... | {'king jr': 1L, 'jr url': ... |\n",
      "| {'ali': 1L, 'midnight': 1L... | {'at user': 1L, 'phone cal... |\n",
      "| {'numth': 1L, 'pain': 1L, ... | {'po the': 1L, 'watch kevi... |\n",
      "| {'is': 1L, 'theocracy': 1L... | {'those flames': 1L, 'a th... |\n",
      "| {'be': 1L, 'death': 1L, 'i... | {'at user': 1L, 'centuri i... |\n",
      "| {'are': 1L, 'all': 1L, 'y'... | {'come ne': 1L, 'go to': 1... |\n",
      "| {'and': 2L, 'just': 1L, 'g... | {'mc and': 1L, 'should go'... |\n",
      "| {'a': 1L, 'on': 1L, 'pat':... | {'at user': 1L, 'on sunday... |\n",
      "| {'kirk': 1L, 'out': 1L, 'j... | {'ne no': 1L, 'tomorrow ca... |\n",
      "| {'and': 1L, 'heart': 1L, '... | {'the cowboy': 1L, 'with a... |\n",
      "| {'and': 1L, 'vampir': 1L, ... | {'vampir diari': 1L, 'at u... |\n",
      "| {'user': 1L, 'newcastl': 1... | {'the victori': 1L, 'jet i... |\n",
      "| {'just': 1L, 'it': 1L, 'nu... | {'with the': 1L, 'annual n... |\n",
      "| {'swfc': 1L, 'last': 1L, '... | {'rhode forget': 1L, 'last... |\n",
      "| {'the': 1L, 'share': 2L, '... | {'one to': 1L, 'share him'... |\n",
      "| {'on': 1L, 'manchest': 1L,... | {'should they': 1L, 'of th... |\n",
      "| {'and': 1L, 'again': 1L, '... | {'alabama vs': 1L, 'at use... |\n",
      "| {'webb': 1L, 'loan': 1L, '... | {'earli lol': 1L, 'at user... |\n",
      "| {'the': 1L, 'google': 1L, ... | {'answer you': 1L, 'you kn... |\n",
      "| {'then': 1L, 'like': 1L, '... | {'at user': 1L, 'it tomoro... |\n",
      "| {'be': 1L, 'wa': 1L, 'that... | {'that wa': 1L, 'of be': 1... |\n",
      "| {'and': 1L, 'be': 1L, 'hob... | {'night hobby': 1L, 'if yo... |\n",
      "| {'from': 1L, 'it': 1L, 'at... | {'sinc we': 1L, 'so good':... |\n",
      "| {'it': 1L, 'an': 1L, 'are'... | {'bigger in': 1L, 'welp i'... |\n",
      "| {'mind': 1L, 'pop': 1L, 'o... | {'too mani': 1L, 'on the':... |\n",
      "| {'fli': 1L, 'flight': 1L, ... | {'the sun': 1L, 'view po':... |\n",
      "| {'week': 2L, 'numth': 1L, ... | {'a po': 1L, 'week ha': 1L... |\n",
      "| {'and': 1L, 'sander': 1L, ... | {'hel url': 1L, 'in van': ... |\n",
      "| {'on': 1L, 'run': 1L, 'for... | {'po for': 1L, 'on sunday'... |\n",
      "| {'a': 1L, 'be': 1L, 'stron... | {'the ne': 1L, 'year in': ... |\n",
      "| {'and': 1L, 'honey': 1L, '... | {'badger daddi': 1L, 'save... |\n",
      "| {'we': 1L, 'except': 1L, '... | {'at user': 1L, 'screw us'... |\n",
      "| {'and': 1L, 'on': 1L, 'pat... | {'hold just': 1L, 'on thur... |\n",
      "| {'penalti': 1L, 'on': 1L, ... | {'of queen': 1L, 'at the':... |\n",
      "| {'school': 2L, 'don': 1L, ... | {'at user': 1L, 'tomorrow ... |\n",
      "| {'a': 1L, 'on': 1L, 'liver... | {'newcatl unit': 1L, 'rate... |\n",
      "| {'don': 2L, 'concord': 1L,... | {'at user': 1L, 'concord t... |\n",
      "| {'isheturnedornot': 1L, 't... | {'po for': 1L, 'game dec':... |\n",
      "| {'and': 1L, 'do': 1L, 'dev... | {'watch devil': 1L, 'to do... |\n",
      "| {'ji': 1L, 'your': 1L, 'nu... | {'party ji': 1L, 'curiosit... |\n",
      "| {'a': 1L, 'on': 2L, 'eveni... | {'at user': 1L, 'm num': 1... |\n",
      "| {'we': 1L, 'wanna': 1L, 'i... | {'the sun': 1L, 'zayn till... |\n",
      "| {'a': 1L, 'about': 1L, 'wi... | {'num num': 1L, 'at num': ... |\n",
      "| {'a': 1L, 'numth': 1L, 'in... | {'a at': 1L, 'talk numth':... |\n",
      "| {'me': 1L, 'love': 1L, 'ma... | {'love me': 1L, 'aaliyah m... |\n",
      "| {'a': 1L, 'ive': 1L, 'catw... | {'may be': 1L, 'kany amp':... |\n",
      "| {'and': 2L, 'on': 1L, 'we'... | {'at user': 3L, 's a': 1L,... |\n",
      "| {'on': 1L, 'nov': 1L, 'lea... | {'can i': 1L, 'num can': 1... |\n",
      "| {'kind': 1L, 'but': 1L, 't... | {'sleep tomorrow': 1L, 'so... |\n",
      "| {'a': 1L, 'and': 1L, 'okay... | {'you mean': 1L, 'at user'... |\n",
      "| {'recipi': 1L, 'be': 1L, '... | {'awards th': 1L, 'golden ... |\n",
      "| {'and': 1L, 'just': 1L, 'r... | {'my chicken': 1L, 'fit ju... |\n",
      "| {'dad': 1L, 'be': 1L, 'and... | {'that moment': 1L, 'at us... |\n",
      "| {'be': 1L, 'univers': 1L, ... | {'will be': 1L, 'the unive... |\n",
      "| {'point': 1L, 'is': 1L, 'a... | {'user hope': 1L, 'from he... |\n",
      "| {'and': 1L, 'v': 1L, 'toni... | {'at user': 1L, 'night v':... |\n",
      "| {'numpeopleyouwanttomeet':... | {'luther king': 1L, 'my gr... |\n",
      "| {'some': 1L, 'it': 2L, 'ha... | {'some kind': 1L, 'said i'... |\n",
      "| {'upto': 1L, 'and': 1L, 'f... | {'minutes pc': 1L, 'num mi... |\n",
      "| {'ve': 1L, 'brown': 1L, 'n... | {'hair is': 1L, 'my hair':... |\n",
      "| {'philli': 1L, 'one': 1L, ... | {'at user': 1L, 'night hop... |\n",
      "| {'pull': 1L, 'qualifi': 1L... | {'world cup': 1L, 'rhode a... |\n",
      "| {'opposit': 1L, 'it': 1L, ... | {'user the': 1L, 'at user'... |\n",
      "| {'be': 1L, 'think': 1L, 'o... | {'tim tebow': 1L, 'jack of... |\n",
      "| {'all': 1L, 'feel': 1L, 'o... | {'or possibl': 1L, 'possib... |\n",
      "| {'the': 1L, 'launch': 1L, ... | {'jan num': 1L, 'on tuesda... |\n",
      "| {'knightley': 2L, 'num': 1... | {'not po': 1L, 'lo angeles... |\n",
      "| {'gari': 1L, 'num': 1L, 'i... | {'a goal': 1L, 'also numst... |\n",
      "| {'just': 1L, 'onc': 1L, 'y... | {'at user': 2L, 'season ha... |\n",
      "| {'competit': 1L, 'europa':... | {'game in': 1L, 'num stoke... |\n",
      "| {'even': 1L, 'them': 1L, '... | {'if they': 1L, 'girl team... |\n",
      "| {'a': 1L, 'the': 1L, 'geor... | {'my last': 1L, 'i think':... |\n",
      "| {'googl': 1L, 'pictur': 1L... | {'m the': 1L, 'if you': 1L... |\n",
      "| {'and': 1L, 'help': 1L, 'i... | {'it may': 1L, 'make sc': ... |\n",
      "| {'knick': 1L, 'for': 1L, '... | {'knick to': 1L, 'to beat'... |\n",
      "| {'anim': 1L, 'practic': 1L... | {'s anim': 1L, 'new num': ... |\n",
      "| {'onc': 1L, 'is': 1L, 'num... | {'for hour': 1L, 'realis t... |\n",
      "| {'txn': 1L, 'ugh': 1L, 'la... | {'bowl last': 1L, 'hewlett... |\n",
      "| {'again': 1L, 'we': 1L, 'm... | {'we ve': 1L, 'at user': 1... |\n",
      "| {'taylor': 1L, 'becaus': 1... | {'match ike': 1L, 'to matc... |\n",
      "| {'and': 2L, 'autumn': 1L, ... | {'come and': 1L, 'that wil... |\n",
      "| {'a': 1L, 'on': 1L, 'le': ... | {'day of': 1L, 'on monday'... |\n",
      "| {'a': 2L, 'be': 1L, 'would... | {'get a': 1L, 'abl to': 1L... |\n",
      "| {'numth': 1L, 'to': 2L, 'i... | {'bands drumlin': 1L, 'to ... |\n",
      "| {'ve': 1L, 'then': 1L, 'do... | {'at user': 1L, 'the eve':... |\n",
      "| {'shop': 1L, 'and': 1L, 'l... | {'to liverpool': 1L, 'tomo... |\n",
      "| {'inning': 1L, 'england': ... | {'the year': 1L, 'num v': ... |\n",
      "| {'week': 1L, 'nfc': 2L, 'n... | {'around the': 1L, 'nfc we... |\n",
      "| {'talkn': 1L, 'is': 1L, 'i... | {'time i': 1L, 'le time': ... |\n",
      "| {'on': 1L, 'video': 3L, 'a... | {'of our': 2L, 'at the': 1... |\n",
      "| {'awar': 1L, 'offici': 1L,... | {'day februari': 1L, 'offi... |\n",
      "| {'and': 1L, 'qtr': 1L, 've... | {'at user': 2L, 'and chalm... |\n",
      "| {'on': 1L, 'await': 1L, 't... | {'second game': 1L, 'ku nu... |\n",
      "| {'a': 1L, 'plea': 1L, 'i':... | {'rob pattinson': 1L, 'a s... |\n",
      "| {'available': 1L, 'on': 1L... | {'de forc': 1L, 'elvis a':... |\n",
      "| {'feb': 1L, 'to': 1L, 'url... | {'xx at': 1L, 'theatr in':... |\n",
      "| {'and': 1L, 've': 1L, 'nlc... | {'if you': 1L, 'thi weeken... |\n",
      "| {'orang': 1L, 'doesn': 1L,... | {'them on': 1L, 'at user':... |\n",
      "| {'and': 1L, 'is': 3L, 'ir'... | {'head into': 1L, 'at the'... |\n",
      "| {'paranorm': 1L, 'and': 1L... | {'m definit': 1L, 'watch d... |\n",
      "| {'and': 1L, 'basketball': ... | {'new thursday': 1L, 'hoes... |\n",
      "| {'be': 1L, 'lb': 1L, 'chri... | {'parti gotta': 1L, 'gotta... |\n",
      "| {'pow': 1L, 'is': 1L, 'it'... | {'tom bradi': 1L, 'the mos... |\n",
      "| {'are': 1L, 'kind': 1L, 'u... | {'black are': 1L, 'at user... |\n",
      "| {'brown': 1L, 'weath': 1L,... | {'long needed': 1L, 'the b... |\n",
      "| {'on': 1L, 'play': 1L, 'fr... | {'m from': 1L, 'at user': ... |\n",
      "| {'shop': 1L, 'el': 1L, 'fo... | {'el spoil': 1L, 'anyth el... |\n",
      "| {'onc': 1L, 'in': 2L, 'sha... | {'at user': 2L, 'a birthda... |\n",
      "| {'numth': 1L, 'in': 1L, 'f... | {'numth seed': 1L, 'after ... |\n",
      "| {'and': 1L, 'just': 1L, 'i... | {'requir and': 1L, 'sat re... |\n",
      "| {'clipper': 1L, 'be': 1L, ... | {'the clipper': 1L, 'odom ... |\n",
      "| {'user': 1L, 'golden': 1L,... | {'january sorri': 1L, 'at ... |\n",
      "| {'behav': 1L, 'just': 1L, ... | {'skype but': 1L, 'at user... |\n",
      "| {'all': 1L, 'it': 1L, 'wal... | {'jobs happi': 1L, 'po all... |\n",
      "| {'at': 1L, 'need': 1L, 'sa... | {'the ne': 1L, 'do with': ... |\n",
      "| {'week': 1L, 'patriot': 1L... | {'week for': 1L, 'for numn... |\n",
      "| {'we': 1L, 'disast': 1L, '... | {'mon disgust': 1L, 'with ... |\n",
      "| {'ve': 1L, 'concord': 1L, ... | {'concord tomorrow': 1L, '... |\n",
      "| {'webb': 1L, 'into': 1L, '... | {'by unit': 1L, 'ard webb'... |\n",
      "| {'play': 1L, 'realli': 1L,... | {'bowl or': 1L, 'sun bowl'... |\n",
      "| {'claro': 1L, 'in': 1L, 'a... | {'at user': 1L, 'lake trai... |\n",
      "| {'or': 1L, 'april': 1L, 'i... | {'wrong i': 1L, 'i think':... |\n",
      "| {'user': 1L, 'there': 1L, ... | {'go to': 1L, 'at user': 1... |\n",
      "| {'knick': 1L, 'gunna': 1L,... | {'knick tomorrow': 1L, 'th... |\n",
      "| {'a': 1L, 'be': 1L, 'rft':... | {'a po': 1L, 'skype with':... |\n",
      "| {'me': 1L, 'king': 1L, 'lu... | {'king jr': 1L, 'dr martin... |\n",
      "| {'held': 1L, 'are': 2L, 'i... | {'time i': 1L, 'the nfc': ... |\n",
      "| {'a': 1L, 'king': 1L, 'lut... | {'littl book': 1L, 'the ur... |\n",
      "| {'exorc': 1L, 'just': 1L, ... | {'at user': 1L, 'which is'... |\n",
      "| {'a': 1L, 'on': 1L, 'such'... | {'a po': 1L, 'lastnight at... |\n",
      "| {'work': 1L, 'may': 1L, 'i... | {'i may': 1L, 'tomorrow is... |\n",
      "| {'qtr': 1L, 'house': 1L, '... | {'v unc': 1L, 'yr amp': 1L... |\n",
      "| {'besi': 1L, 'old': 1L, 'i... | {'persi po': 1L, 'at user'... |\n",
      "| {'manchest': 1L, 'num': 3L... | {'with num': 1L, 'novemb n... |\n",
      "| {'january': 1L, 'episod': ... | {'t wait': 1L, 'wa next': ... |\n",
      "| {'by': 1L, 'banking': 1L, ... | {'banking barclay': 1L, 'i... |\n",
      "| {'and': 1L, 'on': 2L, 'fli... | {'nov num': 1L, 'contraban... |\n",
      "| {'on': 1L, 'play': 1L, 'nu... | {'three stroke': 1L, 'is n... |\n",
      "| {'wa': 1L, 'blew': 1L, 'li... | {'mind atrain': 1L, 'hope ... |\n",
      "| {'a': 1L, 'and': 2L, 'all'... | {'at user': 1L, 'user unio... |\n",
      "| {'and': 1L, 'geaux': 1L, '... | {'at user': 1L, 'to see': ... |\n",
      "| {'and': 1L, 'be': 1L, 'cam... | {'in wisconsin': 1L, 'wisc... |\n",
      "| {'door': 1L, 'url': 1L, 's... | {'game our': 1L, 'in tuesd... |\n",
      "| {'summit': 1L, 'for': 1L, ... | {'at user': 1L, 'jan num':... |\n",
      "| {'video': 1L, 'are': 1L, '... | {'student s': 1L, 'in numt... |\n",
      "| {'numth': 1L, 'season': 1L... | {'of grey': 1L, 'onto the'... |\n",
      "| {'is': 1L, 'it': 1L, 'num'... | {'though prydz': 1L, 'at u... |\n",
      "| {'lfc': 1L, 'ne': 1L, 'at'... | {'our detail': 1L, 'touch ... |\n",
      "| {'and': 1L, 'we': 2L, 'ang... | {'just had': 1L, 'from big... |\n",
      "| {'user': 1L, 'use': 1L, 'h... | {'po travel': 1L, 'those f... |\n",
      "| {'see': 1L, 'num': 1L, 'at... | {'at user': 1L, 'lt num': ... |\n",
      "| {'chix': 1L, 'pot': 1L, 'a... | {'tip do': 1L, 'dip for': ... |\n",
      "| {'and': 2L, 'then': 1L, 'v... | {'if you': 1L, 'you look':... |\n",
      "| {'on': 1L, 'play': 1L, 'fr... | {'po play': 1L, 'nba on': ... |\n",
      "| {'real': 1L, 'a': 1L, 'got... | {'my nigga': 1L, 'real nig... |\n",
      "| {'hooiser': 1L, 'love': 1L... | {'hooiser indianamensbaset... |\n",
      "| {'a': 1L, 'life': 1L, 'pat... | {'at user': 2L, 'game have... |\n",
      "| {'hit': 1L, 'for': 1L, 'pa... | {'pay for': 1L, 'such an':... |\n",
      "| {'and': 1L, 'is': 2L, 'at'... | {'at user': 2L, 'but zelle... |\n",
      "| {'on': 1L, 'rey': 1L, 'lan... | {'unintent rhyme': 1L, 're... |\n",
      "| {'and': 1L, 'drake': 1L, '... | {'at user': 1L, 'with nick... |\n",
      "| {'toronto': 1L, 'again': 1... | {'the sun': 1L, 'grey rain... |\n",
      "| {'and': 1L, 'given': 1L, '... | {'justified bamf': 1L, 'to... |\n",
      "| {'rey': 1L, 'lana': 1L, 'm... | {'to see': 1L, 'del rey': ... |\n",
      "| {'back': 1L, 'on': 1L, 'on... | {'school onli': 1L, 'onli ... |\n",
      "| {'sign': 1L, 'forgot': 1L,... | {'or scandal': 1L, 'm supp... |\n",
      "| {'and': 1L, 'is': 1L, 'it'... | {'two block': 1L, 'may be'... |\n",
      "| {'me': 1L, 'a': 1L, 'georg... | {'a po': 1L, 'anyon wanna'... |\n",
      "| {'notifi': 1L, 'just': 1L,... | {'steeler can': 1L, 'for t... |\n",
      "| {'gone': 1L, 'tonight': 1L... | {'acoust with': 1L, 'may a... |\n",
      "| {'begin': 1L, 'be': 1L, 'g... | {'will be': 1L, 'swirling ... |\n",
      "| {'rush': 1L, 'for': 1L, 't... | {'amp num': 1L, 'po to': 1... |\n",
      "| {'me': 1L, 'jan': 1L, 'we'... | {'ne in': 1L, 'club h': 1L... |\n",
      "| {'escobar': 1L, 'design': ... | {'at user': 1L, 'jewelry r... |\n",
      "| {'rt': 1L, 'numth': 1L, 'k... | {'gt rt': 1L, 'at numth': ... |\n",
      "| {'even': 1L, 'king': 1L, '... | {'even if': 1L, 'if i': 1L... |\n",
      "| {'and': 1L, 'then': 1L, 'o... | {'home to': 1L, 'po tomorr... |\n",
      "| {'a': 1L, 'me': 1L, 'all':... | {'of the': 1L, 'the el': 1... |\n",
      "| {'cbb': 1L, 'ios': 1L, 'up... | {'ios num': 1L, 'num at': ... |\n",
      "| {'webb': 1L, 'gt': 1L, 'ha... | {'v manchest': 1L, 'ard we... |\n",
      "| {'a': 1L, 'we': 1L, 'to': ... | {'t wait': 1L, 'babi can':... |\n",
      "| {'be': 1L, 'swag': 1L, 'to... | {'take an': 1L, 'so swag':... |\n",
      "| {'shine': 1L, 'the': 2L, '... | {'love your': 1L, 'day url... |\n",
      "| {'wa': 1L, 'me': 1L, 'made... | {'me now': 1L, 'through my... |\n",
      "| {'a': 1L, 'arsen': 2L, 'ma... | {'not saturday': 1L, 'the ... |\n",
      "| {'a': 1L, 'be': 1L, 'tomor... | {'a laugh': 1L, 'side look... |\n",
      "| {'and': 1L, 'be': 1L, 'pla... | {'by the': 1L, 'it vancouv... |\n",
      "| {'tough': 1L, 'is': 1L, 'm... | {'tomorrow mark': 1L, 'che... |\n",
      "| {'be': 1L, 'onli': 1L, 'i'... | {'may be': 1L, 'the onli':... |\n",
      "| {'and': 1L, 'then': 1L, 'd... | {'and sam': 1L, 'sam after... |\n",
      "| {'and': 1L, 'am': 1L, 'at'... | {'luck both': 1L, 'at user... |\n",
      "| {'streak': 1L, 'on': 1L, '... | {'find out': 1L, 'po and':... |\n",
      "| {'control': 1L, 'it': 1L, ... | {'not po': 1L, 'at user': ... |\n",
      "| {'we': 2L, 'numrd': 1L, 'l... | {'sugar bowl': 1L, 'player... |\n",
      "| {'me': 1L, 'po': 1L, 'to':... | {'or num': 1L, 't eaten': ... |\n",
      "| {'know': 1L, 'singapore': ... | {'it s': 1L, 'at user': 1L... |\n",
      "| {'a': 1L, 'we': 1L, 'deal'... | {'it in': 1L, 'at user': 1... |\n",
      "| {'tri': 1L, 'do': 1L, 'par... | {'im tri': 1L, 'at user': ... |\n",
      "| {'some': 1L, 'it': 1L, 're... | {'at user': 1L, 'an po': 1... |\n",
      "| {'numth': 1L, 'friday': 1L... | {'know the': 1L, 'jason in... |\n",
      "| {'ad': 1L, 'just': 1L, 'it... | {'pay zinga': 1L, 'the dvd... |\n",
      "| {'case': 1L, 'a': 1L, 'and... | {'it may': 1L, 'andi wa': ... |\n",
      "| {'see': 1L, 'num': 1L, 'at... | {'got num': 1L, 'at user':... |\n",
      "| {'short': 1L, 'to': 1L, 'a... | {'round tie': 1L, 's fa': ... |\n",
      "| {'and': 2L, 'give': 1L, 'y... | {'dj on': 1L, 'tune in': 1... |\n",
      "| {'and': 1L, 'brother': 1L,... | {'brother at': 1L, 'to ble... |\n",
      "| {'opposit': 1L, 'is': 2L, ... | {'a guy': 1L, 'but is': 1L... |\n",
      "| {'awar': 1L, 'recreation':... | {'gather at': 1L, 'all kin... |\n",
      "| {'wiki': 1L, 'we': 1L, 'll... | {'dude po': 1L, 'get back'... |\n",
      "| {'and': 1L, 'tv': 1L, 'of'... | {'poll s': 1L, 'of the': 1... |\n",
      "| {'right': 1L, 'colorado': ... | {'could not': 1L, 'at user... |\n",
      "| {'a': 1L, 'mizzou': 1L, 'v... | {'volleybal game': 1L, 'at... |\n",
      "| {'and': 2L, 'on': 1L, 'to'... | {'obama and': 1L, 'see oba... |\n",
      "| {'diego': 1L, 'rush': 1L, ... | {'amp num': 1L, 'po to': 1... |\n",
      "| {'saturday': 1L, 'be': 1L,... | {'po the': 1L, 'still have... |\n",
      "| {'on': 1L, 'philippin': 1L... | {'rate upgrad': 1L, 'moodi... |\n",
      "| {'kstate': 1L, 'onli': 1L,... | {'fiesta bowl': 1L, 'know ... |\n",
      "| {'webb': 1L, 'give': 1L, '... | {'at user': 1L, 'ard webb'... |\n",
      "| {'flight': 1L, 'vancouv': ... | {'the sun': 1L, 'view po':... |\n",
      "| {'on': 1L, 'giant': 1L, 'a... | {'ne up': 1L, 'on sunday':... |\n",
      "| {'me': 1L, 'advance': 1L, ... | {'halloween can': 1L, 'mak... |\n",
      "| {'ago': 1L, 'on': 2L, 'pre... | {'of waverli': 1L, 'premie... |\n",
      "| {'underworld': 1L, 'url': ... | {'nov num': 1L, 'nump url'... |\n",
      "| {'right': 1L, 'over': 1L, ... | {'numth in': 1L, 'the mvp'... |\n",
      "| {'lfc': 1L, 'is': 1L, 'ard... | {'po the': 1L, 'sunday lfc... |\n",
      "| {'and': 1L, 'tuesday': 1L,... | {'jr librari': 1L, 'to num... |\n",
      "| {'even': 1L, 'be': 1L, 'he... | {'with my': 1L, 'at user':... |\n",
      "| {'and': 1L, 'vampir': 1L, ... | {'vampir diari': 1L, 'at u... |\n",
      "| {'amendment': 1L, '4th': 1... | {'know the': 1L, '20 gave'... |\n",
      "| {'shalwar': 1L, 'aalim': 1... | {'ne shalwar': 1L, 'at use... |\n",
      "| {'mile': 1L, 'and': 1L, 'w... | {'re challenged': 1L, 'if ... |\n",
      "| {'a': 1L, 'on': 1L, 'liver... | {'newcatl unit': 1L, 'rate... |\n",
      "| {'city': 1L, 'relat': 1L, ... | {'numth in': 1L, 'group wi... |\n",
      "| {'clipper': 1L, 'even': 1L... | {'the clipper': 1L, 'the s... |\n",
      "| {'wiki': 1L, 'on': 1L, 'ab... | {'a po': 1L, 'read about':... |\n",
      "| {'a': 1L, 'tuesday': 1L, '... | {'at bolton': 1L, 'at user... |\n",
      "| {'a': 1L, 'and': 1L, 'ben'... | {'etta jame': 1L, 'at user... |\n",
      "| {'origin': 1L, 'jason': 1L... | {'minut of': 1L, 'the orig... |\n",
      "| {'episod': 1L, 'an': 1L, '... | {'i could': 1L, 'of mob': ... |\n",
      "| {'and': 1L, 'cha': 2L, 'sn... | {'night club': 1L, 'at use... |\n",
      "| {'even': 1L, 'me': 1L, 'an... | {'nephew is': 1L, 'is move... |\n",
      "| {'the': 1L, 'about': 1L, '... | {'done talk': 1L, 'the jus... |\n",
      "| {'and': 1L, 'may': 1L, 'wo... | {'i could': 1L, 'may go': ... |\n",
      "| {'a': 1L, 'dwight': 1L, 'm... | {'dwight ard': 1L, 'want a... |\n",
      "| {'just': 2L, 'it': 1L, 'se... | {'at user': 1L, 'but we': ... |\n",
      "| {'and': 1L, 'regardless': ... | {'with ne': 1L, 'afp peopl... |\n",
      "| {'and': 1L, 'is': 2L, 'we'... | {'an po': 1L, 'say we': 1L... |\n",
      "| {'and': 1L, 'a': 1L, 'pas'... | {'sopa and': 1L, 'wa pas':... |\n",
      "| {'blue': 1L, 'and': 1L, 'g... | {'kick in': 1L, 'az sun': ... |\n",
      "| {'me': 1L, 'again': 1L, 'd... | {'a po': 1L, 'me ever': 1L... |\n",
      "| {'user': 1L, 'me': 1L, 'ov... | {'morn so': 1L, 'david bow... |\n",
      "| {'numrd': 1L, 'of': 1L, 'i... | {'numrd anniversari': 1L, ... |\n",
      "| {'in': 1L, 'uk': 1L, 'afte... | {'at user': 2L, 'in uk': 1... |\n",
      "| {'vicar': 1L, 'is': 1L, 'i... | {'nov num': 1L, 'would be'... |\n",
      "| {'a': 1L, 'all': 1L, 'whil... | {'out furnitur': 1L, 'all ... |\n",
      "| {'all': 1L, 'set': 1L, 'is... | {'is big': 1L, 'goal or': ... |\n",
      "| {'a': 2L, 'be': 1L, 'may':... | {'derek fisher': 1L, 'woul... |\n",
      "| {'newcastl': 1L, 'heskey':... | {'user numrd': 1L, 'at use... |\n",
      "| {'a': 1L, 'sdsu': 1L, 'mar... | {'the bc': 1L, 'po up': 1L... |\n",
      "| {'choic': 1L, 'finish': 1L... | {'rehears for': 1L, 'the p... |\n",
      "| {'it': 1L, 'player': 1L, '... | {'lose that': 1L, 'about s... |\n",
      "| {'about': 2L, 'sure': 1L, ... | {'ll need': 1L, 'sat throu... |\n",
      "| {'a': 2L, 'on': 1L, 'liver... | {'newcatl unit': 1L, 'to l... |\n",
      "| {'then': 1L, 'to': 1L, 'be... | {'user at': 1L, 'it s': 1L... |\n",
      "| {'and': 1L, 'num': 1L, 'at... | {'at user': 1L, 'frank gor... |\n",
      "| {'excit': 1L, 'see': 1L, '... | {'at user': 1L, 'to see': ... |\n",
      "| {'a': 1L, 'on': 1L, 'commu... | {'the gop': 1L, 'on happen... |\n",
      "| {'on': 1L, 'play': 1L, 'be... | {'predict po': 1L, 'at use... |\n",
      "| {'ami': 1L, 'be': 1L, 'and... | {'may be': 1L, 'globe awar... |\n",
      "| {'all': 1L, 'is': 2L, 'pla... | {'season n': 1L, 'da mav':... |\n",
      "| {'on': 1L, 'crap': 1L, 'eh... | {'crap eh': 1L, 'am go': 1... |\n",
      "| {'gone': 1L, 'num': 1L, 'b... | {'will be': 1L, 'she can':... |\n",
      "| {'on': 1L, 'letterman': 1L... | {'late s': 1L, 'vice presi... |\n",
      "| {'brazil': 1L, 'style': 1L... | {'the sun': 1L, 'brazil kk... |\n",
      "| {'all': 1L, 'over': 1L, 'c... | {'stoke angri': 1L, 'sat b... |\n",
      "| {'and': 3L, 'both': 1L, 'j... | {'at both': 1L, 'both serv... |\n",
      "| {'and': 1L, 'sanchez': 1L,... | {'num fan': 1L, 'at user':... |\n",
      "| {'see': 1L, 'at': 1L, 'wan... | {'at user': 1L, 'but ne': ... |\n",
      "| {'clipper': 1L, 'and': 1L,... | {'the clipper': 1L, 'not g... |\n",
      "| {'num': 2L, 'at': 1L, 'onl... | {'at user': 1L, 'hat ne': ... |\n",
      "| {'late': 1L, 'in': 1L, 'ni... | {'teamfollowback go': 1L, ... |\n",
      "| {'num': 1L, 'url': 1L, 's'... | {'aww yeah': 1L, 'chester ... |\n",
      "| {'and': 1L, 'notredam': 1L... | {'is rank': 1L, 'numth spo... |\n",
      "| {'cbb': 1L, 'it': 1L, 'was... | {'at user': 1L, 'someth ab... |\n",
      "| {'user': 2L, 'but': 1L, 'm... | {'user at': 1L, 'it in': 1... |\n",
      "| {'and': 1L, 'on': 1L, 'not... | {'on tonight': 1L, 's s': ... |\n",
      "| {'a': 1L, 'on': 1L, 'omg':... | {'happi chelsea': 1L, 'omg... |\n",
      "| {'week': 1L, 'on': 1L, 'el... | {'s lecture': 1L, 'year on... |\n",
      "| {'on': 1L, 'for': 1L, 'big... | {'roll on': 1L, 'on sunday... |\n",
      "| {'own': 1L, 'see': 1L, 'pr... | {'sleep now': 1L, 'with my... |\n",
      "| {'and': 1L, 'on': 1L, 'now... | {'lotr doesn': 1L, 'tonigh... |\n",
      "| {'numth': 1L, 'all': 1L, '... | {'happybirthdaytroianbelli... |\n",
      "| {'classic': 1L, 'nhl': 1L,... | {'is cancelled': 1L, 'ther... |\n",
      "| {'and': 1L, 'golden': 1L, ... | {'fey and': 1L, 'po ne': 1... |\n",
      "| {'flex': 1L, 'matt': 1L, '... | {'matt s': 1L, 'out to': 1... |\n",
      "| {'give': 1L, 'is': 1L, 'of... | {'at user': 1L, 'the nfl':... |\n",
      "| {'a': 2L, 'on': 1L, 'that'... | {'brittania on': 1L, 'stad... |\n",
      "| {'it': 1L, 'in': 1L, 'go':... | {'it s': 1L, 'minut ride':... |\n",
      "| {'is': 1L, 'num': 1L, 'at'... | {'at user': 1L, 'know u': ... |\n",
      "| {'the': 2L, 'qbs': 1L, 'st... | {'history he': 1L, 'nfl hi... |\n",
      "| {'be': 1L, 'confirm': 1L, ... | {'at user': 1L, 'go on': 1... |\n",
      "| {'high': 1L, 'at': 2L, 'su... | {'at the': 1L, '26th frm':... |\n",
      "| {'a': 1L, 'elimin': 1L, 'y... | {'pick your': 1L, 'numersp... |\n",
      "| {'mmmmmmmm': 1L, 'golden':... | {'have to': 1L, 'globe hos... |\n",
      "| {'and': 1L, 'twitter': 1L,... | {'s pizza': 1L, 'twitter f... |\n",
      "| {'is': 1L, 'osu': 1L, 'at'... | {'at user': 1L, 'champions... |\n",
      "| {'and': 1L, 'numth': 2L, '... | {'out with': 1L, 'and numt... |\n",
      "| {'info': 1L, 'awar': 1L, '... | {'full info': 1L, 'info fo... |\n",
      "| {'and': 2L, 'people': 1L, ... | {'tv the': 1L, 'shore is':... |\n",
      "| {'sound': 1L, 'on': 1L, 'd... | {'porcupin tree': 1L, 'vis... |\n",
      "| {'and': 1L, 'never': 1L, '... | {'see santorum': 1L, 'that... |\n",
      "| {'cont': 1L, 'concert': 1L... | {'zayn liam': 1L, 'ask you... |\n",
      "| {'a': 2L, 'bless': 1L, 'al... | {'it certainli': 1L, 'hav ... |\n",
      "| {'downton': 1L, 'it': 1L, ... | {'at user': 1L, 'thought o... |\n",
      "| {'brazil': 1L, 'chile': 1L... | {'amp argentina': 1L, 'the... |\n",
      "| {'fair': 1L, 'it': 1L, 'ru... | {'of toy': 1L, 'running di... |\n",
      "| {'user': 2L, 'lie': 1L, 'd... | {'point u': 1L, 'jr url': ... |\n",
      "| {'el': 1L, 'is': 1L, 'it':... | {'camp nou': 1L, 'at user'... |\n",
      "| {'on': 1L, 'url': 1L, 'rea... | {'amp say': 1L, 'jericho t... |\n",
      "| {'and': 1L, 'both': 1L, 'v... | {'x factor': 1L, 'both epi... |\n",
      "| {'prime': 1L, 'fantasi': 1... | {'ha url': 1L, 'for numnd'... |\n",
      "| {'ratings': 1L, 'cabl': 1L... | {'tnt fx': 1L, 'ratings tn... |\n",
      "| {'and': 1L, 'numth': 1L, '... | {'happi numth': 1L, 'at us... |\n",
      "| {'a': 1L, 'bummer': 1L, 'w... | {'po the': 1L, 'sandi thin... |\n",
      "| {'into': 1L, 'at': 2L, 'in... | {'sir thoma': 1L, 'a frien... |\n",
      "| {'and': 1L, 'fole': 2L, 'a... | {'hi domin': 1L, 'continu ... |\n",
      "| {'don': 1L, 'for': 1L, 'ge... | {'molli hatchet': 1L, 'hat... |\n",
      "| {'readi': 1L, 'some': 1L, ... | {'at user': 1L, 'readi to'... |\n",
      "| {'me': 1L, 'thank': 1L, 'f... | {'at user': 3L, 'is go': 1... |\n",
      "| {'tim': 1L, 'testifyhisgre... | {'want him': 1L, 'get tim'... |\n",
      "| {'footbal': 1L, 'vs': 1L, ... | {'implic chicago': 1L, 'ch... |\n",
      "| {'a': 1L, 'patriot': 1L, '... | {'cut off': 1L, 'num midwa... |\n",
      "| {'cowboy': 1L, 'i': 1L, 'i... | {'will beat': 1L, 'the fal... |\n",
      "| {'patriot': 1L, 'i': 1L, '... | {'i wanna': 1L, 'mon cb': ... |\n",
      "| {'a': 1L, 'on': 1L, 'diffe... | {'you on': 1L, 'for you': ... |\n",
      "| {'we': 1L, 'devil': 1L, 'i... | {'at user': 1L, 'the devil... |\n",
      "| {'see': 1L, 'num': 1L, 'at... | {'at user': 1L, 'lt num': ... |\n",
      "| {'liar': 1L, 'just': 1L, '... | {'a tease': 1L, 'jan num':... |\n",
      "| {'a': 2L, '10': 1L, 'homes... | {'preschool movi': 1L, '9 ... |\n",
      "| {'a': 1L, 'the': 1L, 'marc... | {'in march': 1L, 'a laugh'... |\n",
      "| {'analysi': 1L, 'jinni': 1... | {'recommendations go': 1L,... |\n",
      "| {'and': 1L, 'on': 2L, 'dem... | {'comedi central': 1L, 'on... |\n",
      "| {'saturday': 1L, 'on': 1L,... | {'take the': 1L, 'sinc the... |\n",
      "| {'dad': 1L, 'numth': 1L, '... | {'the hous': 1L, 'bed for'... |\n",
      "| {'conference': 1L, 'reprod... | {'annual conference': 1L, ... |\n",
      "| {'australia': 1L, 'emil': ... | {'australia octob': 1L, 'w... |\n",
      "| {'and': 1L, 'about': 1L, '... | {'to freek': 1L, 'watch de... |\n",
      "| {'tomorrow': 1L, 'i': 1L, ... | {'pic tomorrow': 1L, 'to s... |\n",
      "| {'again': 1L, 'good': 1L, ... | {'is my': 1L, 'pca basketb... |\n",
      "| {'and': 1L, 'on': 1L, 'abo... | {'warm and': 1L, 'link wit... |\n",
      "| {'a': 1L, 'and': 1L, 'patr... | {'friday at': 1L, 'two ret... |\n",
      "| {'cfn': 1L, 'a': 1L, 'cham... | {'an sec': 1L, 'accept a':... |\n",
      "| {'choic': 1L, 'gt': 1L, 'l... | {'to vote': 1L, 'learn url... |\n",
      "| {'orang': 1L, 'sync': 1L, ... | {'that wa': 1L, 'at user':... |\n",
      "| {'sam': 1L, 'for': 1L, 'ga... | {'thompson numnd': 1L, 'ab... |\n",
      "| {'january': 1L, 'ce': 1L, ... | {'at user': 1L, 'ce in': 1... |\n",
      "| {'is': 1L, 'it': 1L, 'one'... | {'but teen': 1L, 'my one':... |\n",
      "| {'numth': 1L, 'look': 1L, ... | {'at user': 3L, 'parti nov... |\n",
      "| {'be': 1L, 'hooked': 1L, '... | {'you d': 1L, 'human at': ... |\n",
      "| {'acc': 1L, 'numth': 1L, '... | {'for hoki': 1L, 'line dif... |\n",
      "| {'mile': 1L, 'le': 1L, 'di... | {'mile discus': 1L, 'satur... |\n",
      "| {'high': 1L, 'at': 2L, 'fr... | {'at user': 1L, 'user come... |\n",
      "| {'set': 1L, 'soon': 1L, 'o... | {'at user': 1L, 'a week': ... |\n",
      "| {'tuesday': 1L, 'sunderlan... | {'at user': 2L, 'if i': 1L... |\n",
      "| {'saturday': 1L, 'afrojack... | {'ticket online': 1L, 'by ... |\n",
      "| {'num': 5L, 'made': 1L, 'j... | {'pourier jr': 1L, 'the nu... |\n",
      "| {'anytim': 1L, 'for': 1L, ... | {'for sunday': 1L, 'sunday... |\n",
      "| {'on': 1L, 'abc': 1L, 'omg... | {'p s': 1L, 'on abc': 1L, ... |\n",
      "| {'and': 1L, 'golden': 1L, ... | {'at user': 1L, 'jan num':... |\n",
      "| {'and': 2L, 'a': 1L, 'we':... | {'all po': 1L, 'you down':... |\n",
      "| {'then': 1L, 'rey': 1L, 'm... | {'with exam': 1L, 'sure i'... |\n",
      "| {'and': 1L, 'on': 1L, 'mon... | {'on monday': 1L, 'someon ... |\n",
      "| {'and': 1L, 'is': 1L, 'bec... | {'at user': 1L, 'that it':... |\n",
      "| {'and': 2L, 'feel': 1L, 'i... | {'woof and': 1L, 'at user'... |\n",
      "| {'fiesta': 1L, 'be': 1L, '... | {'po would': 1L, 'fiesta b... |\n",
      "| {'religi': 1L, 'amend': 1L... | {'doma is': 1L, 'rick sant... |\n",
      "| {'and': 1L, 'have': 1L, 't... | {'state and': 1L, 'at user... |\n",
      "| {'and': 1L, 'all': 1L, 'xc... | {'cold i': 1L, 'at user': ... |\n",
      "| {'c': 1L, 'andi': 1L, 'big... | {'mon you': 1L, 'ne tonigh... |\n",
      "| {'again': 1L, 'liverpool':... | {'again on': 1L, 'at user'... |\n",
      "| {'el': 1L, 'gona': 1L, 'is... | {'go some': 1L, 'at user':... |\n",
      "| {'just': 1L, 'yeah': 1L, '... | {'just let': 1L, 'if you':... |\n",
      "| {'myself': 1L, 'good': 1L,... | {'now goodtim': 1L, 'nowad... |\n",
      "| {'kidrauhl': 1L, 'on': 1L,... | {'po kidrauhl': 1L, 'tour ... |\n",
      "| {'be': 2L, 'lamar': 1L, 'm... | {'in shape': 2L, 'not in':... |\n",
      "| {'ambassador': 1L, 'just':... | {'just the': 1L, 'ambassad... |\n",
      "| {'play': 1L, 'manu': 1L, '... | {'gordon could': 1L, 'dirk... |\n",
      "| {'feel': 1L, 'boston': 1L,... | {'the peopl': 1L, 'now t':... |\n",
      "| {'anatomi': 1L, 'don': 1L,... | {'same time': 1L, 'go to':... |\n",
      "| {'make': 1L, 'choic': 1L, ... | {'speak of': 1L, 'genius l... |\n",
      "| {'career': 1L, 'mathieu': ... | {'tyrann mathieu': 1L, 'di... |\n",
      "| {'is': 1L, 'it': 1L, 'ard'... | {'like wow': 1L, 'in case'... |\n",
      "| {'a': 1L, 'numth': 1L, 'ca... | {'will be': 1L, 'on your':... |\n",
      "| {'a': 1L, 'be': 1L, 'and':... | {'may be': 1L, 'line ever'... |\n",
      "| {'and': 1L, 'longs': 1L, '... | {'an po': 1L, 'numrd and':... |\n",
      "| {'be': 1L, 'head': 1L, 'th... | {'may be': 1L, 'head may':... |\n",
      "| {'a': 1L, 'numth': 1L, 'gr... | {'a po': 1L, 'numth time':... |\n",
      "| {'wa': 1L, 'the': 1L, 'gre... | {'great class': 1L, 'at nu... |\n",
      "| {'do': 1L, 'we': 1L, 'don'... | {'nun el': 1L, 'gotti tryn... |\n",
      "| {'it': 1L, 'one': 2L, 'are... | {'one on': 2L, 'mayb it': ... |\n",
      "| {'nught': 1L, 'don': 1L, '... | {'forget the': 1L, 'of fri... |\n",
      "| {'pre': 1L, 'preorder': 1L... | {'preorder these': 1L, 'on... |\n",
      "| {'and': 1L, 'number': 1L, ... | {'at user': 1L, 'of me': 1... |\n",
      "| {'and': 1L, 'give': 1L, 's... | {'him but': 1L, 'at user':... |\n",
      "| {'and': 1L, 'photo': 1L, '... | {'tomorrow bestintheworld'... |\n",
      "| {'begin': 1L, 'numth': 1L,... | {'begin novemb': 1L, 'up y... |\n",
      "| {'gotti': 1L, 'concert': 1... | {'with my': 1L, 'becaus i'... |\n",
      "| {'numth': 1L, 'from': 1L, ... | {'po for': 1L, 'cast pic':... |\n",
      "| {'and': 2L, 'it': 1L, 'num... | {'at user': 1L, 'would be'... |\n",
      "| {'a': 1L, 'numth': 1L, 'pl... | {'sugar bowl': 1L, 'rank t... |\n",
      "| {'be': 1L, 'iowach': 1L, '... | {'am po': 1L, 'letsgohawk ... |\n",
      "| {'be': 1L, 'grade': 1L, 'i... | {'sugar bowl': 1L, 'a veri... |\n",
      "| {'c': 1L, 'rodger': 1L, 'j... | {'to matt': 1L, 'at user':... |\n",
      "| {'a': 1L, 'head': 1L, 'd':... | {'haha po': 1L, 'to keep':... |\n",
      "| {'gone': 1L, 'cowboy': 1L,... | {'sunday night': 1L, 'the ... |\n",
      "| {'on': 1L, 'besti': 1L, 'm... | {'for the': 1L, 'with the'... |\n",
      "| {'me': 1L, 'a': 1L, 'get':... | {'get a': 1L, 'if you': 1L... |\n",
      "| {'and': 1L, 'ima': 1L, 'we... | {'grey anatomi': 1L, 'thur... |\n",
      "| {'heart': 1L, 'all': 1L, '... | {'if you': 1L, 'at user': ... |\n",
      "| {'ami': 1L, 'do': 1L, 'to'... | {'am go': 1L, 'drive me': ... |\n",
      "| {'just': 1L, 'it': 1L, 'nu... | {'s annual': 1L, 'tell us'... |\n",
      "| {'a': 1L, 'the': 2L, 'from... | {'part of': 1L, 'visit buc... |\n",
      "| {'natur': 1L, 'some': 1L, ... | {'m one': 1L, 'after nov':... |\n",
      "| {'and': 1L, 'on': 1L, 'fro... | {'roosevelt museum': 1L, '... |\n",
      "| {'see': 1L, 'suspens': 1L,... | {'talk about': 1L, 'or sea... |\n",
      "| {'a': 1L, 'championship': ... | {'shot at': 1L, 'nation ch... |\n",
      "| {'becaus': 1L, 'at': 1L, '... | {'at user': 1L, 'user nope... |\n",
      "| {'claim': 1L, 'about': 1L,... | {'won t': 1L, 'he won': 1L... |\n",
      "| {'me': 1L, 'the': 2L, 'pla... | {'at user': 1L, 'an po': 1... |\n",
      "| {'saturday': 1L, 'is': 1L,... | {'t wait': 1L, 'til saturd... |\n",
      "| {'tomorrow': 1L, 'look': 1... | {'s room': 1L, 's s': 1L, ... |\n",
      "| {'a': 1L, 'numth': 1L, 'or... | {'tweet or': 1L, 'or retwe... |\n",
      "| {'and': 1L, 'grade': 1L, '... | {'grade to': 1L, 'year you... |\n",
      "| {'and': 1L, 'on': 1L, 'sta... | {'abt it': 1L, 'comedi cen... |\n",
      "| {'ve': 1L, 'osu': 1L, 'at'... | {'at the': 1L, 'game i': 1... |\n",
      "| {'a': 1L, 'tripl': 1L, 'an... | {'doubl at': 1L, 'at user'... |\n",
      "| {'numd': 1L, 'we': 1L, 'om... | {'can we': 1L, 'drew bree'... |\n",
      "| {'waverli': 1L, 'movi': 1L... | {'place the': 1L, 'at user... |\n",
      "| {'buy': 1L, 'are': 1L, 'in... | {'night consid': 1L, 'at u... |\n",
      "| {'be': 1L, 'just': 1L, 'fo... | {'go to': 1L, 'at user': 1... |\n",
      "| {'the': 1L, 'sure': 1L, 'i... | {'to barcelona': 1L, 'sure... |\n",
      "| {'me': 1L, 'celtic': 1L, '... | {'go to': 1L, 'at user': 1... |\n",
      "| {'and': 1L, 'all': 1L, 'wi... | {'b s': 1L, 'the tray': 1L... |\n",
      "| {'a': 1L, 'great': 1L, 'le... | {'histori gcse': 1L, 'day ... |\n",
      "| {'good': 1L, 'at': 1L, 'in... | {'earli on': 1L, 'numth in... |\n",
      "| {'co': 1L, 'just': 1L, 'nl... | {'on thi': 1L, 'if you': 1... |\n",
      "| {'all': 1L, 'is': 1L, 'it'... | {'the onli': 1L, 'm ask': ... |\n",
      "| {'and': 1L, 'sergei': 1L, ... | {'in paris': 1L, 'to serge... |\n",
      "| {'cyber': 1L, 'ngayon': 1L... | {'at pipa': 1L, 'ngayon cy... |\n",
      "| {'a': 1L, 'awar': 1L, 'abo... | {'about lgs': 1L, 'user ur... |\n",
      "| {'babe': 1L, 'just': 1L, '... | {'won t': 1L, 'see him': 1... |\n",
      "| {'be': 1L, 'after': 1L, 't... | {'after saturday': 1L, 'wa... |\n",
      "| {'we': 1L, 'litani': 1L, '... | {'the saint': 1L, 'of the'... |\n",
      "| {'huge': 1L, 'pas': 1L, 'o... | {'huge day': 1L, 'day of':... |\n",
      "| {'on': 2L, 'num': 1L, 'jus... | {'announc it': 1L, 'at use... |\n",
      "| {'user': 1L, 'on': 1L, 'se... | {'her guest': 1L, 'guest a... |\n",
      "| {'a': 1L, 'great': 1L, 'am... | {'the big': 1L, 't wait': ... |\n",
      "| {'the': 2L, 'underworld': ... | {'halloween the': 1L, 'par... |\n",
      "| {'ago': 1L, 'on': 2L, 'pre... | {'of waverli': 1L, 'episod... |\n",
      "| {'even': 1L, 'all': 1L, 'f... | {'user url': 1L, 'at user'... |\n",
      "| {'a': 1L, 'numth': 1L, 'gu... | {'my guess': 1L, 'that the... |\n",
      "| {'a': 1L, 'nigga': 1L, 'lu... | {'ju have': 1L, 'hawk tomo... |\n",
      "| {'rocket': 1L, 'thunder': ... | {'po the': 1L, 'v rocket':... |\n",
      "| {'ride': 1L, 'the': 1L, 'l... | {'piec of': 1L, 'hello pie... |\n",
      "| {'and': 1L, 'angel': 1L, '... | {'at user': 1L, 'novemb nu... |\n",
      "| {'and': 1L, 'full': 1L, 'm... | {'the friday': 1L, 'in ful... |\n",
      "| {'and': 1L, 'all': 1L, 'tu... | {'have that': 1L, 'i could... |\n",
      "| {'origin': 1L, 'numth': 1L... | {'soton v': 1L, 'monday nu... |\n",
      "| {'and': 1L, 'all': 1L, 'ha... | {'at user': 1L, 'rooki yea... |\n",
      "| {'a': 1L, 'then': 1L, 'alf... | {'morn with': 1L, 'with an... |\n",
      "| {'and': 2L, 'is': 1L, 'thu... | {'at user': 2L, 'rock park... |\n",
      "| {'ne': 1L, 'it': 1L, 'real... | {'numth think': 1L, 'ticke... |\n",
      "| {'files': 1L, 'the': 2L, '... | {'reject are': 1L, 'devil ... |\n",
      "| {'cute': 1L, 'direction': ... | {'ne cute': 1L, 'nummm mor... |\n",
      "| {'sound': 1L, 'actual': 1L... | {'admit that': 1L, 'doesn ... |\n",
      "| {'for': 1L, 'get': 1L, 'pa... | {'po for': 1L, 'kidz po': ... |\n",
      "| {'the': 2L, 'inside': 1L, ... | {'night with': 1L, 'done t... |\n",
      "| {'be': 1L, 'devil': 1L, 'v... | {'may be': 1L, 'ne movi': ... |\n",
      "| {'classic': 1L, 'lock': 1L... | {'at user': 1L, 'a season'... |\n",
      "| {'d': 1L, 'just': 1L, 'mii... | {'gotta ask': 1L, 'just go... |\n",
      "| {'and': 1L, 'is': 1L, 'fuc... | {'still a': 1L, 'with the'... |\n",
      "| {'and': 1L, 'cb': 1L, 'one... | {'at user': 1L, 'everi pol... |\n",
      "| {'and': 1L, 'ksu': 1L, 'us... | {'texa tech': 1L, 'v texa'... |\n",
      "| {'me': 1L, 'be': 1L, 'ver'... | {'may be': 1L, 'be url': 1... |\n",
      "| {'set': 1L, 'it': 1L, 'at'... | {'at user': 1L, 'look funn... |\n",
      "| {'for': 1L, 'watch': 1L, '... | {'numst time': 1L, 'watch ... |\n",
      "| {'her': 1L, 'wa': 1L, 'cow... | {'at user': 1L, 'i wish': ... |\n",
      "| {'all': 1L, 'ya': 1L, 'it'... | {'have a': 1L, 's friday':... |\n",
      "| {'play': 1L, 'fli': 1L, 'o... | {'steeler can': 1L, 'sun t... |\n",
      "| {'saturday': 1L, 'tudor': ... | {'the url': 1L, 're so': 1... |\n",
      "| {'me': 1L, 'houston': 1L, ... | {'tomorrow hope': 1L, 'lin... |\n",
      "| {'all': 1L, 'school': 1L, ... | {'at school': 1L, 'thunder... |\n",
      "| {'and': 1L, 'houston': 1L,... | {'houston may': 1L, 'at us... |\n",
      "| {'then': 1L, 'school': 1L,... | {'at school': 1L, 'with my... |\n",
      "| {'am': 1L, 'wit': 1L, 'at'... | {'at user': 1L, 'user rotf... |\n",
      "| {'and': 3L, 'neighborhood'... | {'come and': 1L, 'u at': 1... |\n",
      "| {'the': 2L, 'all': 1L, 'da... | {'great day': 1L, 'for the... |\n",
      "| {'all': 1L, 'can': 1L, 'at... | {'if you': 1L, 'you still'... |\n",
      "| {'earli': 1L, 'and': 1L, '... | {'records the': 1L, 'lewi ... |\n",
      "| {'and': 1L, 'on': 1L, 'eas... | {'will be': 1L, 'seavrsl t... |\n",
      "| {'user': 1L, 'on': 1L, 'co... | {'at user': 1L, 'naaa i': ... |\n",
      "| {'all': 1L, 'weeknd': 1L, ... | {'all day': 1L, 'the weekn... |\n",
      "| {'be': 1L, 'gotti': 1L, 'y... | {'the biggest': 1L, 'i may... |\n",
      "| {'cbb': 1L, 'is': 2L, 'say... | {'marki say': 1L, 'that me... |\n",
      "| {'a': 1L, 'championship': ... | {'nation championship': 1L... |\n",
      "| {'meme': 1L, 'on': 1L, 'fe... | {'unearthli amp': 1L, 'mem... |\n",
      "| {'all': 1L, 'num': 1L, 'at... | {'at user': 1L, 'too colem... |\n",
      "| {'rva': 1L, 'set': 1L, 'ha... | {'fox morningmarketplace':... |\n",
      "| {'then': 1L, 'eyes': 1L, '... | {'url then': 1L, 'see it':... |\n",
      "| {'and': 1L, 'king': 1L, 'l... | {'king jr': 1L, 'include m... |\n",
      "| {'anatomi': 1L, 'be': 1L, ... | {'at user': 1L, 'with thi'... |\n",
      "| {'em': 1L, 'and': 1L, 'the... | {'prove em': 1L, 'tell me'... |\n",
      "| {'gt': 1L, 'player': 1L, '... | {'career nfc': 1L, 'gt i':... |\n",
      "| {'and': 1L, 'numth': 1L, '... | {'and vogu': 1L, 'eye jude... |\n",
      "| {'months': 1L, 've': 1L, '... | {'he still': 1L, 'of mitt'... |\n",
      "| {'some': 1L, 'date': 1L, '... | {'ha url': 1L, 'him but': ... |\n",
      "| {'tournament': 1L, 'four':... | {'japan num': 1L, 'invit t... |\n",
      "| {'and': 2L, 'is': 1L, 'in'... | {'thi weekend': 1L, 'from ... |\n",
      "| {'on': 2L, 'giant': 2L, 'f... | {'will travel': 1L, 'on ur... |\n",
      "| {'movi': 1L, 'see': 2L, 'i... | {'min i': 1L, 'myself to':... |\n",
      "| {'i': 1L, 'mlk': 1L, 'shou... | {'mlk game': 1L, 'should i... |\n",
      "| {'excit': 1L, 'muah': 1L, ... | {'with my': 1L, 'my girl':... |\n",
      "| {'numth': 1L, 'knows': 1L,... | {'with it': 1L, 'at user':... |\n",
      "| {'and': 1L, 'midnight': 1L... | {'at user': 1L, 'lt num': ... |\n",
      "| {'cutom': 1L, 'at': 1L, 's... | {'buy three': 1L, 'fb spec... |\n",
      "| {'at': 1L, 'want': 1L, 'in... | {'at user': 1L, 'you but':... |\n",
      "| {'for': 1L, 'is': 1L, 'sat... | {'po for': 1L, 't wait': 1... |\n",
      "| {'zumba': 1L, 'feel': 1L, ... | {'zumba but': 1L, 'realli ... |\n",
      "| {'arena': 1L, 'a': 1L, 'bu... | {'at user': 1L, 'ha a': 1L... |\n",
      "| {'even': 1L, 'better': 1L,... | {'at user': 1L, 'and u': 1... |\n",
      "| {'and': 1L, 'are': 1L, 'in... | {'the hous': 1L, 'at user'... |\n",
      "| {'boy': 1L, 'eli': 1L, 'be... | {'with my': 1L, 'at user':... |\n",
      "| {'pr': 1L, 'even': 1L, 'le... | {'give the': 1L, 'the medi... |\n",
      "| {'just': 1L, 'halloween': ... | {'to god': 1L, 'halloween ... |\n",
      "| {'poor': 1L, 'and': 1L, 'w... | {'sopa and': 1L, 'tomorrow... |\n",
      "| {'them': 1L, 'outta': 1L, ... | {'at user': 1L, 'outta the... |\n",
      "| {'i': 1L, 'm': 1L, 'it': 1... | {'game tomorrow': 1L, 'fuc... |\n",
      "| {'user': 1L, 'friendship':... | {'best part': 1L, 'part of... |\n",
      "| {'monday': 1L, 'to': 1L, '... | {'the mod': 1L, 'at user':... |\n",
      "| {'on': 1L, 'br': 1L, 'mond... | {'wed go': 1L, 'the saint'... |\n",
      "| {'and': 1L, 'numth': 1L, '... | {'sooo po': 1L, 'radcliff ... |\n",
      "| {'shop': 1L, 'gone': 1L, '... | {'to gotti': 1L, 'putin it... |\n",
      "| {'and': 1L, 'play': 1L, 'r... | {'motown music': 1L, 'etta... |\n",
      "| {'is': 2L, 'orang': 1L, 'a... | {'at user': 1L, 'with u': ... |\n",
      "| {'a': 1L, 'the': 1L, 'agre... | {'will be': 1L, 'winter cl... |\n",
      "| {'pull': 1L, 'monday': 1L,... | {'becaus of': 1L, 'on mond... |\n",
      "| {'tonight': 1L, 'sen': 1L,... | {'may not': 1L, 'u s': 1L,... |\n",
      "| {'and': 1L, 'help': 1L, 'i... | {'with it': 1L, 'u know': ... |\n",
      "| {'it': 1L, 'at': 1L, 'have... | {'tune in': 1L, 'if you': ... |\n",
      "| {'numth': 1L, 'law': 1L, '... | {'drug test': 1L, 'court h... |\n",
      "| {'result': 1L, 'will': 2L,... | {'be air': 1L, 'at user': ... |\n",
      "| {'rt': 1L, 'a': 1L, 'and':... | {'get a': 1L, 'a rt': 1L, ... |\n",
      "| {'fifa': 1L, 'and': 2L, 'v... | {'final finish': 1L, 'kaka... |\n",
      "| {'on': 1L, 'we': 1L, 'atle... | {'the numth': 1L, 'at user... |\n",
      "| {'high': 1L, 'num': 1L, 't... | {'night well': 1L, 'off ty... |\n",
      "| {'art': 1L, 'just': 1L, 'c... | {'amp curat': 1L, 'weekend... |\n",
      "| {'again': 1L, 'url': 1L, '... | {'in tucson': 1L, 'tucson ... |\n",
      "| {'return': 1L, 'january': ... | {'camper tweet': 1L, 'happ... |\n",
      "| {'a': 1L, 'king': 1L, 'lut... | {'dr martin': 1L, 'appear ... |\n",
      "| {'saturday': 1L, 'a': 1L, ... | {'a po': 1L, 'at user': 1L... |\n",
      "| {'are': 1L, 'on': 1L, 'liv... | {'to liverpool': 1L, 'user... |\n",
      "| {'gone': 1L, 'all': 1L, 'b... | {'thi weekend': 1L, 'down ... |\n",
      "+-------------------------------+-------------------------------+\n",
      "+-------------------------------+-------------------------------+\n",
      "|         3gram features        |        vectors_pos_neg        |\n",
      "+-------------------------------+-------------------------------+\n",
      "| {'with awar day': 1L, 's a... | [0.00897363107651, -0.0307... |\n",
      "| {'england could see': 1L, ... | [-0.0286317002028, -0.0649... |\n",
      "| {'tina fey amp': 1L, 'host... | [-0.00489169172943, 0.0165... |\n",
      "| {'pretti good numst': 1L, ... | [0.0535683631897, -0.02256... |\n",
      "| {'let s po': 1L, 'or cares... | [0.0244785454124, -0.05871... |\n",
      "| {'pari is no': 1L, 'no lon... | [0.0204064827412, -0.01644... |\n",
      "| {'unit will tri': 1L, 'po ... | [0.0174693148583, -0.03296... |\n",
      "| {'amp hope next': 1L, 'go ... | [-0.0079288398847, 0.02765... |\n",
      "| {'spare ticket for': 1L, '... | [0.0781610682607, 0.013849... |\n",
      "| {'inspir outfit today': 1L... | [0.0139341503382, -0.00040... |\n",
      "| {'parad then game': 1L, 'g... | [-0.0818513184786, -0.0472... |\n",
      "| {'they said it': 1L, 'chin... | [-0.00352514418773, -0.070... |\n",
      "| {'it read chelsea': 1L, 'n... | [-0.0114114740863, 0.00078... |\n",
      "| {'ugly but he': 1L, 'want ... | [0.015364870429, -0.009284... |\n",
      "| {'marque next tuesday': 1L... | [-0.0628043562174, 0.00815... |\n",
      "| {'edg down ahead': 1L, 'da... | [0.0415221005678, -0.05113... |\n",
      "| {'close rang iu': 1L, 'gam... | [0.0245452243835, -0.05434... |\n",
      "| {'the fiesta bowl': 1L, 'l... | [0.0289108399302, 0.029915... |\n",
      "| {'might be offensive': 1L,... | [0.0344184078276, -0.03992... |\n",
      "| {'still have a': 1L, 'ha a... | [-0.00626161042601, -0.045... |\n",
      "| {'novemb num and': 1L, 'ph... | [0.00842759013176, -0.0215... |\n",
      "| {'be formal instal': 1L, '... | [-0.00499523151666, -0.008... |\n",
      "| {'and drive to': 1L, 'gott... | [-0.0274584814906, -0.0194... |\n",
      "| {'at user won': 1L, 'beeeo... | [-0.0217245649546, -0.0258... |\n",
      "| {'ha ne wit': 1L, 'daniel ... | [0.0320844911039, -0.02317... |\n",
      "| {'he brought it': 1L, 'he ... | [0.00163832912222, -0.0292... |\n",
      "| {'numth with num': 1L, 'ex... | [0.0047914609313, 0.018524... |\n",
      "| {'so we saw': 1L, 't so ba... | [0.0367637611926, 0.023408... |\n",
      "| {'twitter and fb': 1L, 'an... | [0.0296475868672, -0.02489... |\n",
      "| {'0 end first': 1L, 'zaval... | [-0.0107495198026, 0.03105... |\n",
      "| {'t wait til': 1L, 'til to... | [-0.00961075536907, -0.002... |\n",
      "| {'user texa and': 1L, 'loo... | [-0.0394570454955, 0.01997... |\n",
      "| {'you are in': 1L, 'in van... | [0.0177772343159, -0.04011... |\n",
      "| {'moment for it': 1L, 'num... | [0.0859754160047, -0.05862... |\n",
      "| {'footbal game thi': 1L, '... | [-0.0640803351998, 0.03083... |\n",
      "| {'lakers she s': 1L, '7fac... | [-0.0626631826162, -0.0376... |\n",
      "| {'there wa a': 1L, 'so fri... | [-0.000994636211544, 0.002... |\n",
      "| {'num num over': 1L, 'po s... | [0.00143354723696, -0.0382... |\n",
      "| {'will post a': 1L, 'tucso... | [-0.0189158767462, -0.0708... |\n",
      "| {'at user im': 1L, 'wont s... | [0.0255627799779, -0.04635... |\n",
      "| {'it alreadi here': 1L, 'i... | [0.0264316890389, -0.03230... |\n",
      "| {'no one go': 1L, 'go to s... | [0.000749554892536, 0.0236... |\n",
      "| {'time url via': 1L, 'for ... | [0.022440938279, -0.020606... |\n",
      "| {'on the pvr': 1L, 'contra... | [0.00429225945845, 0.02136... |\n",
      "| {'bernard in the': 1L, 'in... | [0.000913884665351, 0.0132... |\n",
      "| {'make their ufc': 1L, 'ga... | [0.0160265266895, -0.05875... |\n",
      "| {'steal by chalmers': 1L, ... | [0.00761308148503, 0.04701... |\n",
      "| {'anightmareongeorgestreet... | [0.041915461421, -0.051413... |\n",
      "| {'marshal raylan givens': ... | [0.0082965567708, -0.01745... |\n",
      "| {'dsb amp hib': 1L, 'high ... | [0.0696723088622, -0.05243... |\n",
      "| {'iron man s': 1L, 'u have... | [-0.046242531389, 0.017356... |\n",
      "| {'on mon tue': 1L, 'now yo... | [0.0278646666557, 0.031102... |\n",
      "| {'great manag to': 1L, 'th... | [-0.0740343853831, 0.02703... |\n",
      "| {'singers dancers num': 1L... | [0.015154575929, -0.017055... |\n",
      "| {'i cannot wait': 1L, 'to ... | [-0.057760708034, -0.07066... |\n",
      "| {'tomorrow though but': 1L... | [-0.00729121873155, -0.051... |\n",
      "| {'gonna watch grey': 1L, '... | [0.0407713502645, -0.05089... |\n",
      "| {'invit me so': 1L, 'me so... | [0.0370966233313, -0.02500... |\n",
      "| {'ll be buckin': 1L, 'be b... | [0.0677999779582, -0.03246... |\n",
      "| {'i po to': 1L, 'to go tom... | [0.0224489737302, -0.00307... |\n",
      "| {'the first chop': 1L, 'mi... | [-0.0207222420722, 0.00868... |\n",
      "| {'wa schedul for': 1L, 'ex... | [-0.0195748135448, -0.0238... |\n",
      "| {'yep look po': 1L, 'serio... | [0.00363250565715, 0.07353... |\n",
      "| {'on trent for': 1L, 'stok... | [0.081593722105, -0.027564... |\n",
      "| {'pm see you': 1L, 'first ... | [-0.0765614211559, -0.0324... |\n",
      "| {'tuf num final': 1L, 'num... | [0.0226375106722, -0.05164... |\n",
      "| {'ill have somethin': 1L, ... | [0.00962008070201, 0.00777... |\n",
      "| {'user sure absolutely': 1... | [0.0556819327176, 0.047041... |\n",
      "| {'new bos cud': 1L, 'negat... | [0.0442071594298, -0.00745... |\n",
      "| {'at hi hotel': 1L, 'num e... | [-0.0312045440078, -0.0275... |\n",
      "| {'you got to': 1L, 'parti ... | [0.0354982055724, -0.07758... |\n",
      "| {'exit about vancouv': 1L,... | [0.0360682606697, 0.025373... |\n",
      "| {'golden globes air': 1L, ... | [-0.0202464573085, -0.0085... |\n",
      "| {'to plymouth town': 1L, '... | [-0.0811296030879, -0.0050... |\n",
      "| {'the clipper are': 1L, 'a... | [0.0666426867247, 0.007996... |\n",
      "| {'back on at': 1L, 'live b... | [0.0264373235404, -0.13280... |\n",
      "| {'that you can': 1L, 'tmh ... | [0.051424972713, -0.106763... |\n",
      "| {'come out on': 1L, 'hi mi... | [-0.0173052754253, -0.0124... |\n",
      "| {'last night in': 1L, 'sta... | [-0.0183554440737, 0.04481... |\n",
      "| {'laugh must never': 1L, '... | [0.0587440356612, 0.009070... |\n",
      "| {'cbb po now': 1L, 'po on ... | [0.0203617215157, 0.061486... |\n",
      "| {'in credits but': 1L, 'bu... | [-0.00211632833816, 0.0523... |\n",
      "| {'tonight nfl network': 1L... | [-0.000995771144517, -0.01... |\n",
      "| {'edlefsen and matthew': 1... | [-0.00216098129749, -0.037... |\n",
      "| {'url mitt romney': 1L, 'f... | [0.00445909192786, -0.0057... |\n",
      "| {'down in deathvalley': 1L... | [0.0361777804792, -0.03531... |\n",
      "| {'see my big': 1L, 'the pa... | [0.0172809138894, -0.04101... |\n",
      "| {'her for num': 1L, 'i cal... | [-0.0210050586611, -0.0359... |\n",
      "| {'meet tiger wood': 1L, 'i... | [0.0383982434869, -0.05004... |\n",
      "| {'tonight excit for': 1L, ... | [-0.0187372099608, 0.05039... |\n",
      "| {'you ain t': 1L, 'gone do... | [0.0222494378686, 0.006846... |\n",
      "| {'same url therealromney':... | [0.0711143836379, -0.07431... |\n",
      "| {'did romney s': 1L, 's da... | [-0.0535433776677, -0.0104... |\n",
      "| {'premier novemb numnd': 1... | [-0.0192653294653, -0.0308... |\n",
      "| {'a they march': 1L, 'marc... | [-0.00236997543834, 0.0008... |\n",
      "| {'for tht drive': 1L, 'my ... | [0.0584645532072, -0.01806... |\n",
      "| {'donat num to': 1L, 'can ... | [-0.0395443551242, -0.0195... |\n",
      "| {'alleg ne may': 1L, 'play... | [0.0686576813459, 0.000869... |\n",
      "| {'at user watch': 1L, 'act... | [0.00354585656896, -0.0231... |\n",
      "| {'the numth of': 1L, 'litt... | [0.0853784456849, -0.01459... |\n",
      "| {'announc ne alabama': 1L,... | [-0.0395381338894, -0.0291... |\n",
      "| {'go to po': 1L, 'that s o... | [0.0653182193637, 0.051379... |\n",
      "| {'lunch links some': 1L, '... | [0.0489112846553, 0.057829... |\n",
      "| {'call and watch': 1L, 'ro... | [0.0157259684056, -0.07445... |\n",
      "| {'alreadi hit game': 1L, '... | [-0.0152181088924, 0.01524... |\n",
      "| {'in the evenin': 1L, 'all... | [0.0497442893684, 0.005252... |\n",
      "| {'ne than sopa': 1L, 'some... | [-0.0198129545897, -0.0396... |\n",
      "| {'in num game': 1L, 'heske... | [0.0329588092864, -0.01141... |\n",
      "| {'if i shadow': 1L, 'natal... | [0.0175963435322, -0.02257... |\n",
      "| {'my brother liam': 1L, 't... | [-0.038271125406, -0.04493... |\n",
      "| {'other from stratford': 1... | [0.0129731763154, -0.01893... |\n",
      "| {'get to hear': 1L, 'we ge... | [-0.0108871366829, 0.03332... |\n",
      "| {'user gari ablett': 1L, '... | [0.00650568772107, 0.01260... |\n",
      "| {'ne chicken thursday': 1L... | [0.00586730474606, -0.0173... |\n",
      "| {'cheer num the': 1L, 'po ... | [0.00332698598504, 0.05809... |\n",
      "| {'the numth of': 1L, 'll b... | [-0.0109863774851, -0.0717... |\n",
      "| {'snow and low': 1L, 'acco... | [0.103300213814, -0.033662... |\n",
      "| {'may be the': 1L, 'honest... | [0.0686431378126, 0.010609... |\n",
      "| {'said on hmw': 1L, 'qtr o... | [0.0198356471956, -0.05569... |\n",
      "| {'user girl exactli': 1L, ... | [0.0488073378801, -0.06672... |\n",
      "| {'bay head n': 1L, 'n j bo... | [0.0539812520146, -0.10481... |\n",
      "| {'ne to the': 1L, 'at user... | [0.0165476966649, 0.026819... |\n",
      "| {'s friday column': 1L, 'v... | [0.0229482650757, 0.034232... |\n",
      "| {'pump for the': 1L, 'seas... | [-0.0563350282609, -0.0681... |\n",
      "| {'some ne fan': 1L, 'numnd... | [-0.0409817248583, 0.04711... |\n",
      "| {'sinc numth grade': 1L, '... | [-0.0386699028313, 0.02610... |\n",
      "| {'did veri against': 1L, '... | [0.0374273881316, -0.00140... |\n",
      "| {'look forward to': 1L, 'i... | [0.0399971120059, -0.02084... |\n",
      "| {'get alot of': 1L, 'the f... | [0.0453667677939, -0.01309... |\n",
      "| {'he gone have': 1L, 'prol... | [0.017548320815, 0.0544402... |\n",
      "| {'to the texan': 1L, 'texa... | [0.0293124094605, 0.004695... |\n",
      "| {'of the po': 1L, 'seri of... | [-0.00335497665219, -0.033... |\n",
      "| {'v newcastle then': 1L, '... | [0.0153069905937, 0.016721... |\n",
      "| {'are completed the': 1L, ... | [0.0119817443192, -0.04454... |\n",
      "| {'to see the': 1L, 'po a p... | [-0.00344058335759, 0.0208... |\n",
      "| {'all night lonerproblem':... | [0.0230350065976, -0.07340... |\n",
      "| {'are left url': 1L, 'card... | [-0.0121891610324, -0.0523... |\n",
      "| {'so po at': 1L, 'it it wa... | [-0.0102531174198, -0.0131... |\n",
      "| {'numth num go': 1L, 'at n... | [0.0171222370118, -0.05808... |\n",
      "| {'at user u': 1L, 'to chal... | [-0.0763955190778, 0.00190... |\n",
      "| {'to anfield on': 1L, 'sun... | [0.0399562492967, -0.03565... |\n",
      "| {'user mr ard': 1L, 'at us... | [0.0251869261265, -0.01301... |\n",
      "| {'bay head n': 1L, 'n j bo... | [0.0420404858887, -0.10079... |\n",
      "| {'sir terri leahi': 1L, 't... | [0.102598085999, -0.023999... |\n",
      "| {'plagiarism sopa s': 1L, ... | [0.0576439425349, -0.00295... |\n",
      "| {'s episod of': 1L, 'beat ... | [0.035195056349, -0.022341... |\n",
      "| {'wait to go': 1L, 'tcu ga... | [-0.0797883048654, 0.00025... |\n",
      "| {'look forward to': 1L, 't... | [0.0459487624466, 0.036452... |\n",
      "| {'if u wanna': 1L, 'to bra... | [-0.0434775017202, 0.01068... |\n",
      "| {'free throw dasu': 1L, 's... | [-0.0321329347789, 0.07314... |\n",
      "| {'richardson ha the': 1L, ... | [-0.000926241278648, 0.060... |\n",
      "| {'gerrard everi singl': 1L... | [-0.0381348468363, 0.04441... |\n",
      "| {'made thi tuesday': 1L, '... | [-0.0455421507359, 0.00382... |\n",
      "| {'on wnbc there': 1L, 'my ... | [-0.0122946286574, -0.0226... |\n",
      "| {'may have thing': 1L, 'ks... | [0.0940904691815, -0.02094... |\n",
      "| {'the same there': 1L, 't ... | [-0.0261698402464, -0.0021... |\n",
      "| {'titl game ha': 1L, 'ha n... | [-0.00180030323099, -0.022... |\n",
      "| {'there i said': 1L, 'but ... | [-0.00317433243617, 0.0187... |\n",
      "| {'but do get': 1L, 'class ... | [-0.0335783585906, 0.00937... |\n",
      "| {'honey badger in': 1L, 'p... | [0.0695256218314, -0.05797... |\n",
      "| {'play golf he': 1L, 'stan... | [0.00855421088636, 0.03116... |\n",
      "| {'guest moder at': 1L, 'jo... | [8.39525237097e-05, -0.012... |\n",
      "| {'elections rememb the': 1... | [-0.0440785735846, -0.0236... |\n",
      "| {'my polic hat': 1L, 'info... | [0.00032348712557, -0.1060... |\n",
      "| {'po alabama i': 1L, 'aubu... | [0.0563369430602, 0.022802... |\n",
      "| {'season final of': 1L, 'w... | [0.0217046327889, -0.01208... |\n",
      "| {'apewalkin at the': 1L, '... | [0.0295187905431, -0.06822... |\n",
      "| {'dynamit may be': 1L, 'ma... | [0.0468861572444, -0.09713... |\n",
      "| {'may find out': 1L, 'in t... | [0.0879441052675, 0.030318... |\n",
      "| {'numth floor office': 1L,... | [0.0533907637, -0.00782079... |\n",
      "| {'houston tomorrow thank':... | [0.0215766150504, -0.07960... |\n",
      "| {'a jon huntsman': 1L, 'hu... | [-0.00894295331091, -0.001... |\n",
      "| {'go to open': 1L, 'open a... | [0.034237228334, -0.022694... |\n",
      "| {'could be toni': 1L, 'jay... | [-0.0510764457285, -0.0868... |\n",
      "| {'to have po': 1L, 'po are... | [-0.0724569782615, 0.09901... |\n",
      "| {'po upload process': 1L, ... | [0.0441063456237, -0.02391... |\n",
      "| {'of ticket for': 1L, 'use... | [0.0402560271323, 0.020835... |\n",
      "| {'for swansea yesterday': ... | [0.0254642050713, 0.016997... |\n",
      "| {'at watch joy': 1L, 'into... | [-0.0138165634125, 0.00815... |\n",
      "| {'po choice shit': 1L, 'a ... | [0.0600064769387, 0.022210... |\n",
      "| {'call and ha': 1L, 'make ... | [0.0841616988182, -0.10232... |\n",
      "| {'s no park': 1L, 'at user... | [0.0687526240945, -0.08214... |\n",
      "| {'down ahead of': 1L, 'dat... | [0.0415221005678, -0.05113... |\n",
      "| {'guarante if i': 1L, 'spe... | [-0.076935864985, 0.062305... |\n",
      "| {'bowl speech befor': 1L, ... | [0.0208347011358, 0.013231... |\n",
      "| {'come visit me': 1L, 'in ... | [-0.0214735418558, -0.0212... |\n",
      "| {'it s parti': 1L, 'to get... | [0.0159102063626, -0.06119... |\n",
      "| {'ne pretti littl': 1L, 'p... | [0.00515520386398, 0.00951... |\n",
      "| {'i ll shoot': 1L, 'done f... | [0.0584341548383, 0.004127... |\n",
      "| {'devil insid in': 1L, 'fl... | [0.0583764910698, -0.05201... |\n",
      "| {'i walk in': 1L, 'in i st... | [0.00382597232237, 0.01179... |\n",
      "| {'their program bad': 1L, ... | [0.0558778643608, 0.014098... |\n",
      "| {'never say never': 1L, 'p... | [0.0545451939106, 0.018072... |\n",
      "| {'in vancouver await': 1L,... | [-0.00292800017633, -0.021... |\n",
      "| {'address to the': 1L, 'at... | [-0.0260919965804, 0.00335... |\n",
      "| {'today im the': 1L, 'year... | [0.0338233672082, -0.00367... |\n",
      "| {'numth email to': 1L, 'in... | [-0.0251605268568, -0.0764... |\n",
      "| {'are you go': 1L, 'i wann... | [-0.00458051124588, 0.0279... |\n",
      "| {'girlfriend camilla bell'... | [0.0528071634471, -0.07215... |\n",
      "| {'patriots dog beach': 1L,... | [0.0277964808047, -0.03507... |\n",
      "| {'lunch links some': 1L, '... | [0.0407736897469, 0.050881... |\n",
      "| {'lax gt phoenix': 1L, 'gt... | [-0.0781138315797, -0.0245... |\n",
      "| {'go onlin tomorrow': 1L, ... | [0.0120147271082, -0.04093... |\n",
      "| {'in the rose': 1L, 're go... | [-0.00724610127509, -0.034... |\n",
      "| {'not to ne': 1L, 'but i p... | [-0.00612296443433, -0.075... |\n",
      "| {'we ve had': 1L, 'all see... | [0.0977926999331, -0.01794... |\n",
      "| {'s bike shop': 1L, 'shop ... | [0.0114176440984, -0.03266... |\n",
      "| {'bosh may not': 1L, 'eyes... | [-0.0377192758024, -0.0041... |\n",
      "| {'after harvard yale': 1L,... | [0.00314682163298, -0.0103... |\n",
      "| {'our busi live': 1L, 'of ... | [-0.0877859070897, -0.0425... |\n",
      "| {'laugh must never': 1L, '... | [0.0381202474236, 0.020295... |\n",
      "| {'with your at': 1L, 'your... | [0.0344168394804, -0.01308... |\n",
      "| {'upstat anderson sc': 1L,... | [0.0446234829724, -0.02566... |\n",
      "| {'on and im': 1L, 'is on a... | [0.0130048515275, 0.056211... |\n",
      "| {'got po now': 1L, 'tri to... | [0.00619649002329, -0.0812... |\n",
      "| {'time to delet': 1L, 'off... | [-0.0143670095131, -0.0021... |\n",
      "| {'plea plea would': 1L, 'c... | [0.0473338104784, -0.09134... |\n",
      "| {'watch it with': 1L, 'izz... | [0.0305021330714, -0.05139... |\n",
      "| {'look forward to': 1L, 'p... | [0.04801447317, 0.05037889... |\n",
      "| {'king ever said': 1L, 'ho... | [0.0065881810151, 0.022069... |\n",
      "| {'get her licens': 1L, 'me... | [-0.00803841464221, -0.041... |\n",
      "| {'i want to': 1L, 'want to... | [0.0443743094802, 0.004604... |\n",
      "| {'tumblr with twitter': 1L... | [0.0140382898971, 0.037762... |\n",
      "| {'ye it is': 1L, 'tigers a... | [0.045648407191, -0.027183... |\n",
      "| {'dream debut on': 1L, 'ma... | [-0.00507393386215, -0.050... |\n",
      "| {'be num in': 1L, 'll be n... | [-0.0520997866988, -0.0988... |\n",
      "| {'all that money': 1L, 'po... | [0.0386463180184, -0.03392... |\n",
      "| {'you re get': 1L, 'at the... | [0.0639022439718, -0.02550... |\n",
      "| {'fantasi team is': 1L, 'j... | [0.0644126757979, -0.02017... |\n",
      "| {'kaman return thi': 1L, '... | [0.00506745511666, 0.02759... |\n",
      "| {'homework catch up': 1L, ... | [0.0101189268753, -0.00447... |\n",
      "| {'num numpm to': 1L, 'awar... | [0.0395082682371, -0.02364... |\n",
      "| {'rey her face': 1L, 'lana... | [0.0271938759834, -0.06213... |\n",
      "| {'it out guy': 1L, 'breitb... | [-0.0433349199593, 0.00444... |\n",
      "| {'london copper up': 1L, '... | [0.0493875816464, -0.05916... |\n",
      "| {'class that swear': 1L, '... | [-0.0151997571811, -0.0538... |\n",
      "| {'t power po': 1L, 'i bet ... | [0.0367488712072, -0.01075... |\n",
      "| {'on numrd amp': 1L, 'thin... | [0.0271047446877, 0.007360... |\n",
      "| {'date is approach': 1L, '... | [0.0224560070783, 0.030572... |\n",
      "| {'night s concert': 1L, 'w... | [0.037947203964, -0.049213... |\n",
      "| {'dollar drink wizard': 1L... | [-0.0348060429096, -0.0685... |\n",
      "| {'he po numst': 1L, 'miami... | [0.0198216531426, -0.01353... |\n",
      "| {'concert tonight in': 1L,... | [-0.0241097919643, -0.0075... |\n",
      "| {'pipa tomorrow night': 1L... | [-0.0132415983826, -0.0154... |\n",
      "| {'dont see the': 1L, 'resp... | [0.0261618886143, -0.04164... |\n",
      "| {'be ne but': 1L, 'serious... | [0.0169394593686, 0.039853... |\n",
      "| {'a numth round': 1L, 'is ... | [-0.0976658910513, -0.1458... |\n",
      "| {'gonna beat the': 1L, 'th... | [0.0612147375941, -0.00520... |\n",
      "| {'divvi up num': 1L, 'that... | [0.00731415767223, -0.0211... |\n",
      "| {'want a sunday': 1L, 'of ... | [0.0523700714111, 0.053290... |\n",
      "| {'i gave num': 1L, 'peopl ... | [-0.00380727183074, -0.018... |\n",
      "| {'i ne asleep': 1L, 'till ... | [-0.0893588811159, -0.0010... |\n",
      "| {'place in ypsi': 1L, 'go ... | [-0.0103017678484, -0.0478... |\n",
      "| {'at user been': 1L, 'been... | [0.0780498608947, -0.03259... |\n",
      "| {'shant lt num': 1L, 'out ... | [0.00684264861047, -0.0216... |\n",
      "| {'s den yellow': 1L, 'den ... | [-0.00278330850415, -0.024... |\n",
      "| {'at user im': 1L, 'make m... | [0.118379354477, -0.070969... |\n",
      "| {'is on the': 1L, 'now cal... | [0.0204379353672, -0.04793... |\n",
      "| {'on a new': 1L, 'oh the d... | [-0.05723560974, -0.074063... |\n",
      "| {'at user yeah': 1L, 'for ... | [0.067269705236, -0.043321... |\n",
      "| {'movie joy nois': 1L, 'av... | [-0.030947489664, 0.025599... |\n",
      "| {'guy talk about': 1L, 'th... | [-0.019892739132, 0.008007... |\n",
      "| {'now may even': 1L, 'even... | [0.0422213673592, -0.01308... |\n",
      "| {'tey it again': 1L, 'tomo... | [-0.0515357963741, -0.0707... |\n",
      "| {'wa hang out': 1L, 'not j... | [0.00890995841473, 0.04503... |\n",
      "| {'stock market is': 1L, 'c... | [0.0319875665009, -0.02497... |\n",
      "| {'vid to watch': 1L, 'may ... | [0.0488560535014, -0.02608... |\n",
      "| {'laugh must never': 1L, '... | [0.0328361950815, -0.00985... |\n",
      "| {'you cheeki bugger': 1L, ... | [0.0838704183698, -0.03295... |\n",
      "| {'both teams the': 1L, 'an... | [-0.00725818611681, -0.063... |\n",
      "| {'mon google just': 1L, 's... | [-0.00974639505148, 0.0392... |\n",
      "| {'is safe geaux': 1L, 'spo... | [0.0214863270521, -0.01303... |\n",
      "| {'kick off in': 1L, 'centr... | [-0.0249114856124, 0.01199... |\n",
      "| {'premier on disney': 1L, ... | [-0.00298901507631, -8.495... |\n",
      "| {'t know numx': 1L, 'tucso... | [-0.0193470418453, -0.0879... |\n",
      "| {'po num months': 1L, 'not... | [0.00589782791212, 0.00947... |\n",
      "| {'i want to': 1L, 'sat com... | [0.038759175688, -0.068452... |\n",
      "| {'all share the': 1L, 'sam... | [-0.00917957071215, -0.107... |\n",
      "| {'ne w po': 1L, 'you all t... | [-0.0208037830889, 0.00926... |\n",
      "| {'af but it': 1L, 'but it ... | [0.0170526280999, 0.037043... |\n",
      "| {'m beggin you': 1L, 'it p... | [0.0607257038355, -0.07358... |\n",
      "| {'use to fli': 1L, 'been a... | [0.0396091975272, -0.01619... |\n",
      "| {'no okay thanks': 1L, 'a ... | [0.0242212638259, 0.034211... |\n",
      "| {'to the mav': 1L, 'cuban ... | [0.0248705334961, -0.02386... |\n",
      "| {'for stori on': 1L, 'prof... | [0.0724733695388, -0.04430... |\n",
      "| {'go to philli': 1L, 'to g... | [0.0133023168892, -0.03869... |\n",
      "| {'crew presenc becaus': 1L... | [0.0809978097677, 0.019575... |\n",
      "| {'tuesday s newsroom': 1L,... | [0.0608136877418, -0.03185... |\n",
      "| {'t pipe down': 1L, 'down ... | [0.0738221108913, 0.022806... |\n",
      "| {'year la ne': 1L, 'at use... | [0.0312608703971, 0.010761... |\n",
      "| {'at user best': 1L, 'mom ... | [-0.000318112288369, -0.04... |\n",
      "| {'the road at': 1L, 'clark... | [0.035745665431, -0.032316... |\n",
      "| {'ticket for clipper': 1L,... | [0.0832211449742, -0.00555... |\n",
      "| {'convers with tom': 1L, '... | [0.00144106114749, -0.0391... |\n",
      "| {'giant sweep niner': 1L, ... | [-0.0179825089872, -0.0022... |\n",
      "| {'but crunch time': 1L, 'c... | [0.0244193673134, 0.009462... |\n",
      "| {'red hot danc': 1L, 'in e... | [0.0303759220988, -0.00533... |\n",
      "| {'barclay profit driven': ... | [0.00701684178784, 0.02100... |\n",
      "| {'tina fey amp': 1L, 'once... | [-0.00327862915583, -0.017... |\n",
      "| {'kany said bush': 1L, 'fu... | [-0.0238667018712, -0.0449... |\n",
      "| {'tobi isn t': 1L, 'catch ... | [0.00108330999501, -0.0074... |\n",
      "| {'at numpm and': 1L, 'will... | [-0.0333305895329, -0.0462... |\n",
      "| {'a say in': 1L, 'play pic... | [-0.0352206416428, -0.0270... |\n",
      "| {'her numrd marriag': 1L, ... | [0.0295161381364, -0.08732... |\n",
      "| {'me some feedback': 1L, '... | [-0.00222794828005, -0.056... |\n",
      "| {'s bloodi freezing': 1L, ... | [0.0511689335108, -0.01170... |\n",
      "| {'can not wait': 1L, 'of m... | [0.0184252951294, -0.08868... |\n",
      "| {'user liam plea': 1L, 'li... | [0.0658910498023, -0.07008... |\n",
      "| {'boy are gonna': 1L, 'all... | [0.020251326263, -0.033867... |\n",
      "| {'sunderland v villa': 1L,... | [0.00877189915627, 0.02477... |\n",
      "| {'seattl seahawk bleacher'... | [0.0712490901351, -0.01887... |\n",
      "| {'lols you get': 1L, 'for ... | [0.0402045436203, 0.002798... |\n",
      "| {'by rebecca black': 1L, '... | [0.0624851882458, -0.03249... |\n",
      "| {'great manag to': 1L, 'th... | [-0.0740343853831, 0.02703... |\n",
      "| {'of you do': 1L, 'king jr... | [-0.0182785354555, 0.00210... |\n",
      "| {'url the the': 1L, 'melbo... | [0.0418228842318, -0.06563... |\n",
      "| {'at num pm': 1L, 'the san... | [-0.0339212827384, -0.0571... |\n",
      "| {'downton abbey end': 1L, ... | [0.112492062151, 0.0550216... |\n",
      "| {'numst use the': 1L, 'eve... | [0.0519207827747, 0.011043... |\n",
      "| {'friends befor the': 1L, ... | [-0.00407415349036, -0.048... |\n",
      "| {'just watch the': 1L, 'ma... | [-0.0379053726792, -0.0022... |\n",
      "| {'po without granger': 1L,... | [-0.0567827112973, -0.0054... |\n",
      "| {'dozen w re': 1L, 'line t... | [0.0878202393651, 0.015365... |\n",
      "| {'the week and': 1L, 'alex... | [0.0569640882313, -0.00904... |\n",
      "| {'on saturday url': 1L, 'a... | [-0.0279217455536, -0.0329... |\n",
      "| {'bieber i po': 1L, 'at us... | [-0.00468875141814, 0.0202... |\n",
      "| {'kevin durant did': 1L, '... | [-0.012334946543, -0.05110... |\n",
      "| {'january best news': 1L, ... | [0.0243003424257, -0.02249... |\n",
      "| {'on monday she': 1L, 'i r... | [-0.0345929451287, -0.0473... |\n",
      "| {'bill still draft': 1L, '... | [0.00614973343909, 0.03758... |\n",
      "| {'go out drink': 1L, 'unti... | [-0.0534751452506, -0.0709... |\n",
      "| {'it spread so': 1L, 'so t... | [0.0928528681397, 0.009557... |\n",
      "| {'distract by fifa': 1L, '... | [0.135855779052, -0.026448... |\n",
      "| {'user hi there': 1L, 'pum... | [0.00846802163869, 0.02544... |\n",
      "| {'lsu or alabama': 1L, 'to... | [-0.0490649156272, 0.02114... |\n",
      "| {'to liverpool for': 1L, '... | [-0.00533404387534, 0.0207... |\n",
      "| {'lock here num': 1L, 'mar... | [0.104014717042, 0.0489332... |\n",
      "| {'may let jordan': 1L, 'ti... | [0.00381206860766, -0.0088... |\n",
      "| {'it s numnd': 1L, 'think ... | [0.0201837923378, 0.054382... |\n",
      "| {'ne than sopa': 1L, 'phil... | [-0.000549977994524, -0.10... |\n",
      "| {'action lamar odom': 1L, ... | [0.0163853969425, -0.03955... |\n",
      "| {'may have someth': 1L, 'e... | [0.0618578568101, -0.03982... |\n",
      "| {'birthday kendal i': 1L, ... | [0.00206068367697, -0.0867... |\n",
      "| {'some bacon on': 1L, 'wel... | [0.013108342886, 0.0427568... |\n",
      "| {'bottom of the': 1L, 'sig... | [-0.0147167993709, -0.0457... |\n",
      "| {'contraband for the': 1L,... | [-0.0545980222523, -0.0363... |\n",
      "| {'hint num at': 1L, 'cours... | [-0.0202562455088, -0.1265... |\n",
      "| {'william prepar to': 1L, ... | [-0.0294214207679, -0.0197... |\n",
      "| {'i ll find': 1L, 'find it... | [0.0593329370022, 0.010907... |\n",
      "| {'you might not': 1L, 'wan... | [0.00618032272905, -0.0215... |\n",
      "| {'the numst victim': 1L, '... | [-0.0224321819842, 0.02833... |\n",
      "| {'worth d wait': 1L, 'ther... | [-0.014845165424, 0.001047... |\n",
      "| {'box all of': 1L, 'then b... | [-0.0181947238743, -0.0912... |\n",
      "| {'ransomwar virus after': ... | [0.111768566072, -0.044663... |\n",
      "| {'come back in': 1L, 'it c... | [0.0692814663053, -0.02078... |\n",
      "| {'i m guess': 1L, 'best in... | [0.0516168363392, -0.03139... |\n",
      "| {'gyllenhaal direct by': 1... | [0.0464478880167, -0.05095... |\n",
      "| {'off at user': 1L, 'po in... | [-0.0268697310239, 0.04699... |\n",
      "| {'come to culbertson': 1L,... | [-0.0183515753597, 0.10067... |\n",
      "| {'my madonna s': 1L, 'gonn... | [-0.00614439276978, 0.0113... |\n",
      "| {'ball num num': 1L, 'with... | [0.0431276969612, 0.099412... |\n",
      "| {'the celtic may': 1L, 'ma... | [0.136709555984, 0.0005211... |\n",
      "| {'would but she': 1L, 'she... | [0.00434314459562, -0.0357... |\n",
      "| {'to kfc tomorrow': 1L, 'b... | [-0.046199567616, 0.040828... |\n",
      "| {'to the dc': 1L, 'user mi... | [0.0382145904005, -0.02095... |\n",
      "| {'salmon tomorrow in': 1L,... | [0.0341791920364, -0.07378... |\n",
      "| {'bowl game were': 1L, 'wa... | [0.0347582101822, -0.04254... |\n",
      "| {'life in to': 1L, 'have n... | [-0.0139968721196, -0.0170... |\n",
      "| {'at user have': 1L, 'have... | [0.030877424404, -0.020041... |\n",
      "| {'dalla to see': 1L, 'trip... | [0.0248613748699, -0.04180... |\n",
      "| {'intern stutter awar': 1L... | [0.0897499024868, -0.01901... |\n",
      "| {'ne if i': 1L, 'that hall... | [-0.01995350793, -0.038606... |\n",
      "| {'morn movi at': 1L, 'thea... | [0.0354224480689, -0.08187... |\n",
      "| {'in the numst': 1L, 'of c... | [0.0715998783708, -0.01595... |\n",
      "| {'po sunday then': 1L, 'st... | [0.0299513116479, 0.106609... |\n",
      "| {'night of holiday': 1L, '... | [-0.038708563894, -0.05231... |\n",
      "| {'of anarchi and': 1L, 've... | [0.0164827816188, -0.04010... |\n",
      "| {'monday so i': 1L, 'some ... | [-0.00213319552131, -0.040... |\n",
      "| {'taken at anfield': 1L, '... | [0.00125684961677, -0.0734... |\n",
      "| {'believ i thought': 1L, '... | [-0.02034085989, -0.050486... |\n",
      "| {'com out call': 1L, 'in w... | [0.0377022624016, -0.02208... |\n",
      "| {'got it togeth': 1L, 'ale... | [-0.0283674541861, -0.0010... |\n",
      "| {'it po rebecca': 1L, 'reb... | [0.018767632544, -0.001431... |\n",
      "| {'with al green': 1L, 'at ... | [0.0146006243303, -0.02412... |\n",
      "| {'in januari and': 1L, 'on... | [-0.00286795361899, 0.0083... |\n",
      "| {'is thursday and': 1L, 'a... | [0.046067263931, -0.040573... |\n",
      "| {'leagu action on': 1L, 'l... | [0.0590805225074, -0.00966... |\n",
      "| {'user on thi': 1L, 'po sa... | [-0.0222155544907, -0.0244... |\n",
      "| {'but definit not': 1L, 's... | [0.109270535409, 0.0627663... |\n",
      "| {'obama for halloween': 1L... | [0.0125126559287, -0.06345... |\n",
      "| {'golden globe on': 1L, 'h... | [-0.0188199561089, 0.00820... |\n",
      "| {'num rose parad': 1L, 'do... | [-0.0533192940056, -0.0345... |\n",
      "| {'royal rumbl num': 1L, 'r... | [0.0105331130326, -0.03077... |\n",
      "| {'spotifi is all': 1L, 'ma... | [0.0538266114891, -0.09881... |\n",
      "| {'qsu meet will': 1L, 'at ... | [-0.000225348208915, -0.00... |\n",
      "| {'po suit to': 1L, 'he may... | [0.00861806236207, -0.0006... |\n",
      "| {'s stay togeth': 1L, 'cov... | [0.0490892753005, -0.01898... |\n",
      "| {'netflix on friday': 1L, ... | [-0.00541889248416, -0.011... |\n",
      "| {'footbal sunday dure': 1L... | [-0.0201402809471, -0.0338... |\n",
      "| {'think ryback should': 1L... | [0.0647630691528, -0.03754... |\n",
      "| {'to kidrauhl follow': 1L,... | [-0.0170688480139, -0.0492... |\n",
      "| {'can not wait': 1L, 'po d... | [0.037928994745, 0.0090745... |\n",
      "| {'honey badger and': 1L, '... | [0.0165257789195, 0.010810... |\n",
      "| {'game emil heskey': 1L, '... | [0.137989923358, 0.0303102... |\n",
      "| {'the numth annual': 1L, '... | [-0.0494262203574, -0.0905... |\n",
      "| {'at user tim': 1L, 'numrd... | [0.00965119898319, -0.0240... |\n",
      "| {'play for exet': 1L, 'exe... | [0.0187825076282, -0.03806... |\n",
      "| {'to be ob': 1L, 'want to ... | [-0.0610079020262, 0.01963... |\n",
      "| {'to go to': 1L, 'at user ... | [0.0381731763482, 0.019074... |\n",
      "| {'user oh didn': 1L, 't i ... | [0.0102046746761, 0.008094... |\n",
      "| {'the street tomorrow': 1L... | [0.0803043097258, -0.03874... |\n",
      "| {'knew that tomorrow': 1L,... | [-0.00372304022312, -0.013... |\n",
      "| {'s outstand teen': 1L, 'p... | [-0.0301486477256, -0.0079... |\n",
      "| {'llorent do you': 1L, 'hu... | [0.0202110111713, -0.00572... |\n",
      "| {'badger just don': 1L, 'd... | [0.0113706737757, 0.017671... |\n",
      "| {'we went to': 1L, 'at use... | [0.0140141397715, -0.06877... |\n",
      "| {'the percept so': 1L, 'da... | [0.0329078659415, 0.014227... |\n",
      "| {'numst respond and': 1L, ... | [0.0128316991031, -0.07679... |\n",
      "| {'numst unit bk': 1L, 'amp... | [0.0623190812767, -0.03933... |\n",
      "| {'wvu skip give': 1L, 'dow... | [-0.00497255753726, 0.0017... |\n",
      "| {'i didn t': 1L, 'the firs... | [-0.0336066782475, 0.02147... |\n",
      "| {'that tomorrow if': 1L, '... | [0.0228057913482, -0.01736... |\n",
      "| {'let the debat': 1L, 'the... | [0.0430422089994, -0.02664... |\n",
      "| {'the nfl thi': 1L, 'thi a... | [0.0636576861143, -0.02663... |\n",
      "| {'amp that s': 1L, 'in a w... | [0.0017618361162, -0.04638... |\n",
      "| {'hear some1 say': 1L, 'th... | [-0.0438764132559, -0.0439... |\n",
      "| {'may never know': 1L, 'yo... | [0.0477906242013, -0.06335... |\n",
      "| {'polls hammer it': 1L, 'r... | [0.0160592328757, 0.001946... |\n",
      "| {'hip injury san': 1L, 'sa... | [-0.0333090797067, -0.0667... |\n",
      "| {'flow mondays dj': 1L, 'd... | [0.0239859838039, -0.04283... |\n",
      "| {'janet stay over': 1L, 'w... | [-0.027206780389, 0.012128... |\n",
      "| {'him in concert': 1L, 's ... | [0.0233431980014, 0.012401... |\n",
      "| {'decemb numth num': 1L, '... | [0.0204662382603, 0.007545... |\n",
      "| {'okc is still': 1L, 'to t... | [0.0173537842929, 0.051414... |\n",
      "| {'come out til': 1L, 'moth... | [-0.0395882204175, -0.0063... |\n",
      "| {'napoleon dynamit in': 1L... | [0.018048947677, 0.0424784... |\n",
      "| {'at user even': 1L, 'a ne... | [0.0681934207678, -0.05786... |\n",
      "| {'the chicken liver': 1L, ... | [0.0684585422277, 0.074420... |\n",
      "| {'wa in wichita': 1L, 'to ... | [-0.00195095757954, -0.033... |\n",
      "| {'after it wa': 1L, 'janua... | [-0.0470743663609, -0.0537... |\n",
      "| {'touchdown num num': 1L, ... | [0.0425999835134, 0.064247... |\n",
      "| {'the day u': 1L, 'if no t... | [-0.0617338344455, -0.0198... |\n",
      "| {'anfield wa actual': 1L, ... | [-0.0475117042661, 0.01494... |\n",
      "| {'go to sc': 1L, 'just ski... | [0.0458213575184, -0.03183... |\n",
      "| {'user plea justin': 1L, '... | [0.0133730657399, -0.01817... |\n",
      "| {'just agre to': 1L, 'tomo... | [0.00072604912566, -0.0355... |\n",
      "| {'monday and tuesday': 1L,... | [-0.00187921209726, -0.042... |\n",
      "| {'numnd half of': 1L, 'liv... | [0.0131053328514, -0.04423... |\n",
      "| {'onlin on friday': 1L, 't... | [-0.0208642017096, -0.0742... |\n",
      "| {'januari for the': 1L, 'e... | [-0.0847998708487, -0.0086... |\n",
      "| {'is daniel radcliff': 1L,... | [-0.0453336797655, 0.00186... |\n",
      "| {'bowl in januari': 1L, 'g... | [0.0399497784674, 0.007272... |\n",
      "| {'i ll ne': 1L, 'user talk... | [-0.0060039726086, -0.0076... |\n",
      "| {'poehler are host': 1L, '... | [0.0101907104254, 0.025660... |\n",
      "| {'so i think': 1L, 'jersey... | [0.0307562462986, -0.00848... |\n",
      "| {'sec on at': 1L, 'user is... | [0.00200979225338, -0.0481... |\n",
      "| {'pride of the': 1L, 'marc... | [0.0976342111826, -0.03785... |\n",
      "| {'go po you': 1L, 'at user... | [0.0424533560872, 0.027284... |\n",
      "| {'lt num url': 1L, 'lol at... | [-0.0734079703689, -0.0136... |\n",
      "| {'get me everytim': 1L, 'n... | [-0.0143435504287, 0.02506... |\n",
      "| {'have sum christian': 1L,... | [-0.0663351491094, -0.0401... |\n",
      "| {'are play on': 1L, 'for i... | [-0.0465503782034, 0.00259... |\n",
      "| {'the first week': 1L, 'fr... | [-0.0452160835266, -0.0047... |\n",
      "| {'to be so': 1L, 'by eleve... | [-0.031385358423, -0.03602... |\n",
      "| {'impact by both': 1L, 'ha... | [0.0119964350015, -0.03321... |\n",
      "| {'next up no': 1L, 'tonigh... | [-0.0454708300531, -0.0149... |\n",
      "| {'he also ha': 1L, 'traine... | [-0.0260433349758, 0.01372... |\n",
      "| {'cohort tina fey': 1L, 'f... | [-0.0473861508071, -0.0568... |\n",
      "| {'town should a': 1L, 'wis... | [-0.0828543752432, -0.0405... |\n",
      "| {'knew that tomorrow': 1L,... | [-0.0306838583201, 0.00869... |\n",
      "| {'news the ne': 1L, 'park ... | [0.0664760097861, -0.00646... |\n",
      "| {'new york i': 1L, 'numth ... | [-0.0414777398109, -0.0146... |\n",
      "| {'and i think': 1L, 'user ... | [0.0983309596777, -0.02392... |\n",
      "| {'in la liga': 1L, 'book f... | [0.0650396943092, -0.01990... |\n",
      "| {'drive to huntsville': 1L... | [0.000993757159449, -0.010... |\n",
      "| {'peopl may ne': 1L, 'shor... | [0.0540748760104, -0.04848... |\n",
      "| {'make newcatl unit': 1L, ... | [0.0326091349125, -0.09151... |\n",
      "| {'on tv justified': 1L, 'n... | [0.0137980226427, -0.01517... |\n",
      "| {'a sunday night': 1L, 'po... | [-0.00468307500705, 0.0080... |\n",
      "| {'monday rose thebachelor'... | [0.0285981427878, -0.04205... |\n",
      "| {'confirm sunday with': 1L... | [-0.0155681092292, -0.0200... |\n",
      "| {'contraband kate beckinsa... | [-0.0157355312258, 0.03280... |\n",
      "| {'rochdal on tuesday': 1L,... | [0.0206375736743, -0.03047... |\n",
      "| {'on live at': 1L, 'at use... | [0.00757558643818, -0.0911... |\n",
      "| {'costum i m': 1L, 'numrd ... | [0.0926775485277, -0.01378... |\n",
      "| {'think dynasti league': 1... | [0.0786441862583, 0.017441... |\n",
      "| {'jersey shore and': 1L, '... | [0.039035294205, -0.066524... |\n",
      "| {'team rolling huge': 1L, ... | [0.0087616853416, -0.02492... |\n",
      "| {'onli the numth': 1L, 'th... | [-0.100099235773, 0.026431... |\n",
      "| {'bold prediction cowboy':... | [-0.0361406579614, -0.0381... |\n",
      "| {'purpl tomorrow at': 1L, ... | [-0.0215540155768, -0.0086... |\n",
      "| {'neck be one': 1L, 'respo... | [0.0288061778992, -0.04903... |\n",
      "| {'waiver by the': 1L, 'on ... | [0.0141605399549, -0.01690... |\n",
      "| {'about zayn and': 1L, 'ha... | [0.00038954016054, 0.02964... |\n",
      "| {'a num numth': 1L, 'in ne... | [-0.0353711768985, 0.05833... |\n",
      "| {'dsb amp hib': 1L, 'high ... | [0.0657594352961, -0.04482... |\n",
      "| {'the game stat': 1L, 'of ... | [0.0389242991805, 0.007811... |\n",
      "| {'you talk about': 1L, 'ab... | [0.080249324441, 0.0124984... |\n",
      "| {'some pressur doe': 1L, '... | [-0.0258754082024, 0.01893... |\n",
      "| {'no chipotl today': 1L, '... | [0.0390585660934, -0.03873... |\n",
      "| {'in on the': 1L, 'go in o... | [0.0598950460553, 0.004385... |\n",
      "| {'madonna s choic': 1L, 'f... | [-0.100224658847, 0.052066... |\n",
      "| {'we got swansea': 1L, 'aw... | [0.0357153862715, 0.022868... |\n",
      "| {'numst offici vacat': 1L,... | [-0.0723208412528, 0.05956... |\n",
      "| {'num skeleton holla': 1L,... | [0.00546374497935, -0.0767... |\n",
      "| {'w community park': 1L, '... | [-0.0243326164782, -0.1118... |\n",
      "| {'earli thursday morn': 1L... | [0.016894845292, -0.014880... |\n",
      "| {'the same level': 1L, 'st... | [0.0446224138141, 0.125531... |\n",
      "| {'drop ani new': 1L, 'octo... | [-0.0484599098563, -0.0383... |\n",
      "| {'ha some differences': 1L... | [0.0532854348421, -0.02089... |\n",
      "| {'team url via': 1L, 'be p... | [-0.00118370482232, -0.033... |\n",
      "| {'marri for the': 1L, 'tim... | [-0.0233929827809, -0.0271... |\n",
      "| {'he s a': 1L, 'ne ne a': ... | [0.00387861463241, -0.0150... |\n",
      "| {'from jordan rhode': 1L, ... | [-0.0430830456316, 0.03537... |\n",
      "| {'on a thursday': 1L, 'at ... | [-0.00427203951403, -0.009... |\n",
      "| {'so i can': 1L, 'food at ... | [0.0394923612475, -0.03218... |\n",
      "| {'not see mass': 1L, 'with... | [0.0421385653317, -0.01863... |\n",
      "| {'numst po answer': 1L, 'o... | [-0.0730141699314, 0.00951... |\n",
      "| {'care buckey would': 1L, ... | [-0.00902824196965, 0.0246... |\n",
      "| {'po tomorrow please': 1L,... | [0.0127508919686, -0.01140... |\n",
      "| {'spireit debut against': ... | [-0.00619042292237, -0.041... |\n",
      "| {'extend earli voting': 1L... | [0.0232900660485, -0.02233... |\n",
      "| {'user white collar': 1L, ... | [0.0440857969224, -0.04303... |\n",
      "| {'ryan mallett in': 1L, 'w... | [0.0444723181427, 0.006419... |\n",
      "| {'think it s': 1L, 'amazin... | [0.0814813748002, -0.00910... |\n",
      "| {'season is it': 1L, 'po t... | [-0.00618212064728, -0.024... |\n",
      "| {'so i can': 1L, 'live in ... | [-0.0643494874239, -0.0946... |\n",
      "| {'need to know': 1L, 'xc r... | [0.0481556653976, -0.03271... |\n",
      "| {'in everton game': 1L, 'm... | [0.0105720637366, -0.01732... |\n",
      "| {'wa gone stand': 1L, 'she... | [-0.00307813822292, -0.005... |\n",
      "| {'back on at': 1L, 'long b... | [0.00248303869739, -0.0254... |\n",
      "| {'truck is still': 1L, 'my... | [0.00633217906579, -0.0566... |\n",
      "| {'we might play': 1L, 'pla... | [0.0168708469719, 0.008069... |\n",
      "| {'t watch tvd': 1L, 'quick... | [-0.0124506140128, -0.0108... |\n",
      "| {'i not bet': 1L, 'it ne m... | [0.0259711388499, -0.05902... |\n",
      "| {'so po to': 1L, 'the boy ... | [-0.0122934151441, -0.0483... |\n",
      "| {'number num job': 1L, 'nu... | [0.0626271218061, 0.006853... |\n",
      "| {'i ve seen': 1L, 'thing i... | [0.0526129230857, -0.02228... |\n",
      "| {'num minut if': 1L, 'to p... | [-0.0527004189789, 0.03344... |\n",
      "| {'dame to go': 1L, 'po thi... | [0.00763448467478, 0.07198... |\n",
      "| {'vs bobcat game': 1L, 'ga... | [0.00239249318838, 0.04939... |\n",
      "| {'s the last': 1L, 'chief ... | [-0.025971557945, 0.062827... |\n",
      "| {'go back to': 1L, 'back t... | [0.00565928407013, -0.0035... |\n",
      "| {'hope we get': 1L, 'to go... | [0.00670060049742, 0.04202... |\n",
      "| {'sunday plea bring': 1L, ... | [0.0707695558667, -0.10750... |\n",
      "| {'night po thi': 1L, 'bell... | [-0.0613175965846, 0.06159... |\n",
      "| {'portray of the': 1L, 'th... | [-0.0148073621094, 0.01279... |\n",
      "| {'may soon be': 1L, 'for p... | [0.00478160427883, -0.0103... |\n",
      "| {'pray for judi': 1L, 'wit... | [0.0233928244561, -0.05835... |\n",
      "| {'be packag to': 1L, 'num ... | [0.0054033966735, -0.06294... |\n",
      "| {'tomorrow wssu parad': 1L... | [0.0611200332642, -0.02567... |\n",
      "| {'po banter ne': 1L, 'abou... | [0.00541024794802, 0.04688... |\n",
      "| {'the huntsman director': ... | [0.0577211603522, -0.02075... |\n",
      "| {'wasn t pasocc': 1L, 'num... | [0.036514673382, -0.085479... |\n",
      "| {'note is on': 1L, 'tumblr... | [-0.0443953759968, -0.0109... |\n",
      "| {'the train hustl': 1L, 'w... | [0.0376883707941, 0.029093... |\n",
      "| {'sick i mean': 1L, 'liste... | [0.0184119623154, -0.04383... |\n",
      "| {'to go to': 1L, 'the po r... | [-0.0542822703719, 0.05762... |\n",
      "| {'a month in': 1L, 'for go... | [-0.0299591645598, -0.0029... |\n",
      "| {'we re in': 1L, 'home we ... | [-0.0109566925094, -0.0127... |\n",
      "| {'tomorrow mark the': 1L, ... | [0.0107471961528, -0.04984... |\n",
      "| {'on congress some': 1L, '... | [0.0345496945083, 0.005828... |\n",
      "| {'do homework i': 1L, 'hom... | [0.0586703903973, -0.04092... |\n",
      "| {'at shelter in': 1L, 'go ... | [0.0169077664614, -0.00371... |\n",
      "| {'finish the b': 1L, 'at n... | [0.0285739842802, 0.029353... |\n",
      "| {'in may lol': 1L, 'some o... | [0.0298969615251, -0.01862... |\n",
      "| {'the manag some': 1L, 'su... | [0.0857027098536, -0.01188... |\n",
      "| {'saturday afternoon go': ... | [-0.0135461986065, -0.0205... |\n",
      "| {'birthday at user': 1L, '... | [-0.0128596192226, -0.0781... |\n",
      "| {'exclusive on the': 1L, '... | [-0.00127253856044, 0.0057... |\n",
      "| {'were on the': 1L, 'micha... | [-0.00945986807346, -0.053... |\n",
      "| {'call in the': 1L, 'cbb m... | [-0.0242914557457, -0.0427... |\n",
      "| {'u on saturday': 1L, 'he ... | [-0.0525202043355, -0.0062... |\n",
      "| {'to make a': 1L, 'make a ... | [0.0801047757268, -0.04679... |\n",
      "| {'t think you': 1L, 'take ... | [0.0463695786893, -0.05557... |\n",
      "| {'at user i': 1L, 'i get i... | [0.104784555733, -0.023778... |\n",
      "| {'littl thing on': 1L, 's ... | [-0.0154374074191, -0.0287... |\n",
      "| {'you stay past': 1L, 'sta... | [0.0189013108611, 0.013636... |\n",
      "| {'offer a ne': 1L, 'but do... | [-0.0135585498065, -0.0273... |\n",
      "| {'kany s numth': 1L, 'be a... | [0.0294833481312, 0.007981... |\n",
      "| {'numth of novemb': 1L, 'o... | [-0.0146230077371, -0.0373... |\n",
      "| {'watch red tail': 1L, 're... | [0.0195540394634, -0.02004... |\n",
      "| {'hustl from januari': 1L,... | [0.0101163089275, -0.02355... |\n",
      "| {'po to but': 1L, 'd norma... | [0.0275582112372, -0.01504... |\n",
      "| {'at user at': 1L, 'my po ... | [-0.00608074804768, -0.084... |\n",
      "| {'dead next saturday': 1L,... | [-0.0168314855546, -0.0301... |\n",
      "| {'have to see': 1L, 'to se... | [-0.023789903149, 0.021410... |\n",
      "| {'care for po': 1L, 'and m... | [0.020688418299, 0.0173483... |\n",
      "| {'drake song lmao': 1L, 'u... | [0.0213103741407, -0.01118... |\n",
      "| {'i take it': 1L, 'refere ... | [0.0320903323591, -0.05066... |\n",
      "| {'respons to rest': 1L, 'd... | [0.0134181138128, -0.08582... |\n",
      "| {'walk distanc from': 1L, ... | [-0.0196350254118, -0.0760... |\n",
      "| {'fx po now': 1L, 'watch f... | [0.048881560564, -0.005538... |\n",
      "| {'green get in': 1L, 'grea... | [-0.0201619938016, -0.0339... |\n",
      "| {'hahahahaha mademynight u... | [0.0640747323632, 0.001352... |\n",
      "| {'check out the': 1L, 'and... | [0.0671791359782, -0.07378... |\n",
      "| {'him out there': 1L, 'odo... | [-0.0110863624141, -0.0089... |\n",
      "| {'tomorrow morningg xx': 1... | [-0.00909408833832, -0.004... |\n",
      "| {'so i can': 1L, 'can stop... | [-0.030443476513, 0.023982... |\n",
      "| {'the manches url': 1L, 't... | [0.0847888365388, -0.04952... |\n",
      "| {'to get that': 1L, 'jerse... | [0.0275006759912, -0.03259... |\n",
      "| {'went to a': 1L, 'hustl t... | [-0.00856494251639, -0.052... |\n",
      "| {'positive you and': 1L, '... | [-0.0352770425379, 0.03713... |\n",
      "| {'with our po': 1L, 'num i... | [0.0275787711143, 0.023358... |\n",
      "| {'backflip gerbils liam': ... | [-0.00906067434698, -0.029... |\n",
      "| {'to go with': 1L, 'game s... | [-0.0274044778198, 0.04395... |\n",
      "| {'about matt flynn': 1L, '... | [0.0182103738189, -0.00232... |\n",
      "| {'it is one': 1L, 'bowl wa... | [-0.0113765727729, -0.0104... |\n",
      "| {'receiv a golden': 1L, 's... | [0.0109841972589, 0.010128... |\n",
      "| {'a po citi': 1L, 'still t... | [-0.000682048499584, 0.055... |\n",
      "| {'time in a': 1L, 'year to... | [0.0378112830222, -0.01208... |\n",
      "| {'seattl seahawk bleacher'... | [0.08831294626, -0.0603008... |\n",
      "| {'at user come': 1L, 'from... | [0.0657594352961, -0.04482... |\n",
      "| {'mlk jr on': 1L, 'just qu... | [0.0317673794925, 0.025830... |\n",
      "| {'i m po': 1L, 'last seaso... | [0.00911859516054, 0.01431... |\n",
      "| {'you then they': 2L, 'lov... | [-0.0246350541711, -0.0230... |\n",
      "| {'t see pardew': 1L, 'pard... | [-0.00599709153175, 0.0238... |\n",
      "| {'from le mile': 1L, 'have... | [0.027697134763, -0.071612... |\n",
      "| {'did get your': 1L, 'on n... | [0.0652839913964, -0.01893... |\n",
      "| {'think we re': 1L, 'we re... | [0.0742016881704, 0.023555... |\n",
      "| {'a a guest': 1L, 'user be... | [0.0450747981668, 0.034140... |\n",
      "| {'host by tina': 1L, 'ami ... | [-0.0369301885366, -0.0595... |\n",
      "| {'of fox and': 1L, 'numth ... | [0.0462012067437, -0.03288... |\n",
      "| {'couldn t withstand': 1L,... | [0.0363130569458, 0.022427... |\n",
      "| {'keep the po': 1L, 'fans ... | [0.0585015155375, 0.038063... |\n",
      "| {'may be the': 1L, 'ne man... | [0.00497858226299, -0.0031... |\n",
      "| {'hustle d with': 1L, 'to ... | [0.00694589084014, -0.0045... |\n",
      "| {'centr tonight to': 1L, '... | [-0.0133528504521, 0.04987... |\n",
      "| {'to do with': 1L, 'am i s... | [0.0472575798631, 0.024201... |\n",
      "| {'ne it come': 1L, 'dwight... | [0.0146632166579, -0.02403... |\n",
      "| {'m go down': 1L, 'unit ga... | [0.00206454982981, -0.0309... |\n",
      "| {'laugh must never': 1L, '... | [0.0573045611382, 0.011154... |\n",
      "| {'doe not po': 1L, 'knight... | [-0.0179156959057, 0.01970... |\n",
      "| {'to new y': 1L, 'pittsbur... | [0.0258584693074, -0.03637... |\n",
      "| {'some of those': 1L, 'mor... | [0.068199954927, -0.003665... |\n",
      "| {'wanna remind the': 1L, '... | [0.0448827147484, 0.006655... |\n",
      "| {'with five versu': 1L, 'n... | [0.00529016461223, -0.0679... |\n",
      "| {'views sta url': 1L, 'gam... | [0.0169071350247, -0.04907... |\n",
      "| {'the vanish on': 1L, 'or ... | [0.0651068240404, -0.04535... |\n",
      "| {'po for tomorrow': 1L, 'g... | [-0.0144179007038, -0.0015... |\n",
      "| {'next npsf webcast': 1L, ... | [0.0357157550752, -0.05123... |\n",
      "| {'num of suarez': 1L, 'on ... | [-0.0282242763788, -0.0052... |\n",
      "| {'down to georgia': 1L, 'i... | [-0.0661947354674, -0.0681... |\n",
      "| {'abl to be': 1L, 'with it... | [0.0362008102238, -0.00027... |\n",
      "| {'the race for': 1L, 'in t... | [-0.0183113981038, -0.0335... |\n",
      "| {'a po one': 1L, 'sam it s... | [0.044753048569, -0.031767... |\n",
      "| {'pre match brief': 1L, 'b... | [0.0636879056692, -0.08577... |\n",
      "| {'now all i': 1L, 'town to... | [0.0311753712595, -0.00791... |\n",
      "| {'black went crawl': 1L, '... | [0.115099430084, -0.002809... |\n",
      "| {'friday heart warm': 1L, ... | [0.0275641251355, 0.050053... |\n",
      "| {'a po go': 1L, 'po go to'... | [-0.0577609464526, 0.09178... |\n",
      "| {'justin bieber concert': ... | [0.00402874639258, 0.01920... |\n",
      "| {'ninja got u': 1L, 'my ni... | [-0.0344521589577, -0.0101... |\n",
      "| {'ne sunday night': 1L, 'u... | [-0.0378538742661, -0.0555... |\n",
      "| {'with mlk jr': 1L, 'my fa... | [0.01030624751, -0.0580130... |\n",
      "| {'on tmh a': 1L, 'zayn and... | [0.000711623579264, -0.044... |\n",
      "| {'have the same': 1L, 'jus... | [0.0142878862098, -0.05420... |\n",
      "| {'it s 32': 1L, 'lana del ... | [0.0248310044408, -0.01601... |\n",
      "| {'at user pipa': 1L, 'user... | [-0.00564012816176, -0.003... |\n",
      "| {'bosh drink a': 1L, 'burp... | [-0.0466556102037, 0.03218... |\n",
      "| {'go to watch': 1L, 'and g... | [-0.00485750846565, 0.0077... |\n",
      "| {'the upcom sex': 1L, 'dec... | [-0.0444540642202, -0.0518... |\n",
      "| {'game at guiseley': 1L, '... | [-0.0235335249454, -0.0975... |\n",
      "| {'a ransomwar virus': 1L, ... | [0.110515214503, -0.064454... |\n",
      "| {'at user don': 1L, 'ne wi... | [0.0131468903273, 0.017252... |\n",
      "| {'away saturday and': 1L, ... | [-0.0011859764345, -0.0220... |\n",
      "| {'my house respect': 1L, '... | [0.00857330299914, -0.0490... |\n",
      "| {'insid the exorcis': 1L, ... | [0.0252785030752, -0.02715... |\n",
      "| {'philippin but the': 1L, ... | [0.0587923415005, -0.06762... |\n",
      "| {'user at least': 1L, 'the... | [0.0247825607657, 0.017454... |\n",
      "| {'wooow it s': 1L, 'po asl... | [0.00245519378223, 0.02334... |\n",
      "| {'the lo angel': 1L, 'carp... | [0.0207460541278, -0.01229... |\n",
      "| {'have to move': 1L, 'just... | [-0.0182361099869, -0.0792... |\n",
      "| {'give it away': 1L, 'in c... | [0.0318973138928, 0.012032... |\n",
      "| {'they call me': 1L, 'tebo... | [0.0126713300124, -0.04854... |\n",
      "| {'to tell ya': 1L, 'have a... | [0.0638962909579, -0.01559... |\n",
      "| {'name is john': 1L, 'lett... | [0.00413780007511, 0.03119... |\n",
      "| {'on hi side': 1L, 'club o... | [-0.00152769812848, -0.021... |\n",
      "| {'friday find a': 1L, 'all... | [0.0118267377838, -0.03911... |\n",
      "| {'year damian lillard': 1L... | [0.0519871003926, 0.056687... |\n",
      "| {'newcatl s trip': 1L, 'de... | [0.0152688995004, -0.06025... |\n",
      "| {'be a po': 1L, 'to train ... | [0.0611127950251, 0.047367... |\n",
      "| {'user have a': 1L, 'a row... | [-0.0139349605888, -0.0165... |\n",
      "| {'german girl po': 1L, 'll... | [-0.00410622172058, -0.046... |\n",
      "| {'inform came out': 1L, 'o... | [0.0309397857636, -0.02488... |\n",
      "| {'with you url': 1L, 'thi ... | [-0.023014716804, 0.024863... |\n",
      "| {'and alex no': 1L, 'matt ... | [0.0411808975041, -0.02614... |\n",
      "| {'man just sat': 1L, 'the ... | [-0.0483183301985, -0.0544... |\n",
      "| {'numnd the heskey': 1L, '... | [0.0450160130858, -0.07667... |\n",
      "| {'we take torr': 1L, 'ne h... | [-0.00627293763682, -0.016... |\n",
      "| {'is anybodi go': 1L, 'tob... | [-0.0638246834278, 0.05125... |\n",
      "| {'at user oh': 1L, 'toss a... | [0.0953381434083, -0.00176... |\n",
      "| {'dsb amp hib': 1L, 'high ... | [0.0657594352961, -0.04482... |\n",
      "| {'theme friend marathon': ... | [0.0359237380326, -0.10761... |\n",
      "| {'ne my as': 1L, 'at user ... | [0.0759290978312, -0.03214... |\n",
      "| {'don t s': 1L, 's up to':... | [0.0279609430581, -0.03483... |\n",
      "| {'that s the': 1L, 'the po... | [-0.0122354272753, 0.03035... |\n",
      "| {'take on sec': 1L, 'at nu... | [0.0318537950516, -0.02020... |\n",
      "| {'then on the': 1L, 'monda... | [0.0151390172541, 0.009281... |\n",
      "| {'nation freebooth awar': ... | [0.0803309679031, -0.01681... |\n",
      "| {'spent the last': 1L, 'co... | [0.0188916195184, 0.083106... |\n",
      "| {'fyi wife ha': 1L, 'reill... | [0.0356762185693, -0.03896... |\n",
      "| {'of num yr': 1L, 'portsmo... | [-0.0524637438357, -0.0330... |\n",
      "| {'look forward to': 1L, 'n... | [0.0891828984022, 0.007399... |\n",
      "| {'plea kenda releas': 1L, ... | [0.0383098572493, -0.01056... |\n",
      "| {'date saturday or': 1L, '... | [-0.00864183716476, 0.0185... |\n",
      "| {'is uva just': 1L, 'for t... | [0.0162781476974, 0.015872... |\n",
      "| {'and po movi': 1L, 'at sb... | [0.0300624798983, -0.00657... |\n",
      "| {'ll tell everyth': 1L, 't... | [0.0848544165492, -0.00256... |\n",
      "| {'you ll never': 1L, 'time... | [0.0698731839657, -0.00240... |\n",
      "| {'move to california': 1L,... | [-0.0694402530789, -0.1394... |\n",
      "| {'the border with': 1L, 'e... | [0.0768307596445, 0.004979... |\n",
      "| {'bosqu believ sunday': 1L... | [0.0436252020299, -0.00476... |\n",
      "| {'be ne of': 1L, 'our twee... | [0.0191727280617, 0.011575... |\n",
      "| {'bree talk about': 1L, 'v... | [-0.0385540388525, -0.0601... |\n",
      "| {'live on espn': 1L, 'meet... | [-0.0274042487144, -0.0575... |\n",
      "| {'giggl while make': 1L, '... | [0.0128615451977, -0.04625... |\n",
      "| {'tuesday and i': 1L, 'the... | [0.03165910393, 0.01971491... |\n",
      "| {'free but you': 1L, 'but ... | [0.0177145581692, -0.02967... |\n",
      "| {'site doctor or': 1L, 'ch... | [0.0481381267309, -0.05001... |\n",
      "| {'buke togeth gether': 1L,... | [0.0451404862106, 0.029886... |\n",
      "| {'hotel to travel': 1L, 'g... | [0.0319725424051, -0.04327... |\n",
      "| {'travel to fulham': 1L, '... | [0.0292835962027, -0.01342... |\n",
      "| {'the onli candidate': 1L,... | [-0.00484411651269, 0.0053... |\n",
      "| {'for newcastl utd': 1L, '... | [0.0054062041454, -0.00183... |\n",
      "| {'the hawk game': 1L, 'ne ... | [-0.0448861531913, 0.01208... |\n",
      "| {'all thi sat': 1L, 'user ... | [-0.00886666588485, -0.034... |\n",
      "| {'at user drake': 1L, 'me ... | [0.0391316562891, -0.00657... |\n",
      "| {'laugh must never': 1L, '... | [0.0215105488896, 0.018501... |\n",
      "| {'get my camaro': 1L, 'a t... | [0.0315345562994, 0.040139... |\n",
      "| {'the numth time': 1L, 'be... | [-0.0411055013537, 0.01547... |\n",
      "| {'for throwback thursday':... | [0.0337440259755, 0.043424... |\n",
      "| {'game num pt': 1L, 'domin... | [-0.0922802239656, 0.07174... |\n",
      "| {'behind motor boatin': 1L... | [-0.00188948493451, -0.045... |\n",
      "| {'check the go': 1L, 'gari... | [0.0256257280707, -0.01504... |\n",
      "| {'ally im so': 1L, 'monday... | [-0.00498207565397, -0.006... |\n",
      "| {'po against the': 1L, 'be... | [-0.0310820024461, 0.06882... |\n",
      "| {'is sat next': 1L, 'tomor... | [-0.019148638472, -0.00883... |\n",
      "| {'jet get big': 1L, 'on nu... | [0.0882855504751, -0.02957... |\n",
      "| {'invad nlc for': 1L, 'n p... | [0.00823994912207, -0.0879... |\n",
      "| {'tide victory brow': 1L, ... | [-0.0229009948671, -0.0302... |\n",
      "| {'must po ne': 1L, 'po ne ... | [-0.0289918594062, 0.05449... |\n",
      "| {'friday and saturday': 1L... | [-0.017585510388, 0.013981... |\n",
      "| {'for the manu': 1L, 'oh d... | [-0.0239867698401, -0.0200... |\n",
      "| {'the tv sinc': 1L, 'haven... | [0.0125287780538, -0.07829... |\n",
      "| {'talk if you': 1L, 'cell ... | [0.035821530968, 0.0282444... |\n",
      "| {'becaus it s': 1L, 'it s ... | [0.0446612350643, 0.000152... |\n",
      "| {'call u superstitiou': 1L... | [0.0131776966155, -0.00296... |\n",
      "| {'ago they probabl': 1L, '... | [0.0145734902471, 0.046327... |\n",
      "| {'at user my': 1L, 'numnd ... | [-0.0271731391549, -0.0165... |\n",
      "| {'po against the': 1L, 'ag... | [0.014087067917, 0.0258439... |\n",
      "| {'to black light': 1L, 'ca... | [0.0360293574631, -0.01959... |\n",
      "| {'and with the': 1L, 'pat ... | [0.00537290563807, -0.0062... |\n",
      "| {'the globe url': 1L, 'int... | [0.132702708244, -0.037684... |\n",
      "| {'may never know': 1L, 'bo... | [0.117839209735, -0.048017... |\n",
      "| {'liam louis zayn': 1L, 'l... | [0.0664865300059, 0.050152... |\n",
      "| {'in a while': 1L, 'amp in... | [-0.000252975150943, 0.024... |\n",
      "| {'never end po': 1L, 'the ... | [0.0580370351672, -0.01980... |\n",
      "| {'against a small': 1L, 'w... | [-0.0154370935634, -0.0363... |\n",
      "| {'sunday is the': 1L, 'jk ... | [-0.0131494179368, -0.0105... |\n",
      "| {'po it feel': 1L, 'grade ... | [0.00391274923459, -0.0870... |\n",
      "| {'classic on thursda': 1L,... | [0.0122158406302, -0.07669... |\n",
      "| {'everyon let s': 1L, 'shi... | [-0.00288423313759, 0.0189... |\n",
      "| {'ard po say': 1L, 'no mor... | [0.0659370869398, -0.01792... |\n",
      "| {'pat amp pipa': 1L, 'num ... | [-0.0220106672496, -0.0009... |\n",
      "| {'don t tweet': 1L, 'to wi... | [-0.00812317617238, 0.0082... |\n",
      "| {'have been santorum': 1L,... | [0.0115249669179, 0.042482... |\n",
      "| {'titl game ha': 1L, 'game... | [-0.00583848357201, -0.024... |\n",
      "| {'to finish red': 1L, 'tom... | [0.00564902927727, 0.05806... |\n",
      "| {'a pre game': 1L, 'got ti... | [0.00810305122286, 0.01150... |\n",
      "| {'hand in the': 1L, 'rz to... | [-0.00975828897208, -0.086... |\n",
      "| {'my numnd goal': 1L, 'the... | [-0.0224842783064, 0.07201... |\n",
      "| {'red tail for': 1L, 'nw r... | [0.0170275699347, 0.006752... |\n",
      "| {'the horizon be': 1L, 'a ... | [0.034409057349, -0.006047... |\n",
      "| {'realli wanna see': 1L, '... | [-0.0387758277357, -0.0474... |\n",
      "| {'will get beat': 1L, 'beg... | [0.0173534825444, -0.01201... |\n",
      "| {'and the bumblebees': 1L,... | [-0.0684582665563, 0.05365... |\n",
      "| {'v bama thi': 1L, 'bama t... | [-0.00741318194196, 0.0147... |\n",
      "| {'except themselves lot': ... | [0.0400149561465, -0.00977... |\n",
      "| {'time is it': 1L, 'promo ... | [0.0314219035208, -0.02388... |\n",
      "| {'get ne make': 1L, 'tomor... | [0.040377844125, -0.003864... |\n",
      "| {'num tie w': 1L, 'afc off... | [-0.0767803490162, 0.03670... |\n",
      "| {'be tell my': 1L, 'to loo... | [0.0793828815222, 0.074055... |\n",
      "| {'numth quarter geaux': 1L... | [0.0139635084197, -0.02312... |\n",
      "| {'jason is my': 1L, 'is my... | [0.0187048707157, -0.01898... |\n",
      "| {'with drew befor': 1L, 'd... | [0.0122677292675, 0.003664... |\n",
      "| {'hope i the': 1L, 'bieber... | [0.071470528841, 0.0300523... |\n",
      "| {'a peyton man': 1L, 'be a... | [-0.0766346678138, -0.0063... |\n",
      "| {'dsb amp hib': 1L, 'high ... | [0.05067801103, -0.0570162... |\n",
      "| {'galleria in houston': 1L... | [0.0137502076104, 0.015209... |\n",
      "| {'hye will have': 1L, 'par... | [0.10388314724, 0.00077290... |\n",
      "| {'were go to': 1L, 'at use... | [0.0150212319568, -0.00448... |\n",
      "| {'will prove himself': 1L,... | [0.0105703948066, -0.02547... |\n",
      "| {'a well mani': 1L, 'i ve ... | [0.0770917013288, -0.02259... |\n",
      "| {'honey badger video': 1L,... | [0.0508342571557, -0.04368... |\n",
      "| {'user and mayb': 1L, 'but... | [0.0265611223876, -0.06406... |\n",
      "| {'go to be': 1L, 'ne i go'... | [0.0579716637731, -0.00220... |\n",
      "| {'quarter westbrook honey'... | [0.00805219542235, 0.04263... |\n",
      "| {'num num tomorrow': 1L, '... | [-0.000356156378984, 0.011... |\n",
      "| {'with a new': 1L, 'i just... | [-0.039566911757, -0.09674... |\n",
      "| {'pictur with chri': 1L, '... | [0.054038003087, -0.003859... |\n",
      "| {'happen to the': 1L, 'in ... | [0.0432291664183, 0.003579... |\n",
      "| {'may be wear': 1L, 'but l... | [0.045157853514, -0.063220... |\n",
      "| {'richardson did more': 1L... | [0.0235125496984, 0.032717... |\n",
      "| {'here thi weekend': 1L, '... | [-0.0258783958852, 0.00606... |\n",
      "| {'h open up': 1L, 'wa gr n... | [0.0241418741643, -0.01285... |\n",
      "| {'tucson on the': 1L, 'in ... | [0.0217317249626, -0.08739... |\n",
      "| {'num on saturday': 1L, 'n... | [0.0496718920767, -0.03033... |\n",
      "| {'fuckin birthday to': 1L,... | [0.00288391858339, -0.0450... |\n",
      "| {'didn t march': 1L, 'hi n... | [-0.0215065907687, 0.01460... |\n",
      "| {'wait to go': 1L, 'meet t... | [-0.043868906796, -0.02902... |\n",
      "| {'of labor on': 1L, 'num n... | [0.0402328073978, 0.001395... |\n",
      "| {'take place in': 1L, 'for... | [0.0322786122561, -0.08564... |\n",
      "| {'in bel air': 1L, 'on tue... | [0.0151798613369, 0.000331... |\n",
      "| {'have a po': 1L, 'underwo... | [-0.0452613122761, 0.04792... |\n",
      "| {'sec team have': 1L, 'in ... | [0.0298416502774, -0.03679... |\n",
      "| {'in january are': 1L, 're... | [0.0936587229371, -0.01915... |\n",
      "| {'hoodi from at': 1L, 'ali... | [0.0282582044601, 0.004709... |\n",
      "| {'mugshot look po': 1L, 'l... | [0.0310805626214, -0.05154... |\n",
      "| {'in dc xfactor': 1L, 'd c... | [0.026030426845, 0.0140899... |\n",
      "| {'a ne sexual': 1L, 'user ... | [0.0723633393645, -0.03607... |\n",
      "| {'tomorrow so bring': 1L, ... | [0.0767621174455, -0.04798... |\n",
      "| {'in swansea thursday': 1L... | [-0.0609828159213, 0.01838... |\n",
      "| {'for detail at': 1L, 'fb ... | [0.0723531246185, -0.08734... |\n",
      "| {'last rehears day': 1L, '... | [0.00192815170158, 0.02284... |\n",
      "| {'ben ha said': 1L, 'at us... | [-0.0276287011802, -0.0078... |\n",
      "| {'greek poet odyssea': 1L,... | [-0.0134771065786, -0.0015... |\n",
      "| {'to baylor on': 1L, 'bayl... | [-0.120270684361, 0.114763... |\n",
      "| {'florida need to': 1L, 'l... | [0.0833832919598, 0.025710... |\n",
      "| {'my brother would': 1L, '... | [-0.0885828286409, -0.0082... |\n",
      "| {'but the onli': 1L, 'is f... | [-0.00680745299906, -0.048... |\n",
      "| {'is gunna po': 1L, 'still... | [0.0397998839617, 0.026529... |\n",
      "| {'mlk jr po': 1L, 'powel g... | [-0.0294145774096, 0.07140... |\n",
      "| {'the same a': 1L, 'an po ... | [-0.0268230196089, 0.02341... |\n",
      "| {'num years iowa': 1L, 'fr... | [-0.0389905869961, 0.02192... |\n",
      "| {'num for num': 1L, 'trent... | [0.00792374834418, -0.0808... |\n",
      "| {'to make landfal': 1L, 't... | [0.057122644037, -0.016255... |\n",
      "| {'you here in': 1L, 'are i... | [0.0344902537763, -0.00701... |\n",
      "| {'father march with': 1L, ... | [0.012043167837, -0.052627... |\n",
      "| {'po start call': 1L, 'may... | [0.0128927119076, -0.04064... |\n",
      "| {'look po im': 1L, 'watch ... | [-0.00543091166764, 0.0447... |\n",
      "| {'new under the': 1L, 'po ... | [0.0204155538231, 0.026928... |\n",
      "| {'death valley saturday': ... | [0.0160526242107, -0.03627... |\n",
      "| {'the contraband tomorrow'... | [-0.00435252487659, 0.0134... |\n",
      "| {'should go to': 1L, 'the ... | [-0.0246763769537, 0.01259... |\n",
      "| {'pat fan yet': 1L, 'after... | [0.00351426750422, -0.0097... |\n",
      "| {'mouth ne tomorrow': 1L, ... | [0.0470353923738, 0.007523... |\n",
      "| {'heart and more': 1L, 'wi... | [0.0571588389575, 0.040529... |\n",
      "| {'on tv saturday': 1L, 'he... | [0.0133512532339, -0.00508... |\n",
      "| {'jet in po': 1L, 'victori... | [0.0489591546357, -0.02926... |\n",
      "| {'s ing just': 1L, 'num k ... | [-0.0030686785467, -0.0420... |\n",
      "| {'pray wednesday can': 1L,... | [-0.0123091274872, 0.07775... |\n",
      "| {'please your share': 1L, ... | [0.0686594247818, -0.05154... |\n",
      "| {'should they po': 1L, 're... | [0.0226140394807, -0.02083... |\n",
      "| {'at user im': 1L, 'the al... | [0.0602166540921, -0.09502... |\n",
      "| {'is on loan': 1L, 'loan t... | [-0.00994303356856, -0.018... |\n",
      "| {'you are ne': 1L, 'know y... | [0.051210783422, -0.072220... |\n",
      "| {'in it like': 1L, 'tomoro... | [0.0917818024755, -0.04106... |\n",
      "| {'be human lol': 1L, 'lol ... | [-0.0363744087517, 0.01102... |\n",
      "| {'and if you': 1L, 'for so... | [0.0354150906205, -0.01528... |\n",
      "| {'amp tyl sinc': 1L, 'good... | [0.0177493859082, -0.02382... |\n",
      "| {'o o tomorrow': 1L, 'texa... | [-0.028716398403, 0.033806... |\n",
      "| {'in my mind': 1L, 'from m... | [0.0716234669089, 0.000743... |\n",
      "| {'view po window': 1L, 'mo... | [0.0631202831864, -0.05714... |\n",
      "| {'nov numth nba': 1L, 'nba... | [-0.0211018081754, 0.00798... |\n",
      "| {'the huntsman director': ... | [0.0435746982694, -0.02653... |\n",
      "| {'to get realli': 1L, 'sti... | [-0.0226903520525, 0.05554... |\n",
      "| {'ne wa similar': 1L, 'row... | [0.0387497097254, 0.029007... |\n",
      "| {'honey badger daddi': 1L,... | [-0.00355787482113, 0.0172... |\n",
      "| {'ne of depth': 1L, 'of de... | [0.111128330231, -0.010124... |\n",
      "| {'are off wednesday': 1L, ... | [-0.0647702589631, 0.02506... |\n",
      "| {'queen num num': 1L, 'pla... | [-0.0656780451536, 0.03962... |\n",
      "| {'tomorrow school s': 1L, ... | [0.055933393538, 0.0300230... |\n",
      "| {'face fit fight': 1L, 'tr... | [0.0174573436379, -0.06121... |\n",
      "| {'m bring out': 1L, 'at us... | [0.0724853798747, -0.00572... |\n",
      "| {'num and yes': 1L, 'po fo... | [0.00960926245898, 0.01918... |\n",
      "| {'island tomorrow url': 1L... | [0.049847651273, -0.033596... |\n",
      "| {'at user out': 1L, 'numrd... | [0.0210531596094, -0.05074... |\n",
      "| {'hib plate spot': 1L, 'pl... | [0.0727701336145, -0.08995... |\n",
      "| {'till we see': 1L, 'we se... | [-0.0283048283309, 0.02893... |\n",
      "| {'num about a': 1L, 'game ... | [-0.0531537309289, -0.0364... |\n",
      "| {'the offic with': 1L, 'bl... | [-0.0263053346425, -0.0873... |\n",
      "| {'rest in peace': 1L, 'som... | [0.143462523818, -0.094480... |\n",
      "| {'catwoman may be': 1L, 'e... | [0.0496831089258, -0.04668... |\n",
      "| {'parad on new': 1L, 'marc... | [0.0147467441857, -0.06643... |\n",
      "| {'onkyo servic centr': 1L,... | [0.0651605352759, -0.06770... |\n",
      "| {'tried but ipod': 1L, 'at... | [0.0652696788311, -0.01387... |\n",
      "| {'gonna have a': 1L, 'nums... | [0.0518850721419, -0.01507... |\n",
      "| {'b demil award': 1L, 'boa... | [0.0162309296429, -0.01887... |\n",
      "| {'jean that fit': 1L, 'fit... | [-0.00969616789371, -0.000... |\n",
      "| {'i didn t': 1L, 'go to be... | [-0.0578692965209, -0.0577... |\n",
      "| {'at the univers': 1L, 'jr... | [-0.0413758940995, -0.0276... |\n",
      "| {'arsenal push on': 1L, 'u... | [0.0503468587995, 0.016632... |\n",
      "| {'on or side': 1L, 'or sid... | [-0.00150901137386, -0.001... |\n",
      "| {'boys mention numpeopleyo... | [-0.0968244150281, -0.0406... |\n",
      "| {'to school until': 1L, 'o... | [-0.00855581648648, 0.0263... |\n",
      "| {'num ferri friday': 1L, '... | [-0.0491308830678, 0.00706... |\n",
      "| {'rey next halloween': 1L,... | [0.0230231974274, -0.02771... |\n",
      "| {'at user so': 1L, 'philli... | [0.000292390934192, -0.013... |\n",
      "| {'friday s world': 1L, 'fo... | [0.0765000060201, -0.07668... |\n",
      "| {'po isn t': 1L, 'opposit ... | [0.061686437577, -0.034385... |\n",
      "| {'po dude that': 1L, 'that... | [0.0387947820127, 0.013362... |\n",
      "| {'still feel ne': 1L, 'ne ... | [-0.0201536174864, 0.01778... |\n",
      "| {'second half of': 1L, 'nu... | [0.0187565237284, 0.027272... |\n",
      "| {'doe not po': 1L, 'knight... | [-0.0379067771137, 0.02603... |\n",
      "| {'numst game of': 1L, 'gar... | [0.0176424831152, 0.052080... |\n",
      "| {'i m addicted': 1L, 'addi... | [0.0629332736135, -0.01275... |\n",
      "| {'beat hajduk split': 1L, ... | [-0.00242837611586, -0.032... |\n",
      "| {'play tomorrow i': 1L, 'i... | [-0.0615201741457, -0.0779... |\n",
      "| {'go to georgia': 1L, 'the... | [0.0498524457216, 0.037599... |\n",
      "| {'costum i m': 1L, 'm the ... | [0.12665399909, -0.0312359... |\n",
      "| {'and make sc': 1L, 'sperr... | [0.0212752111256, -0.10345... |\n",
      "| {'can t wait': 1L, 'beat t... | [-0.0342456325889, -0.0249... |\n",
      "| {'with new num': 1L, 'nbc ... | [-0.039015032351, -0.04654... |\n",
      "| {'he cri for': 1L, 'cri fo... | [0.0116506256163, 0.004833... |\n",
      "| {'than today almost': 1L, ... | [0.0204211995006, 0.047622... |\n",
      "| {'tomorrow latest we': 1L,... | [0.0873444229364, -0.09211... |\n",
      "| {'j green becaus': 1L, 'do... | [0.0013672033092, 0.020328... |\n",
      "| {'new autumn wrap': 1L, 'd... | [0.0377911254764, -0.05651... |\n",
      "| {'a le day': 1L, 'endur th... | [-0.0682317763567, 0.03762... |\n",
      "| {'not be abl': 1L, 'to get... | [-0.024125020951, 0.072722... |\n",
      "| {'i want to': 1L, 'to go t... | [0.0718849301338, 0.030547... |\n",
      "| {'user that s': 1L, 'the e... | [0.0149157959968, -0.00382... |\n",
      "| {'to liverpool shop': 1L, ... | [0.0333755798638, -0.01683... |\n",
      "| {'the numrd test': 1L, 'by... | [0.0123246880248, 0.022052... |\n",
      "| {'week wednesday url': 1L,... | [0.0810041055083, 0.013122... |\n",
      "| {'le time i': 1L, 'night f... | [0.00389970326796, 0.01988... |\n",
      "| {'end of dec': 1L, 'of our... | [-0.0107354465872, -0.0150... |\n",
      "| {'forev alon day': 1L, 'si... | [-0.028121938929, -0.03262... |\n",
      "| {'team in dah': 1L, 'at us... | [0.0488359071314, -0.04209... |\n",
      "| {'tech await on': 1L, 'ku ... | [-0.0415146313608, -0.0077... |\n",
      "| {'of daniel radcliff': 1L,... | [0.0442028827965, -0.13370... |\n",
      "| {'the s on': 1L, 'sunday t... | [0.0144630037248, 0.048233... |\n",
      "| {'tucson az feb': 1L, 'to ... | [0.0399282835424, -0.00737... |\n",
      "| {'you ve never': 1L, 'reco... | [0.0518497601151, -0.00299... |\n",
      "| {'at user after': 1L, 'ora... | [0.0842680260539, -0.01923... |\n",
      "| {'vernon davi is': 1L, 'th... | [-0.0160114187747, 0.01630... |\n",
      "| {'paranorm activ ye': 1L, ... | [0.0282497815788, 0.019197... |\n",
      "| {'you ratchet hoes': 1L, '... | [-0.00554139213637, -0.047... |\n",
      "| {'m po lb': 1L, 'the shit ... | [0.0459142066538, 0.036328... |\n",
      "| {'the most pow': 1L, 'time... | [-0.0292799398303, 0.02252... |\n",
      "| {'of stupid ugh': 1L, 'wro... | [0.038886629045, 0.0074340... |\n",
      "| {'weath back the': 1L, 'mo... | [0.021535763517, 0.0686903... |\n",
      "| {'guy on march': 1L, 'seco... | [-0.0552091486752, 0.01164... |\n",
      "| {'go shop for': 1L, 'park ... | [0.0446640886366, 0.027829... |\n",
      "| {'at user share': 1L, 'it ... | [0.0156624298543, 0.010605... |\n",
      "| {'numth seed they': 1L, 't... | [0.0256506633013, -0.01474... |\n",
      "| {'requir and such': 1L, 'm... | [0.0145338745788, 0.053060... |\n",
      "| {'numth spot in': 1L, 'the... | [0.0530088841915, 0.011262... |\n",
      "| {'at user wrong': 1L, 'gol... | [-0.00191221572459, 0.0143... |\n",
      "| {'skype but averi': 1L, 'b... | [0.0281033851206, 0.012113... |\n",
      "| {'jobs happi it': 1L, 'po ... | [-0.0696878060699, -0.0043... |\n",
      "| {'i m sure': 1L, 'my need ... | [0.0455313138664, 0.009243... |\n",
      "| {'player of the': 1L, 'wee... | [0.00380876800045, 0.02303... |\n",
      "| {'live in the': 1L, 'the r... | [0.0139323528856, -0.08720... |\n",
      "| {'ne the concord': 1L, 'i ... | [0.077761054039, 0.0311828... |\n",
      "| {'against arsenal look': 1... | [0.0372271873057, -0.04384... |\n",
      "| {'bowl if we': 1L, 't in a... | [0.10801140219, 0.02069251... |\n",
      "| {'the moment go': 1L, 'mom... | [0.0312411803752, -0.01198... |\n",
      "| {'you re jerry': 1L, 'you ... | [0.127715110779, -0.004531... |\n",
      "| {'to go to': 1L, 'to the p... | [0.0149100702256, -0.01459... |\n",
      "| {'heat is gunna': 1L, 'gun... | [0.0213019978255, 0.037340... |\n",
      "| {'night might a': 1L, 'tou... | [-0.0109169790521, 0.00884... |\n",
      "| {'library on yelp': 1L, 'o... | [-0.0486875064671, 0.02353... |\n",
      "| {'talk about niner': 1L, '... | [-0.0637579858303, 0.12900... |\n",
      "| {'the week we': 1L, 'frida... | [-0.00209711748175, -0.006... |\n",
      "| {'devil insid which': 1L, ... | [0.0306309312582, 0.017227... |\n",
      "| {'solo po to': 1L, 'po on ... | [-0.0181053820997, 0.07275... |\n",
      "| {'support the packer': 1L,... | [0.00596493016928, -0.0041... |\n",
      "| {'turn the po': 1L, 'po wa... | [0.0206177849323, 0.016147... |\n",
      "| {'persi po po': 1L, 'think... | [-0.0155514990911, -0.0195... |\n",
      "| {'numst num cristiano': 1L... | [-0.0621949285269, -0.0364... |\n",
      "| {'wa next level': 1L, 'hal... | [-0.0547307170928, 0.02351... |\n",
      "| {'bcs wednesday continu': ... | [0.0470101758838, 0.025844... |\n",
      "| {'demand on nov': 1L, 'amp... | [-0.0224569942802, -0.0172... |\n",
      "| {'mickelson remain second'... | [-0.0381000004709, 0.00958... |\n",
      "| {'is it januari': 1L, 'it ... | [0.0183759778738, 0.018414... |\n",
      "| {'tomorrow mon and': 1L, '... | [0.00451626395807, -0.0599... |\n",
      "| {'po on saturday': 1L, 'we... | [0.0217790491879, 0.001678... |\n",
      "| {'s last stop': 1L, 'the n... | [-0.0845868065953, 0.01279... |\n",
      "| {'tuesday s wizard': 1L, '... | [0.0228046551347, -0.04950... |\n",
      "| {'regist for the': 1L, 'at... | [0.0470075756311, -0.11924... |\n",
      "| {'so po of': 1L, 'numth us... | [-0.0376181192696, 0.00433... |\n",
      "| {'the numth season': 1L, '... | [0.125487864017, -0.009185... |\n",
      "| {'num to play': 1L, 'nummo... | [0.0241310726851, 0.033380... |\n",
      "| {'spoke to a': 1L, 'a shei... | [-0.0326953195035, -0.0118... |\n",
      "| {'saturday night we': 1L, ... | [-0.0285950992256, -0.0339... |\n",
      "| {'those florida boys': 1L,... | [0.0240455642343, 0.035220... |\n",
      "| {'user plea justin': 1L, '... | [0.0133730657399, -0.01817... |\n",
      "| {'pot wanna make': 1L, 'fo... | [0.0733739435673, -0.01178... |\n",
      "| {'is num num': 1L, 'numst ... | [0.0227225087583, 0.017611... |\n",
      "| {'nba on thursday': 1L, 't... | [-0.0259789619595, -0.0050... |\n",
      "| {'ne bread with': 1L, 'my ... | [-0.0117409443483, -0.0031... |\n",
      "| {'basketbal game tonight':... | [0.0188469532877, -0.03997... |\n",
      "| {'for the pat': 1L, 'envir... | [0.052514180541, -0.047517... |\n",
      "| {'raw hit newcastle': 1L, ... | [0.052729703486, 0.0176838... |\n",
      "| {'i agre but': 1L, 'andi s... | [0.0368396453559, -0.01906... |\n",
      "| {'rey on the': 1L, 'lana d... | [-0.000677299511153, -0.01... |\n",
      "| {'b cuz i': 1L, 'a game sa... | [0.0231785904616, -0.02474... |\n",
      "| {'rainy ne day': 1L, 'is g... | [0.0365517325699, -0.03989... |\n",
      "| {'around with raylan': 1L,... | [0.0296657364815, -0.03239... |\n",
      "| {'in london so': 1L, 'much... | [0.0712060108781, 0.003456... |\n",
      "| {'ur skype numst': 1L, 'us... | [0.0303400475532, 0.012894... |\n",
      "| {'s or scandal': 1L, 'no n... | [0.0164209920913, -0.00676... |\n",
      "| {'doe sinc it': 1L, 'kiddo... | [0.031247716397, 0.0135598... |\n",
      "| {'po concert saturday': 1L... | [-0.0371869243681, 0.05010... |\n",
      "| {'morning thank you': 1L, ... | [0.0365378335118, -0.02065... |\n",
      "| {'at user num': 1L, 'acous... | [0.0656498000026, -0.03600... |\n",
      "| {'let the debat': 1L, 'the... | [0.0430422089994, -0.02664... |\n",
      "| {'num for num': 1L, 'for n... | [-0.0289345663041, 0.00777... |\n",
      "| {'the ne in': 1L, 'jan for... | [0.04691343382, 0.00787563... |\n",
      "| {'trunk s by': 1L, 'po coc... | [0.00732724787667, -0.0362... |\n",
      "| {'you have at': 1L, 'at us... | [0.0458170697093, -0.05931... |\n",
      "| {'plant my appl': 1L, 'eve... | [-0.0420219451189, 0.02841... |\n",
      "| {'and talk shit': 1L, 'tom... | [-0.00572494696826, 0.0160... |\n",
      "| {'the el classico': 1L, 'a... | [0.0660100877285, 0.037712... |\n",
      "| {'at user ios': 1L, 'ios n... | [0.0516544878483, -0.03281... |\n",
      "| {'ard webb will': 1L, 'wes... | [-0.0171521641314, -0.0540... |\n",
      "| {'re go to': 1L, 'we re go... | [-0.0233588609844, -0.0080... |\n",
      "| {'pic with him': 1L, 'take... | [0.0432431064546, -0.01287... |\n",
      "| {'disco ball shine': 1L, '... | [0.0861573219299, -0.02042... |\n",
      "| {'see plymouth is': 1L, 'm... | [-0.0531627163291, -0.0527... |\n",
      "| {'arsen fc v': 1L, 'fc v m... | [0.0327957831323, -0.05116... |\n",
      "| {'side look forward': 1L, ... | [0.0103194443509, 0.010686... |\n",
      "| {'deserv to po': 1L, 'half... | [0.00450447155163, 0.06643... |\n",
      "| {'mark the start': 1L, 'to... | [-0.0354491099715, 0.00729... |\n",
      "| {'the onli person': 1L, 'm... | [-0.0511096455157, 0.00668... |\n",
      "| {'then go to': 1L, 'sam af... | [0.0297063495964, 0.014425... |\n",
      "| {'of luck both': 1L, 'same... | [0.0252691991627, 0.028535... |\n",
      "| {'podcast tomorrow url': 1... | [-0.000972245412413, -0.02... |\n",
      "| {'me of the': 1L, 'of numn... | [0.035261772573, 0.0018798... |\n",
      "| {'you re po': 1L, 'po our ... | [0.0459276102483, -0.00468... |\n",
      "| {'s with me': 1L, 'or num ... | [0.0100359926, 0.018958311... |\n",
      "| {'happi birthday i': 1L, '... | [-0.0037844395265, -0.1167... |\n",
      "| {'january mayb we': 1L, 'g... | [0.057487949729, -0.063819... |\n",
      "| {'to get out': 1L, 'go ply... | [-0.00966497976333, -0.015... |\n",
      "| {'with some po': 1L, 'at n... | [-0.0210050102323, -0.0217... |\n",
      "| {'i didn t': 1L, 'the numt... | [-0.0439737848938, -0.0177... |\n",
      "| {'an ad for': 1L, 'for the... | [0.0507921166718, -0.11490... |\n",
      "| {'andi wa hide': 1L, 'wa h... | [0.0547357797623, 0.002163... |\n",
      "| {'numst i wnt': 1L, 'him f... | [0.0456733368337, -0.01216... |\n",
      "| {'fa cup first': 1L, 'roun... | [0.0363078340888, -0.01285... |\n",
      "| {'nw s po': 1L, 'him a lis... | [0.0112345144153, -0.01651... |\n",
      "| {'continu to bless': 1L, '... | [-0.0504749640822, -0.0863... |\n",
      "| {'for a guy': 1L, 'gurl fi... | [0.0406601242721, 0.055178... |\n",
      "| {'all kind gather': 1L, 'b... | [0.050387263298, 0.0166867... |\n",
      "| {'the wiki with': 1L, 'que... | [0.0495641455054, -0.01772... |\n",
      "| {'po tv poll': 1L, 'poll s... | [-0.0213466957211, -0.0474... |\n",
      "| {'america she could': 1L, ... | [0.00538467708975, -0.0060... |\n",
      "| {'black v po': 1L, 'a mizz... | [0.00964589510113, -0.0169... |\n",
      "| {'and tom to': 1L, 'on sat... | [0.0299123432487, -0.02449... |\n",
      "| {'trent richardson rush': ... | [-0.0275418814272, -0.0285... |\n",
      "| {'po the wvu': 1L, 'that g... | [-0.0690274611115, 0.07777... |\n",
      "| {'inch up on': 1L, 's cred... | [0.122595258057, -0.026637... |\n",
      "| {'kstate can probabl': 1L,... | [0.0270021799952, -0.00925... |\n",
      "| {'odd that man': 1L, 'are ... | [0.022560114041, -0.007439... |\n",
      "| {'view po window': 1L, 'mo... | [0.0529437065125, -0.04995... |\n",
      "| {'of the redskins': 1L, 'n... | [0.0148801933974, 0.018815... |\n",
      "| {'malik thank in': 1L, 'ha... | [0.00120162218809, -0.0199... |\n",
      "| {'on num octob': 1L, 'num ... | [0.00952836871147, -0.0106... |\n",
      "| {'samhain ritual journey':... | [-0.0369671098888, -0.0201... |\n",
      "| {'he qual for': 1L, 'it be... | [0.0178759861737, -0.02278... |\n",
      "| {'of sunday lfc': 1L, 'ahe... | [0.0248649530113, 0.015696... |\n",
      "| {'is thi come': 1L, 'from ... | [-0.0250169504434, -0.0147... |\n",
      "| {'user head to': 1L, 'po e... | [-0.0618466027081, -0.0303... |\n",
      "| {'on tv saturday': 1L, 'he... | [0.0133512532339, -0.00508... |\n",
      "| {'polic hi consent': 1L, '... | [0.0262633487582, -0.06150... |\n",
      "| {'user haha i': 1L, 'hifz ... | [0.0576291717589, -0.03603... |\n",
      "| {'you po it': 1L, 'and if ... | [-0.0525427013636, 0.01518... |\n",
      "| {'face fit fight': 1L, 'ba... | [0.0201019868255, -0.06529... |\n",
      "| {'hib relat suspensions': ... | [0.0156773012131, -0.05614... |\n",
      "| {'there s fluiditi': 1L, '... | [0.00164253276307, -0.0190... |\n",
      "| {'about mafioso s': 1L, 'm... | [-0.031700681895, 0.016169... |\n",
      "| {'po lcfc at': 1L, 'watch ... | [-0.00933792255819, -0.018... |\n",
      "| {'franklin and etta': 1L, ... | [0.00681425817311, -0.0100... |\n",
      "| {'from the lake': 1L, 'lak... | [-0.0339463502169, -0.0342... |\n",
      "| {'of mob wives': 1L, 'sat ... | [0.0140760103241, -0.03349... |\n",
      "| {'provid see you': 1L, 'ch... | [0.0497563295066, -0.00799... |\n",
      "| {'to florida tomorrow': 1L... | [-0.0646554753184, -0.0684... |\n",
      "| {'i will see': 1L, 'justin... | [-0.0289297606796, 0.02164... |\n",
      "| {'go to hell': 1L, 'americ... | [0.0148792145774, 0.027012... |\n",
      "| {'dwight ard is': 1L, 'mid... | [0.094284914434, -0.003225... |\n",
      "| {'see in a': 1L, 'bowl in ... | [0.086755335331, -0.031343... |\n",
      "| {'and url welovethai': 1L,... | [0.0651597753167, -0.04095... |\n",
      "| {'is an po': 1L, 'no more ... | [-0.0253670345992, 0.01718... |\n",
      "| {'wa pas in': 1L, 'in the ... | [0.00324543239549, -0.0345... |\n",
      "| {'az sun url': 1L, 'garden... | [0.0326642543077, -0.05002... |\n",
      "| {'may a po': 1L, 'you don ... | [0.0430390313268, -0.06782... |\n",
      "| {'so i can': 1L, 'tell me ... | [0.111838236451, -0.101201... |\n",
      "| {'of white collar': 1L, 'i... | [0.119433000684, -0.028919... |\n",
      "| {'watch new jersey': 1L, '... | [0.028529079631, -0.084760... |\n",
      "| {'the numth of': 1L, 'next... | [-0.0157452765852, 0.07058... |\n",
      "| {'it all up': 1L, 'a bath ... | [0.0460110493004, 0.010756... |\n",
      "| {'tonight ne the': 1L, 's ... | [-0.0251315142959, 0.02034... |\n",
      "| {'may soon be': 1L, 'be cr... | [0.012793767266, -0.009322... |\n",
      "| {'newcastl jet v': 1L, 'me... | [0.10723400116, -0.0619030... |\n",
      "| {'resum sdsu get': 1L, 'sd... | [0.0330410934985, 0.027857... |\n",
      "| {'thursday finish rehears'... | [-0.0208699498326, -0.0254... |\n",
      "| {'to po player': 1L, 'tob ... | [0.0172311048955, 0.017147... |\n",
      "| {'just sat through': 1L, '... | [0.0843633636832, -0.00965... |\n",
      "| {'demba ba is': 1L, 'a cal... | [0.0177034977823, -0.09441... |\n",
      "| {'i got coursework': 1L, '... | [0.0216551050544, -0.01011... |\n",
      "| {'in to monday': 1L, 'wa u... | [0.0102337906137, 0.011259... |\n",
      "| {'to see the': 1L, 'girl e... | [-0.019768288359, -0.06751... |\n",
      "| {'end of communism': 1L, '... | [0.0560190267861, 0.072395... |\n",
      "| {'predict po now': 1L, 'in... | [0.0697853043675, 0.008769... |\n",
      "| {'host the golden': 1L, 'p... | [0.0360878929496, -0.00125... |\n",
      "| {'numst game of': 1L, 'mav... | [9.13387993933e-05, -0.035... |\n",
      "| {'crap eh i': 1L, 'to anfi... | [0.00244192918763, 0.00601... |\n",
      "| {'be gone lt': 1L, 'there ... | [-0.0147707900032, 0.02886... |\n",
      "| {'with david letterman': 1... | [0.00872047524899, -0.0339... |\n",
      "| {'po here i': 1L, 'here i ... | [0.0118767851964, -0.08901... |\n",
      "| {'angri bloke will': 1L, '... | [-0.0156920216978, -0.0080... |\n",
      "| {'join u for': 1L, 'num nu... | [-0.0341839045286, -0.0006... |\n",
      "| {'your num fan': 1L, 'sanc... | [0.0867457091808, 0.009857... |\n",
      "| {'user in falmouth': 1L, '... | [0.0304255858064, 0.028294... |\n",
      "| {'okay but not': 1L, 'gonn... | [0.0326796323061, 0.050689... |\n",
      "| {'against sheff wed': 1L, ... | [-0.0226126071066, -0.0109... |\n",
      "| {'loui thursday night': 1L... | [0.0032614916563, 0.014818... |\n",
      "| {'tyler s num': 1L, 'num s... | [0.0586631670594, -0.00585... |\n",
      "| {'po that notredam': 1L, '... | [-0.0272991564125, 0.03197... |\n",
      "| {'at user aka': 1L, 'po bc... | [0.0621276944876, -0.01557... |\n",
      "| {'she may have': 1L, 'user... | [0.00969180837274, -0.0179... |\n",
      "| {'look forward to': 1L, 'n... | [0.0574964620173, 0.037785... |\n",
      "| {'drunk chelsea happi': 1L... | [-0.0134274670854, -0.0425... |\n",
      "| {'jon huntsman s': 1L, 'en... | [0.0142301134765, -0.02043... |\n",
      "| {'weekend for the': 1L, 'r... | [0.00322968326509, -0.0037... |\n",
      "| {'will see bigbang': 1L, '... | [-0.0250144600868, -0.0435... |\n",
      "| {'until februari now': 1L,... | [-0.00773461861536, 0.0010... |\n",
      "| {'happybirthdaytroianbelli... | [0.0335105471313, -0.02523... |\n",
      "| {'i may or': 1L, 'may or m... | [0.0317551009357, -0.01152... |\n",
      "| {'oh yeah i': 1L, 're po n... | [0.0174111295491, 0.021948... |\n",
      "| {'i will make': 1L, 's mat... | [0.0267712976784, -0.05418... |\n",
      "| {'nba s onlin': 1L, 'your ... | [0.0377540141344, -0.10182... |\n",
      "| {'the el classico': 1L, 'n... | [0.0470163300633, 0.094050... |\n",
      "| {'it s most': 1L, 'get a l... | [-0.0567419826984, 0.06830... |\n",
      "| {'marin will b': 1L, 'will... | [-0.020856840536, -0.03277... |\n",
      "| {'toni romo ha': 1L, 'the ... | [-0.00297495955601, -0.007... |\n",
      "| {'can you confirm': 1L, 'g... | [0.00458439812064, -0.0317... |\n",
      "| {'dsb amp hib': 1L, 'high ... | [0.05067801103, -0.0570162... |\n",
      "| {'num num url': 1L, 'elimi... | [0.0525297150016, -0.05271... |\n",
      "| {'mmmmmmmm i may': 1L, 'ju... | [0.019662104547, -0.039150... |\n",
      "| {'twitter first one': 1L, ... | [-0.044827580452, -0.08256... |\n",
      "| {'the run for': 1L, 's gam... | [-0.0220294035971, 0.01071... |\n",
      "| {'numth grade and': 1L, 'w... | [-0.0146110039204, -0.0579... |\n",
      "| {'game breast cancer': 1L,... | [0.0327909700572, -0.03939... |\n",
      "| {'num of num': 1L, 'shore ... | [-0.00706505402923, -0.039... |\n",
      "| {'you d po': 1L, 'tree lig... | [0.0414407067001, -0.02778... |\n",
      "| {'debat url i': 1L, 'down ... | [0.127297013998, -0.033455... |\n",
      "| {'i may ask': 1L, 'at user... | [0.0535281859338, -0.04554... |\n",
      "| {'is a po': 1L, 'sunday ma... | [0.00157653610222, 0.04178... |\n",
      "| {'the thought of': 1L, 'on... | [-0.00387670123018, -0.037... |\n",
      "| {'amp up brazil': 1L, 'up ... | [0.0286924354732, 0.011915... |\n",
      "| {'toy fair for': 1L, 'didn... | [0.0340344682336, -0.02892... |\n",
      "| {'best to stop': 1L, 'you ... | [0.044626288116, -0.037538... |\n",
      "| {'to watch the': 1L, 'the ... | [0.0694351494312, 0.003369... |\n",
      "| {'talk ryback win': 1L, 'r... | [-0.00320876273327, -0.013... |\n",
      "| {'both episodes tomorrow':... | [0.0288228690624, 0.068135... |\n",
      "| {'report trent richardson'... | [0.0395127460361, -0.06828... |\n",
      "| {'with nba son': 1L, 's ca... | [0.0136450612918, -0.02281... |\n",
      "| {'brother at user': 1L, 'b... | [-0.0519817508757, -0.1289... |\n",
      "| {'po the niners': 1L, 'thi... | [-0.0345779918134, 0.05849... |\n",
      "| {'gig tomorrow do': 1L, 'a... | [-0.00421612896025, -0.065... |\n",
      "| {'school a drew': 1L, 'fol... | [-0.0298042465001, 0.00489... |\n",
      "| {'t ne with': 1L, 'ne with... | [-0.00524310115725, 0.0330... |\n",
      "| {'readi to tast': 1L, 'm o... | [0.0160219464451, -0.01786... |\n",
      "| {'februari num is': 1L, 'f... | [0.0847260206938, -0.06873... |\n",
      "| {'the jaguar may': 1L, 'ge... | [0.00921101216227, -0.0258... |\n",
      "| {'reach implic chicago': 1... | [-0.00299073103815, -0.026... |\n",
      "| {'num midway through': 1L,... | [0.0502984113991, -0.04803... |\n",
      "| {'i guarante it': 1L, 'nit... | [-0.0314290560782, -0.0180... |\n",
      "| {'i wanna watch': 1L, 'cb ... | [0.0270054452121, 0.031057... |\n",
      "| {'lunch links some': 1L, '... | [0.026103887707, 0.0292679... |\n",
      "| {'the devil insid': 1L, 'u... | [0.156130045652, -0.047507... |\n",
      "| {'user plea justin': 1L, '... | [0.0133730657399, -0.01817... |\n",
      "| {'for pretti littl': 1L, '... | [-0.0385121889412, 0.00249... |\n",
      "| {'homeschool s book': 1L, ... | [0.0480050481856, -0.04021... |\n",
      "| {'have a laugh': 1L, 'watc... | [0.0323906056583, 0.028093... |\n",
      "| {'semant discoveri amp': 1... | [-0.0109242536128, -0.0273... |\n",
      "| {'and it also': 1L, 'at us... | [-0.0103496937081, -0.0011... |\n",
      "| {'been num day': 1L, 'it w... | [-0.0467406995595, 0.07591... |\n",
      "| {'sinc numth grade': 1L, '... | [-0.0350287295878, -0.0974... |\n",
      "| {'england will be': 1L, 'c... | [-0.0281046628952, -0.0374... |\n",
      "| {'octob num url': 1L, 'wit... | [0.0373572297394, -0.04631... |\n",
      "| {'and i m': 1L, 'about to ... | [0.157946780324, -0.019168... |\n",
      "| {'damn i forgot': 1L, 'tom... | [-0.0222101267427, 0.05161... |\n",
      "| {'be back with': 1L, 'with... | [0.0820735692978, -0.08093... |\n",
      "| {'with global warm': 1L, '... | [0.0124374851584, 0.009240... |\n",
      "| {'at noon est': 1L, 'noon ... | [0.0313862189651, -0.02204... |\n",
      "| {'question will you': 1L, ... | [0.0178791768849, -0.05971... |\n",
      "| {'nomin selena at': 1L, 'a... | [-0.0082799103111, -0.0114... |\n",
      "| {'perform after snl': 1L, ... | [0.0491009019315, -0.00770... |\n",
      "| {'will get the': 1L, 'i th... | [0.0590099282563, -0.01969... |\n",
      "| {'to ce in': 1L, 'at user ... | [0.00582157075405, -0.0153... |\n",
      "| {'i cannot wait': 1L, 'can... | [-0.0175721868873, -0.0650... |\n",
      "| {'with u at': 1L, 'parti n... | [0.023764565587, 0.0097893... |\n",
      "| {'human at user': 1L, 'wat... | [0.0849827006459, -0.01433... |\n",
      "| {'bowl game and': 1L, 'cha... | [0.0636010617018, 0.013735... |\n",
      "| {'s ne with': 1L, 'lsu s l... | [-0.0679427832365, -0.0257... |\n",
      "| {'dsb amp hib': 1L, 'high ... | [0.0657594352961, -0.04482... |\n",
      "| {'at user yep': 1L, 'down ... | [0.0188491232693, 0.001799... |\n",
      "| {'everton midweek before':... | [-0.00951350852847, 0.0058... |\n",
      "| {'thi saturday with': 1L, ... | [0.00759688345715, 0.07345... |\n",
      "| {'free throw dasu': 1L, 's... | [-0.0321329347789, 0.07314... |\n",
      "| {'paper back page': 1L, 'p... | [0.0760276690125, -0.05351... |\n",
      "| {'on abc family': 1L, 'omg... | [-0.0150553630665, 0.00872... |\n",
      "| {'woohoo thi will': 1L, 'w... | [0.0193144064397, -0.01802... |\n",
      "| {'on mw num': 1L, 'on xbox... | [-0.0184875428677, 0.04793... |\n",
      "| {'ll be busi': 1L, 'go see... | [0.0532655604184, 0.040944... |\n",
      "| {'may someon get': 1L, 'sn... | [0.0418321788311, -0.03604... |\n",
      "| {'is thursday and': 1L, 'a... | [0.046067263931, -0.040573... |\n",
      "| {'wa go to': 1L, 'but at u... | [0.0236801635474, -0.03171... |\n",
      "| {'speech befor the': 1L, '... | [-0.0285877585411, 0.00932... |\n",
      "| {'ne against gays': 1L, 't... | [0.0377673394978, 0.009167... |\n",
      "| {'to nc state': 1L, 'but h... | [0.0569135248661, -0.10238... |\n",
      "| {'will take that': 1L, 'ta... | [0.0153510151431, 0.016498... |\n",
      "| {'ne tonight big': 1L, 'ir... | [0.0270273592323, 0.027152... |\n",
      "| {'you again on': 1L, 'meet... | [-0.00388909783214, 0.0119... |\n",
      "| {'tomorrow po bolton': 1L,... | [0.0338024497032, 0.031641... |\n",
      "| {'about it more': 1L, 'yea... | [0.0372957065701, 0.004392... |\n",
      "| {'playground with aaliyah'... | [-0.00388386100531, -0.023... |\n",
      "| {'hi po kidrauhl': 1L, 'on... | [0.0440154261887, 0.006180... |\n",
      "| {'shape after skip': 1L, '... | [0.046296492219, 0.0361144... |\n",
      "| {'factor po to': 1L, 'the ... | [0.0419874079525, -0.04525... |\n",
      "| {'without gordon could': 1... | [-0.104032680392, 0.006748... |\n",
      "| {'n j boston': 1L, 'feel a... | [0.0167658664286, -0.10044... |\n",
      "| {'the same time': 1L, 'to ... | [0.00547678489238, 0.09469... |\n",
      "| {'he s a': 1L, 'like he ma... | [0.0664916411042, 0.007455... |\n",
      "| {'did cuz he': 1L, 'cuz he... | [0.0341092906892, -0.00429... |\n",
      "| {'go ham in': 1L, 'unaware... | [0.0190876983106, 0.030713... |\n",
      "| {'will be talk': 1L, 'duke... | [-0.0778777748346, -0.0666... |\n",
      "| {'give a damn': 1L, 'hairc... | [0.0853569284081, -0.04443... |\n",
      "| {'i think the': 1L, 'hawk ... | [-0.0321893692017, 0.04494... |\n",
      "| {'just off the': 1L, 'at u... | [-0.0360884070396, -0.0482... |\n",
      "| {'keep get po': 1L, 'po ar... | [0.0110538601875, 0.021476... |\n",
      "| {'wa ne befor': 1L, 'great... | [-0.035572052002, -0.07237... |\n",
      "| {'nun el to': 1L, 'gotti t... | [-0.0374830625951, 0.00763... |\n",
      "| {'at numpm mayb': 1L, 'on ... | [-0.0304091274738, 0.02296... |\n",
      "| {'of friday nught': 1L, 't... | [0.0178429353982, 0.011752... |\n",
      "| {'one final chanc': 1L, 'o... | [0.00796405877918, 0.00204... |\n",
      "| {'gurls teenag dream': 1L,... | [0.00355126080103, -0.0014... |\n",
      "| {'came to my': 1L, 'and he... | [-0.0368518196046, -0.1172... |\n",
      "| {'chanc me and': 1L, 'me a... | [-0.00297757633962, -0.046... |\n",
      "| {'great way to': 1L, 'numt... | [0.0229133684188, -0.01290... |\n",
      "| {'feel po by': 1L, 'po by ... | [0.0381341055036, 0.017907... |\n",
      "| {'new season of': 1L, 'fro... | [0.00960346404463, -0.0472... |\n",
      "| {'would be hi': 1L, 'user ... | [-0.0135447615758, -0.0061... |\n",
      "| {'numth rank team': 1L, 'p... | [0.0481350794435, -0.00664... |\n",
      "| {'go to indiana': 1L, 'po ... | [-0.0913439020514, 0.10538... |\n",
      "| {'numth grade it': 1L, 'to... | [-0.0211272072047, -0.0309... |\n",
      "| {'c mon firsttak': 1L, 'co... | [0.0583525151014, 0.006087... |\n",
      "| {'blanket to keep': 1L, 'p... | [0.0392271801829, 0.036000... |\n",
      "| {'the falcon sunday': 1L, ... | [-0.057008087635, -0.03581... |\n",
      "| {'tail with the': 1L, 'for... | [-0.0341673009098, -0.0657... |\n",
      "| {'chanc i watch': 1L, 'a c... | [0.0135263036937, -0.02401... |\n",
      "| {'not gonna make': 1L, 'ma... | [0.0667944550514, -0.07805... |\n",
      "| {'if you watch': 1L, 'reci... | [-0.0140246869996, 0.01218... |\n",
      "| {'to do maths': 1L, 'one t... | [-0.00483546731994, -0.047... |\n",
      "| {'file tell us': 1L, 'nov ... | [0.0311620291322, -0.05166... |\n",
      "| {'of lat minut': 1L, 'repo... | [-0.062216386199, -0.01629... |\n",
      "| {'user do it': 1L, 'nov nu... | [0.000214791638427, 0.0048... |\n",
      "| {'theodor roosevelt museum... | [0.0449228920043, -0.04102... |\n",
      "| {'to see he': 1L, 'to talk... | [-0.0184007305652, -0.0148... |\n",
      "| {'video of the': 1L, 'shot... | [0.0539292730391, -0.02961... |\n",
      "| {'at user nope': 1L, 'comp... | [0.0261462442577, -0.03620... |\n",
      "| {'father march with': 1L, ... | [0.0115580558777, -0.02204... |\n",
      "| {'can po me': 1L, 'at user... | [-0.0275880396366, -0.0030... |\n",
      "| {'lsu geaux tiger': 1L, 'b... | [-0.0259347278625, 0.01519... |\n",
      "| {'tomorrow night the': 1L,... | [0.0120823942125, -0.02685... |\n",
      "| {'tweet or retweet': 1L, '... | [0.0252226460725, -0.03574... |\n",
      "| {'to sophomor year': 1L, '... | [0.0295295938849, 0.031737... |\n",
      "| {'into sg from': 1L, 'mild... | [-0.00596470525488, 0.0195... |\n",
      "| {'i ve seen': 1L, 'of the ... | [0.0450642071664, 0.026469... |\n",
      "| {'tomorrow becaus andi': 1... | [0.047564599663, -0.041997... |\n",
      "| {'drew bree the': 1L, 'mak... | [0.00736936042085, -0.0753... |\n",
      "| {'of waverli place': 1L, '... | [0.0190906133503, -0.03381... |\n",
      "| {'at user pat': 1L, 'long ... | [0.0456932410598, 0.016269... |\n",
      "| {'not watch today': 1L, 'a... | [0.0591730289161, -0.05140... |\n",
      "| {'think the num': 1L, 'sur... | [0.0717753693461, 0.045188... |\n",
      "| {'wednesday dtake me': 1L,... | [0.0059711560607, -0.02326... |\n",
      "| {'tomorrow sparkingat numm... | [0.0101016573608, 0.016713... |\n",
      "| {'a le day': 1L, 'day of h... | [-0.0590429604053, -0.0216... |\n",
      "| {'pick to finish': 1L, 'sf... | [0.0199595354497, -0.01491... |\n",
      "| {'at nlc just': 1L, 'user ... | [0.00406561372802, -0.0377... |\n",
      "| {'i m ask': 1L, 'watch mis... | [0.0282836444676, 0.014148... |\n",
      "| {'prayer go out': 1L, 'ne ... | [0.014250151813, 0.0758513... |\n",
      "| {'first boom charot': 1L, ... | [-0.00904561020434, -0.098... |\n",
      "| {'a moment to': 1L, 'to le... | [0.0558254122734, -0.03336... |\n",
      "| {'see him till': 1L, 'tucs... | [-0.0789969787002, -0.0094... |\n",
      "| {'rollin ani more': 1L, 't... | [0.00531501229852, -0.0307... |\n",
      "| {'freshman po u': 1L, 'lit... | [-0.0362557582557, -0.0191... |\n",
      "| {'of nba tomorrow': 1L, 'd... | [-0.00674658594653, -0.019... |\n",
      "| {'gb hou game': 1L, 'dure ... | [-0.040082950145, -0.06754... |\n",
      "| {'dvr s amp': 1L, 's at us... | [-0.0287621375173, -0.0612... |\n",
      "| {'the big night': 1L, 'gre... | [-0.0303371176124, -0.0172... |\n",
      "| {'wait for tomorrow': 1L, ... | [-0.0635512545705, -0.0328... |\n",
      "| {'on num octob': 1L, 'num ... | [0.00922370515764, -0.0107... |\n",
      "| {'for saturday can': 1L, '... | [0.0330667234957, -0.07240... |\n",
      "| {'a ne in': 1L, 'ne in the... | [0.0588010624051, -0.01718... |\n",
      "| {'tomorrow nigga luv': 1L,... | [0.0650653988123, 0.029082... |\n",
      "| {'thunder sunday so': 1L, ... | [0.0427242293954, -0.01420... |\n",
      "| {'ne piec of': 1L, 'ever n... | [0.0254136566073, -0.01927... |\n",
      "| {'to be here': 1L, 'po bri... | [-0.0179787687957, 0.02362... |\n",
      "| {'full juice the': 1L, 'ju... | [-0.0121056484058, 0.00206... |\n",
      "| {'have that stuff': 1L, 't... | [-0.0191379580647, -0.0190... |\n",
      "| {'nice one sky': 1L, 'ever... | [0.0351421870291, -0.05268... |\n",
      "| {'quarterback and he': 1L,... | [0.0565578602254, 0.021436... |\n",
      "| {'for a honda': 1L, 'in ke... | [0.00564280897379, 0.00789... |\n",
      "| {'fri ne knell': 1L, '30 r... | [0.0545406080782, -0.03843... |\n",
      "| {'mav on nov': 1L, 'lakers... | [0.0538466572762, 0.049809... |\n",
      "| {'are you listen': 1L, 'op... | [-0.00997594650835, 0.0191... |\n",
      "| {'cute num day': 1L, 'num ... | [0.00930171087384, 0.00072... |\n",
      "| {'doesn t sound': 1L, 'tha... | [0.0122609334067, -0.01633... |\n",
      "| {'s numnd birthday': 1L, '... | [-0.049957562238, 0.016274... |\n",
      "| {'done the ring': 1L, 'ins... | [-0.0277977064252, -0.0454... |\n",
      "| {'the ne movi': 1L, 'i ve ... | [0.119670048356, -0.073654... |\n",
      "| {'lock tomorrow with': 1L,... | [0.020903730765, -0.026832... |\n",
      "| {'mi padr tomorrow': 1L, '... | [0.0252624005079, -0.01874... |\n",
      "| {'with the heat': 1L, 'hea... | [-0.0201575066894, 0.10154... |\n",
      "| {'one word benghazi': 1L, ... | [0.0334146283567, -0.04235... |\n",
      "| {'sat bama v': 1L, 'lsu or... | [-0.00228923303075, 0.0053... |\n",
      "| {'you realli take': 1L, 't... | [0.103222727776, -0.044858... |\n",
      "| {'countri look funny': 1L,... | [0.0429981201887, -0.05166... |\n",
      "| {'red tail for': 1L, 'watc... | [0.0195540394634, -0.02004... |\n",
      "| {'i wish at': 1L, 'wa go t... | [-0.00535758491606, 0.0299... |\n",
      "| {'a po one': 1L, 'friday t... | [0.0062792878598, 0.023541... |\n",
      "| {'hotel to travel': 1L, 'p... | [-0.0158572960645, -0.0048... |\n",
      "| {'so po about': 1L, 'tudor... | [0.0536782331765, -0.02189... |\n",
      "| {'num tomorrow hope': 1L, ... | [0.0946745127439, 0.005958... |\n",
      "| {'much about the': 1L, 'at... | [-0.0214658007026, 0.01570... |\n",
      "| {'may num san': 1L, 'll be... | [0.0583099126816, -0.03206... |\n",
      "| {'wednesday nd friday': 1L... | [-0.0014476776123, -0.0197... |\n",
      "| {'you are googl': 1L, 'a n... | [0.0431825332344, -0.00860... |\n",
      "| {'with u at': 1L, 'bloodi ... | [-0.0532381981611, 0.00429... |\n",
      "| {'cup great day': 1L, 'at ... | [0.0599736832082, -0.06715... |\n",
      "| {'to listen wait': 1L, 'on... | [-0.0829936340451, -0.0289... |\n",
      "| {'pad to career': 1L, 'to ... | [-0.0402261018753, -0.0711... |\n",
      "| {'on sun night': 1L, 'seav... | [-0.0232273247093, -0.0519... |\n",
      "| {'ipod and im': 1L, 'i wro... | [0.0270158722997, -0.06596... |\n",
      "| {'weeknd all day': 1L, 'al... | [-0.0108598219231, -0.0268... |\n",
      "| {'may not be': 1L, 'be the... | [0.0485409423709, -0.04562... |\n",
      "| {'into cbb in': 1L, 'user ... | [0.0343366377056, 0.064555... |\n",
      "| {'year in a': 1L, 'a row t... | [-0.0195198003203, -0.0247... |\n",
      "| {'like thi tri': 1L, 'sabr... | [0.0123013351113, -0.00956... |\n",
      "| {'num min on': 1L, 'hib or... | [-0.0482197105885, 0.02976... |\n",
      "| {'of my new': 1L, 'sweeps ... | [-0.0115303872153, -0.0721... |\n",
      "| {'father march with': 1L, ... | [0.032465621829, -0.047757... |\n",
      "| {'user and christmas': 1L,... | [-0.00874982122332, 0.0023... |\n",
      "| {'so i can': 1L, 'i can go... | [0.0253957137465, -0.02548... |\n",
      "| {'be a urban': 1L, 'run wa... | [0.0615041442215, -0.03013... |\n",
      "| {'num of num': 1L, 'tast t... | [-0.052638117224, 0.021519... |\n",
      "| {'numth po of': 1L, 'paint... | [0.0232849325985, -0.05488... |\n",
      "| {'the last few': 1L, 'he s... | [0.0094874612987, 0.031063... |\n",
      "| {'tim tebow date': 1L, 'ca... | [0.030894363299, -0.029864... |\n",
      "| {'nigeria beach soccer': 1... | [-0.00871416553855, 0.0123... |\n",
      "| {'and a ne': 1L, 'onli thi... | [0.0497358739376, 0.020497... |\n",
      "| {'the new york': 1L, 'game... | [0.00568440556526, -0.0663... |\n",
      "| {'see ne friday': 1L, 'bee... | [-0.0342062748969, 0.02349... |\n",
      "| {'to the mlk': 1L, 'i go t... | [-0.061036914587, 0.033565... |\n",
      "| {'girl wvu excit': 1L, 'my... | [-0.0826393589377, -0.0698... |\n",
      "| {'numth gen knows': 1L, 't... | [0.0397756621242, -0.04993... |\n",
      "| {'po your day': 1L, 'po nu... | [-0.0159778557718, -0.0050... |\n",
      "| {'three bundl amp': 1L, 'f... | [-0.0219315681607, -0.0478... |\n",
      "| {'you but i': 1L, 'not get... | [0.036341920495, -0.025917... |\n",
      "| {'saturday texa is': 1L, '... | [-0.0239018425345, -0.0147... |\n",
      "| {'realli feel it': 1L, 'no... | [0.0933467671275, -0.00594... |\n",
      "| {'the burg lockwood': 1L, ... | [0.00578243657947, -0.0562... |\n",
      "| {'that mean grey': 1L, 'it... | [0.0503569543362, -0.07100... |\n",
      "| {'here and yesterday': 1L,... | [-0.00367481517605, 0.0044... |\n",
      "| {'can not wait': 1L, 'eli ... | [-0.028024064377, -0.06089... |\n",
      "| {'wednesday even to': 1L, ... | [0.00315256416798, 0.00377... |\n",
      "| {'po than devil': 1L, 'num... | [0.0429153256118, 0.019067... |\n",
      "| {'at pipa sopa': 1L, 'curr... | [0.0336921848357, 0.046586... |\n",
      "| {'ne outta the': 1L, 'user... | [0.0328928120434, -0.00812... |\n",
      "| {'fuckin wit it': 1L, 'gam... | [0.0661878585815, -0.00578... |\n",
      "| {'with at user': 1L, 'frie... | [0.0147421061993, -0.04780... |\n",
      "| {'chri jericho bring': 1L,... | [0.0153585420921, 0.006293... |\n",
      "| {'to br saturday': 1L, 'we... | [-0.0359838269651, -0.0093... |\n",
      "| {'wa sooo po': 1L, 'daniel... | [-0.000396193005145, 0.032... |\n",
      "| {'gone have ne': 1L, 'arou... | [0.0623472779989, 0.010250... |\n",
      "| {'etta jame is': 1L, 'i op... | [0.0295148584992, -0.02133... |\n",
      "| {'user that there': 1L, 'g... | [0.0520426668227, 0.009531... |\n",
      "| {'winter classic will': 1L... | [-0.0382077246904, 0.07521... |\n",
      "| {'sandy i ne': 1L, 'on mon... | [-0.0233937706798, -0.0615... |\n",
      "| {'to hold eisen': 1L, 'u s... | [-0.0251600295305, -0.0193... |\n",
      "| {'u know more': 1L, 'you w... | [0.0217003505677, -0.00082... |\n",
      "| {'can t make': 1L, 'make i... | [-0.0101694837213, -0.0766... |\n",
      "| {'in the numth': 1L, 'flor... | [0.0214225854725, -0.10518... |\n",
      "| {'s s will': 1L, 'be air i... | [0.0123219387606, -0.02565... |\n",
      "| {'get a rt': 1L, 'po pat p... | [0.00563040468842, -0.0141... |\n",
      "| {'first season on': 1L, 's... | [-0.0273895040154, 0.02658... |\n",
      "| {'at peopl s': 1L, 'theri ... | [0.0775160491467, 0.022030... |\n",
      "| {'to too mani': 1L, 'type ... | [0.0692242011428, -0.00360... |\n",
      "| {'to abc rn': 1L, 'just af... | [0.00316053093411, -0.0197... |\n",
      "| {'day in tucson': 1L, 'nic... | [0.0502635985613, -0.07120... |\n",
      "| {'return with new': 1L, 'e... | [0.0398007817566, -0.04552... |\n",
      "| {'dream poster promot': 1L... | [0.000730469822884, -0.010... |\n",
      "| {'way to at': 1L, 'a po fi... | [0.0182440560311, 0.009201... |\n",
      "| {'folk are up': 1L, 'my fo... | [-0.0178828090429, -0.0519... |\n",
      "| {'num num all': 1L, 'to th... | [0.0323419980705, -0.00206... |\n",
      "+-------------------------------+-------------------------------+\n",
      "+-------------------------------+-------------------------------+\n",
      "|      vectors_pos_neutral      |      vectors_neutral_neg      |\n",
      "+-------------------------------+-------------------------------+\n",
      "| [-0.0340698510408, 0.00550... | [-0.0333484858274, 0.02722... |\n",
      "| [0.0126458723098, 0.040749... | [0.0757853090763, -0.03816... |\n",
      "| [-0.00323612615466, 0.0161... | [0.00410536723211, -0.0184... |\n",
      "| [-0.00350528699346, 0.0355... | [0.0402210988104, 0.029302... |\n",
      "| [0.0191367994994, 0.048501... | [-0.0291852150112, -0.0238... |\n",
      "| [0.0770083069801, 0.014658... | [0.00387389515527, -0.0211... |\n",
      "| [-0.014029850252, 0.055823... | [0.00705665396526, -0.0041... |\n",
      "| [0.03723564744, 0.07315213... | [0.0559082403779, 0.018202... |\n",
      "| [0.0554437711835, -0.00328... | [-0.0035795441363, 0.00440... |\n",
      "| [-0.0157517101616, 0.10149... | [0.0428548231721, -0.01192... |\n",
      "| [0.0937720164657, 0.137938... | [-0.0286162048578, 0.08409... |\n",
      "| [0.0160249806941, 0.009067... | [0.0135629205033, -0.06445... |\n",
      "| [-0.0388217382133, 0.13732... | [-0.0239051729441, -0.0091... |\n",
      "| [0.0781776160002, 0.006504... | [0.0195207297802, -0.00015... |\n",
      "| [0.0328673161566, 0.085571... | [0.0500219687819, -0.00788... |\n",
      "| [0.0751530230045, 0.085608... | [0.0305965542793, -0.01579... |\n",
      "| [0.0440375357866, 0.104551... | [0.0204967111349, -0.02048... |\n",
      "| [-0.0275931209326, 0.08036... | [-0.0393749810755, 0.01743... |\n",
      "| [0.0532221421599, 0.044513... | [-0.00333232246339, -0.026... |\n",
      "| [0.112000562251, 0.0664507... | [0.0242167320102, 0.009771... |\n",
      "| [0.0986633822322, 0.048257... | [0.0471818633378, 0.033014... |\n",
      "| [0.0571757331491, 0.136397... | [0.0360365882516, 0.013633... |\n",
      "| [0.07530259341, 0.09736503... | [0.0133241927251, 0.057924... |\n",
      "| [0.0960254669189, 0.061399... | [-0.0463104657829, 0.03115... |\n",
      "| [0.0174739006907, 0.024203... | [0.0530918054283, -0.00413... |\n",
      "| [0.0417297445238, 0.128264... | [0.0505260117352, -0.01637... |\n",
      "| [0.0108018033206, 0.041988... | [0.000550590455532, 0.0288... |\n",
      "| [0.0212536379695, 0.002952... | [0.00868621189147, -0.0431... |\n",
      "| [0.0268171336502, 0.037582... | [0.0114587573335, 0.010629... |\n",
      "| [-0.0145075554028, 0.09867... | [0.0313180498779, -0.08576... |\n",
      "| [0.0212203674018, 0.056807... | [-0.0458427332342, 0.01023... |\n",
      "| [0.048686940223, 0.1740906... | [0.0380529426038, 0.066502... |\n",
      "| [0.147113546729, 0.0715789... | [0.0821139663458, 0.026832... |\n",
      "| [0.130550816655, 0.1255357... | [0.0536887347698, 0.031370... |\n",
      "| [0.0479133762419, 0.140436... | [0.0515654012561, 0.027717... |\n",
      "| [-0.0474051125348, 0.11450... | [0.0423019118607, -0.06599... |\n",
      "| [0.0673742070794, 0.058505... | [0.0365007594228, 0.038102... |\n",
      "| [-0.0324739776552, 0.11890... | [-0.00711592286825, 0.0096... |\n",
      "| [0.08907148242, 0.03399644... | [0.0647646486759, 0.015857... |\n",
      "| [0.116755798459, 0.0365845... | [-0.0557866543531, 0.02137... |\n",
      "| [0.136846572161, 0.1002431... | [0.0322881788015, 0.032457... |\n",
      "| [0.0456909947097, -0.00594... | [-0.0439579822123, 0.00281... |\n",
      "| [0.0797062292695, 0.123297... | [0.0131523059681, 0.044953... |\n",
      "| [0.0346958227456, 0.099144... | [-0.0209776107222, 0.06700... |\n",
      "| [0.0714279934764, 0.068364... | [0.0303195752203, -0.01824... |\n",
      "| [-0.00640721805394, 0.0328... | [-0.0351630039513, -0.0145... |\n",
      "| [-0.0820262059569, 0.08045... | [-0.00794633384794, 0.0048... |\n",
      "| [0.302471369505, 0.1621081... | [0.0770488977432, 0.081372... |\n",
      "| [0.0495002903044, -0.02003... | [0.0309238694608, 0.025571... |\n",
      "| [0.0429625995457, 0.070564... | [-0.00695245340466, 0.0095... |\n",
      "| [0.0422504730523, 0.069560... | [-0.026536218822, -0.00090... |\n",
      "| [0.0767754241824, 0.025862... | [0.0101550873369, -0.02634... |\n",
      "| [0.034508574754, 0.1022575... | [-0.0185636952519, 0.04907... |\n",
      "| [0.0191031657159, 0.021385... | [0.0129486937076, 0.007373... |\n",
      "| [0.0407169088721, 0.094467... | [-0.0766849964857, -0.0210... |\n",
      "| [0.0496589057148, 0.050477... | [-0.0465162135661, 0.04213... |\n",
      "| [0.106772989035, 0.1141737... | [0.0369012691081, 0.067869... |\n",
      "| [0.0342458598316, 0.069195... | [-0.0284575801343, -0.0143... |\n",
      "| [0.0744208917022, 0.058546... | [-0.00717246159911, -0.004... |\n",
      "| [0.0823245644569, 0.011370... | [-0.0645975768566, 0.02611... |\n",
      "| [0.051116976887, 0.0795835... | [0.074923209846, 0.0153273... |\n",
      "| [0.0436612218618, 0.015123... | [0.0457446239889, -0.00921... |\n",
      "| [-0.0389510653913, 0.04403... | [0.01631209068, 0.01428877... |\n",
      "| [0.00757506489754, 0.07461... | [0.0630695149302, 0.064636... |\n",
      "| [0.0568192712963, 0.168099... | [0.0445063449442, 0.045743... |\n",
      "| [0.107743524015, 0.0747822... | [0.0633016526699, 0.002570... |\n",
      "| [0.0442727357149, 0.124774... | [-0.000745618483052, 0.033... |\n",
      "| [0.044318664819, 0.0344223... | [-0.0238853562623, 0.02914... |\n",
      "| [0.0323806107044, 0.098388... | [0.00575026869774, 0.00862... |\n",
      "| [0.0145047456026, 0.034282... | [0.033474419266, -0.036524... |\n",
      "| [-0.0428242199123, 0.08351... | [0.00975232757628, -0.0141... |\n",
      "| [0.0446105636656, 0.080353... | [-0.00624920753762, 0.0479... |\n",
      "| [0.0078388499096, 0.056829... | [-0.0270777475089, 0.02629... |\n",
      "| [0.12715639174, 0.11401455... | [-0.0482741035521, 0.05575... |\n",
      "| [0.0950979068875, 0.046702... | [0.00107176357415, 0.01929... |\n",
      "| [0.0718867853284, 0.074067... | [-0.00520892813802, 0.0327... |\n",
      "| [0.0941058620811, 0.091529... | [0.105515137315, -0.045039... |\n",
      "| [-0.0072910822928, 0.01482... | [0.000926111242734, -0.049... |\n",
      "| [-0.0199374817312, 0.10264... | [-0.0302088465542, 0.03226... |\n",
      "| [-0.0260154716671, 0.01694... | [0.0453272499144, -0.02966... |\n",
      "| [0.0582299567759, 0.086567... | [-0.0494915209711, 0.04039... |\n",
      "| [-0.0738647952676, -0.0033... | [0.0313366055489, -0.02136... |\n",
      "| [0.0734732374549, 0.159097... | [0.021192131564, 0.0898641... |\n",
      "| [0.124620199203, 0.1574414... | [0.102683901787, 0.1114156... |\n",
      "| [-0.0162207912654, 0.00770... | [0.0116162179038, -0.00716... |\n",
      "| [0.256857812405, 0.1647803... | [0.06987760216, 0.06895405... |\n",
      "| [0.141008764505, 0.0739887... | [0.0183495413512, 0.033424... |\n",
      "| [0.0353787876666, 0.059160... | [0.0220242198557, 0.033938... |\n",
      "| [0.0212080795318, 0.107623... | [0.00252878549509, -0.0012... |\n",
      "| [0.0743306055665, 0.081291... | [-0.0105186104774, 0.03230... |\n",
      "| [0.0810946002603, 0.161101... | [0.0141888810322, 0.026337... |\n",
      "| [0.271311253309, 0.1452049... | [0.081140846014, 0.0561103... |\n",
      "| [0.129414111376, 0.0746115... | [0.0310840103775, 0.002977... |\n",
      "| [0.103595674038, 0.1172784... | [0.0629093721509, 0.013427... |\n",
      "| [-0.051804818213, 0.077209... | [0.00572727248073, 0.01898... |\n",
      "| [0.0708145722747, 0.068675... | [0.0425116308033, 0.069663... |\n",
      "| [0.0949553996325, 0.121142... | [-0.00950696878135, -0.009... |\n",
      "| [-0.0271751526743, 0.07173... | [-0.0315506719053, -0.0635... |\n",
      "| [0.070525161922, 0.1025646... | [0.0584287084639, -0.00333... |\n",
      "| [-0.0215968545526, 0.08376... | [-0.00126346142497, 0.0170... |\n",
      "| [0.045031696558, -0.013668... | [0.0185149963945, -0.00635... |\n",
      "| [0.113783873618, 0.1077477... | [-0.0134539203718, 0.02505... |\n",
      "| [0.0518455095589, -0.01600... | [0.0724128857255, 0.040368... |\n",
      "| [0.0695727393031, 0.050821... | [0.0671833604574, 0.008035... |\n",
      "| [0.0133413430303, 0.099086... | [0.0411946475506, -0.00385... |\n",
      "| [0.00578485243022, 0.01525... | [0.0318000651896, 0.007844... |\n",
      "| [0.0213056933135, 0.026901... | [0.0307167731225, 0.027995... |\n",
      "| [0.0584340728819, 0.068821... | [0.0124493110925, 0.026442... |\n",
      "| [0.0967252627015, 0.088375... | [0.0201544426382, 0.061146... |\n",
      "| [0.0800810307264, 0.077120... | [0.0288296863437, -0.02600... |\n",
      "| [0.0567878000438, 0.036915... | [0.00832525081933, 0.01611... |\n",
      "| [0.00542399147525, 0.02958... | [-0.023567115888, -0.03542... |\n",
      "| [0.0420686490834, 0.074951... | [-0.0214603487402, -0.0530... |\n",
      "| [-0.0237376932055, 0.03168... | [0.0234243087471, -0.01739... |\n",
      "| [0.088646158576, 0.0893301... | [0.0166509132832, -0.01569... |\n",
      "| [0.0746575817466, 0.046186... | [0.0263031776994, -0.02392... |\n",
      "| [0.15995977819, 0.11398333... | [0.0589164942503, 0.062141... |\n",
      "| [0.0515282787383, -0.05785... | [0.0362056456506, -0.00246... |\n",
      "| [-0.000170843297383, 0.017... | [0.0397629477084, 0.000150... |\n",
      "| [0.0663542151451, 0.074802... | [0.0633721277118, -0.03290... |\n",
      "| [-0.00545474607497, -0.044... | [0.0230533890426, -0.02296... |\n",
      "| [0.0938894972205, 0.100413... | [-0.0330156385899, 0.04004... |\n",
      "| [-0.0271323174238, -0.0082... | [0.0491750463843, -0.02427... |\n",
      "| [0.0106035573408, -0.00971... | [0.0466925241053, -0.00772... |\n",
      "| [-0.0120758293197, 0.15407... | [-0.040860965848, 0.004753... |\n",
      "| [-0.0504540130496, -0.0164... | [0.0774909257889, -0.06595... |\n",
      "| [0.048789434135, 0.1240460... | [0.0186377651989, 0.090462... |\n",
      "| [0.115077465773, 0.1032441... | [-0.019216902554, 0.057720... |\n",
      "| [0.0756268054247, 0.067591... | [0.0172152388841, -0.00188... |\n",
      "| [0.066669434309, 0.1543312... | [-1.813005656e-05, -0.0088... |\n",
      "| [0.112988114357, 0.1620956... | [0.0259371772408, 0.036328... |\n",
      "| [-0.0377098172903, 0.07155... | [-0.00407336140051, -0.031... |\n",
      "| [-0.0600782521069, 0.05017... | [0.00250961259007, 0.00274... |\n",
      "| [0.0664013698697, 0.033456... | [0.0829375088215, -0.01289... |\n",
      "| [0.0345918685198, 0.036297... | [0.00689297309145, 0.00920... |\n",
      "| [0.0041744383052, 0.199782... | [0.0232379417866, 0.091832... |\n",
      "| [0.0237164329737, 0.114642... | [0.0172281339765, 0.056984... |\n",
      "| [0.10329439491, 0.08492890... | [0.0270313099027, 0.008624... |\n",
      "| [0.0713220536709, -0.00292... | [0.0102703077719, -0.05501... |\n",
      "| [0.245870426297, 0.2245858... | [-0.0775993466377, 0.03410... |\n",
      "| [0.213136255741, 0.1482900... | [0.0573516711593, 0.061572... |\n",
      "| [0.0400777161121, 0.041561... | [-0.00367042329162, -0.013... |\n",
      "| [0.0379994250834, -0.02163... | [0.0399970971048, -0.02103... |\n",
      "| [0.0614061318338, 0.007424... | [-0.0026870383881, 0.03172... |\n",
      "| [0.21321901679, 0.14139470... | [-0.00363049004227, 0.0528... |\n",
      "| [0.0626707673073, 0.010829... | [0.0419228561223, 0.019472... |\n",
      "| [0.0742921978235, 0.154470... | [-0.0616429150105, 0.03877... |\n",
      "| [-0.0276961699128, 0.05141... | [0.0130690336227, 0.030382... |\n",
      "| [0.0795039981604, 0.081531... | [-0.0118240509182, 0.01653... |\n",
      "| [-0.00734286243096, 0.0102... | [-0.0211412664503, -0.0348... |\n",
      "| [0.18296533823, 0.08668591... | [0.0512932874262, -0.03089... |\n",
      "| [-0.0328830145299, 0.07780... | [0.0372438691556, 0.090518... |\n",
      "| [-0.0023150248453, 0.02698... | [0.0480181612074, -0.03410... |\n",
      "| [-0.0291213300079, 0.03045... | [-0.00159033946693, -0.000... |\n",
      "| [0.185853242874, 0.1255441... | [0.081769824028, 0.0285479... |\n",
      "| [0.0154539300129, 0.110380... | [0.00143605028279, 0.04732... |\n",
      "| [0.0570441149175, 0.041738... | [0.0814103633165, 0.005824... |\n",
      "| [-0.00578424567357, 0.0033... | [-0.00199648318812, 0.0047... |\n",
      "| [0.0661241933703, 0.075185... | [-0.0153923500329, 0.06246... |\n",
      "| [0.115694478154, 0.0387288... | [0.0542740486562, -0.01071... |\n",
      "| [0.0357393398881, 0.085714... | [0.0140295699239, -0.04299... |\n",
      "| [0.063580609858, 0.0666820... | [0.0195248853415, 0.016897... |\n",
      "| [0.00235841167159, 0.08559... | [-0.0161451175809, 0.00379... |\n",
      "| [-0.0300414916128, 0.08245... | [0.069815710187, -0.030402... |\n",
      "| [0.0397054962814, -0.02030... | [-0.0308090373874, -0.0285... |\n",
      "| [0.0709525868297, 0.160702... | [0.066341213882, -0.007061... |\n",
      "| [-0.028914982453, 0.002672... | [0.000977739691734, -0.101... |\n",
      "| [-0.0361690074205, 0.06189... | [0.018652651459, -0.010901... |\n",
      "| [0.0243878625333, 0.068395... | [-0.0292002856731, 0.00983... |\n",
      "| [0.0811640247703, 0.161283... | [0.0468944571912, 0.024433... |\n",
      "| [0.0095890192315, 0.025296... | [0.0247396882623, -0.01493... |\n",
      "| [0.0176073685288, 0.153738... | [0.042081721127, 0.0144150... |\n",
      "| [0.105201177299, 0.0120330... | [0.0391592122614, 0.003228... |\n",
      "| [0.0040853456594, -0.01306... | [0.00934133119881, -0.0693... |\n",
      "| [-0.0128861144185, 0.05972... | [-0.0847449675202, 0.01056... |\n",
      "| [0.0981290489435, 0.048539... | [0.0362885631621, 0.004159... |\n",
      "| [0.0833437666297, 0.052494... | [-0.0158647801727, 0.02153... |\n",
      "| [-0.000546769646462, 0.018... | [0.0569407343864, 0.003999... |\n",
      "| [0.0715522766113, 0.139835... | [-0.0166646745056, 0.02513... |\n",
      "| [0.0796025320888, 0.095108... | [0.055459998548, 0.0092150... |\n",
      "| [-0.0499555468559, -0.0574... | [0.0524082779884, -0.04577... |\n",
      "| [0.15389226377, 0.09367713... | [0.0217025931925, 0.074395... |\n",
      "| [0.0751530230045, 0.085608... | [0.0305965542793, -0.01579... |\n",
      "| [0.0198133029044, 0.090337... | [-0.0781751051545, 0.03048... |\n",
      "| [-0.0054190675728, 0.04589... | [0.0158324558288, -0.00323... |\n",
      "| [0.113097004592, 0.1301239... | [0.00104600808118, -0.0403... |\n",
      "| [0.082011193037, 0.1181131... | [0.0356889739633, 0.018881... |\n",
      "| [-0.0369638502598, 0.05688... | [0.00874098669738, -0.0255... |\n",
      "| [0.0349194332957, 0.112921... | [0.0802749097347, -0.05340... |\n",
      "| [0.0823442861438, 0.187528... | [0.0279702153057, 0.012910... |\n",
      "| [-0.00501005491242, 0.1227... | [-0.0172085147351, -0.0219... |\n",
      "| [-0.0130505673587, 0.08251... | [0.0507601648569, 0.023054... |\n",
      "| [-0.0450378060341, 0.05814... | [0.0807120501995, -0.01445... |\n",
      "| [0.0872677788138, 0.100652... | [0.0088264150545, 0.035910... |\n",
      "| [0.0508551113307, 0.021967... | [0.0852941423655, 0.015148... |\n",
      "| [0.0243360046297, 0.060105... | [-0.0132431350648, -0.0495... |\n",
      "| [0.0673408061266, 0.046316... | [0.073919840157, -0.060992... |\n",
      "| [0.0548653006554, 0.050515... | [-0.0251599550247, 0.04256... |\n",
      "| [0.0418254286051, 0.043837... | [0.0697778537869, 0.023815... |\n",
      "| [-0.0256701856852, 0.04579... | [0.000202197043109, -0.008... |\n",
      "| [0.0471550039947, -0.01292... | [0.0724128857255, 0.040368... |\n",
      "| [-0.0132520031184, 0.03173... | [-0.0401547923684, 0.07499... |\n",
      "| [0.0442582517862, 0.043347... | [0.0051533319056, 0.004584... |\n",
      "| [0.064491070807, 0.0547508... | [-0.0871762782335, 0.02653... |\n",
      "| [-0.00855950266123, 0.0588... | [-0.0195428337902, 0.00702... |\n",
      "| [-0.0185342803597, -0.0058... | [0.0391216501594, -0.03946... |\n",
      "| [0.0197434816509, -0.02700... | [-0.000238111621002, 0.006... |\n",
      "| [0.0284039005637, 0.040606... | [0.0129661113024, -0.04584... |\n",
      "| [0.0890324935317, 0.076912... | [-0.0180615149438, 0.00514... |\n",
      "| [-0.0226246267557, 0.06618... | [-0.00508035486564, -0.002... |\n",
      "| [-0.0672385618091, 0.01796... | [0.0379857122898, -0.04607... |\n",
      "| [0.119264781475, 0.0757509... | [0.0108825489879, 0.015687... |\n",
      "| [0.16003473103, 0.11162333... | [0.0505024790764, 0.074771... |\n",
      "| [0.0935852378607, 0.031550... | [-0.0726365670562, 0.05370... |\n",
      "| [0.0371679067612, -0.01477... | [0.0472307689488, -0.00515... |\n",
      "| [0.0249563790858, 0.019055... | [0.0330170467496, 0.046668... |\n",
      "| [0.0561529248953, 0.005092... | [0.0126199685037, -0.00187... |\n",
      "| [0.0249389577657, 0.033607... | [0.033114887774, 0.0582332... |\n",
      "| [-0.0116417165846, 0.03682... | [0.0117136696354, 0.027164... |\n",
      "| [-0.0309304799885, 0.04232... | [0.056537207216, -0.050179... |\n",
      "| [0.0159862805158, 0.009590... | [0.00133089954033, 0.00851... |\n",
      "| [0.0511423237622, 0.003878... | [0.00690682884306, 0.01335... |\n",
      "| [-0.018346497789, 0.057747... | [0.0543091185391, -0.01257... |\n",
      "| [0.038438603282, 0.0802323... | [0.000245826202445, 0.0105... |\n",
      "| [0.0279789175838, 0.072931... | [0.0489830039442, -0.01402... |\n",
      "| [0.0568436309695, 0.069011... | [-0.00349199236371, 0.0229... |\n",
      "| [0.0714530274272, 0.091939... | [-0.026271302253, 0.019196... |\n",
      "| [-0.0086429072544, -0.0263... | [-0.00924306642264, -0.007... |\n",
      "| [-0.0608446449041, 0.10890... | [0.0211516916752, 0.026915... |\n",
      "| [-0.125174626708, 0.024276... | [0.0575437434018, 0.008429... |\n",
      "| [0.0560050345957, 0.110919... | [0.00151314714458, -0.0037... |\n",
      "| [0.171550408006, 0.0103401... | [0.0453084632754, 0.042739... |\n",
      "| [0.00384563789703, 0.03087... | [0.0605469122529, -0.04234... |\n",
      "| [0.0775121748447, 0.084398... | [0.0454818084836, 0.006225... |\n",
      "| [0.0674174875021, 0.036081... | [0.0720843598247, 0.056420... |\n",
      "| [0.0471316128969, 0.081293... | [-0.00841948203743, -0.016... |\n",
      "| [0.0210101120174, 0.071413... | [-0.0155719285831, 0.05196... |\n",
      "| [-0.0670968666673, 0.06466... | [0.00690477108583, -0.0353... |\n",
      "| [-0.0282316990197, 0.01094... | [-0.00849700346589, 0.0235... |\n",
      "| [0.151571765542, 0.2299638... | [0.0701040402055, 0.071072... |\n",
      "| [0.137598112226, 0.1001912... | [-0.0257221385837, 0.02806... |\n",
      "| [0.0629539117217, 0.126471... | [0.00740717165172, -0.0032... |\n",
      "| [0.0931528806686, 0.048648... | [-0.0295924432576, 0.00074... |\n",
      "| [0.10916223377, 0.13770189... | [0.0375144258142, 0.062023... |\n",
      "| [-0.00463844789192, 0.0689... | [0.0114217242226, -0.01502... |\n",
      "| [0.0115030407906, 0.021442... | [-0.0076966448687, -0.0088... |\n",
      "| [-0.113819979131, -0.02319... | [-0.0656828954816, -0.0265... |\n",
      "| [0.128293052316, 0.0161363... | [-0.0123852938414, 0.08762... |\n",
      "| [0.0530409961939, 0.087507... | [0.00914780702442, 0.05084... |\n",
      "| [0.0413416214287, 0.055261... | [-0.014320621267, -0.00348... |\n",
      "| [-0.0178381185979, 0.01051... | [-0.0185606833547, -0.0141... |\n",
      "| [0.038371309638, 0.1636993... | [-0.0401218309999, 0.07015... |\n",
      "| [0.0891140326858, 0.102927... | [0.0231805406511, 0.058784... |\n",
      "| [-0.00111832295079, 0.0276... | [0.0652149766684, -0.01322... |\n",
      "| [0.0823863297701, 0.005557... | [0.0534242093563, -0.00321... |\n",
      "| [0.059527490288, 0.1502180... | [0.0495304912329, 0.009307... |\n",
      "| [0.111256591976, 0.0454349... | [0.00142747513019, 0.00194... |\n",
      "| [0.022112313658, 0.0433382... | [0.0329651869833, 0.015112... |\n",
      "| [0.0115951038897, 0.178745... | [0.0588509812951, 0.081698... |\n",
      "| [0.067833468318, 0.0667214... | [0.00955957360566, -0.0068... |\n",
      "| [0.0286984406412, 0.064142... | [-0.0413303263485, -0.0207... |\n",
      "| [0.0292433127761, 0.052460... | [-0.0401793979108, -0.0062... |\n",
      "| [-0.0283469501883, 0.02677... | [0.0295172557235, -0.01572... |\n",
      "| [0.136910274625, 0.1335698... | [-0.0119476895779, 0.02654... |\n",
      "| [0.0803612470627, 0.095311... | [0.0497342310846, -0.03572... |\n",
      "| [-0.052756883204, -0.04115... | [0.0269178394228, -0.00779... |\n",
      "| [0.00479660090059, 0.00179... | [0.0915299504995, 0.065460... |\n",
      "| [0.00112167000771, 0.04336... | [0.0746002420783, -0.01478... |\n",
      "| [0.180080398917, 0.0835123... | [0.0487640686333, 0.049357... |\n",
      "| [0.0803103819489, 0.132150... | [0.0518032684922, 0.060703... |\n",
      "| [0.0115473307669, 0.090411... | [0.0281573571265, 0.034940... |\n",
      "| [0.0571890063584, 0.166608... | [-0.0195112153888, 0.10095... |\n",
      "| [0.0123631237075, 0.112411... | [-0.0132289435714, 0.05808... |\n",
      "| [0.0195174515247, 0.092053... | [0.014229063876, -0.011054... |\n",
      "| [-0.0266662947834, 0.03044... | [-0.0688638389111, -0.0010... |\n",
      "| [0.00898621883243, 0.02081... | [0.0242824684829, 0.015380... |\n",
      "| [0.278827339411, 0.2244027... | [0.0492455549538, 0.066542... |\n",
      "| [0.0182601995766, 0.025625... | [0.0855585560203, -0.01162... |\n",
      "| [0.0894157215953, 0.078636... | [-0.0347806736827, 0.02684... |\n",
      "| [0.023828510195, 0.0645544... | [0.0362313017249, 0.028968... |\n",
      "| [0.0817382335663, 0.006285... | [0.0119211236015, 0.039277... |\n",
      "| [0.0814689397812, 0.088560... | [0.0359413400292, 0.005988... |\n",
      "| [0.0696528479457, 0.065638... | [0.0694558396935, -0.00096... |\n",
      "| [0.109688036144, 0.0756247... | [0.0437512136996, 0.024100... |\n",
      "| [0.0508645363152, 0.018127... | [0.00355114717968, 0.04246... |\n",
      "| [0.259928226471, 0.1219202... | [-0.00388722121716, 0.0343... |\n",
      "| [0.0625139400363, 0.062417... | [0.0155273890123, 0.015756... |\n",
      "| [-0.0389112234116, 0.07574... | [0.0497681237757, -0.01565... |\n",
      "| [0.0546487905085, 0.085222... | [-0.0220586173236, 0.00401... |\n",
      "| [0.024377701804, 0.1290501... | [-0.0229344516993, 0.01041... |\n",
      "| [0.0278987120837, 0.064308... | [0.0447597838938, -0.02681... |\n",
      "| [-0.0376642420888, 0.10287... | [0.0268995705992, -0.06113... |\n",
      "| [0.0636346489191, 0.065283... | [0.0325913317502, 0.016023... |\n",
      "| [0.086700104177, 0.1331569... | [0.00715092848986, 0.02064... |\n",
      "| [-0.00617666961625, 0.0658... | [-0.01071621757, -0.014074... |\n",
      "| [0.0137577336282, 0.032395... | [0.0301349330693, 0.009114... |\n",
      "| [0.0211026072502, 0.062001... | [0.0301302615553, 0.042709... |\n",
      "| [0.0752459168434, 0.061977... | [0.0287867803127, 0.077130... |\n",
      "| [0.0651932507753, 0.047948... | [0.00238565634936, 0.02273... |\n",
      "| [0.0279571004212, 0.118353... | [0.0433722399175, 0.027982... |\n",
      "| [0.0171436704695, 0.010077... | [0.0100461822003, 0.003804... |\n",
      "| [0.0373085513711, -0.04616... | [0.0577574893832, 0.034774... |\n",
      "| [0.0117513360456, 0.123558... | [0.0636635050178, -0.04275... |\n",
      "| [0.136897578835, 0.0598788... | [0.0431006923318, 0.002954... |\n",
      "| [-0.00243653985672, -0.020... | [-0.0129841165617, -0.0283... |\n",
      "| [-0.0180724430829, 0.07231... | [0.032342735678, -0.006808... |\n",
      "| [0.236168473959, 0.1918946... | [0.0180778205395, 0.043929... |\n",
      "| [0.031805627048, 0.0299723... | [0.0469474494457, 0.029956... |\n",
      "| [0.0682883188128, 0.093544... | [0.00804270710796, -0.0233... |\n",
      "| [-0.0226289704442, 0.00356... | [-0.0140115087852, -0.0010... |\n",
      "| [0.0566791296005, 0.037236... | [0.0976237654686, 0.005525... |\n",
      "| [0.125202864408, 0.0201607... | [0.00365774543025, -0.0416... |\n",
      "| [0.0915791988373, 0.164620... | [0.0659291222692, 0.017326... |\n",
      "| [0.034508574754, 0.1022575... | [-0.0185636952519, 0.04907... |\n",
      "| [0.0435395427048, 0.043723... | [0.030959341675, 0.0503897... |\n",
      "| [0.0463490821421, 0.073708... | [0.0716707631946, 0.022269... |\n",
      "| [0.0195125974715, 0.110793... | [0.0204892642796, 0.009788... |\n",
      "| [0.112790450454, 0.1260585... | [-0.025816116482, -0.00555... |\n",
      "| [0.143741294742, 0.1050670... | [-0.106817103922, 0.013402... |\n",
      "| [0.110546179116, 0.1092623... | [0.0839310511947, 0.010845... |\n",
      "| [0.0560393929482, 0.098332... | [0.00380094815046, 0.01729... |\n",
      "| [0.0282014179975, 0.194762... | [6.63890168653e-05, 0.0399... |\n",
      "| [-0.0130027430132, 0.03579... | [-0.0594874359667, 0.00359... |\n",
      "| [0.0195772554725, 0.018661... | [0.0304595902562, 0.010859... |\n",
      "| [-0.00258609349839, 0.0477... | [-0.00586819183081, 0.0139... |\n",
      "| [0.114024080336, 0.1100969... | [0.0112748080865, 0.042628... |\n",
      "| [0.000281695276499, 0.0888... | [-0.00440033851191, -0.023... |\n",
      "| [0.0174281615764, 0.047067... | [0.0743173658848, -0.00299... |\n",
      "| [0.0279444213957, 0.098444... | [0.0314633399248, -0.01376... |\n",
      "| [-0.040276132524, 0.071594... | [-0.0195170715451, 0.01164... |\n",
      "| [0.0861811637878, 0.092587... | [-0.0427433550358, -0.0055... |\n",
      "| [0.0737378299236, 0.037892... | [0.0689755529165, -0.00517... |\n",
      "| [0.0760366246104, 0.106385... | [0.0504158660769, 0.040500... |\n",
      "| [0.0356921851635, 0.101021... | [0.00506544206291, 0.05029... |\n",
      "| [0.0637525618076, 0.052445... | [-0.0164314713329, 0.05146... |\n",
      "| [0.00218684133142, 0.07237... | [0.0330441221595, 0.045330... |\n",
      "| [-0.0284234564751, 0.01007... | [0.0043292590417, 0.001634... |\n",
      "| [0.0807967185974, 0.042194... | [0.0257629752159, 0.022296... |\n",
      "| [-0.0450135320425, 0.02754... | [0.045990601182, -0.022120... |\n",
      "| [0.0814841985703, 0.003310... | [0.0648097544909, 0.014972... |\n",
      "| [-0.0278488397598, 0.12689... | [0.0405968241394, 0.030588... |\n",
      "| [0.14665915072, 0.09388142... | [0.0364819280803, 0.031973... |\n",
      "| [0.164365082979, 0.1066041... | [0.0444820299745, 0.029410... |\n",
      "| [-0.0360393077135, 0.05022... | [0.00561625976115, 0.01330... |\n",
      "| [0.0587975867093, 0.127120... | [0.0808182060719, -0.00829... |\n",
      "| [0.0636257976294, 0.202814... | [0.0674627125263, 0.078151... |\n",
      "| [0.0489344522357, 0.071183... | [0.104778900743, -0.023172... |\n",
      "| [0.0128350174055, 0.069494... | [0.0328833311796, 0.009295... |\n",
      "| [0.0512110218406, 0.002655... | [-0.0430485419929, -0.0046... |\n",
      "| [0.228950768709, 0.0846922... | [0.0128136062995, 0.051229... |\n",
      "| [-0.00838736351579, -0.017... | [-0.0243188142776, -0.0518... |\n",
      "| [-0.0401045382023, 0.02732... | [-0.0198190528899, -0.0159... |\n",
      "| [0.0917473137379, 0.108344... | [0.000638124009129, 0.0304... |\n",
      "| [0.00723049556836, 0.08448... | [0.0410942472517, 0.050948... |\n",
      "| [0.0420021824539, 0.090510... | [-0.0128412861377, 0.00749... |\n",
      "| [0.134088814259, 0.1323188... | [0.0446485690773, 0.052713... |\n",
      "| [0.0689524039626, 0.033345... | [0.0618081688881, -0.00667... |\n",
      "| [0.0761377289891, 0.137380... | [-0.0428659357131, 0.02491... |\n",
      "| [0.142162516713, 0.0955451... | [-0.0266949813813, -0.0033... |\n",
      "| [0.0282305367291, 0.077992... | [-0.00349842803553, 0.0128... |\n",
      "| [-0.0628569349647, 0.22821... | [0.0227096509188, 0.043612... |\n",
      "| [0.00753668928519, -0.0375... | [0.00298166950233, -0.0224... |\n",
      "| [0.0747939571738, 0.031374... | [0.0741270557046, -0.01296... |\n",
      "| [0.0199910979718, 0.079449... | [-0.00625388883054, -0.027... |\n",
      "| [0.220240712166, 0.1751509... | [0.0388444922864, 0.105722... |\n",
      "| [0.0122727826238, 0.048489... | [0.00886223930866, -0.0191... |\n",
      "| [0.151489913464, 0.2232542... | [0.0679092034698, 0.137479... |\n",
      "| [0.03313594684, 0.07924173... | [0.0115350242704, 0.056659... |\n",
      "| [0.139882385731, 0.1221709... | [-0.00837451405823, 0.0005... |\n",
      "| [0.0970724523067, 0.050654... | [0.00952903926373, 0.03219... |\n",
      "| [0.060389328748, 0.0090953... | [0.0924258008599, -0.04343... |\n",
      "| [0.00764489313588, 0.05725... | [0.0285557899624, -0.01282... |\n",
      "| [0.0361948795617, 0.084032... | [0.0584135726094, -0.01054... |\n",
      "| [0.0362265147269, 0.024168... | [0.0224226210266, -0.01116... |\n",
      "| [0.147125944495, 0.0605136... | [-0.0325052812696, -0.0004... |\n",
      "| [0.0756427422166, 0.198954... | [0.0145339481533, 0.079023... |\n",
      "| [0.0575764402747, 0.041052... | [0.133639112115, -0.068325... |\n",
      "| [0.0430735126138, 0.115724... | [-0.0206767357886, -0.0275... |\n",
      "| [0.268883228302, 0.3054476... | [0.088462933898, 0.0442513... |\n",
      "| [0.0238777697086, 0.106276... | [-0.0183158181608, -0.0383... |\n",
      "| [-0.0027318475768, -0.0024... | [0.00938704423606, -0.0470... |\n",
      "| [0.023557735607, 0.1228840... | [0.0187052041292, -0.03190... |\n",
      "| [0.0664664432406, 0.102018... | [0.0571785494685, 0.017740... |\n",
      "| [0.0402374081314, 0.100481... | [0.0425393655896, 0.008356... |\n",
      "| [0.0257059969008, 0.040548... | [-0.0360156297684, 0.00510... |\n",
      "| [0.107361339033, 0.0268875... | [0.054693736136, -0.048803... |\n",
      "| [0.0176383312792, 0.091684... | [0.0279071852565, 0.031622... |\n",
      "| [0.106797851622, 0.1659862... | [0.062109336257, 0.0876042... |\n",
      "| [0.00997730158269, 0.02380... | [-0.0181960370392, -0.0072... |\n",
      "| [0.0763073638082, -0.02153... | [0.052608795464, 0.0369270... |\n",
      "| [0.00324553321116, 0.04673... | [-0.013151248917, -0.01652... |\n",
      "| [-0.0712441876531, 0.06927... | [-0.018160643056, 0.000369... |\n",
      "| [0.132644966245, 0.1114807... | [0.0354723967612, 0.039276... |\n",
      "| [0.109229072928, 0.0202977... | [0.0490315854549, 0.024987... |\n",
      "| [0.020023111254, 0.1087429... | [-0.0271406173706, 0.05897... |\n",
      "| [0.00376409385353, 0.03286... | [-0.0316264629364, 0.02350... |\n",
      "| [-0.000285190471914, 0.128... | [0.0036300746724, -0.00173... |\n",
      "| [0.0458933338523, 0.069675... | [0.000528742384631, 0.0167... |\n",
      "| [0.0419250652194, 0.144094... | [0.0347464680672, 0.026719... |\n",
      "| [0.13207834959, -0.0345026... | [0.059498809278, -0.003234... |\n",
      "| [0.0271845087409, 0.090987... | [-0.039964761585, 0.001679... |\n",
      "| [-0.0866911262274, 0.01773... | [-0.0101396040991, 0.01939... |\n",
      "| [-0.0148426173255, 0.04475... | [-0.0209513418376, 0.00947... |\n",
      "| [0.123225152493, -0.019021... | [0.0104172183201, 0.020842... |\n",
      "| [0.0219243168831, 0.061245... | [0.0461616776884, 0.009729... |\n",
      "| [0.0460013598204, 0.081194... | [0.0693010762334, -0.03027... |\n",
      "| [0.0770455151796, 0.191065... | [0.0693646818399, -0.00400... |\n",
      "| [-0.0423919036984, 0.07067... | [-0.00222863606177, 0.0238... |\n",
      "| [0.214404791594, 0.1321976... | [-0.0315626375377, 0.07894... |\n",
      "| [0.000859219930135, 0.0393... | [0.031960695982, 0.0072386... |\n",
      "| [0.175163045526, 0.1403357... | [0.0112693896517, 0.073828... |\n",
      "| [0.0470511466265, 0.069085... | [0.0433085672557, 0.017027... |\n",
      "| [-0.0106347696856, 0.04761... | [-0.0157809499651, 0.03956... |\n",
      "| [0.0668233335018, 0.038242... | [-0.0211196970195, 0.01989... |\n",
      "| [0.0715546086431, 0.074058... | [0.0343005359173, 0.058206... |\n",
      "| [0.0620826892555, 0.123818... | [-0.00348538835533, -0.011... |\n",
      "| [0.069725394249, 0.0497385... | [0.0011674775742, -0.02741... |\n",
      "| [0.0374367795885, 0.016572... | [-0.00642846804112, 0.0112... |\n",
      "| [-0.0255980696529, 0.05120... | [-0.00265393848531, -0.048... |\n",
      "| [-0.0412720218301, -0.0174... | [-0.00435322197154, -0.035... |\n",
      "| [0.0795863121748, 0.049826... | [0.0304979756474, 0.058676... |\n",
      "| [0.041308183223, 0.0702028... | [0.0362322814763, 0.030340... |\n",
      "| [0.070742495358, 0.0500916... | [0.00547484494746, 0.01743... |\n",
      "| [0.058188341558, 0.0240269... | [0.0833109468222, 0.000606... |\n",
      "| [-0.0125722084194, 0.04542... | [-0.000524042523466, -0.00... |\n",
      "| [0.0328052900732, 0.096015... | [0.0102195525542, -0.03234... |\n",
      "| [0.10262041539, 0.04698561... | [0.0312510579824, -0.00933... |\n",
      "| [-0.0010661095148, 0.07087... | [0.00760946888477, 0.05053... |\n",
      "| [0.0107674412429, 0.071725... | [0.0759539455175, 0.011493... |\n",
      "| [0.113744035363, 0.1213828... | [0.0311964638531, 0.066157... |\n",
      "| [-0.0485660545528, 0.03349... | [-0.0320448838174, -0.0005... |\n",
      "| [0.0266589485109, 0.087144... | [0.0302979853004, -0.02904... |\n",
      "| [0.0192257408053, 0.045792... | [0.0887520462275, 0.002221... |\n",
      "| [0.0206662230194, 0.058181... | [-0.0304295588285, 0.03546... |\n",
      "| [0.0793124586344, 0.052150... | [0.00849265791476, 0.03992... |\n",
      "| [-0.0314649268985, 0.13556... | [-0.0102594271302, 0.04975... |\n",
      "| [0.117299921811, 0.0698713... | [-0.00160297646653, 0.0065... |\n",
      "| [0.0590813942254, 0.078631... | [-0.0291025526822, 0.05061... |\n",
      "| [0.0299612376839, 0.049689... | [0.0514375343919, 0.004820... |\n",
      "| [0.030036309734, 0.0258929... | [-0.0143337398767, -0.0111... |\n",
      "| [-0.0591747872531, 0.05427... | [0.0135958576575, -0.00135... |\n",
      "| [0.00176409713458, 0.02328... | [0.0134358918294, -0.04456... |\n",
      "| [-0.0421076305211, 0.07313... | [0.0362947806716, -0.03358... |\n",
      "| [0.0650647133589, 0.123852... | [0.0220828056335, 0.008200... |\n",
      "| [0.030087871477, 0.0663618... | [-0.0406517349184, 0.02585... |\n",
      "| [0.0878443717957, 0.020548... | [0.146983250976, 0.0579512... |\n",
      "| [0.0246991496533, 0.075636... | [-0.000227941200137, -0.00... |\n",
      "| [0.103227414191, 0.0920669... | [0.0531993880868, 0.006386... |\n",
      "| [0.028889881447, 0.0726674... | [0.0213505681604, -0.03202... |\n",
      "| [0.0139250354841, 0.072802... | [-0.0355825833976, 0.00215... |\n",
      "| [0.0541128627956, 0.104405... | [0.0113199353218, -0.01714... |\n",
      "| [-0.00919053889811, 0.0326... | [0.0361928828061, 0.003793... |\n",
      "| [0.0322521142662, 0.165808... | [0.00278736883774, 0.04160... |\n",
      "| [0.0179775897413, 0.015684... | [-0.0472910478711, 0.00685... |\n",
      "| [0.0690929293633, 0.009199... | [-0.0270715411752, 0.01311... |\n",
      "| [0.138330221176, 0.1367873... | [0.105919830501, 0.1044757... |\n",
      "| [0.0132031626999, 0.040755... | [0.0479523129761, -0.01196... |\n",
      "| [0.017981486395, 0.0007438... | [-0.00373679492623, -0.046... |\n",
      "| [0.0744866207242, 0.177168... | [0.0598489418626, 0.051642... |\n",
      "| [-0.0311556719244, 0.04638... | [-0.029840817675, 0.019619... |\n",
      "| [0.0303869061172, 0.093584... | [-0.0100881885737, -0.0169... |\n",
      "| [-0.0143230063841, 0.16484... | [0.0459362156689, 0.019228... |\n",
      "| [-0.00646619638428, 0.2504... | [0.024628572166, 0.0366173... |\n",
      "| [0.0220851488411, 0.071671... | [0.017923463136, -0.005576... |\n",
      "| [0.0518963634968, 0.169802... | [0.0323830060661, 0.054194... |\n",
      "| [-0.0143787460402, 0.10910... | [0.0641739442945, 0.036201... |\n",
      "| [-0.01045820117, 0.0229523... | [0.00460766302422, -0.0302... |\n",
      "| [0.0284708533436, 0.118830... | [0.0294066257775, 0.080906... |\n",
      "| [0.0122381309047, 0.104049... | [-0.0190414022654, -0.0125... |\n",
      "| [0.0480362921953, 0.062961... | [-0.00886501371861, 0.0104... |\n",
      "| [0.0436516776681, 0.072595... | [0.0352588072419, 0.042373... |\n",
      "| [-0.0503196194768, 0.04594... | [0.0415826290846, -0.03701... |\n",
      "| [0.0916449427605, -0.02213... | [0.0552189722657, -0.07543... |\n",
      "| [-0.0750350356102, -0.0414... | [0.101602762938, 0.0025980... |\n",
      "| [0.00588338961825, 0.08294... | [0.0213478785008, -0.00591... |\n",
      "| [-0.0131844924763, -0.0782... | [-0.0135995578021, -0.0275... |\n",
      "| [0.0793243795633, 0.064031... | [-0.0129326600581, 0.03662... |\n",
      "| [0.073186725378, 0.0736938... | [0.0326673015952, 0.032761... |\n",
      "| [-0.0270522534847, 0.12759... | [0.00390715477988, 0.09263... |\n",
      "| [-0.0155414128676, 0.04842... | [0.0213050637394, -0.01483... |\n",
      "| [0.174384757876, 0.1072593... | [0.0306741334498, 0.045464... |\n",
      "| [0.00328088295646, 0.16074... | [-0.0041246493347, 0.10284... |\n",
      "| [0.0845542252064, 0.203567... | [0.0142774861306, -0.00368... |\n",
      "| [0.0773248225451, 0.119536... | [0.0695250928402, 0.058798... |\n",
      "| [0.103139393032, 0.0652249... | [0.0703589990735, 0.003542... |\n",
      "| [0.175064459443, 0.0350119... | [0.0370998345315, -0.03655... |\n",
      "| [0.00417830096558, 0.02810... | [0.0737527310848, -0.02552... |\n",
      "| [0.131624728441, 0.0257503... | [0.0753201395273, 0.023184... |\n",
      "| [-0.0420406758785, 0.05229... | [-0.0232750046998, 0.04286... |\n",
      "| [0.0508113875985, 0.117542... | [0.0250763110816, 0.091958... |\n",
      "| [0.0691236630082, 0.109834... | [0.0677447319031, 0.069585... |\n",
      "| [-0.00778823019937, 0.0187... | [-0.0133151141927, -0.0291... |\n",
      "| [0.0685815960169, 0.061062... | [0.0629624798894, 0.013761... |\n",
      "| [-0.000154273322551, 0.007... | [0.00149237900041, 0.00436... |\n",
      "| [-0.0212390720844, -0.0182... | [0.0057858619839, -0.02761... |\n",
      "| [0.0437247045338, 0.064359... | [-0.00407257443294, -0.006... |\n",
      "| [0.0158369466662, 0.041244... | [0.0703080743551, -0.02855... |\n",
      "| [-0.0549901127815, -0.0030... | [0.0248756818473, -0.01844... |\n",
      "| [-0.0266504604369, 0.18224... | [-0.105635017157, 0.001265... |\n",
      "| [0.0623384527862, 0.044097... | [0.0530513711274, 0.040593... |\n",
      "| [0.235054150224, 0.1198969... | [-0.00514694815502, -0.025... |\n",
      "| [-0.0025059315376, 0.05734... | [-0.0044950414449, -0.0262... |\n",
      "| [0.0663454830647, 0.091608... | [-0.0285049136728, 0.00809... |\n",
      "| [-0.0185634456575, 0.07476... | [-0.0300072599202, -0.0174... |\n",
      "| [-0.000957002223004, 0.181... | [0.0222951322794, 0.042250... |\n",
      "| [0.0662578120828, 0.237183... | [-0.00142123864498, 0.0729... |\n",
      "| [0.103024363518, 0.0374676... | [0.0407364889979, 0.068368... |\n",
      "| [0.0507886745036, 0.123285... | [0.0481149107218, 0.034885... |\n",
      "| [-0.0907578319311, 0.08496... | [0.00704089691862, -0.0160... |\n",
      "| [0.0840405672789, -0.00558... | [0.101242132485, -0.053950... |\n",
      "| [0.113215446472, 0.1174108... | [0.0747227370739, 0.021858... |\n",
      "| [-0.0238843690604, 0.04718... | [-0.0307603627443, 0.01737... |\n",
      "| [-0.0831575468183, 0.00854... | [0.00971748214215, -0.0237... |\n",
      "| [0.0187760740519, 0.064926... | [-0.0127134388313, 0.03830... |\n",
      "| [0.263503015041, 0.1652257... | [-0.00219742953777, 0.0603... |\n",
      "| [0.052870053798, 0.0654521... | [-0.0220456831157, 0.02201... |\n",
      "| [0.0596736930311, -0.02955... | [0.0218142662197, 0.027062... |\n",
      "| [-0.00188289827202, 0.0551... | [-0.00861202646047, -0.026... |\n",
      "| [0.00850433576852, -0.0006... | [-0.0275940503925, -0.0410... |\n",
      "| [0.0518941283226, 0.060649... | [0.00273105385713, 0.07511... |\n",
      "| [0.0886359885335, 0.103937... | [0.0580856911838, -0.00730... |\n",
      "| [0.0472794435918, 0.015224... | [0.0450987257063, -0.00010... |\n",
      "| [0.058459341526, 0.0991843... | [-0.0299288462847, 0.01104... |\n",
      "| [0.074644215405, 0.0499730... | [-0.00484723038971, -0.020... |\n",
      "| [0.0209227446467, -0.04627... | [0.0461331531405, 0.013282... |\n",
      "| [0.0325538106263, 0.073340... | [0.0308146178722, 0.009678... |\n",
      "| [0.0616770088673, 5.275011... | [0.0240939501673, 0.049472... |\n",
      "| [0.0500375851989, 0.090963... | [-0.0485868863761, 0.02504... |\n",
      "| [0.0931829363108, 0.112547... | [-0.0140143092722, -0.0065... |\n",
      "| [-0.0437455959618, 0.05246... | [0.0200654119253, -0.06197... |\n",
      "| [0.0321034826338, 0.093190... | [-0.0107393199578, 0.06969... |\n",
      "| [0.0531074367464, 0.102956... | [-0.0111953429878, 0.01064... |\n",
      "| [0.0129457106814, 0.065047... | [0.0325603894889, -0.01550... |\n",
      "| [0.0503038465977, -0.00746... | [-0.00675397366285, 0.0302... |\n",
      "| [0.101591981947, 0.0302219... | [0.0202788151801, 0.013079... |\n",
      "| [0.171718120575, 0.1946695... | [0.0542682446539, 0.012288... |\n",
      "| [0.0648992732167, 0.098322... | [0.053018566221, 0.0311130... |\n",
      "| [0.0498234443367, 0.029387... | [0.050448667258, -0.053990... |\n",
      "| [-0.0183761194348, 0.11454... | [-0.0411045029759, 0.00253... |\n",
      "| [0.00897005200386, 0.04433... | [-0.0288128778338, 0.04225... |\n",
      "| [0.0355261974037, 0.147986... | [-0.0206610504538, 0.03776... |\n",
      "| [-0.0439037382603, 0.09647... | [-0.00610015913844, 0.0300... |\n",
      "| [0.0432157628238, 0.070224... | [-0.019569190219, 0.055478... |\n",
      "| [-0.0324959680438, 0.07372... | [-0.0301002822816, -0.0207... |\n",
      "| [0.110994145274, 0.0254876... | [0.0270587392151, 0.010605... |\n",
      "| [-0.0205583386123, 0.07877... | [0.056233741343, 0.0177713... |\n",
      "| [0.0323954038322, 0.097407... | [0.0553332641721, 0.070624... |\n",
      "| [0.0132969841361, -0.04678... | [0.0373113751411, -0.02163... |\n",
      "| [-0.00226040394045, 0.0680... | [0.024067664519, -0.019047... |\n",
      "| [-0.104539565742, 0.039035... | [-0.0116850277409, -0.0573... |\n",
      "| [0.0411637350917, -0.00919... | [-0.0542517900467, -0.0137... |\n",
      "| [-0.0149617390707, 0.04207... | [-0.0122356461361, -0.0020... |\n",
      "| [0.0518604516983, 0.105440... | [0.0565864183009, 0.027268... |\n",
      "| [-0.0525519028306, -0.0013... | [-0.00128606916405, -0.027... |\n",
      "| [0.0514081269503, 0.031530... | [-0.0510751008987, 0.01704... |\n",
      "| [-0.00494099361822, 0.1614... | [0.00521725974977, 0.06532... |\n",
      "| [0.00316605088301, 0.07817... | [0.0104181589559, 0.028644... |\n",
      "| [0.0603635087609, 0.063667... | [-0.0613038502634, 0.03541... |\n",
      "| [-0.00547114759684, 0.0735... | [-0.0316779464483, -0.0725... |\n",
      "| [0.141875058413, 0.1736724... | [-0.0887039452791, 0.04546... |\n",
      "| [0.0936589241028, 0.093911... | [0.0578483715653, 0.009731... |\n",
      "| [0.0453187376261, 0.128458... | [0.0422668717802, 0.006571... |\n",
      "| [0.0560417063534, 0.099446... | [0.0192618221045, 0.049523... |\n",
      "| [0.0455684363842, 0.029624... | [-0.00994143914431, -0.000... |\n",
      "| [0.0304062124342, 0.059042... | [0.0288166534156, 0.020984... |\n",
      "| [-0.0133795076981, -0.0296... | [0.011882818304, -0.003713... |\n",
      "| [0.14974398911, 0.04252281... | [0.00865174178034, 0.02763... |\n",
      "| [0.0483835302293, 0.117633... | [0.000479517126223, 0.0874... |\n",
      "| [0.174140021205, 0.1213596... | [0.0215795859694, 0.024973... |\n",
      "| [-0.0237799976021, 0.00044... | [0.0360780321062, -0.01773... |\n",
      "| [0.077113352716, 0.1820751... | [0.0640112087131, 0.065192... |\n",
      "| [0.137782841921, 0.0623223... | [0.0357417650521, 0.026771... |\n",
      "| [0.0169391520321, 0.052961... | [-0.0327701233327, -0.0446... |\n",
      "| [0.0802255794406, 0.091806... | [-0.0162479504943, -0.0001... |\n",
      "| [0.0333656109869, -0.02003... | [-0.00307181174867, -0.001... |\n",
      "| [0.105021670461, 0.1078785... | [0.0151131702587, 0.016007... |\n",
      "| [-0.00153410155326, 0.0006... | [-0.0233567040414, -0.0063... |\n",
      "| [-0.0128785297275, 0.09652... | [-0.0370156131685, -0.0068... |\n",
      "| [0.0433857552707, 0.031648... | [-0.038009930402, -0.02454... |\n",
      "| [-0.0321924835443, -0.0021... | [-0.0103607736528, -0.0185... |\n",
      "| [-0.00640612049028, 0.0692... | [-0.0274871587753, -0.0586... |\n",
      "| [0.00177816743962, 0.18969... | [0.0731822922826, 0.060903... |\n",
      "| [-0.0581759139895, 0.05876... | [-0.0488933511078, -0.0312... |\n",
      "| [0.0431974753737, 0.117077... | [0.0189399253577, 0.000118... |\n",
      "| [0.11656871438, 0.05420286... | [0.0672555565834, -0.10002... |\n",
      "| [0.10481081903, 0.21219767... | [0.0392326824367, 0.111005... |\n",
      "| [0.0268802884966, 0.067653... | [-0.00944170728326, -0.008... |\n",
      "| [-0.0154745606706, 0.04889... | [-0.0550516918302, -0.0183... |\n",
      "| [0.0892716646194, 0.073100... | [0.0148160243407, 0.046606... |\n",
      "| [0.196049720049, 0.1424390... | [0.0195350982249, 0.046612... |\n",
      "| [-0.0178422685713, 0.08733... | [0.0856044963002, -0.01828... |\n",
      "| [0.0603833645582, 0.116743... | [-0.0155050400645, 0.05671... |\n",
      "| [0.122444331646, 0.0600198... | [0.0340263284743, 0.027989... |\n",
      "| [-0.0648352205753, 0.13741... | [0.00311822234653, -0.0157... |\n",
      "| [0.1615550071, 0.018064169... | [0.0349914617836, 0.055587... |\n",
      "| [0.0668874084949, 0.048048... | [0.0553619042039, -0.01670... |\n",
      "| [0.0428474396467, -0.02253... | [-0.0138289434835, 0.00431... |\n",
      "| [0.208593204618, 0.1185580... | [-0.0142252380028, 0.09190... |\n",
      "| [0.044590562582, 0.0743135... | [-0.0681652650237, 0.05088... |\n",
      "| [0.0567785017192, -0.01279... | [0.0641068741679, 0.051958... |\n",
      "| [0.122136227787, 0.1094264... | [-0.0200593266636, 0.03618... |\n",
      "| [0.0181143600494, 0.110086... | [0.0435840897262, -0.00082... |\n",
      "| [-0.0451164282858, 0.05529... | [-0.0183601956815, -0.0006... |\n",
      "| [0.0276134312153, 0.059122... | [-0.00729277031496, 0.0246... |\n",
      "| [-0.0312778577209, 0.04448... | [0.00432727951556, -0.0298... |\n",
      "| [0.0778756886721, 0.055540... | [0.00788033660501, 0.02254... |\n",
      "| [-0.0415904074907, 0.08989... | [0.00892613176256, 0.02878... |\n",
      "| [0.0530654042959, 0.085318... | [0.0417751260102, 0.000632... |\n",
      "| [0.0260953661054, 0.086301... | [0.0709105283022, -0.00182... |\n",
      "| [0.00948459841311, 0.08927... | [-0.0118443211541, 0.00465... |\n",
      "| [0.101273179054, 0.1101488... | [0.0422819256783, -0.00706... |\n",
      "| [0.177497968078, 0.1068360... | [0.116728186607, 0.0334416... |\n",
      "| [0.0437247045338, 0.064359... | [-0.00407257443294, -0.006... |\n",
      "| [0.0540275722742, 0.106455... | [0.0561769604683, 0.024468... |\n",
      "| [0.0375870801508, 0.081324... | [0.0512365289032, -0.00900... |\n",
      "| [-0.114986680448, -0.06845... | [0.0425749830902, -0.09662... |\n",
      "| [0.0488879643381, 0.134869... | [0.007343551144, 0.0448616... |\n",
      "| [0.0647272095084, 0.028872... | [0.0361915715039, 0.022584... |\n",
      "| [-0.0431897677481, 0.06175... | [-0.0112954014912, -0.0080... |\n",
      "| [0.0110326157883, 0.029682... | [-0.0417450629175, -0.0086... |\n",
      "| [0.0644556209445, 0.075734... | [-0.0137227065861, 0.00810... |\n",
      "| [-0.021823624149, 0.092762... | [-0.00452016573399, 0.0311... |\n",
      "| [0.106601521373, 0.1116313... | [-0.00101947784424, 0.0433... |\n",
      "| [-0.0481625013053, 0.04946... | [0.00909596215934, -0.0090... |\n",
      "| [0.014012189582, 0.1382170... | [0.00203017727472, 0.01362... |\n",
      "| [0.095000103116, 0.1124662... | [0.0390418097377, 0.021602... |\n",
      "| [0.0313499495387, 0.073952... | [0.0155854783952, 0.036693... |\n",
      "| [-0.00173416384496, 0.0547... | [-0.0342551209033, -0.0336... |\n",
      "| [0.0871964469552, 0.137587... | [0.0240393374115, 0.112139... |\n",
      "| [0.0800752192736, 0.090896... | [0.0036344784312, -0.01424... |\n",
      "| [0.0818875730038, 0.166805... | [-0.0100253988057, 0.03813... |\n",
      "| [-0.0758920460939, 0.02236... | [0.0376952029765, -0.05028... |\n",
      "| [-0.0393010862172, 0.05609... | [0.0183707922697, 0.011466... |\n",
      "| [-0.0235293805599, -0.0660... | [0.060052935034, -0.002676... |\n",
      "| [0.102900370955, 0.0131072... | [0.0342055559158, 0.017765... |\n",
      "| [0.0932590588927, 0.044439... | [0.0284174662083, -0.02033... |\n",
      "| [0.0185554232448, 0.074403... | [0.0148332798854, 0.006503... |\n",
      "| [0.0613225996494, 0.028406... | [0.0554547272623, 0.026587... |\n",
      "| [0.122411824763, 0.0853498... | [-0.00594295188785, 0.0315... |\n",
      "| [0.137628510594, 0.0741129... | [-0.0130964638665, 0.08315... |\n",
      "| [0.0190880410373, 0.022319... | [0.100115761161, -0.017336... |\n",
      "| [0.0157907176763, 0.046116... | [-0.00407236162573, -0.052... |\n",
      "| [-0.0377632901073, 0.07174... | [0.0545477457345, -0.04120... |\n",
      "| [0.0612567551434, 0.064115... | [-0.0299076139927, -0.0140... |\n",
      "| [0.0116760665551, 0.005463... | [0.00513440091163, -0.0386... |\n",
      "| [0.172584086657, 0.0877021... | [0.0985181033611, 0.002065... |\n",
      "| [0.0403106212616, -0.05638... | [-0.0167312566191, 0.02272... |\n",
      "| [0.0836670696735, 0.060634... | [-0.0598953962326, 0.03834... |\n",
      "| [0.0426398068666, 0.142882... | [0.0837756916881, 0.001725... |\n",
      "| [0.052663538605, 0.0662664... | [0.0493002869189, 0.040631... |\n",
      "| [0.124630481005, 0.0569229... | [-0.0691843107343, 0.04166... |\n",
      "| [0.168634474277, 0.0952187... | [-0.0105078537017, 0.04434... |\n",
      "| [0.0883897840977, 0.153132... | [-0.00243166973814, -0.030... |\n",
      "| [0.153582945466, 0.3225191... | [0.0370369441807, 0.111450... |\n",
      "| [0.0234102960676, 0.039028... | [0.0740306004882, 0.007114... |\n",
      "| [0.0582178980112, 0.125277... | [0.0524725541472, 0.022178... |\n",
      "| [0.0300195869058, 0.089650... | [-0.000652204442304, -0.02... |\n",
      "| [0.0926497653127, 0.120900... | [-0.0165883395821, 0.05871... |\n",
      "| [0.021073281765, 0.0892473... | [0.01005227305, -0.0133667... |\n",
      "| [0.0827114805579, 0.008380... | [-0.0875114277005, 0.04060... |\n",
      "| [0.0509054251015, 0.087103... | [0.0341784991324, 0.018843... |\n",
      "| [-0.0287373065948, 0.09273... | [0.0190460458398, -0.02378... |\n",
      "| [0.117946542799, 0.1473684... | [0.0694294497371, 0.021021... |\n",
      "| [0.094833932817, 0.0848840... | [0.0753655657172, 0.076798... |\n",
      "| [0.138402476907, 0.1771245... | [-0.00854543875903, 0.0662... |\n",
      "| [0.0682562142611, 0.160935... | [0.0125083969906, 0.043287... |\n",
      "| [0.203123196959, 0.2814905... | [0.046197026968, 0.1210389... |\n",
      "| [0.0392429977655, 0.087670... | [0.00908707454801, 0.00189... |\n",
      "| [0.0239020213485, 0.006962... | [0.0891171097755, -0.03693... |\n",
      "| [0.0792575404048, 0.080093... | [0.00143837661017, -0.0363... |\n",
      "| [0.0035289786756, 0.046557... | [-0.0319388695061, 0.01887... |\n",
      "| [-0.0354852899909, 0.08794... | [0.073413990438, 0.0221893... |\n",
      "| [0.00181150354911, 0.01415... | [0.0497913956642, -0.05330... |\n",
      "| [-0.0525634177029, -0.0224... | [-0.0330449156463, -0.0242... |\n",
      "| [0.0497761964798, -0.09919... | [0.0990597531199, -0.08496... |\n",
      "| [0.0480756126344, 0.014457... | [0.00483726756647, -0.0177... |\n",
      "| [0.0184063240886, 0.094536... | [0.055236928165, 0.0150754... |\n",
      "| [0.0891471281648, 0.090241... | [0.0454753153026, 0.023474... |\n",
      "| [0.0618069134653, -0.01182... | [-0.00128313712776, -0.036... |\n",
      "| [0.0804696977139, 0.093819... | [-0.0435653068125, -0.0278... |\n",
      "| [0.0247180089355, 0.040533... | [-0.0505799055099, 0.01572... |\n",
      "| [-0.0109498500824, 0.03527... | [-0.0183937009424, 0.01276... |\n",
      "| [0.0528987981379, 0.029608... | [-0.0558216795325, -0.0090... |\n",
      "| [0.0704052448273, 0.107790... | [0.0322675779462, 0.003244... |\n",
      "| [-0.0166002493352, -0.0013... | [0.047687292099, -0.021067... |\n",
      "| [0.0166347604245, 0.004755... | [0.00227470393293, -0.0030... |\n",
      "| [0.131260707974, 0.0767343... | [0.0104017360136, 0.052970... |\n",
      "| [-0.013603608124, 0.082429... | [0.0822674930096, 0.004826... |\n",
      "| [0.0577766373754, 0.048076... | [0.0492298305035, 0.013792... |\n",
      "| [-0.0232447851449, 0.08647... | [-0.0206695292145, -0.0374... |\n",
      "| [0.0609085597098, 0.083216... | [-0.067277289927, 0.035275... |\n",
      "| [0.0679745897651, 0.059331... | [0.0132677285001, 0.019099... |\n",
      "| [0.0437247045338, 0.064359... | [-0.00407257443294, -0.006... |\n",
      "| [0.00262748007663, 0.05161... | [0.0509683340788, -0.02272... |\n",
      "| [0.101352162659, 0.0701785... | [0.0149447219446, 0.021194... |\n",
      "| [0.110671296716, 0.0400661... | [0.0275962445885, 0.001807... |\n",
      "| [0.116028748453, 0.0727597... | [0.00948790647089, 0.07670... |\n",
      "| [0.0692282095551, 0.059342... | [0.0438730716705, 0.045620... |\n",
      "| [0.0174592547119, 0.070400... | [-0.00905013177544, 0.0298... |\n",
      "| [0.0256399419159, 0.063823... | [-0.00636622309685, -0.021... |\n",
      "| [0.0129826804623, 0.112033... | [0.030978480354, 0.0237624... |\n",
      "| [0.0624593123794, 0.017741... | [0.0666659921408, -0.03624... |\n",
      "| [-0.0167460571975, 0.08793... | [0.0119494730607, -0.05258... |\n",
      "| [-0.0305457916111, -0.0493... | [0.0506634637713, -0.00642... |\n",
      "| [0.0372588299215, 0.023332... | [-0.0178188458085, 0.00587... |\n",
      "| [6.22160732746e-05, 0.0413... | [0.0190802719444, 0.001399... |\n",
      "| [0.100154832006, 0.0789669... | [0.00932217109948, 0.09420... |\n",
      "| [0.0604301430285, 0.051405... | [-0.00255185062997, 0.0166... |\n",
      "| [0.0857540220022, 0.060313... | [-0.00933147966862, 0.0068... |\n",
      "| [0.0106094675139, 0.076702... | [-0.0147265950218, 0.00334... |\n",
      "| [-0.0529615394771, 0.07114... | [-0.0804396867752, -0.0322... |\n",
      "| [0.029411830008, 0.0459074... | [0.0503387786448, 0.058105... |\n",
      "| [-0.0167785380036, -0.0222... | [0.0171361789107, 0.009524... |\n",
      "| [0.0806383118033, 0.116287... | [0.0199668649584, 0.024789... |\n",
      "| [0.0706579461694, 0.090668... | [0.0533316843212, 0.057463... |\n",
      "| [0.0199421662837, 0.078946... | [0.00362404203042, 0.03253... |\n",
      "| [-0.0188282765448, 0.10677... | [0.0176767446101, 0.027502... |\n",
      "| [-0.0338302999735, 0.06088... | [0.0222006887197, -0.00343... |\n",
      "| [0.0517013520002, -0.05003... | [-0.00150931021199, 0.0121... |\n",
      "| [0.0959789901972, -0.03241... | [0.0511707663536, -0.01863... |\n",
      "| [0.0693496316671, 0.036573... | [0.0746544748545, 0.011155... |\n",
      "| [0.0426083989441, -0.06749... | [0.0391922220588, -0.00064... |\n",
      "| [0.0719070881605, 0.044699... | [0.022969385609, 0.0445268... |\n",
      "| [0.0480707585812, 0.077739... | [0.00760495755821, -0.0138... |\n",
      "| [0.0277377702296, 0.079710... | [0.0278336908668, 0.018088... |\n",
      "| [0.0585104748607, 0.114953... | [-0.0216794312, 0.08141122... |\n",
      "| [0.0508534088731, 0.044448... | [0.028136478737, -0.020331... |\n",
      "| [0.0203410126269, 0.006350... | [0.0108636636287, 0.020904... |\n",
      "| [-0.0692211911082, 0.00768... | [0.0390094183385, -0.04642... |\n",
      "| [-0.0155571931973, -0.0196... | [-0.0227041840553, -0.0391... |\n",
      "| [-0.0180279184133, 0.09147... | [0.0268535222858, -0.02838... |\n",
      "| [0.0748081877828, -0.02340... | [-0.0180096775293, -0.0160... |\n",
      "| [0.0088870190084, 0.164853... | [0.00670442264527, 0.00259... |\n",
      "| [0.0165090467781, 0.027178... | [-0.0194389149547, -0.0014... |\n",
      "| [0.0514496080577, 0.068344... | [-0.02717679739, -0.013380... |\n",
      "| [0.0649389326572, 0.024155... | [-0.0473420619965, -0.0307... |\n",
      "| [0.0267763976008, 0.131402... | [-0.00093530252343, 0.0679... |\n",
      "| [0.0224878974259, 0.081430... | [0.00821517501026, 0.04122... |\n",
      "| [0.14869864285, 0.06716740... | [0.0333040133119, 0.026955... |\n",
      "| [0.00171719118953, 0.00149... | [0.0368982665241, -0.01880... |\n",
      "| [-0.0327440686524, 0.05181... | [0.0171346347779, 0.019243... |\n",
      "| [-0.0442368499935, 0.08347... | [-0.0202379170805, 0.02145... |\n",
      "| [0.126907899976, 0.1496987... | [0.0338861756027, 0.030324... |\n",
      "| [0.0650382190943, 0.119332... | [0.0578114800155, 0.025036... |\n",
      "| [0.0315635688603, 0.108917... | [0.0797570496798, 0.003196... |\n",
      "| [0.0639897882938, 0.055840... | [0.0447811000049, -0.00986... |\n",
      "| [0.0275043379515, 0.025014... | [0.058697167784, -0.070787... |\n",
      "| [0.0271719451994, 0.080228... | [0.00702651077881, -0.0483... |\n",
      "| [-0.0134884649888, 0.06280... | [-0.0516417697072, -0.0508... |\n",
      "| [0.0286920350045, 0.043521... | [0.0121838059276, -0.02425... |\n",
      "| [-0.0196750722826, 0.09507... | [-0.0511043295264, 0.02312... |\n",
      "| [0.0754040926695, 0.103442... | [0.0353713817894, 0.034748... |\n",
      "| [-0.0553218238056, 0.07917... | [-0.0292761661112, -0.0042... |\n",
      "| [0.103902310133, 0.1220413... | [0.0692692101002, 0.004021... |\n",
      "| [0.0162600893527, 0.030125... | [0.0351380184293, -0.01456... |\n",
      "| [0.0442333370447, 0.066383... | [-0.0387597866356, 0.02165... |\n",
      "| [-0.00115737912711, 0.0188... | [0.0821608677506, -0.05610... |\n",
      "| [-0.013759046793, -0.00112... | [0.0356281138957, 0.006533... |\n",
      "| [0.0770942047238, 0.170530... | [0.0175900794566, 0.078458... |\n",
      "| [-0.0365747772157, 0.08611... | [0.0173239596188, -0.04929... |\n",
      "| [0.0925433561206, 0.146267... | [0.0542471781373, 0.073336... |\n",
      "| [0.0590083673596, 0.021622... | [0.0366796404123, 0.052143... |\n",
      "| [-0.00902257673442, 0.0517... | [-0.0624848455191, -0.0541... |\n",
      "| [0.111334100366, 0.1075414... | [0.0549444779754, 0.015306... |\n",
      "| [0.0214937608689, 0.127824... | [0.00264967908151, 0.02789... |\n",
      "| [-0.0119675425813, 0.04375... | [-0.0347418896854, 0.03778... |\n",
      "| [0.0422528274357, 0.120032... | [0.0212768577039, 0.016232... |\n",
      "| [0.0582867115736, 0.027911... | [0.0859107673168, -0.01452... |\n",
      "| [0.03804172948, 0.12182174... | [0.0103017957881, 0.011175... |\n",
      "| [0.0932871326804, 0.163043... | [0.0230908002704, 0.042980... |\n",
      "| [0.107055194676, 0.1113678... | [0.0113370735198, 0.032449... |\n",
      "| [-0.043220307678, 0.070298... | [0.00522994389758, -0.0054... |\n",
      "| [-0.0460698641837, 0.20314... | [0.084260456264, 0.0473414... |\n",
      "| [0.0111494278535, 0.022689... | [0.0465290285647, -0.01773... |\n",
      "| [-0.0305729880929, -0.0198... | [0.0285064484924, -0.01313... |\n",
      "| [0.00184123043437, 0.02578... | [-0.023462947458, 0.000558... |\n",
      "| [0.0131368208677, 0.107092... | [-0.0222453474998, 0.01788... |\n",
      "| [-0.00517827877775, 0.0547... | [0.0156391523778, 0.012765... |\n",
      "| [-0.0105718923733, 0.09821... | [0.00704620778561, -0.0302... |\n",
      "| [0.0817787572742, 0.063724... | [0.0469793267548, 0.053226... |\n",
      "| [-0.00386886787601, 0.0985... | [-0.0291484743357, 0.06505... |\n",
      "| [-0.0741172358394, 0.05681... | [0.00956315081567, -0.0699... |\n",
      "| [-0.0208878722042, 0.06463... | [0.00172466575168, 0.09350... |\n",
      "| [-0.0140186948702, 0.06106... | [-0.0205134879798, 0.04148... |\n",
      "| [0.142546772957, 0.1405675... | [0.0309797395021, 0.036742... |\n",
      "| [0.038436666131, 0.1056557... | [0.0300213117152, 0.039285... |\n",
      "| [0.1782271415, 0.122018203... | [0.0590271428227, 0.057646... |\n",
      "| [0.0447640307248, 0.073099... | [0.027361607179, -0.009855... |\n",
      "| [-0.0171159077436, 0.08116... | [-0.031878720969, -0.03418... |\n",
      "| [0.0231642816216, 0.031420... | [0.0290034487844, 0.028512... |\n",
      "| [0.0681915134192, 0.036807... | [0.0374748669565, 0.015928... |\n",
      "| [0.0547124147415, 0.030836... | [0.00162683427334, 0.01343... |\n",
      "| [0.142668545246, 0.0601631... | [0.00705134868622, 0.02861... |\n",
      "| [0.120679110289, 0.0126084... | [0.0581805892289, -0.04475... |\n",
      "| [0.106576345861, 0.0241133... | [0.0904041752219, 0.041654... |\n",
      "| [0.0484808124602, 0.068482... | [0.085774384439, -0.058348... |\n",
      "| [-0.0130580905825, 0.04585... | [-0.0602889843285, 0.01128... |\n",
      "| [-0.0246703624725, 0.03070... | [-0.0276669114828, -0.0097... |\n",
      "| [0.0188458971679, 0.077275... | [-0.0270995628089, 0.01820... |\n",
      "| [0.020288715139, 0.0218321... | [0.0265996456146, -0.03522... |\n",
      "| [0.23733265698, 0.11994747... | [0.109848916531, 0.0478077... |\n",
      "| [0.0437954179943, 0.129462... | [0.0845491886139, -0.00243... |\n",
      "| [0.0254928413779, 0.027751... | [0.0232441630214, 0.015152... |\n",
      "| [0.105254635215, 0.1135057... | [0.0271693356335, 0.025973... |\n",
      "| [0.0372450500727, 0.049292... | [-0.0302364788949, 0.02458... |\n",
      "| [0.0655604973435, 0.110706... | [0.0194276124239, -0.01435... |\n",
      "| [0.189355239272, 0.0210397... | [0.0316794067621, 0.082210... |\n",
      "| [0.0468485206366, 0.028748... | [0.00201466842555, 0.01842... |\n",
      "| [0.00651611760259, 0.00939... | [0.00599466916174, -0.0311... |\n",
      "| [-0.0580972358584, 0.06157... | [0.0156216667965, -0.02060... |\n",
      "| [0.0323404110968, 0.036425... | [-0.0366931930184, 0.00681... |\n",
      "| [0.0269457567483, 0.079238... | [0.0288881771266, 0.014393... |\n",
      "| [-0.0431084409356, 0.01634... | [0.0432060547173, 0.010105... |\n",
      "| [0.029062487185, 0.0991294... | [-0.00700972927734, 0.0151... |\n",
      "| [0.0220492780209, 0.104216... | [-0.0276319980621, 0.04088... |\n",
      "| [-0.0364178419113, 0.06022... | [0.0289323627949, -0.00492... |\n",
      "| [-0.0229136832058, 0.09860... | [-0.0226055197418, -0.0318... |\n",
      "| [0.181440323591, 0.1156137... | [-0.0238161347806, 0.06834... |\n",
      "| [0.0719315782189, 0.024990... | [0.10982657969, -0.0405264... |\n",
      "| [-0.0355778336525, 0.04186... | [0.0618589147925, -0.03680... |\n",
      "| [0.253272414207, 0.1857594... | [0.0383427925408, 0.052665... |\n",
      "| [0.0483263321221, 0.035730... | [0.00912639033049, 0.01283... |\n",
      "| [0.0503961071372, 0.216179... | [-0.00684135500342, 0.0412... |\n",
      "| [0.256688445807, 0.1297413... | [0.0592066049576, 0.033506... |\n",
      "| [-0.020570108667, 0.076366... | [-0.0307114180177, -0.0011... |\n",
      "| [-0.0409843586385, 0.07814... | [0.00924835074693, -0.0410... |\n",
      "| [0.046383086592, 0.0508694... | [0.0100276246667, 0.040993... |\n",
      "| [-0.0567377880216, 0.02194... | [-0.183470696211, -0.08494... |\n",
      "| [0.0656348317862, 0.060858... | [0.0177531782538, 0.036364... |\n",
      "| [-0.00565065257251, 0.1191... | [-0.0218611210585, -0.0159... |\n",
      "| [0.044370226562, 0.0687374... | [0.0022738778498, 0.009939... |\n",
      "| [0.112025305629, 0.0398146... | [-0.0137546220794, 0.01138... |\n",
      "| [0.0100347474217, -0.01390... | [-0.0204171556979, -0.0633... |\n",
      "| [0.0139073133469, 0.029376... | [0.0155304502696, 0.005273... |\n",
      "| [-0.0247345976532, 0.00598... | [0.0109910434112, 0.004083... |\n",
      "| [-0.0720487982035, -0.0123... | [-0.0504076480865, 0.03803... |\n",
      "| [0.0442374125123, 0.047933... | [0.02235250175, 0.00761648... |\n",
      "| [0.0376024693251, 0.029671... | [-0.0147352814674, -0.0329... |\n",
      "| [0.0691975876689, 0.035246... | [0.0699686259031, 0.005979... |\n",
      "| [0.0411960221827, 0.040997... | [0.0127729754895, -0.01157... |\n",
      "| [-0.0175913479179, 0.06431... | [-0.0119845308363, 0.04559... |\n",
      "| [0.0368453897536, 0.046886... | [0.0215943511575, 0.006654... |\n",
      "| [0.121451243758, 0.1763255... | [0.0188658349216, 0.099889... |\n",
      "| [0.0507524423301, 0.043675... | [-0.0125096589327, 0.03270... |\n",
      "| [0.0623476617038, 0.106214... | [0.0171817000955, 0.051313... |\n",
      "| [0.0429881773889, 0.108774... | [-0.00355628645048, 0.0187... |\n",
      "| [0.0546838305891, 0.051631... | [0.00290749385022, 0.02854... |\n",
      "| [-0.0140374964103, 0.03269... | [0.0374460108578, -0.01726... |\n",
      "| [0.0479905195534, 0.084415... | [0.00632679322734, 0.05525... |\n",
      "| [0.151404336095, 0.0551141... | [0.0673616603017, 0.022785... |\n",
      "| [0.0832295268774, 0.026729... | [0.0499790795147, -0.03146... |\n",
      "| [0.00423342036083, 0.12074... | [-0.0231319963932, -0.0070... |\n",
      "| [0.0230410825461, 0.050568... | [0.0419961847365, 0.021324... |\n",
      "| [0.0393901504576, 0.055810... | [0.000518300745171, 0.0191... |\n",
      "| [0.278255701065, 0.2377466... | [0.0709213465452, 0.123821... |\n",
      "| [0.0854376852512, 0.098355... | [-0.0020191797521, 0.02527... |\n",
      "| [-0.0336516164243, 0.00587... | [-0.00587164144963, -0.053... |\n",
      "| [0.133678600192, 0.0229565... | [0.00283640879206, 0.02832... |\n",
      "| [0.0407125763595, 0.112802... | [0.0284891370684, 0.020635... |\n",
      "| [0.152193054557, 0.1581606... | [0.027692688629, 0.0629693... |\n",
      "| [0.110068485141, 0.1066057... | [0.0595435686409, -0.00208... |\n",
      "| [0.0277981460094, 0.082949... | [0.0143583184108, 0.002219... |\n",
      "| [-0.043079175055, 0.023731... | [-0.00650037685409, -0.009... |\n",
      "| [0.0266303531826, 0.040030... | [0.0467811748385, -0.01056... |\n",
      "| [-0.0338421203196, 0.06124... | [0.0052558905445, -0.07550... |\n",
      "| [0.0628792122006, 0.079977... | [0.0515679717064, 0.025640... |\n",
      "| [-0.0298692528158, 0.11372... | [-0.0663732737303, 0.00041... |\n",
      "| [-0.0606990940869, 0.05118... | [-0.00826030783355, -0.010... |\n",
      "| [0.0346007980406, 0.091207... | [-0.00249299942516, 0.0461... |\n",
      "| [0.0149931004271, 0.025650... | [0.0232725795358, -0.05645... |\n",
      "| [0.0496926903725, 0.126474... | [-0.0473277121782, 0.00850... |\n",
      "| [-0.0528362244368, 0.09434... | [-0.00296134408563, -0.047... |\n",
      "| [0.182083696127, 0.0909206... | [-0.0146612627432, 0.06483... |\n",
      "| [0.031916346401, 0.0341676... | [-0.000227201322559, 0.005... |\n",
      "| [0.22789144516, 0.11375369... | [-0.0138675868511, 0.07920... |\n",
      "| [0.0853974223137, 0.099155... | [0.0190248321742, 0.029826... |\n",
      "| [0.142416730523, 0.0951555... | [0.0661310330033, 0.067691... |\n",
      "| [0.104364916682, 0.0951404... | [0.0591976568103, 0.046842... |\n",
      "| [0.097069889307, 0.0823451... | [0.0477025732398, -0.01817... |\n",
      "| [0.0387744382024, -0.00482... | [-0.058577299118, 0.028962... |\n",
      "| [0.170118018985, 0.2864087... | [0.0483926534653, 0.160536... |\n",
      "| [0.0595692507923, 0.075137... | [0.0512710921466, -0.01198... |\n",
      "| [-0.0761974826455, 0.02230... | [0.114908717573, -0.065721... |\n",
      "| [0.0160310342908, 0.028995... | [0.0714908242226, -0.02596... |\n",
      "| [0.131329327822, 0.1220314... | [0.0309511888772, -0.02641... |\n",
      "| [0.097748786211, -0.080807... | [0.16420198977, -0.0022845... |\n",
      "| [0.103258356452, 0.0893722... | [0.000979310134426, 0.0517... |\n",
      "| [0.0645870342851, 0.055093... | [-0.0355183072388, 0.04087... |\n",
      "| [0.0464176647365, 0.038591... | [0.0288037862629, 0.021480... |\n",
      "| [-0.00782150682062, 0.0738... | [-0.0303384345025, 0.00584... |\n",
      "| [0.0576065406203, 0.090564... | [0.0261179544032, 0.003431... |\n",
      "| [0.138366818428, 0.0019006... | [0.0303692519665, 0.048009... |\n",
      "| [0.0164623185992, 0.099907... | [-0.000236175386817, -0.01... |\n",
      "| [0.0398049913347, 0.115915... | [0.0384241454303, 0.019297... |\n",
      "| [-0.0377450734377, 0.03072... | [0.050404727459, -0.026968... |\n",
      "| [-0.0318747311831, -0.0086... | [0.0162358116359, -0.04667... |\n",
      "| [0.037866499275, 0.0859599... | [-0.0492239743471, 0.07498... |\n",
      "| [0.018193943426, -0.012353... | [0.0307260453701, -0.06264... |\n",
      "| [0.0365452431142, 0.095380... | [0.0459000766277, 0.043660... |\n",
      "| [0.178599476814, 0.1093299... | [0.0244976412505, 0.056415... |\n",
      "| [0.03322545439, 0.06109153... | [0.0343209281564, 0.010933... |\n",
      "| [0.131816014647, -0.006544... | [-0.0109882829711, 0.00949... |\n",
      "| [0.0238929465413, 0.089114... | [-0.0313051119447, 0.03256... |\n",
      "| [0.0294691380113, 0.129040... | [0.0734467431903, -0.02120... |\n",
      "| [-0.0353565774858, 0.04601... | [0.0115737421438, 0.018562... |\n",
      "| [-0.00229034153745, 0.0965... | [0.0231031943113, -0.01268... |\n",
      "| [0.128872007132, 0.1024060... | [0.0545618012547, 0.024445... |\n",
      "| [0.0321651361883, 0.008731... | [0.0694462358952, 0.001358... |\n",
      "| [0.0503897331655, 0.187785... | [0.00514779333025, -0.0056... |\n",
      "| [0.0485894270241, 0.083628... | [0.0138380117714, -0.03890... |\n",
      "| [0.0719255805016, 0.017850... | [0.0881654247642, -0.00717... |\n",
      "| [-0.0232180915773, 0.00264... | [-0.00575960380957, -0.073... |\n",
      "| [0.109681025147, 0.0398932... | [-0.0618996806443, 0.07401... |\n",
      "| [0.0295791625977, 0.033579... | [0.0549628436565, 0.021147... |\n",
      "| [-0.0273785609752, 0.08884... | [0.0577935129404, 0.013749... |\n",
      "| [0.0313066095114, 0.093897... | [0.051081430167, -0.006709... |\n",
      "| [0.232599452138, 0.0599501... | [0.115001045167, 0.0211863... |\n",
      "| [0.011147717014, 0.0761326... | [-0.0225763376802, -0.0074... |\n",
      "| [0.0984592437744, -5.57642... | [0.0263074282557, 0.025585... |\n",
      "| [-0.0384954847395, 0.09636... | [0.0375778004527, -0.01691... |\n",
      "| [0.058579094708, 0.0195002... | [-0.0237008593976, 0.00750... |\n",
      "| [-0.0351838730276, 0.08465... | [-0.0235094893724, 0.01002... |\n",
      "| [0.022513082251, 0.0805825... | [0.00300359725952, -0.0432... |\n",
      "| [0.116582468152, 0.0294813... | [-0.040273539722, 0.030051... |\n",
      "| [-0.00637410115451, 0.0480... | [-0.0368908941746, -0.0263... |\n",
      "| [0.0469841472805, 0.094234... | [0.0194673314691, 0.002426... |\n",
      "| [-0.0164669845253, 0.09200... | [0.00679021980613, 0.02490... |\n",
      "| [-0.00670928368345, 0.1101... | [-0.0240511372685, -0.0383... |\n",
      "| [-0.0485779009759, 0.04767... | [-0.0219440627843, 0.00534... |\n",
      "| [0.0693893060088, 0.033341... | [0.0724099427462, 0.010527... |\n",
      "| [0.0128754228354, 0.006654... | [0.0362232141197, 0.000257... |\n",
      "| [0.0731627345085, 0.013876... | [-0.0058237425983, -0.0122... |\n",
      "| [-0.0106311542913, 0.03760... | [-0.0218100398779, 0.01648... |\n",
      "| [0.0928929075599, 0.031222... | [0.0259793940932, 0.034673... |\n",
      "| [-0.0133541459218, 0.01738... | [0.0619757138193, -0.01112... |\n",
      "| [0.0990307182074, 0.069225... | [-0.00793315377086, 0.0206... |\n",
      "| [-0.0234485268593, 0.08433... | [0.0103686284274, 0.008175... |\n",
      "| [0.0291250534356, 0.089747... | [-0.000672269961797, 0.038... |\n",
      "| [0.0588670149446, 0.036775... | [0.0491024181247, 0.015779... |\n",
      "| [0.0420382395387, 0.067160... | [0.0205023139715, -0.00205... |\n",
      "| [-0.0294786263257, 0.09472... | [-0.00292517570779, -0.008... |\n",
      "| [0.0995655208826, 0.060328... | [0.0567389652133, -0.02728... |\n",
      "| [0.0904304310679, 0.019737... | [0.031735021621, 0.0457031... |\n",
      "| [0.0210570972413, 0.093274... | [0.0560157038271, -0.02102... |\n",
      "| [-0.00771455047652, 0.0184... | [-0.086839556694, 0.006375... |\n",
      "| [0.119196824729, 0.1146491... | [-0.0138780390844, -0.0255... |\n",
      "| [0.029155485332, 0.0797769... | [0.0527277588844, 0.007319... |\n",
      "| [-0.0913282334805, 0.03685... | [-0.0589529834688, -0.0001... |\n",
      "| [0.00076906819595, 0.02283... | [-0.0234447550029, -0.0150... |\n",
      "| [0.0400742515922, 0.032001... | [-0.00793973822147, -0.021... |\n",
      "| [0.0697442665696, 0.123781... | [-0.0104590849951, 0.04963... |\n",
      "| [-0.0768226459622, 0.07172... | [-0.0288934726268, -8.2687... |\n",
      "| [-0.0100112324581, 0.07135... | [-0.0293677989393, 0.03142... |\n",
      "| [0.0142878824845, 0.118373... | [-0.0148534439504, 0.04372... |\n",
      "| [-0.00865919142962, 0.0731... | [0.0265462193638, 0.016775... |\n",
      "| [0.0241157915443, 0.008164... | [0.0697419792414, 0.005878... |\n",
      "| [-0.011508885771, 0.078613... | [0.0186582822353, -0.00168... |\n",
      "| [-0.00389872584492, 0.0885... | [0.0461454205215, 0.003278... |\n",
      "| [0.0514305979013, 0.100373... | [-0.00668626837432, 0.0480... |\n",
      "| [0.128307148814, 0.0281662... | [-0.0422210767865, 0.03611... |\n",
      "| [0.141656026244, 0.0974344... | [-0.0378979071975, 0.06060... |\n",
      "| [0.0817782506347, 0.155719... | [-0.0171022340655, 0.10099... |\n",
      "| [0.112014889717, 0.1135526... | [-0.00897436402738, 0.0696... |\n",
      "| [0.0153799587861, 0.039296... | [0.0146997580305, 0.023419... |\n",
      "| [-0.0608869865537, 0.05959... | [-0.00190459564328, 0.0099... |\n",
      "| [0.0484708659351, 0.027821... | [0.0197695381939, -0.02585... |\n",
      "| [0.0599369481206, 0.054827... | [-0.0105667822063, -0.0457... |\n",
      "| [-0.0261372085661, 0.00665... | [-0.0551918372512, -0.0384... |\n",
      "| [0.0368472374976, 0.066390... | [0.0113249756396, 0.041125... |\n",
      "| [0.0268394947052, 0.039914... | [-0.071197964251, -0.02558... |\n",
      "| [0.0583782568574, 0.103732... | [0.00298006599769, -0.0042... |\n",
      "| [-0.0437412373722, 0.10429... | [0.00145726592746, -0.0583... |\n",
      "| [0.0324620679021, 0.057097... | [-0.00402141036466, 0.0339... |\n",
      "| [0.0789849683642, 0.065992... | [-0.0413914434612, 0.09465... |\n",
      "| [0.0153371514753, 0.091351... | [0.01797234267, 0.00805623... |\n",
      "| [-0.131107032299, 0.043795... | [0.0247119534761, -0.03013... |\n",
      "| [-0.0350936837494, 0.04108... | [-0.0163157526404, -0.0175... |\n",
      "| [0.114758946002, 0.0862966... | [0.0463308393955, 0.046816... |\n",
      "| [0.0404588244855, 0.106068... | [-0.00789404287934, 0.0362... |\n",
      "| [-0.0185749810189, 0.11359... | [0.00563263706863, 0.01883... |\n",
      "| [0.133136808872, 0.2002526... | [0.0480489805341, 0.032980... |\n",
      "| [0.0641282573342, 0.132511... | [-0.0227018259466, -0.0223... |\n",
      "| [-0.0146408602595, 0.02051... | [0.00699172262102, -0.0074... |\n",
      "| [-0.0137573862448, 0.07796... | [0.0909997224808, -0.03130... |\n",
      "| [0.0460635237396, 0.067466... | [-0.0338946394622, -0.0303... |\n",
      "| [-0.0558912791312, -0.0140... | [0.00466145621613, -0.0446... |\n",
      "| [0.125373318791, 0.0988184... | [0.0139364935458, 0.040051... |\n",
      "| [0.052924092859, 0.0453764... | [0.00897784996778, -0.0206... |\n",
      "| [0.030087871477, 0.0663618... | [-0.0406517349184, 0.02585... |\n",
      "| [0.0672618299723, 0.112015... | [-0.00660716509447, 0.0373... |\n",
      "| [-0.0748715400696, 0.06995... | [0.0230380613357, 0.064641... |\n",
      "| [0.070688970387, 0.1288671... | [0.0227542743087, 0.057629... |\n",
      "| [-0.019799599424, 0.043861... | [-0.00218222220428, -0.002... |\n",
      "| [0.107802212238, 0.0738905... | [0.0412335917354, 0.009415... |\n",
      "| [0.085533246398, 0.0890824... | [0.0635417252779, 0.029480... |\n",
      "| [0.079356148839, 0.1400112... | [0.0157958287746, 0.071362... |\n",
      "| [0.0490830652416, 0.022467... | [0.0541276708245, -0.00012... |\n",
      "| [0.042725097388, -0.024384... | [0.0115897133946, -0.01529... |\n",
      "| [0.0739351883531, 0.165973... | [0.0497342050076, 0.055963... |\n",
      "| [-0.0112781710923, 0.08306... | [0.00962467491627, 0.01970... |\n",
      "| [-0.094231531024, 0.062969... | [-0.0109413927421, -0.0602... |\n",
      "| [0.0438779778779, -0.05902... | [-0.0567442402244, 0.01029... |\n",
      "| [0.0997578129172, 0.084697... | [-0.021618032828, 0.034148... |\n",
      "| [0.0341271050274, 0.026527... | [0.0376417897642, -0.01841... |\n",
      "| [0.0281861238182, 0.113974... | [-0.042151093483, 0.009133... |\n",
      "| [0.0527342148125, 0.033846... | [-0.0747635960579, 0.02616... |\n",
      "| [-0.0420493148267, 0.03409... | [-0.011069778353, 0.006666... |\n",
      "| [0.269780427217, 0.2302953... | [0.0677933692932, 0.023225... |\n",
      "| [0.070742495358, 0.0500916... | [0.00547484494746, 0.01743... |\n",
      "| [-0.0572060570121, -0.0020... | [-0.0526635125279, 0.00276... |\n",
      "| [-0.0225013978779, 0.08574... | [-0.0073674977757, 0.04939... |\n",
      "| [0.075262337923, 0.0735146... | [-0.00183354818728, -0.015... |\n",
      "| [0.276262849569, 0.2140243... | [0.0502260364592, 0.092006... |\n",
      "| [0.0450349524617, 0.066365... | [-0.0159925408661, 0.01042... |\n",
      "| [0.0468730852008, 0.085605... | [-0.029800131917, 0.032983... |\n",
      "| [-0.068486019969, 0.001186... | [0.0241468250751, 0.001179... |\n",
      "| [0.26112189889, 0.17710731... | [0.0299997907132, 0.086615... |\n",
      "| [0.101885393262, 0.0764439... | [0.0541737377644, 0.043103... |\n",
      "| [0.0881542786956, 0.165512... | [-0.0483914427459, 0.06708... |\n",
      "| [0.189423978329, 0.1153573... | [-0.0180952847004, 0.07939... |\n",
      "| [0.0690027028322, 0.078873... | [0.0678380578756, 0.021629... |\n",
      "| [0.0228099953383, 0.039632... | [0.0924903005362, -0.02134... |\n",
      "| [0.078876376152, 0.0890281... | [0.0202835164964, 0.037382... |\n",
      "| [-0.0029404386878, 0.11497... | [-0.00791893806309, 0.0521... |\n",
      "| [-0.102661550045, 0.015265... | [-0.0140068056062, -0.0523... |\n",
      "| [-0.0155898733065, 0.17371... | [-0.0675134137273, -0.0126... |\n",
      "| [0.0322144515812, 0.044269... | [-0.033825468272, 0.005261... |\n",
      "| [0.12021625042, 0.04693064... | [-0.0166013035923, 0.05122... |\n",
      "| [-0.0279331747442, 0.02058... | [-0.0374183952808, 0.00430... |\n",
      "| [0.065556704998, -0.035063... | [0.0490441322327, -0.02008... |\n",
      "| [0.0673599243164, 0.125571... | [0.0587554760277, 0.008186... |\n",
      "| [0.0100347734988, 0.067201... | [-0.0216387268156, 0.02036... |\n",
      "| [0.0263483989984, 0.022380... | [-0.0110265826806, -0.0030... |\n",
      "| [0.0845693722367, 0.083014... | [0.0346181057394, -1.30939... |\n",
      "| [0.0846326053143, 0.095446... | [0.0576641820371, -0.06722... |\n",
      "| [0.119198486209, 0.0300659... | [-0.0659021884203, 0.01380... |\n",
      "| [0.0645270124078, 0.091076... | [0.0493382327259, 0.046018... |\n",
      "| [-0.0550505593419, 0.05498... | [0.0410462059081, -0.01846... |\n",
      "| [0.0534701682627, 0.069409... | [0.0701688602567, -0.04367... |\n",
      "| [0.0560322701931, 0.002361... | [0.0332709290087, 0.001467... |\n",
      "| [0.0757448524237, 0.076494... | [0.0291196983308, -0.02818... |\n",
      "| [0.0495177656412, 0.081326... | [0.0429564863443, 0.030182... |\n",
      "| [0.0938127413392, 0.090878... | [-0.00723570724949, 0.0356... |\n",
      "| [0.0998894944787, 0.073182... | [0.00074927759124, 0.00617... |\n",
      "| [-0.0167981740087, 0.05501... | [0.0239664539695, 0.005812... |\n",
      "| [-0.0083148824051, 0.01642... | [0.0370779521763, -0.01895... |\n",
      "| [0.147549256682, 0.0946960... | [-0.01014402695, 0.0262963... |\n",
      "| [0.130935296416, 0.1523916... | [0.0388070121408, 0.060740... |\n",
      "| [-0.000952691363636, -0.00... | [0.0371221266687, -0.01720... |\n",
      "| [0.14442974329, 0.15555521... | [0.0546111427248, 0.095605... |\n",
      "| [0.156339764595, 0.0465282... | [0.0015719224466, 0.056247... |\n",
      "| [-0.0554808489978, 0.00794... | [-0.0329416021705, 0.01678... |\n",
      "| [0.0576994717121, 0.137975... | [-0.0486670210958, 0.03879... |\n",
      "| [0.071002073586, -0.009599... | [0.0371520668268, -0.00152... |\n",
      "| [0.0446758717299, 0.073639... | [-0.0217710044235, 0.00435... |\n",
      "| [0.0613289065659, 0.062469... | [-0.0284265391529, 0.01872... |\n",
      "| [0.0378072932363, 0.037852... | [0.0427126325667, -0.00767... |\n",
      "| [-0.0356951504946, 0.06373... | [0.00286894873716, 0.02784... |\n",
      "| [0.02529367432, 0.02915084... | [0.0228542685509, -0.00390... |\n",
      "| [-0.000882479769643, 0.077... | [0.0238065756857, -0.03364... |\n",
      "| [0.101968064904, 0.1089990... | [0.0924602150917, 0.043335... |\n",
      "| [-0.0165160540491, 0.09484... | [0.0157639924437, -0.02140... |\n",
      "| [0.0167466662824, 0.042832... | [-0.0102448994294, -0.0039... |\n",
      "| [0.027566505596, 0.0731399... | [0.0495318211615, 0.044754... |\n",
      "| [0.13213005662, 0.12521843... | [-0.0338820256293, 0.05065... |\n",
      "| [0.0479905195534, 0.084415... | [0.00632679322734, 0.05525... |\n",
      "| [-0.0662141814828, -0.0066... | [-0.0511213243008, -0.0779... |\n",
      "| [0.0575183555484, 0.042669... | [0.0306185260415, -0.00325... |\n",
      "| [0.00998246017843, 0.04900... | [-0.0849937945604, -0.0340... |\n",
      "| [0.0345247276127, 0.035591... | [-0.000227201322559, 0.005... |\n",
      "| [-0.0568756051362, -0.0831... | [0.0304444022477, -0.03233... |\n",
      "| [-0.00142921286169, 0.0509... | [0.0374503955245, -0.01933... |\n",
      "| [-0.0438427142799, 0.07209... | [-0.0867970064282, 0.01113... |\n",
      "| [0.0603530406952, 0.132776... | [0.0535398907959, 0.038591... |\n",
      "| [0.0780296474695, 0.076122... | [0.0254044756293, 0.053439... |\n",
      "| [-0.0252823233604, 0.08965... | [0.0220287814736, -0.02579... |\n",
      "| [-0.0152964890003, 0.10009... | [-0.0159289576113, 0.00592... |\n",
      "| [0.0376659631729, 0.113763... | [-0.00135349354241, 0.0409... |\n",
      "| [0.0177041869611, -0.00990... | [-0.0760627835989, 0.00632... |\n",
      "| [0.063259139657, 0.1109734... | [-0.0045229694806, 0.00953... |\n",
      "| [-0.0247137937695, -0.0354... | [0.0959776490927, -0.08288... |\n",
      "| [0.0521004907787, 0.059835... | [-0.00950448215008, 0.0670... |\n",
      "| [0.110452823341, 0.0172335... | [-0.0400487110019, 0.05240... |\n",
      "| [0.0585348457098, 0.050177... | [0.0317208766937, 0.028884... |\n",
      "| [0.00798924732953, 0.07664... | [0.0210904590786, 0.007359... |\n",
      "| [0.0114550208673, -0.01551... | [0.0490491390228, -0.01794... |\n",
      "| [0.0338414125144, 0.104092... | [0.0944335386157, 0.023197... |\n",
      "| [0.00431797793135, -0.0004... | [-0.0197835359722, -0.0381... |\n",
      "| [0.152210816741, 0.0487837... | [-0.00596026657149, 0.0656... |\n",
      "| [0.0583267956972, 0.112998... | [0.078522130847, -0.003616... |\n",
      "| [0.0584718175232, 0.056171... | [0.0527113750577, 0.002901... |\n",
      "| [-0.0282891038805, 0.04822... | [-0.0231629647315, -0.0408... |\n",
      "| [-0.10560259223, 0.0564698... | [0.041501224041, 0.0208567... |\n",
      "| [-0.00649376399815, -0.016... | [0.00745747238398, -0.0311... |\n",
      "| [0.00873560458422, -0.0484... | [0.0372924096882, -0.01564... |\n",
      "| [0.206397086382, 0.0180362... | [0.102683186531, -0.008693... |\n",
      "| [-0.0398411825299, 0.01752... | [-0.0144516211003, -0.0334... |\n",
      "| [0.0258925780654, 0.044915... | [0.0332248955965, 0.013608... |\n",
      "| [-0.053700145334, 0.072326... | [-0.0175373256207, -0.0151... |\n",
      "| [0.0179439652711, 0.086441... | [-0.0432644821703, 0.02315... |\n",
      "| [0.0263896081597, 0.065718... | [-0.0497814826667, 0.01624... |\n",
      "| [0.13616746664, 0.16203540... | [0.0114498035982, 0.040046... |\n",
      "| [0.062711134553, 0.1113731... | [0.00964900758117, 0.04181... |\n",
      "| [0.0974700972438, 0.058504... | [0.0484465248883, 0.012313... |\n",
      "| [0.0347744114697, 0.035364... | [-0.0210007335991, -0.0540... |\n",
      "| [0.061628498137, 0.1374479... | [0.0337418392301, -0.00871... |\n",
      "| [0.0723956823349, 0.045558... | [0.0635035410523, 0.029225... |\n",
      "| [0.0233537629247, 0.033840... | [0.0618151500821, 0.005196... |\n",
      "| [0.0553608350456, 0.086658... | [-0.0146535960957, -0.0504... |\n",
      "| [0.00219234428369, 0.08423... | [0.0158818252385, -0.07299... |\n",
      "| [0.138877287507, 0.0439831... | [0.0449803806841, -0.00167... |\n",
      "| [-0.0138642592356, 0.00819... | [0.0341561250389, -0.00382... |\n",
      "| [-0.00118224648759, 0.0975... | [-0.00585446273908, 0.0197... |\n",
      "| [0.00441330065951, 0.05180... | [-0.00776668125764, -0.009... |\n",
      "| [0.0324900858104, 0.082447... | [0.0318940058351, 0.015227... |\n",
      "| [0.0832063034177, 0.081242... | [0.0158008430153, 0.035038... |\n",
      "| [0.00845697987825, 0.05841... | [-0.0150337480009, 0.04378... |\n",
      "| [0.00428867945448, 0.11620... | [0.035238776356, -0.014263... |\n",
      "| [0.103793784976, 0.1294887... | [0.0181250683963, 0.055293... |\n",
      "| [0.178262174129, 0.0419999... | [0.0220133047551, 0.087315... |\n",
      "| [-0.0733344182372, 0.10010... | [-0.000716846028809, 0.019... |\n",
      "| [0.0681267678738, 0.037257... | [0.0102644572034, 0.014817... |\n",
      "| [0.079569272697, 0.0948138... | [0.0331470817327, 0.039362... |\n",
      "| [0.00108763342723, 0.05361... | [0.00748814083636, 0.04011... |\n",
      "| [0.0273020844907, 0.052613... | [-0.00258196890354, 0.0455... |\n",
      "| [0.0281957387924, 0.062825... | [-0.0213223919272, -0.0067... |\n",
      "| [0.0133754126728, 0.107725... | [-0.0230911262333, -0.0009... |\n",
      "| [0.016034565866, 0.1309392... | [-0.0192066673189, 0.06024... |\n",
      "| [0.0607957057655, 0.073811... | [-0.0502763427794, -0.0031... |\n",
      "| [0.0460596866906, 0.107667... | [0.017482381314, 0.0128812... |\n",
      "| [0.0208847317845, 0.078268... | [-0.0178373064846, 0.00804... |\n",
      "| [-0.0021357210353, 0.02815... | [-0.0260737761855, 0.01897... |\n",
      "| [0.00342526542954, 0.08191... | [0.030258577317, 0.0467128... |\n",
      "| [0.0394759736955, 0.034551... | [0.0234773084521, -0.02842... |\n",
      "| [-0.036595068872, 0.075270... | [-0.0079728718847, 0.00341... |\n",
      "| [0.0307216327637, 0.099150... | [-0.0778722018003, 0.02746... |\n",
      "| [0.0623651370406, 0.093367... | [0.000483944138978, -0.011... |\n",
      "| [0.00208124518394, 0.04212... | [-0.00865605380386, 0.0025... |\n",
      "| [0.189733818173, 0.1310305... | [0.0319829285145, 0.040756... |\n",
      "| [-0.0171159077436, 0.08116... | [-0.031878720969, -0.03418... |\n",
      "| [0.00821876805276, -0.0509... | [0.0536484643817, 0.008710... |\n",
      "| [0.054131668061, 0.0420901... | [0.0531202927232, 0.015702... |\n",
      "| [-0.0412458330393, 0.00071... | [0.0473480075598, -0.05464... |\n",
      "| [0.0939453616738, 0.137513... | [0.0464114770293, 0.000660... |\n",
      "| [0.0482688620687, 0.038528... | [0.0296761523932, -0.00116... |\n",
      "| [0.099974155426, 0.0684009... | [0.0446019507945, 0.039242... |\n",
      "| [0.0212446302176, -0.00331... | [0.0188172552735, -0.03532... |\n",
      "| [0.0845124796033, 0.120834... | [0.0780698210001, 0.020970... |\n",
      "| [0.0929131880403, 0.053386... | [0.00954168476164, 0.06381... |\n",
      "| [0.176825240254, 0.0776505... | [0.0547990016639, 0.042548... |\n",
      "| [-0.0394118800759, 0.04988... | [-0.0456630848348, -0.0312... |\n",
      "| [0.0731997862458, 0.125294... | [0.012358058244, 0.0372268... |\n",
      "| [0.0641472265124, 0.077863... | [0.0413197763264, 0.022562... |\n",
      "| [-0.0198914539069, 0.06488... | [-0.00444375397637, 0.0060... |\n",
      "| [0.0810696929693, 0.107750... | [-0.0319334603846, 0.01744... |\n",
      "| [0.0852726399899, 0.069873... | [0.0500499270856, -0.00062... |\n",
      "| [0.0555801689625, 0.046190... | [0.044501375407, -0.017507... |\n",
      "| [0.114857554436, 0.0439830... | [-0.0321421585977, 0.03318... |\n",
      "| [0.047439981252, 0.0110655... | [0.0486381910741, -0.08612... |\n",
      "| [0.08515778929, 0.05686386... | [0.0486193969846, 0.006855... |\n",
      "| [0.122938483953, 0.0615836... | [0.0427666604519, -0.05875... |\n",
      "| [-0.00174767151475, 0.0264... | [0.0536313690245, 0.008179... |\n",
      "| [0.123921357095, 0.0799748... | [0.0181283690035, -0.03129... |\n",
      "| [-0.0731681138277, 0.07358... | [-0.00818464532495, -0.022... |\n",
      "| [0.0862551927567, 0.031484... | [-0.00500899041072, 0.0430... |\n",
      "| [0.0436238832772, 0.056890... | [0.0133397504687, 0.031333... |\n",
      "| [0.103760227561, 0.0890511... | [0.0446854457259, 0.030645... |\n",
      "| [0.0506931170821, -0.00429... | [-0.0328357703984, 0.03796... |\n",
      "| [0.052463453263, 0.0633114... | [0.0689962878823, 0.036683... |\n",
      "| [0.032948654145, 0.1090930... | [0.0550295710564, -0.02390... |\n",
      "| [0.0498381182551, -0.00161... | [0.0203653778881, 0.039641... |\n",
      "| [0.0581900477409, 0.085913... | [0.00525754550472, 0.07104... |\n",
      "| [0.0483338013291, -0.03831... | [0.0302180312574, 0.045445... |\n",
      "| [0.19468152523, 0.10022366... | [0.0433603674173, 0.006613... |\n",
      "| [0.030087871477, 0.0663618... | [-0.0406517349184, 0.02585... |\n",
      "| [-0.0061040725559, 0.04261... | [-0.0421327501535, -0.0146... |\n",
      "| [0.0590810552239, 0.055673... | [0.0521464720368, 0.023541... |\n",
      "| [-0.00640534237027, 0.0337... | [0.0123981600627, 0.009151... |\n",
      "| [0.0529075413942, 0.063145... | [-0.00903137214482, 0.0201... |\n",
      "| [0.0773460119963, 0.060782... | [0.0403810217977, 0.015656... |\n",
      "| [-0.0436978116632, 0.14197... | [-0.00217112596147, 0.0012... |\n",
      "| [-0.0684494003654, -0.0361... | [-0.0277675911784, -0.0381... |\n",
      "| [0.0456244572997, 0.050578... | [0.06026571244, 0.01012444... |\n",
      "| [0.108541287482, 0.0285178... | [0.0873077362776, 0.032748... |\n",
      "| [0.0665369927883, 0.092123... | [0.0555420666933, 0.066665... |\n",
      "| [0.0181509554386, 0.069540... | [-0.0435072779655, 0.01953... |\n",
      "| [0.177610054612, 0.1200236... | [0.026024216786, 0.0810801... |\n",
      "| [0.0770140662789, 0.074474... | [0.00565481092781, 0.00025... |\n",
      "| [0.0260162707418, 0.020402... | [0.0193546433002, -0.02456... |\n",
      "| [0.0828096494079, 0.033842... | [0.0742293894291, -0.00407... |\n",
      "| [0.0654848441482, 0.071521... | [0.00769013119861, 0.03069... |\n",
      "| [0.0790000334382, 0.098519... | [0.0452466011047, 0.015973... |\n",
      "| [0.0217835716903, -0.06209... | [-0.0264808293432, -0.0772... |\n",
      "| [0.257058680058, 0.1734028... | [-0.0353168696165, 0.01235... |\n",
      "| [0.0360666140914, 0.053802... | [-0.0354126319289, -0.0217... |\n",
      "| [0.129494249821, 0.1314133... | [0.0204472616315, 0.018179... |\n",
      "| [0.104313224554, 0.0680052... | [0.0328709073365, -0.00841... |\n",
      "| [0.080027654767, 0.0980449... | [0.0686409026384, 0.007103... |\n",
      "| [0.0362640991807, 0.089246... | [0.0197359640151, 0.048354... |\n",
      "| [0.0437247045338, 0.064359... | [-0.00407257443294, -0.006... |\n",
      "| [0.0165628734976, 0.052993... | [-0.035265929997, 0.000685... |\n",
      "| [0.0783189982176, 0.186714... | [-0.0035182274878, 0.02538... |\n",
      "| [0.0783971920609, 0.007726... | [-0.0315275788307, 0.01195... |\n",
      "| [-0.00734286243096, 0.0102... | [-0.0211412664503, -0.0348... |\n",
      "| [0.0320649296045, 0.098274... | [0.00393065623939, 0.01560... |\n",
      "| [0.0231003146619, 0.027060... | [-0.00912196841091, 0.0187... |\n",
      "| [0.061337698251, 0.0646897... | [0.0569978952408, 0.014326... |\n",
      "| [0.177777647972, 0.1182624... | [1.7784002921e-05, 0.05723... |\n",
      "| [0.0114647429436, -0.01325... | [-0.125244140625, 0.018470... |\n",
      "| [0.0772625952959, 0.030778... | [-0.00069974642247, 0.0077... |\n",
      "| [0.107361339033, 0.0268875... | [0.054693736136, -0.048803... |\n",
      "| [0.0821299776435, 0.127088... | [0.0267417691648, 0.056892... |\n",
      "| [0.0437756367028, 0.202393... | [0.0353579521179, 0.053070... |\n",
      "| [-0.0289841406047, 0.04583... | [-0.0178464017808, -0.0174... |\n",
      "| [0.170354142785, -0.008433... | [0.0677064061165, 0.017760... |\n",
      "| [0.0931188315153, 0.047489... | [0.0111848218367, 0.004913... |\n",
      "| [0.0540797002614, 0.057224... | [0.012409992516, 0.0328778... |\n",
      "| [0.056697268039, 0.0634534... | [0.0252840127796, -0.01449... |\n",
      "| [0.0223683342338, 0.028702... | [0.00268239760771, 0.01565... |\n",
      "| [0.124815039337, 0.0822340... | [-0.0516802147031, 0.04238... |\n",
      "| [0.0233369618654, 0.077104... | [0.011217141524, 0.0540800... |\n",
      "| [0.0293927770108, 0.098087... | [0.0384873338044, 0.009284... |\n",
      "| [-0.0460960231721, 0.01758... | [0.0124351233244, -0.02104... |\n",
      "| [0.0130694610998, -0.06297... | [0.0236506257206, -0.01917... |\n",
      "| [0.050425581634, 0.1475542... | [-0.00629905425012, -0.003... |\n",
      "| [0.0430841855705, 0.016804... | [0.0279547218233, 0.006556... |\n",
      "| [0.0930114537477, 0.097354... | [-0.0516582801938, 0.11247... |\n",
      "| [-0.00958735961467, -0.018... | [0.0123086320236, -0.04260... |\n",
      "| [-0.0583833530545, 0.05746... | [-0.0341776870191, -0.0116... |\n",
      "| [-0.126237809658, -0.08053... | [-0.0194260105491, -0.0088... |\n",
      "| [-0.0319866873324, 0.09203... | [0.00625618547201, 0.00483... |\n",
      "| [0.0100192576647, 0.022021... | [-0.00171553564724, -0.002... |\n",
      "| [-0.0244836471975, -0.0367... | [-0.0764705762267, -0.0801... |\n",
      "| [0.0714767426252, 0.095006... | [0.0239595137537, 0.002930... |\n",
      "| [0.0195223707706, 0.062447... | [-0.0332794338465, 0.03643... |\n",
      "| [-0.0124945221469, 0.07181... | [0.0344733893871, 0.019237... |\n",
      "| [-0.0489768981934, 0.03637... | [-0.00532803824171, -0.003... |\n",
      "| [-0.00663081370294, 0.0661... | [0.0459500402212, 0.007741... |\n",
      "| [0.0871969535947, 0.141028... | [0.0181920062751, 0.109978... |\n",
      "| [0.0361994802952, 0.012239... | [0.0152255706489, -0.01942... |\n",
      "| [0.0412040613592, 0.085143... | [0.0566971190274, -0.00939... |\n",
      "| [0.0146330427378, 0.032280... | [0.0232091899961, -0.01592... |\n",
      "| [0.0642600134015, 0.080729... | [-0.0416026189923, 0.01761... |\n",
      "| [0.0365660376847, 0.035831... | [0.0109720407054, 0.023720... |\n",
      "| [0.0355487130582, -0.00224... | [-0.0081303762272, -0.0370... |\n",
      "| [0.0909965187311, 0.083924... | [0.0966836512089, 0.006150... |\n",
      "| [0.0259557366371, 0.110734... | [0.0774455964565, -0.01908... |\n",
      "| [0.0555381700397, 0.097530... | [-0.00502102822065, -0.000... |\n",
      "| [0.0283989533782, 0.056154... | [-0.0865686386824, 0.03053... |\n",
      "| [-0.0223263632506, 0.00974... | [-0.00711667351425, 0.0332... |\n",
      "| [0.201754614711, 0.0989806... | [0.0362798534334, 0.069524... |\n",
      "| [0.00890618748963, 0.04167... | [-0.0234039649367, -0.0166... |\n",
      "| [0.0712218880653, 0.185079... | [0.0436498448253, 0.064730... |\n",
      "| [-0.0795366838574, 0.17823... | [0.0103838853538, 0.026956... |\n",
      "| [0.169569209218, 0.1358865... | [0.00841932184994, 0.06408... |\n",
      "| [0.00771520426497, 0.12440... | [-0.0186233706772, 0.03347... |\n",
      "| [0.11512568593, 0.11624073... | [-0.0387912616134, 0.06288... |\n",
      "| [0.239128082991, 0.1459567... | [0.0287630520761, 0.055059... |\n",
      "| [0.0525877773762, 0.056607... | [0.0276472792029, -0.01063... |\n",
      "| [0.033902913332, 0.0999438... | [0.01977080293, 0.00253240... |\n",
      "| [0.0452404618263, 0.073576... | [0.0143517404795, -0.00126... |\n",
      "| [0.0896094068885, -0.00061... | [0.0840205028653, 0.021406... |\n",
      "| [0.110120102763, 0.1362469... | [0.021105427295, 0.0500371... |\n",
      "| [0.0444041751325, 0.047856... | [0.0386807397008, 0.024892... |\n",
      "| [0.0158107448369, 0.014660... | [0.0224276538938, -0.04102... |\n",
      "| [-0.0128415878862, 0.00239... | [-0.0104427533224, -0.0153... |\n",
      "| [0.0479286275804, 0.157242... | [-0.00620014686137, 0.0446... |\n",
      "| [0.057947460562, 0.0763635... | [-0.0543910451233, 0.03121... |\n",
      "| [0.0837756916881, 0.109124... | [0.0540997982025, 0.070494... |\n",
      "| [0.0576653257012, 0.016099... | [-0.0296312402934, -0.0298... |\n",
      "| [0.00437379721552, 0.04009... | [-0.0425754152238, -0.0939... |\n",
      "| [0.0249248817563, 0.106681... | [-0.0357626900077, 0.08181... |\n",
      "| [0.0326526127756, 0.057178... | [0.0158326476812, -0.01062... |\n",
      "| [0.13119982183, 0.06163464... | [-0.00055593252182, 0.0032... |\n",
      "| [-0.0270330347121, 0.03387... | [0.031231302768, 0.0002031... |\n",
      "| [0.136561796069, 0.2025975... | [0.0267527345568, 0.092353... |\n",
      "| [0.0291761234403, 0.091992... | [-0.00882750656456, 0.0390... |\n",
      "| [0.191768303514, 0.1283789... | [0.0406240336597, 0.035035... |\n",
      "| [0.161204338074, 0.0246867... | [-0.0526425242424, 0.03682... |\n",
      "| [0.215911656618, 0.2003671... | [0.0315033607185, 0.035154... |\n",
      "| [0.0133172236383, 0.114667... | [-0.0212360303849, 0.03865... |\n",
      "| [0.0409009680152, 0.089494... | [-0.0159932617098, 0.03826... |\n",
      "| [-0.00357612455264, 0.0471... | [0.0150210661814, 0.020727... |\n",
      "| [0.00679694814608, 0.05281... | [0.016029573977, -0.040511... |\n",
      "| [0.0176003780216, 0.010839... | [0.0314771793783, 0.016854... |\n",
      "| [-0.0361467525363, 0.07090... | [-0.0708308815956, -0.0095... |\n",
      "| [-0.0414224863052, -0.0191... | [0.0886756032705, -0.03194... |\n",
      "| [0.0919269472361, 0.070183... | [0.0359962582588, 0.007495... |\n",
      "| [0.0681703835726, 0.067973... | [-0.0704333856702, 0.03877... |\n",
      "| [0.00982938706875, 0.00912... | [-0.0830120146275, 0.07048... |\n",
      "| [0.0679494738579, 0.081200... | [-0.00873318128288, 0.0180... |\n",
      "| [0.0608652867377, 0.053818... | [0.00605260767043, 0.04657... |\n",
      "| [0.0905016064644, 0.145976... | [0.0462426804006, 0.051975... |\n",
      "| [0.0378549285233, 0.111537... | [0.0318333357573, 0.016351... |\n",
      "| [-0.0334353037179, 0.03544... | [-0.0155656514689, -0.0083... |\n",
      "| [0.0845316946507, 0.112875... | [0.00589026138186, 0.08955... |\n",
      "| [-0.00109322404023, 0.0682... | [0.021140223369, -0.029429... |\n",
      "| [0.290656626225, 0.1617486... | [-0.00390577479266, 0.0708... |\n",
      "| [0.0468895062804, 0.013282... | [0.0692907571793, 0.026899... |\n",
      "| [0.0907003134489, 0.013673... | [0.0191949680448, 0.041908... |\n",
      "| [0.0351468324661, 0.061856... | [0.0422571636736, 0.027363... |\n",
      "| [0.00467342836782, 0.13478... | [0.0359290614724, -0.01173... |\n",
      "| [0.00660127820447, 0.06960... | [-0.0350206941366, -0.0164... |\n",
      "| [0.135167270899, 0.0503586... | [-0.0140480427071, -0.0535... |\n",
      "| [0.0208198055625, 0.074064... | [0.0405271053314, 0.009938... |\n",
      "| [0.116903826594, 0.0935171... | [0.0640742629766, 0.010372... |\n",
      "| [0.0254619363695, 0.067644... | [-0.0279721990228, -0.0248... |\n",
      "| [0.0269706249237, 0.046062... | [0.0440336950123, 0.054941... |\n",
      "| [0.0449946559966, 0.065492... | [0.0167922917753, 0.001949... |\n",
      "| [0.0189928952605, 0.017120... | [-0.0269763767719, -0.0682... |\n",
      "| [0.022681536153, 0.1257313... | [-0.0256913304329, -0.0242... |\n",
      "| [0.0934013724327, 0.093804... | [0.0201610289514, 0.077718... |\n",
      "| [0.0568787455559, 0.115032... | [0.0518647022545, 0.023760... |\n",
      "| [-0.00199575535953, 0.0610... | [-0.0313735641539, -0.0049... |\n",
      "| [0.00504718907177, 0.25149... | [0.0585841797292, 0.065402... |\n",
      "| [0.0410988554358, 0.034919... | [0.0943956896663, -0.05032... |\n",
      "| [0.0626704022288, 0.089669... | [-0.00710377702489, 0.0644... |\n",
      "| [0.127548471093, 0.0448921... | [0.0284644067287, 0.037311... |\n",
      "| [-0.0302547942847, 0.12493... | [-0.0657582506537, 0.03100... |\n",
      "| [0.0753440707922, 0.024836... | [0.0503573976457, 0.050533... |\n",
      "| [0.0489067994058, 0.038313... | [0.0408729352057, 0.020410... |\n",
      "| [0.134399086237, 0.0566287... | [0.022999163717, 0.0344063... |\n",
      "| [0.0652813613415, 0.076632... | [0.024606436491, 0.0182848... |\n",
      "| [0.00177816743962, 0.18969... | [0.0731822922826, 0.060903... |\n",
      "| [0.104648567736, 0.0925591... | [0.0945492908359, 0.012486... |\n",
      "| [-0.0127546712756, -0.0037... | [0.0676667019725, -0.03522... |\n",
      "| [-0.0302736051381, 0.01050... | [0.0208315979689, -0.01401... |\n",
      "| [0.0503061302006, 0.090695... | [-0.00907382555306, 0.0623... |\n",
      "| [0.0602241232991, 0.041761... | [0.0543116889894, -0.02298... |\n",
      "| [0.038043551147, 0.0683714... | [-0.0452713631094, -0.0327... |\n",
      "| [0.158168047667, 0.0628377... | [-0.0295561552048, 0.02397... |\n",
      "| [0.0514899976552, 0.082705... | [-0.00474211899564, 0.0311... |\n",
      "| [0.0573541373014, 0.053736... | [0.0313871353865, 0.006529... |\n",
      "| [0.0230116583407, 0.082083... | [-0.0397639535367, 0.00467... |\n",
      "| [0.144123077393, 0.0994354... | [0.00125645287335, 0.06489... |\n",
      "| [0.00153814873192, 0.12488... | [-0.0844416990876, 0.03585... |\n",
      "| [-0.013373484835, 0.089869... | [0.0527667328715, 0.024416... |\n",
      "| [0.107777312398, 0.1810359... | [0.0856897234917, 0.166762... |\n",
      "| [0.107716031373, 0.0641523... | [0.0262025482953, 0.019384... |\n",
      "| [0.0825034156442, 0.157746... | [0.0066041671671, 0.132069... |\n",
      "| [0.0875884816051, -0.02944... | [0.0306235216558, -0.00635... |\n",
      "| [0.0307580269873, 0.013285... | [-0.0230301078409, -0.0317... |\n",
      "| [0.00678326748312, 0.04137... | [0.0599183067679, -0.02210... |\n",
      "| [0.0149758644402, -0.00241... | [0.0580188930035, 0.011806... |\n",
      "| [0.0452996827662, 0.105209... | [-0.0265711210668, 3.53154... |\n",
      "| [0.0131229674444, 0.096789... | [0.0458203069866, 0.019541... |\n",
      "| [0.095740519464, -0.007515... | [0.0660826936364, 0.021066... |\n",
      "| [0.0177624076605, 0.101082... | [-0.00590224191546, 0.0268... |\n",
      "| [-0.00648274226114, 0.1112... | [0.0476954951882, -0.01185... |\n",
      "| [-0.0374170355499, 0.01458... | [-0.00627299724147, -0.015... |\n",
      "| [-0.0313824675977, 0.09210... | [-0.0686372816563, -0.0191... |\n",
      "| [0.0363476201892, 0.128071... | [0.0928896069527, 0.030490... |\n",
      "| [0.0185179729015, 0.033357... | [0.0187435913831, -0.04167... |\n",
      "| [0.0763812959194, -0.00361... | [0.0125235076994, -0.02129... |\n",
      "| [0.0368743911386, 0.035829... | [0.0357958599925, 0.006962... |\n",
      "| [0.0118223531172, 0.042563... | [0.0277679730207, 0.028577... |\n",
      "| [0.0404585339129, 0.031179... | [0.105463810265, -0.000852... |\n",
      "| [-0.00124753150158, 0.0486... | [-0.0391892120242, -0.0051... |\n",
      "| [0.172270521522, 0.1819141... | [0.00388079998083, 0.08369... |\n",
      "| [0.11590167135, 0.23511400... | [0.10445933789, 0.02351457... |\n",
      "| [0.202600121498, 0.0529909... | [-0.0298156253994, 0.04743... |\n",
      "| [-0.0242478251457, 0.11949... | [0.0431044586003, 0.000144... |\n",
      "| [-0.0339915752411, -0.0279... | [-0.0350596345961, -0.0844... |\n",
      "| [0.131120756269, 0.0286922... | [-0.0164519660175, -0.0295... |\n",
      "| [0.12678091228, 0.07810508... | [-0.0525480955839, 0.05161... |\n",
      "| [0.0916588306427, 0.069951... | [0.0406021885574, 0.090507... |\n",
      "| [0.201618716121, 0.1662739... | [0.0596568472683, 0.080275... |\n",
      "| [0.0833986029029, 0.026817... | [-0.00113606045488, -0.034... |\n",
      "| [0.0928720906377, 0.100596... | [0.0454708337784, -0.03059... |\n",
      "| [0.155993402004, 0.1594512... | [0.0163843631744, 0.010875... |\n",
      "| [0.0596146844327, -0.00827... | [0.00480338511989, 0.00703... |\n",
      "| [-0.0266653783619, 0.03451... | [0.0106254825369, -0.02398... |\n",
      "| [-0.0523355305195, 0.07284... | [0.00067677377956, 0.04522... |\n",
      "| [0.153404742479, 0.0778208... | [0.0077169816941, 0.080208... |\n",
      "| [0.148368924856, 0.1307381... | [0.0754632875323, 0.088562... |\n",
      "| [0.0744108632207, 0.190732... | [0.0455294363201, 0.051311... |\n",
      "| [0.0595555119216, 0.107823... | [0.0315516293049, 0.011628... |\n",
      "| [0.127746924758, 0.1681000... | [0.0312889963388, 0.067390... |\n",
      "| [-0.00130574125797, 0.0199... | [-0.00227263034321, -0.013... |\n",
      "| [0.0423841774464, 0.094670... | [-0.0393984690309, 0.00811... |\n",
      "| [-0.0152843696997, 0.13158... | [-0.0017898430815, 0.00506... |\n",
      "| [0.0657257065177, 0.099459... | [0.0348675921559, -0.00368... |\n",
      "| [-0.0830837264657, 0.07453... | [0.00661138305441, 0.01929... |\n",
      "| [0.0249747522175, 0.113848... | [0.0389023311436, -0.07631... |\n",
      "| [0.0409280918539, 0.114824... | [-0.017138261348, -0.01577... |\n",
      "| [0.0549906529486, 0.076502... | [-0.067828528583, -0.03596... |\n",
      "| [0.104989916086, 0.0888596... | [0.0166808925569, 0.065086... |\n",
      "| [0.0476115010679, 0.019436... | [0.034848228097, 0.0020860... |\n",
      "| [0.110017389059, 0.1931121... | [-0.00346597284079, 0.1268... |\n",
      "| [0.100570827723, 0.1612925... | [-0.0254627130926, 0.05512... |\n",
      "| [-0.00693040341139, 0.1790... | [0.0795788839459, -0.01168... |\n",
      "| [0.0779490098357, 0.015394... | [0.0395480841398, 0.012898... |\n",
      "| [-0.028530664742, 0.069174... | [0.00215672701597, -0.0246... |\n",
      "| [0.0308791399002, 0.125152... | [0.00967402942479, 0.04922... |\n",
      "| [0.09510242939, 0.07147707... | [0.0440210290253, 0.053712... |\n",
      "| [-0.0376054421067, 0.04181... | [0.0141719700769, -0.03359... |\n",
      "| [0.0641965717077, -0.01271... | [0.030812099576, -0.003503... |\n",
      "| [0.0244111437351, 0.129910... | [-0.0036391261965, 0.03765... |\n",
      "| [0.111305497587, 0.1218455... | [0.014609885402, 0.0047549... |\n",
      "| [-0.0876613855362, 0.13754... | [0.016259836033, 0.0087778... |\n",
      "+-------------------------------+-------------------------------+\n",
      "+-------------------------------+-------------------------------+\n",
      "|       vectors_pos_ornot       |       vectors_neg_ornot       |\n",
      "+-------------------------------+-------------------------------+\n",
      "| [0.0936123132706, 0.024268... | [-0.07299657166, 0.0437016... |\n",
      "| [0.0310404226184, -0.02571... | [-0.0893728807569, -0.1290... |\n",
      "| [0.0521051809192, -0.01476... | [-0.00431049615145, -0.017... |\n",
      "| [0.0271889418364, 0.019986... | [-0.034230992198, 0.030675... |\n",
      "| [-0.00646200822666, 0.0597... | [-0.0308717675507, -0.0002... |\n",
      "| [0.0576002709568, 0.047451... | [0.00517074670643, -0.0121... |\n",
      "| [0.0488021373749, 0.017187... | [-0.0163234639913, 0.04145... |\n",
      "| [0.0889856144786, 0.003392... | [-0.142204552889, -0.00826... |\n",
      "| [0.0594770945609, 0.212507... | [-0.0564132295549, 0.04702... |\n",
      "| [0.0479829572141, 0.069598... | [-0.0416305139661, 0.02045... |\n",
      "| [0.0704684481025, 0.020197... | [-0.0682658404112, 0.02890... |\n",
      "| [0.0146769089624, -0.05402... | [-0.0622315332294, -0.1277... |\n",
      "| [-0.0537872351706, 0.01518... | [-0.0454203784466, 0.00119... |\n",
      "| [0.00254950812086, -0.0241... | [-0.00436542415991, -0.011... |\n",
      "| [0.0182450376451, 0.040746... | [-0.0288491435349, 0.00786... |\n",
      "| [0.0542540363967, 0.030084... | [-0.0665976926684, -0.0279... |\n",
      "| [0.026996826753, 0.0370020... | [-0.047036331147, -0.00248... |\n",
      "| [-0.018021767959, 0.076743... | [-0.0490678437054, -0.0330... |\n",
      "| [0.0853564068675, 0.048229... | [-0.132881894708, -0.02546... |\n",
      "| [0.0116210011765, 0.016354... | [-0.102783419192, -0.02057... |\n",
      "| [0.0538201630116, -0.00841... | [0.00701528135687, 0.06831... |\n",
      "| [0.0867714211345, 0.066560... | [-0.0419341251254, -0.0227... |\n",
      "| [0.0970849692822, 0.016452... | [-0.042419437319, 0.066048... |\n",
      "| [0.101243637502, 0.0572594... | [-0.0704415813088, 0.02056... |\n",
      "| [0.0231729261577, 0.018584... | [-0.017846332863, -0.00222... |\n",
      "| [0.0052294828929, 0.052132... | [-0.058231215924, -0.01929... |\n",
      "| [0.0477666445076, 0.112651... | [-0.0297830067575, 0.02990... |\n",
      "| [-0.0226550474763, 0.02929... | [0.0281168930233, 0.020069... |\n",
      "| [0.0529368631542, 0.024525... | [-0.023940814659, 0.005777... |\n",
      "| [-0.0736709758639, 0.06077... | [-3.74785486201e-05, -0.03... |\n",
      "| [0.0485805459321, 0.032270... | [-0.0234000682831, -0.0119... |\n",
      "| [0.0379065796733, -0.00754... | [-0.046261485666, 0.016159... |\n",
      "| [0.124670282006, 0.0509314... | [-0.151313021779, -0.02445... |\n",
      "| [0.124394036829, 0.0748194... | [0.0510101728141, 0.047411... |\n",
      "| [0.11227953434, 0.03449710... | [-0.120206832886, 0.018136... |\n",
      "| [0.0309520028532, -0.06930... | [-0.0796692147851, -0.0312... |\n",
      "| [0.122234225273, 0.0038966... | [-0.0758288279176, 0.00653... |\n",
      "| [-0.00345079740509, 0.0325... | [-0.0684961527586, -0.0033... |\n",
      "| [0.0884653106332, -0.04472... | [-0.0246133282781, -0.0802... |\n",
      "| [0.049087729305, 0.0196292... | [0.0111409192905, -0.00852... |\n",
      "| [0.165871888399, 0.0792256... | [-0.0525588393211, -0.0407... |\n",
      "| [0.0540855899453, 0.061650... | [-0.0811733454466, 0.06843... |\n",
      "| [0.0219772942364, 0.064773... | [0.012473301962, -0.012954... |\n",
      "| [0.0108199603856, 0.038803... | [-0.0436330810189, 0.02513... |\n",
      "| [0.00772834382951, 0.01468... | [0.0239679887891, 0.006363... |\n",
      "| [0.00464450661093, -0.0292... | [-0.0182236414403, -0.0179... |\n",
      "| [-0.0454008765519, 0.04985... | [-0.00570985116065, 0.0011... |\n",
      "| [0.241050854325, 0.1277170... | [-0.092529989779, 0.006055... |\n",
      "| [0.0956655591726, 0.021167... | [-0.0507108084857, -0.0382... |\n",
      "| [0.0582186318934, 0.037852... | [-0.117241688073, -0.00681... |\n",
      "| [-0.00209273770452, 0.0398... | [-0.00258591887541, 0.0443... |\n",
      "| [0.161389917135, 0.0162143... | [-0.0384669601917, -0.0554... |\n",
      "| [-0.0367235727608, 0.06225... | [-0.0940043106675, 0.00349... |\n",
      "| [0.133847549558, -0.032525... | [-0.0804955363274, -0.0705... |\n",
      "| [0.00762840220705, 0.04280... | [-0.00713091203943, 0.0302... |\n",
      "| [0.0292428489774, -0.03711... | [0.0293163713068, 0.017936... |\n",
      "| [0.201059922576, 0.0224150... | [-0.0542242079973, -0.0490... |\n",
      "| [-0.0147841898724, 0.05316... | [-0.0405303277075, 0.02019... |\n",
      "| [0.0485108606517, 0.026394... | [-0.0253405272961, 0.00703... |\n",
      "| [0.0458501577377, -0.03803... | [-0.0366170778871, -0.0195... |\n",
      "| [0.0390625186265, 0.025937... | [-0.0345330499113, -0.0163... |\n",
      "| [0.0702367275953, -0.01731... | [-0.0449166186154, -0.0812... |\n",
      "| [0.0348388701677, -0.01807... | [-0.0331494100392, 0.02139... |\n",
      "| [0.0811302885413, 0.070169... | [-0.0230639725924, 0.06818... |\n",
      "| [0.0522276312113, 0.015027... | [-0.0579978041351, -0.0346... |\n",
      "| [0.170814216137, -0.009371... | [-0.0223539471626, -0.0319... |\n",
      "| [0.0507843345404, 0.076966... | [-0.0552632883191, 0.01540... |\n",
      "| [-0.00939424242824, -0.027... | [0.0421089455485, -0.02008... |\n",
      "| [-0.0300067588687, 0.05224... | [-0.0386019386351, -0.0151... |\n",
      "| [0.0911409631371, 0.017851... | [-0.129804626107, -0.05860... |\n",
      "| [-0.0809862911701, 0.00191... | [-0.105530425906, -0.07322... |\n",
      "| [-0.0180483590811, -0.0145... | [0.0414987429976, 0.088186... |\n",
      "| [-0.0462329126894, 0.01627... | [0.022948147729, -0.013773... |\n",
      "| [0.147617384791, 0.0039443... | [-0.169772624969, 0.056958... |\n",
      "| [0.158740475774, 0.0718016... | [-0.0647749528289, 0.01701... |\n",
      "| [0.0317727662623, 0.057438... | [-0.0831178054214, -0.0039... |\n",
      "| [0.168316677213, -0.019489... | [-0.062415394932, -0.02945... |\n",
      "| [0.00829205755144, 0.01830... | [-0.116971492767, -0.02958... |\n",
      "| [-0.0117595866323, 0.02862... | [-0.0516470707953, 0.00243... |\n",
      "| [-0.0316828377545, 0.01088... | [-0.019875664264, 0.034827... |\n",
      "| [0.060673519969, 0.0070717... | [0.0364810675383, 0.029580... |\n",
      "| [0.157448887825, -0.102669... | [-0.0304620210081, -0.0393... |\n",
      "| [0.090959481895, 0.0172686... | [-0.0985664352775, 0.03309... |\n",
      "| [0.141652584076, 0.0767071... | [-0.109115123749, -0.01627... |\n",
      "| [0.0452598594129, -0.01440... | [-0.0260027553886, -0.0215... |\n",
      "| [0.206501871347, 0.1169298... | [-0.0281835068017, 0.02850... |\n",
      "| [0.132511109114, 0.0447104... | [-0.0526860356331, -0.0375... |\n",
      "| [0.101020209491, -0.023931... | [0.0274318158627, -0.01385... |\n",
      "| [0.0701786130667, 0.042859... | [-0.0947470515966, -0.0098... |\n",
      "| [0.0872956514359, 0.079686... | [-0.0694038420916, 0.09506... |\n",
      "| [0.0419655740261, 0.081416... | [-0.00239401054569, -0.006... |\n",
      "| [0.210164263844, 0.1149193... | [-0.00433507794514, 0.0452... |\n",
      "| [0.141470894217, -0.030251... | [0.00970999430865, -0.0371... |\n",
      "| [0.0842099562287, -0.01399... | [-0.108864881098, -0.05115... |\n",
      "| [-0.0363654755056, 0.00686... | [-0.0632238686085, -0.0145... |\n",
      "| [0.148872643709, 0.0141429... | [-0.0357396230102, 0.02212... |\n",
      "| [0.0509511493146, 0.030510... | [-0.0116405915469, 0.03933... |\n",
      "| [-0.0807884633541, 0.03389... | [-0.0140489265323, 0.02250... |\n",
      "| [0.072682544589, 0.0050399... | [-0.0506880655885, -0.0361... |\n",
      "| [0.0377771221101, 0.015587... | [0.0397163368762, -0.03100... |\n",
      "| [0.0812456831336, 0.021665... | [-0.0540851689875, 0.03801... |\n",
      "| [0.0785870030522, 0.074030... | [0.0517764389515, 0.037682... |\n",
      "| [0.0501722283661, 0.023051... | [-0.03013574332, -0.011054... |\n",
      "| [0.0597711391747, 0.040936... | [0.0206471048295, -0.00889... |\n",
      "| [0.0174676496536, 0.040212... | [-0.023066630587, -0.02558... |\n",
      "| [-0.00587348360568, 0.0198... | [0.0086285341531, 0.028597... |\n",
      "| [0.0216574128717, 0.007313... | [-0.0463952831924, 0.00786... |\n",
      "| [-0.00261493609287, 0.0060... | [-0.0785410925746, -0.0207... |\n",
      "| [0.109150744975, 0.0848578... | [0.00293594156392, 0.03726... |\n",
      "| [0.059999473393, 0.0154777... | [-0.0306425690651, -0.0124... |\n",
      "| [0.0540183857083, 0.011144... | [0.0337372533977, 0.010598... |\n",
      "| [-0.0125490231439, 0.07583... | [-0.000446429854492, 0.033... |\n",
      "| [-0.0738247931004, 2.77236... | [0.0364250354469, -0.00749... |\n",
      "| [-0.0667254403234, 0.01437... | [-0.0248359553516, -0.1021... |\n",
      "| [0.112772420049, 0.0516069... | [-0.0555271841586, -0.0183... |\n",
      "| [0.0528120025992, 0.003234... | [-0.0443259961903, -0.0164... |\n",
      "| [0.12151196599, 0.07101032... | [-0.0166956391186, -0.0204... |\n",
      "| [0.0772632583976, 0.018220... | [0.0481544546783, 0.015324... |\n",
      "| [0.0922462269664, 0.019701... | [-0.0270107444376, -0.0008... |\n",
      "| [0.11334964633, 0.00608796... | [-0.0314862020314, -0.0379... |\n",
      "| [0.0265543833375, -0.02399... | [-0.00358789460734, -0.056... |\n",
      "| [0.0177648700774, 0.105906... | [0.0660390034318, 0.032616... |\n",
      "| [-0.00504148518667, -0.013... | [-0.0293831601739, 0.00997... |\n",
      "| [0.121571697295, -0.027826... | [0.0191840454936, -0.10303... |\n",
      "| [-0.0421035066247, 0.03904... | [0.0343963727355, -0.00719... |\n",
      "| [-0.0182295050472, -0.0161... | [0.0328418277204, -0.08178... |\n",
      "| [0.11502739042, 0.09024947... | [-0.165293112397, 0.053823... |\n",
      "| [0.0780861675739, 0.009887... | [-0.00707091810182, 0.0307... |\n",
      "| [0.0352764055133, -0.01856... | [0.0105673018843, -0.01370... |\n",
      "| [-0.0380172431469, 0.04982... | [0.0265186671168, -0.00480... |\n",
      "| [0.103299982846, 0.0760112... | [-0.0770329385996, 0.05248... |\n",
      "| [-0.0135914683342, -0.0132... | [-0.0577011294663, 0.01095... |\n",
      "| [0.0243834313005, 0.084222... | [-0.0402764044702, -0.0461... |\n",
      "| [0.0865095555782, -0.00146... | [-0.0974271297455, -0.0570... |\n",
      "| [0.0941549390554, 0.013782... | [-0.00205396232195, -0.006... |\n",
      "| [-0.0163011159748, 0.00854... | [-0.0278507508337, 0.01336... |\n",
      "| [0.0143929310143, 0.042981... | [-0.0742516368628, -0.0296... |\n",
      "| [0.0574743114412, -0.00589... | [-0.0279335249215, 0.03997... |\n",
      "| [0.161456689239, -0.029951... | [-0.0477807968855, -0.0197... |\n",
      "| [0.113822191954, 0.1043181... | [-0.0137450052425, 0.12308... |\n",
      "| [0.256216526031, 0.1865748... | [-0.0580122843385, 0.06304... |\n",
      "| [0.106818653643, -0.000814... | [-0.0275422651321, -0.0032... |\n",
      "| [0.0398770198226, -0.00664... | [-0.0202869959176, -0.0818... |\n",
      "| [0.121901750565, 0.0628530... | [-0.0470653101802, -0.0052... |\n",
      "| [0.148728400469, 0.0768447... | [0.00470467470586, -0.0284... |\n",
      "| [0.072748541832, 0.0476234... | [-0.0904660373926, 0.00970... |\n",
      "| [0.0869694724679, 0.076449... | [-0.165327861905, 0.057995... |\n",
      "| [0.0601209998131, 0.035801... | [-0.0820819288492, 0.03453... |\n",
      "| [0.0402483977377, 0.040730... | [-0.0151228606701, 0.04188... |\n",
      "| [-0.0770476609468, 0.07398... | [0.0401378273964, -0.06271... |\n",
      "| [0.104075789452, 0.1241466... | [0.0216146726161, -0.03040... |\n",
      "| [0.0254755616188, 0.097490... | [-0.111069701612, 0.029754... |\n",
      "| [0.0117367915809, -0.02892... | [0.0049684625119, -0.03460... |\n",
      "| [0.0442374534905, 0.028715... | [-0.0702160671353, 0.01346... |\n",
      "| [0.115856185555, 0.0869712... | [0.024059029296, 0.1058465... |\n",
      "| [0.00706813950092, -0.0032... | [0.0281641036272, 0.102969... |\n",
      "| [0.087086558342, -0.018874... | [-0.0642670989037, -0.0131... |\n",
      "| [0.0879293456674, -0.00998... | [-0.0261549949646, 0.00221... |\n",
      "| [0.0976215973496, 0.025186... | [-0.0596188791096, -0.0075... |\n",
      "| [0.0994650274515, 0.024712... | [0.0032407194376, -0.04723... |\n",
      "| [0.0469859614968, 0.045498... | [-0.0520125888288, -0.0229... |\n",
      "| [0.0958153232932, 0.070139... | [-0.00864856410772, 0.0170... |\n",
      "| [0.110576629639, 0.1115818... | [-0.0337418951094, 0.03826... |\n",
      "| [0.0140080107376, -0.03552... | [0.0462479330599, -0.05011... |\n",
      "| [0.0287815742195, 0.001044... | [0.0426394492388, 0.027574... |\n",
      "| [0.0890634804964, -0.05099... | [-0.0948429256678, -0.0238... |\n",
      "| [-0.0182269681245, -0.0511... | [-0.0399377681315, -0.0078... |\n",
      "| [-0.0294742602855, 0.01474... | [0.00696538388729, 0.00425... |\n",
      "| [0.00546293100342, -0.0321... | [-0.0428617596626, 0.03619... |\n",
      "| [0.0665165707469, 0.060525... | [0.0148242227733, -0.01869... |\n",
      "| [0.103154137731, 0.0346457... | [-0.0795045346022, -0.0032... |\n",
      "| [0.00974067300558, 0.02931... | [-0.0121547579765, 0.05032... |\n",
      "| [0.103995129466, 0.0802995... | [-0.0321494676173, -0.0496... |\n",
      "| [-0.0354794859886, -0.0513... | [0.0120275672525, -0.01215... |\n",
      "| [0.0276199430227, 0.022768... | [-0.0201135613024, 0.08609... |\n",
      "| [0.125167116523, -0.018714... | [-0.075971364975, -0.04896... |\n",
      "| [0.118357896805, 0.0594414... | [-0.0269750468433, -0.0088... |\n",
      "| [0.125542491674, -0.009754... | [-0.01452776324, -0.000221... |\n",
      "| [0.0315107926726, -0.03317... | [-0.061912138015, -0.01235... |\n",
      "| [0.0617981813848, 0.016548... | [-0.00943122245371, 0.0577... |\n",
      "| [-0.0377970598638, -0.0071... | [-0.0215033162385, 0.02783... |\n",
      "| [0.223486021161, 0.0467239... | [-0.08050275594, 0.0344004... |\n",
      "| [0.0542540363967, 0.030084... | [-0.0665976926684, -0.0279... |\n",
      "| [-0.00740051921457, 0.0998... | [0.0717018917203, 0.044806... |\n",
      "| [0.0549397282302, -0.01351... | [-0.025424959138, 0.012752... |\n",
      "| [0.0445256233215, 0.028613... | [-0.0923923030496, 0.02694... |\n",
      "| [0.111380256712, 0.0665702... | [-0.120725631714, -0.08557... |\n",
      "| [0.0893402770162, 0.091399... | [0.0226226914674, -0.01816... |\n",
      "| [0.15273809433, -0.0011417... | [-0.0674466639757, 0.03976... |\n",
      "| [0.0297816451639, 0.043437... | [-0.107720181346, -0.03737... |\n",
      "| [0.0317463539541, -0.00843... | [-0.129744514823, -0.02587... |\n",
      "| [0.0224428568035, -0.01647... | [-0.0431562699378, 0.05847... |\n",
      "| [0.024388320744, -0.058795... | [0.000571438809857, -0.004... |\n",
      "| [0.112734496593, 0.0149818... | [-0.0460731163621, 0.00163... |\n",
      "| [0.135887876153, 0.0102708... | [-0.0573738589883, -0.0082... |\n",
      "| [0.0388188585639, 0.013951... | [-0.0112451370806, -0.1124... |\n",
      "| [0.101069837809, 0.0114934... | [-0.0287442300469, -0.0120... |\n",
      "| [0.0487663522363, 0.060607... | [-0.0608440637589, 0.00068... |\n",
      "| [0.0690767765045, -0.00437... | [-0.077235840261, -0.01412... |\n",
      "| [0.0795561373234, 0.086836... | [-0.177063018084, 0.044671... |\n",
      "| [0.0464699827135, 0.021725... | [-0.0302007347345, -0.0075... |\n",
      "| [0.142488911748, 0.0697840... | [-0.131390690804, -0.02630... |\n",
      "| [0.144910082221, -0.042007... | [0.00172751233913, -0.0339... |\n",
      "| [0.0431727319956, 0.034397... | [-0.0823944509029, -0.0096... |\n",
      "| [-0.018558928743, 0.045473... | [-0.00810467638075, -0.007... |\n",
      "| [-0.0551256760955, -0.0330... | [-0.00885984674096, 0.0483... |\n",
      "| [0.124097086489, -0.031345... | [-0.0329246744514, -0.0322... |\n",
      "| [-0.052304841578, -0.02397... | [-0.0513555705547, -0.0302... |\n",
      "| [0.0405569262803, 0.082644... | [-0.0813143476844, 0.02043... |\n",
      "| [-0.0533729493618, 0.02194... | [-0.0859294831753, 0.00071... |\n",
      "| [-0.0526483915746, -0.0289... | [-0.00304918573238, 0.0295... |\n",
      "| [0.103420436382, 0.1025915... | [-0.0855439603329, 0.08184... |\n",
      "| [0.132672026753, 0.0317475... | [-0.017316006124, -0.00178... |\n",
      "| [0.07529630512, 0.08536167... | [-0.00879100430757, 0.0527... |\n",
      "| [0.0735072046518, -0.03061... | [-0.0348098613322, -0.0418... |\n",
      "| [0.0337014868855, 0.021979... | [-0.0592503920197, 0.03574... |\n",
      "| [0.0381035581231, 0.002084... | [-0.0508577562869, 0.02994... |\n",
      "| [0.0245073027909, -0.01237... | [0.0526428930461, -0.00469... |\n",
      "| [0.0906255543232, 0.042405... | [-0.0936706289649, 0.03430... |\n",
      "| [0.0315705053508, -0.04740... | [-0.0417810827494, -0.0172... |\n",
      "| [0.0302073061466, -0.01509... | [0.0056399544701, 0.001017... |\n",
      "| [0.0622847005725, 0.019948... | [0.00198912015185, 0.00051... |\n",
      "| [0.0882323160768, 0.051876... | [-0.0670451670885, -0.0016... |\n",
      "| [0.111624836922, 0.0462482... | [0.0252229198813, 0.032416... |\n",
      "| [0.0777403563261, -0.00089... | [-0.0636697560549, -0.0226... |\n",
      "| [-0.0603348091245, 0.07000... | [-0.138054594398, -0.04967... |\n",
      "| [-0.111539550126, 0.082674... | [0.0395692959428, 0.042292... |\n",
      "| [0.102346107364, 0.0096404... | [-0.0594938062131, -0.0001... |\n",
      "| [0.102958709002, 0.0107409... | [-0.0309926345944, -0.0276... |\n",
      "| [0.105395302176, 0.0760919... | [-0.0736341029406, 0.07441... |\n",
      "| [0.0281074848026, 0.098861... | [-0.0691527053714, 0.04067... |\n",
      "| [0.145013317466, 0.0114635... | [-0.0247699227184, 0.00919... |\n",
      "| [0.0314600095153, -0.01361... | [-0.0603969544172, -0.0498... |\n",
      "| [0.123677738011, 0.0444488... | [-0.0569928586483, 0.03886... |\n",
      "| [0.124648764729, -0.017703... | [-0.0716792792082, -0.0520... |\n",
      "| [0.0182791575789, 0.020951... | [-0.0554959848523, 0.02213... |\n",
      "| [0.0513236708939, 0.031833... | [-0.0233286805451, 0.07656... |\n",
      "| [-0.0791164413095, -0.0139... | [0.00400519091636, 0.05224... |\n",
      "| [0.111017249525, -0.111122... | [-0.111218355596, -0.02826... |\n",
      "| [0.156319901347, 0.0432760... | [-0.159653350711, 0.046720... |\n",
      "| [0.0102860610932, 0.084154... | [-0.0488301180303, -0.0164... |\n",
      "| [0.034045226872, 0.0342502... | [-0.0479823835194, 0.00821... |\n",
      "| [0.0940398648381, 0.042791... | [-0.0731725171208, 0.02506... |\n",
      "| [-0.00186508591287, 0.0403... | [-0.00410563638434, -0.030... |\n",
      "| [0.0813621655107, 0.022102... | [-0.0245203524828, -0.0822... |\n",
      "| [0.00203584693372, 0.03595... | [0.0289535503834, 0.059237... |\n",
      "| [-0.194370597601, 0.010895... | [-0.157186895609, -0.01056... |\n",
      "| [0.138426735997, 0.0918517... | [-0.0172859933227, 0.07994... |\n",
      "| [0.0607058554888, 0.069772... | [0.00588271208107, 0.01626... |\n",
      "| [0.0960611626506, -0.01342... | [-0.0503558702767, 0.05099... |\n",
      "| [0.0119400629774, -0.01350... | [-0.0542346462607, -0.0600... |\n",
      "| [-0.0141028054059, 0.07503... | [-0.00812620669603, 0.0513... |\n",
      "| [0.0558377802372, 0.029708... | [-0.0448952168226, 0.03920... |\n",
      "| [0.0249257404357, -0.00048... | [-0.0377807170153, -0.0238... |\n",
      "| [0.030991403386, 0.0378158... | [-0.0203251317143, 0.00976... |\n",
      "| [0.0511317998171, -0.01022... | [-0.109566748142, -0.00979... |\n",
      "| [0.104842789471, 0.0927361... | [0.00204361998476, -0.0064... |\n",
      "| [0.0442548692226, 0.025624... | [-0.0432970821857, 0.03116... |\n",
      "| [0.000957453798037, -0.010... | [-0.0786919519305, -0.0078... |\n",
      "| [0.00216168304905, 0.02581... | [-0.00856903474778, -0.091... |\n",
      "| [0.00574614712968, 0.01091... | [-0.0236591696739, 0.00820... |\n",
      "| [0.00649243639782, 0.04135... | [0.0414528921247, 0.048569... |\n",
      "| [0.056946888566, -0.069693... | [-0.0327600650489, -0.0276... |\n",
      "| [0.0345894545317, 0.076196... | [-0.0615710541606, -0.0345... |\n",
      "| [0.0966105163097, 0.066787... | [-0.016908083111, -0.05059... |\n",
      "| [-0.0177729595453, -0.0620... | [-0.0242452751845, -0.0246... |\n",
      "| [0.0914455503225, -0.01431... | [-0.112334296107, -0.00552... |\n",
      "| [-0.0609993077815, -0.0062... | [-0.0230131261051, 0.03782... |\n",
      "| [-0.0159698314965, 0.05243... | [-0.0124166710302, -0.0057... |\n",
      "| [0.0579481348395, 0.047293... | [-0.105383053422, 0.019697... |\n",
      "| [0.00738564878702, 0.08083... | [-0.0117336492985, 0.01253... |\n",
      "| [0.0127569111064, 0.013856... | [-0.114590160549, 0.054123... |\n",
      "| [0.0145775033161, 0.023839... | [-0.0641575530171, 0.03797... |\n",
      "| [0.0493437424302, -0.01675... | [-0.0504333265126, -0.0735... |\n",
      "| [-0.0633231326938, -0.0716... | [-0.0835722386837, -0.0012... |\n",
      "| [0.0483690835536, 0.005321... | [-0.00747349159792, 0.0530... |\n",
      "| [0.143799632788, 0.1206174... | [-0.113969407976, 0.027658... |\n",
      "| [0.0785776376724, -0.02248... | [-0.102841481566, -0.06302... |\n",
      "| [0.0936687588692, 0.080829... | [0.00855744071305, 0.00670... |\n",
      "| [0.00972563307732, 0.03942... | [-0.039493881166, 0.033260... |\n",
      "| [0.0714797750115, 0.039405... | [-0.0187018942088, 0.04356... |\n",
      "| [0.121062234044, 0.1109419... | [-0.0669404342771, 0.01370... |\n",
      "| [0.0866937041283, 0.057707... | [0.00855276361108, 0.02163... |\n",
      "| [0.0383056141436, 0.047181... | [-0.015445751138, 0.022913... |\n",
      "| [0.117543458939, 0.0306634... | [-0.0312031116337, 0.01076... |\n",
      "| [0.126131489873, 0.0845793... | [0.0657329559326, 0.057957... |\n",
      "| [0.085516512394, 0.0093375... | [-0.0201424602419, 0.01046... |\n",
      "| [-0.0653699710965, 0.03525... | [-0.0111311329529, 0.04352... |\n",
      "| [0.014002289623, 0.0222399... | [0.0417335964739, -0.08312... |\n",
      "| [0.0960649996996, 0.031340... | [-0.0673454403877, -0.0431... |\n",
      "| [-0.0110684912652, 0.00272... | [-0.0236499588937, 0.02847... |\n",
      "| [0.00161425210536, -0.0355... | [-0.0956321880221, -0.0407... |\n",
      "| [0.138969510794, 0.0607411... | [-0.100628018379, -0.08961... |\n",
      "| [0.0601911991835, 0.042235... | [-0.00858782138675, -0.021... |\n",
      "| [-0.0117995850742, 0.02977... | [-0.0248406846076, 0.01148... |\n",
      "| [0.0198342129588, 0.014133... | [-0.00762786669657, 0.0096... |\n",
      "| [0.0404263325036, 0.068760... | [-0.128628179431, -0.01013... |\n",
      "| [0.0657299533486, 0.050158... | [-0.0882687792182, 0.03021... |\n",
      "| [0.0790682211518, 0.022852... | [0.0278527271003, 0.028165... |\n",
      "| [0.0667608529329, 0.000503... | [-0.0714111626148, -0.0316... |\n",
      "| [0.1109502092, 0.029761165... | [-0.00409871945158, -0.006... |\n",
      "| [0.121802441776, -0.026718... | [-0.0868243202567, -0.0361... |\n",
      "| [-0.012224541977, 0.011123... | [-0.0283952504396, 0.03184... |\n",
      "| [0.118334978819, -0.050130... | [0.0881520360708, 0.049190... |\n",
      "| [0.0375768952072, -0.03991... | [-0.0508163049817, -0.0345... |\n",
      "| [2.68615167442e-05, 0.0008... | [-0.0869454219937, 0.05220... |\n",
      "| [0.168384745717, 0.0989629... | [-0.0467556379735, -0.0338... |\n",
      "| [0.0861506462097, 0.043402... | [-0.0454143323004, 0.03069... |\n",
      "| [0.0600966662169, 0.010064... | [-0.0485328696668, -0.0378... |\n",
      "| [0.0526506975293, 0.004330... | [-0.0789511203766, 0.01810... |\n",
      "| [0.0243385788053, -0.02809... | [-0.105324305594, 0.064033... |\n",
      "| [0.147242277861, 0.0348372... | [-0.0109950611368, -0.0473... |\n",
      "| [0.19637170434, -0.0324360... | [-0.200423628092, 0.022765... |\n",
      "| [-0.0367235727608, 0.06225... | [-0.0940043106675, 0.00349... |\n",
      "| [0.0926593616605, 0.033479... | [-0.0585450269282, 0.02059... |\n",
      "| [0.0823749527335, 0.041399... | [-0.0827700644732, -0.0607... |\n",
      "| [0.000265469570877, -0.012... | [-0.169182673097, -0.00506... |\n",
      "| [0.16411831975, 0.15670378... | [-0.00813297461718, -0.014... |\n",
      "| [0.0567134544253, 0.051499... | [0.0777691230178, 0.073856... |\n",
      "| [0.162726864219, 0.0426065... | [-0.0751248821616, -0.0467... |\n",
      "| [0.0616976171732, 0.023645... | [-0.0446777641773, -0.0060... |\n",
      "| [0.0350048802793, 0.072407... | [-0.0858754739165, -0.0038... |\n",
      "| [0.056310929358, 0.1168873... | [-0.102337539196, -0.05113... |\n",
      "| [0.0648925080895, 0.012297... | [-0.0536650270224, -0.1022... |\n",
      "| [-0.0173173397779, -0.0115... | [-0.0557674057782, -0.0240... |\n",
      "| [0.139308348298, 0.1025761... | [0.0438675917685, 0.012069... |\n",
      "| [-0.0432424582541, -0.0603... | [-0.0368080437183, -0.0970... |\n",
      "| [0.0801984071732, -0.01495... | [-0.01920645684, -0.008321... |\n",
      "| [0.0629603639245, 0.029053... | [-0.0292437449098, 0.02273... |\n",
      "| [0.0129811437801, 0.054445... | [0.051169205457, 0.0042628... |\n",
      "| [0.016243012622, -0.028186... | [-0.0548016391695, -0.0064... |\n",
      "| [0.0614663250744, 0.089045... | [-0.000871594354976, -0.01... |\n",
      "| [0.0889676213264, -0.00489... | [-0.0371328182518, 0.04078... |\n",
      "| [0.00352238421328, 0.01376... | [-0.0202435161918, -0.0340... |\n",
      "| [0.062986522913, 0.0080287... | [-0.0327111333609, -0.0006... |\n",
      "| [0.136932164431, 0.0733890... | [-0.029395468533, -0.00629... |\n",
      "| [0.103457391262, 0.0093351... | [-0.0967471897602, 0.07605... |\n",
      "| [0.0762426331639, 0.005027... | [0.00791397597641, 0.01891... |\n",
      "| [0.0495117008686, -0.06998... | [-0.0324147008359, 0.00289... |\n",
      "| [0.00385212292895, -0.0010... | [-0.0902884230018, -0.0518... |\n",
      "| [-0.0118195014074, 0.05900... | [-0.0509129799902, 0.00661... |\n",
      "| [0.0901285111904, 0.053390... | [-0.016747739166, 0.016704... |\n",
      "| [0.122345931828, 0.0455403... | [-0.000776403700002, -0.02... |\n",
      "| [-0.071304872632, 0.028668... | [-0.0194804146886, 0.04693... |\n",
      "| [0.0830048471689, -0.06319... | [-0.0457386262715, -0.0377... |\n",
      "| [-0.0138772781938, 0.04495... | [-0.045615375042, 0.109642... |\n",
      "| [0.0519846342504, -0.02021... | [-0.05906079337, -0.085305... |\n",
      "| [0.0325030609965, 0.050680... | [-0.0156461261213, 0.06002... |\n",
      "| [0.0737319365144, 0.033150... | [0.0550532713532, 0.053091... |\n",
      "| [0.128859043121, 0.1014765... | [-0.0854904726148, 0.03692... |\n",
      "| [-0.0121976146474, 0.00537... | [-0.0188646800816, -0.0323... |\n",
      "| [0.00895097106695, 0.01541... | [-0.00475675193593, 0.0080... |\n",
      "| [0.0459649898112, 0.045445... | [0.0172736868262, -0.04011... |\n",
      "| [0.117222756147, 0.0272284... | [-0.0773482099175, -0.0304... |\n",
      "| [0.0407820977271, 0.019221... | [-0.0781352445483, -0.0083... |\n",
      "| [0.0625480189919, 0.019850... | [0.0578806512058, 0.027075... |\n",
      "| [0.156893730164, -0.044228... | [-0.0903518646955, -0.0217... |\n",
      "| [0.0367269963026, 0.050688... | [-0.0625521317124, 0.09815... |\n",
      "| [0.0493060238659, 0.026622... | [-0.0945379659534, -0.0527... |\n",
      "| [0.0588554665446, 0.041701... | [-0.0309968683869, 0.01356... |\n",
      "| [0.0804608017206, 0.080147... | [-0.107146911323, 0.012740... |\n",
      "| [0.0274815056473, -0.02041... | [0.0430537126958, -0.02680... |\n",
      "| [0.064017124474, 0.0026484... | [0.0138011081144, -0.01151... |\n",
      "| [0.0209021642804, 0.003465... | [0.0524616464972, -0.01381... |\n",
      "| [0.205884218216, 0.0990642... | [-0.0719055533409, 0.03373... |\n",
      "| [0.114936061203, 0.0002031... | [-0.0905245319009, -0.0253... |\n",
      "| [0.11131952703, 0.04953870... | [-0.0613446608186, 0.01891... |\n",
      "| [-0.00101976981387, 0.0043... | [0.013397696428, 0.0022272... |\n",
      "| [0.0919261574745, 0.101139... | [-0.0104383993894, -0.0528... |\n",
      "| [0.142754822969, 0.0152062... | [-0.120012953877, -0.01053... |\n",
      "| [0.141875877976, -0.040500... | [-0.0682484358549, -0.0825... |\n",
      "| [-0.00243711099029, -0.008... | [-0.0513672418892, -0.0774... |\n",
      "| [0.0794591382146, 0.014152... | [-0.121200852096, 0.006642... |\n",
      "| [0.0666328147054, 0.042352... | [-0.0174563657492, 0.00341... |\n",
      "| [0.158465862274, 0.0879655... | [0.0786265209317, 0.050087... |\n",
      "| [0.0651339590549, 0.022950... | [-0.0102665377781, 0.06612... |\n",
      "| [0.0154961235821, -0.13442... | [-0.0424307882786, -0.0570... |\n",
      "| [0.0213283486664, 0.022402... | [-0.00898891035467, 0.0430... |\n",
      "| [0.101312421262, 0.1346729... | [-0.139027312398, 0.024215... |\n",
      "| [-0.0349120385945, -0.0228... | [-0.0211900826544, 0.01991... |\n",
      "| [0.00559309218079, 0.00648... | [-0.0213498342782, -0.0735... |\n",
      "| [-0.0513140186667, 0.00123... | [-0.015939315781, 0.009979... |\n",
      "| [0.0788985937834, -0.01778... | [-0.0589368641376, -0.0630... |\n",
      "| [0.16828738153, -0.0208298... | [-0.134171918035, -0.00725... |\n",
      "| [0.0632946565747, 0.012895... | [-0.0424459576607, 0.00807... |\n",
      "| [0.118387244642, -0.032207... | [-0.0879969149828, -0.0777... |\n",
      "| [0.121783621609, 0.1142580... | [-0.0551702491939, 0.03200... |\n",
      "| [0.0244967378676, 0.041686... | [-0.00460161920637, -0.001... |\n",
      "| [-0.0695871040225, 0.04973... | [0.0218340512365, 0.017004... |\n",
      "| [0.0866979286075, 0.016018... | [-0.07700330019, 0.0052714... |\n",
      "| [0.0397834554315, -0.00206... | [-0.0596041567624, 0.00095... |\n",
      "| [-0.0271142981946, -0.0432... | [-0.129802957177, -0.00275... |\n",
      "| [0.110288605094, 0.0117315... | [0.0180337820202, -0.03423... |\n",
      "| [0.0787223950028, 0.022690... | [-0.0956036746502, -0.0091... |\n",
      "| [0.0279071517289, 0.024050... | [-0.116463899612, 0.071158... |\n",
      "| [-0.00921583268791, 0.0132... | [0.0235347524285, -0.00300... |\n",
      "| [0.0933365151286, 0.029142... | [-0.104373209178, 0.045317... |\n",
      "| [0.071616768837, -0.030930... | [-0.083733767271, -0.00405... |\n",
      "| [0.0918729826808, 0.058996... | [-0.0672830194235, 0.06664... |\n",
      "| [0.181443765759, 0.0325245... | [0.007886858657, 0.0747901... |\n",
      "| [0.00883391033858, 0.02976... | [-0.022299092263, 0.033665... |\n",
      "| [-0.0164793655276, 0.06362... | [-0.0221996940672, 0.03012... |\n",
      "| [-0.00573087716475, 0.0069... | [-0.00038081620005, 0.0144... |\n",
      "| [0.189136520028, 0.1310457... | [-0.0489212535322, -0.0491... |\n",
      "| [0.0109552741051, -0.00907... | [-0.0810936763883, -0.0688... |\n",
      "| [-0.031453307718, 0.033443... | [-0.00120652501937, -0.026... |\n",
      "| [0.0574267059565, 0.066513... | [-0.0518760830164, 0.08127... |\n",
      "| [-0.0548967458308, 0.06589... | [-0.0469367280602, -0.0330... |\n",
      "| [0.150738269091, 0.0993585... | [-0.100888252258, 0.085687... |\n",
      "| [0.0536051727831, -0.02068... | [-0.0168514680117, 0.00877... |\n",
      "| [0.115569978952, 0.0924363... | [-0.0387701764703, 0.04702... |\n",
      "| [0.120279580355, 0.0164964... | [-0.0923526287079, -0.0343... |\n",
      "| [0.095289349556, -0.021471... | [-0.0875262022018, -0.0081... |\n",
      "| [0.0340903401375, 0.000256... | [-0.0619179047644, -0.0577... |\n",
      "| [0.110760882497, 0.0513521... | [0.0283470451832, -0.04070... |\n",
      "| [-0.0189417917281, -0.0178... | [-0.0827715173364, -0.0312... |\n",
      "| [0.0270308572799, 0.000612... | [0.0378153696656, 0.027003... |\n",
      "| [0.0148746129125, 0.013025... | [0.00931249093264, 0.00563... |\n",
      "| [0.000190470833331, -0.003... | [-0.0453763566911, 0.01426... |\n",
      "| [0.0431143455207, 0.009112... | [-0.000744903751183, -0.03... |\n",
      "| [0.0469519495964, 0.049925... | [-0.0300777330995, -0.0002... |\n",
      "| [0.0953501537442, 0.027463... | [-0.0636525154114, -0.0090... |\n",
      "| [0.0693788006902, 0.059290... | [-0.0940863713622, 0.03617... |\n",
      "| [0.131257489324, 0.0182513... | [-0.0339990071952, -0.0648... |\n",
      "| [0.0416426323354, -0.01277... | [-0.108297102153, 0.004427... |\n",
      "| [-0.036815520376, -0.04151... | [-0.0424029044807, 0.04409... |\n",
      "| [0.0161335933954, 0.012532... | [-0.0423347391188, -0.0558... |\n",
      "| [0.142392814159, 0.0200722... | [-0.0772170871496, 0.05444... |\n",
      "| [0.089216940105, 0.0505607... | [-0.13681165874, -0.057691... |\n",
      "| [0.0934081971645, 0.073742... | [-0.0833566039801, -0.0504... |\n",
      "| [-0.0176535323262, 0.00836... | [-0.0811292603612, 0.01348... |\n",
      "| [0.0910398438573, -0.01068... | [-0.0379393324256, -0.0175... |\n",
      "| [-0.0188963487744, -0.0216... | [0.00153014203534, -0.0255... |\n",
      "| [0.00800150912255, 0.05292... | [0.0227219369262, -0.00774... |\n",
      "| [0.0605059042573, 0.066647... | [-0.0588645301759, 0.00040... |\n",
      "| [0.0194065794349, 0.084513... | [-0.0912293419242, 0.10856... |\n",
      "| [0.0484042316675, 0.051676... | [0.0389076508582, -0.01703... |\n",
      "| [0.022820379585, 0.1005385... | [0.039483524859, 0.0611421... |\n",
      "| [0.117824532092, -0.025439... | [-0.114847838879, -0.01858... |\n",
      "| [0.0254263486713, 0.007790... | [-0.241931065917, 3.589224... |\n",
      "| [-0.00249104434624, 0.0054... | [-0.0521278083324, 0.00537... |\n",
      "| [0.111591920257, -0.069307... | [-0.0535518676043, 0.03239... |\n",
      "| [-0.00404063332826, -0.039... | [-0.0268343742937, -0.0438... |\n",
      "| [0.110943183303, 0.0848408... | [-0.112730041146, 0.070506... |\n",
      "| [0.0413723438978, 0.051801... | [-0.0421677008271, 0.08298... |\n",
      "| [0.335468679667, 0.0034320... | [-0.0695879980922, 0.06791... |\n",
      "| [0.0874417722225, 0.062818... | [-0.00760557129979, 0.0420... |\n",
      "| [0.105280324817, 0.1320243... | [-0.0483124032617, -0.0641... |\n",
      "| [0.080706782639, -0.118310... | [-0.0517759136856, -0.0472... |\n",
      "| [0.0523022226989, 0.042241... | [-0.0548380240798, -0.0497... |\n",
      "| [0.0489598065615, 0.000261... | [-0.0608333572745, -0.0285... |\n",
      "| [0.0325636006892, -0.02035... | [-0.0434984900057, 0.03266... |\n",
      "| [-0.00483154086396, 0.0763... | [0.0130820889026, 0.024141... |\n",
      "| [0.0373997017741, 0.002832... | [0.00180842913687, 0.02292... |\n",
      "| [0.0507366210222, -0.07353... | [0.0830978304148, -0.00701... |\n",
      "| [0.10292455554, 0.06174015... | [-0.0226180162281, 0.01997... |\n",
      "| [0.0689409673214, 0.080298... | [-0.10103186965, -0.069910... |\n",
      "| [0.0967510938644, -0.04003... | [-0.029060151428, 0.019431... |\n",
      "| [-0.0256380736828, 0.03330... | [-0.035028450191, -0.00448... |\n",
      "| [-0.0621454045177, -0.0285... | [-0.0656741335988, -0.0120... |\n",
      "| [0.0252098925412, -0.00974... | [-0.0115401772782, 0.05612... |\n",
      "| [-0.044486567378, 0.020035... | [-0.0929485186934, 0.05378... |\n",
      "| [-0.0448771119118, 0.03504... | [-0.036197964102, -0.09045... |\n",
      "| [0.00735965650529, 0.00388... | [0.0157405007631, 0.053505... |\n",
      "| [0.0301833692938, 0.038801... | [-0.0473350062966, -0.0079... |\n",
      "| [0.112127155066, 0.0231505... | [-0.108712434769, 0.013987... |\n",
      "| [-0.0637036636472, -0.0066... | [0.0384403690696, -0.04631... |\n",
      "| [0.0373232923448, 0.048687... | [-0.105041898787, -0.03300... |\n",
      "| [0.0791186019778, 0.005032... | [-0.0329299941659, -0.0050... |\n",
      "| [0.0712171047926, 0.001996... | [-0.0752684995532, 0.01680... |\n",
      "| [0.0802490860224, 0.083089... | [-0.00260261213407, -0.012... |\n",
      "| [0.0448804609478, -0.03165... | [-0.00226543424651, 0.0071... |\n",
      "| [0.114833936095, -0.067718... | [0.030452305451, -0.039367... |\n",
      "| [0.147590503097, -0.099421... | [-0.126414448023, -0.08096... |\n",
      "| [0.134775623679, -0.016862... | [-0.117330141366, 0.015242... |\n",
      "| [0.00977481808513, -0.0163... | [0.0730574056506, -0.00973... |\n",
      "| [0.0569658614695, 0.006860... | [-0.125254929066, 0.037657... |\n",
      "| [0.0625025555491, 0.035971... | [-0.0551032051444, 0.01145... |\n",
      "| [0.0781014412642, 0.036599... | [-0.0823437348008, 0.11762... |\n",
      "| [0.046413321048, -0.015359... | [-0.0743179470301, 0.04311... |\n",
      "| [0.215827122331, 0.0883787... | [0.0257962048054, 0.080717... |\n",
      "| [0.0579819194973, 0.016676... | [-0.0867584869266, 0.05132... |\n",
      "| [0.0649777129292, 0.056050... | [-0.0464581586421, -0.0237... |\n",
      "| [0.10531835258, 0.05263224... | [-0.12899838388, -0.017795... |\n",
      "| [0.118727244437, -0.014625... | [0.0755863785744, -0.06653... |\n",
      "| [0.15175576508, 0.08931133... | [0.0513256452978, -0.00865... |\n",
      "| [-0.035993937403, -0.06643... | [-0.0321238487959, -0.1072... |\n",
      "| [0.0601505152881, 0.053274... | [-0.0477780811489, -0.0511... |\n",
      "| [-0.0702878907323, -0.0382... | [0.111338526011, 0.0466416... |\n",
      "| [0.00283650751226, 0.07078... | [-0.101405717432, 0.080057... |\n",
      "| [0.0644503906369, 0.059265... | [0.000176341694896, 0.0040... |\n",
      "| [-0.0863409712911, 0.00265... | [-0.0936980620027, 0.05026... |\n",
      "| [0.110125139356, -0.012432... | [-0.123565286398, 0.005759... |\n",
      "| [0.113487884402, -0.022858... | [-0.0126039618626, 0.02563... |\n",
      "| [-0.00563803967088, 0.0093... | [0.0116770043969, -0.06802... |\n",
      "| [0.0448147915304, 0.044528... | [-0.0847820267081, -0.0162... |\n",
      "| [0.00110221118666, 0.03679... | [-0.0274823717773, -0.0848... |\n",
      "| [0.0159903876483, -0.00032... | [0.0275089349598, 0.022233... |\n",
      "| [-0.00409607496113, -0.022... | [0.103326484561, 0.0580309... |\n",
      "| [0.161496162415, 0.0256378... | [-0.086929500103, -0.04160... |\n",
      "| [0.132295280695, 0.0974058... | [0.0121489893645, -0.03399... |\n",
      "| [0.00350674893707, 0.01158... | [0.0207241680473, 0.050581... |\n",
      "| [0.0717285275459, 0.045404... | [0.00509632425383, 0.04241... |\n",
      "| [0.013498426415, -0.011536... | [-0.0762161165476, 0.00493... |\n",
      "| [0.0992544665933, 0.078602... | [-0.0991027727723, 0.04636... |\n",
      "| [0.00559560954571, 0.01701... | [-0.0847490504384, 0.02191... |\n",
      "| [0.124911136925, 0.0280162... | [-0.144944489002, 0.007318... |\n",
      "| [0.0694912672043, 0.067151... | [0.0828778371215, -0.01081... |\n",
      "| [-0.0633521676064, 0.05861... | [-0.0343761257827, -0.0517... |\n",
      "| [0.0453422516584, -0.05728... | [0.0208444204181, 0.056036... |\n",
      "| [0.0890205055475, 0.044606... | [-0.065517924726, 0.007935... |\n",
      "| [0.0360499583185, -0.00761... | [0.00941018108279, 0.00452... |\n",
      "| [0.0126720024273, -0.02276... | [0.0204297564924, -0.04269... |\n",
      "| [0.131691962481, 0.1103377... | [-0.055712159723, 0.018102... |\n",
      "| [0.176369145513, 0.0798265... | [-0.09994276613, 0.0734846... |\n",
      "| [0.00038406252861, 0.04179... | [0.0352600067854, -0.01112... |\n",
      "| [0.097891420126, -0.024474... | [-0.0639617741108, -0.0251... |\n",
      "| [0.0296963509172, -0.01349... | [-0.00816694181412, 0.0608... |\n",
      "| [0.0102224322036, -0.02213... | [-0.0026703865733, 0.05322... |\n",
      "| [0.0714361444116, 0.051815... | [0.0217944830656, -0.00332... |\n",
      "| [0.0134768839926, 0.030315... | [-0.0181057192385, -0.0140... |\n",
      "| [0.10452747345, 0.03364326... | [-0.0245879832655, -0.0218... |\n",
      "| [0.130606651306, 0.0776157... | [-0.0996610075235, -0.0201... |\n",
      "| [0.0670454278588, 0.040409... | [0.0174743812531, -0.00133... |\n",
      "| [0.182705119252, -0.067482... | [-0.0144196916372, 0.02700... |\n",
      "| [0.0492675304413, -0.00348... | [-0.0515861399472, -0.0360... |\n",
      "| [-0.0966024547815, 0.02233... | [-0.184407457709, -0.04385... |\n",
      "| [0.061713386327, 0.0460945... | [-0.0813238546252, -0.0052... |\n",
      "| [0.0290456842631, 0.050305... | [-0.0380546189845, 0.04545... |\n",
      "| [-0.0109957223758, -0.0274... | [-0.115437924862, -0.03586... |\n",
      "| [0.021477798, 0.0290913842... | [-0.102237008512, 0.015234... |\n",
      "| [0.0176356546581, 0.063226... | [0.0243831146508, 0.008459... |\n",
      "| [0.0128665333614, 0.002072... | [-0.0503745116293, 0.02184... |\n",
      "| [0.141305044293, 0.0088943... | [-0.0529406964779, 0.08550... |\n",
      "| [0.0706762522459, 0.043500... | [0.0554228499532, 0.078685... |\n",
      "| [0.127821058035, 0.0387713... | [0.0224600955844, 0.014846... |\n",
      "| [0.135164186358, 0.0704092... | [0.0409378297627, 0.105077... |\n",
      "| [-0.0752852410078, -0.0812... | [0.0190447382629, 0.005142... |\n",
      "| [0.088198363781, 0.0696218... | [0.0103136710823, 0.044044... |\n",
      "| [0.0754746198654, 0.044201... | [-0.067694209516, 0.045770... |\n",
      "| [0.121692404151, 0.0303368... | [-0.162959054112, -0.04967... |\n",
      "| [0.00190033658873, 0.00612... | [-0.0255160238594, 0.01540... |\n",
      "| [0.1010729298, 0.047441821... | [-0.0697032362223, 0.05735... |\n",
      "| [0.0226684771478, 0.030705... | [-0.102590538561, -0.00204... |\n",
      "| [0.163393393159, 0.0388128... | [-0.0678249374032, 0.02745... |\n",
      "| [-0.103506200016, -0.07061... | [-0.0299058295786, 0.01746... |\n",
      "| [0.0732090473175, 0.042298... | [-0.113662414253, 0.016915... |\n",
      "| [0.0412126109004, 0.026843... | [-0.12367361784, -0.032766... |\n",
      "| [0.0101587520912, 0.044894... | [-0.0429511331022, 0.03808... |\n",
      "| [-0.0707955211401, 0.01270... | [-0.0407743044198, -0.0110... |\n",
      "| [0.0761426612735, -0.00096... | [0.000855805759784, 0.0069... |\n",
      "| [0.0132587254047, 0.035958... | [0.0160514656454, 0.044424... |\n",
      "| [0.10215575248, 0.00513545... | [-0.107103399932, -0.01125... |\n",
      "| [0.0600342825055, 0.001919... | [-0.147752761841, -0.04702... |\n",
      "| [0.0740714892745, 0.008105... | [-0.0746594145894, 0.07659... |\n",
      "| [0.0102399075404, -0.01837... | [-0.00890954397619, 0.0645... |\n",
      "| [0.0104050431401, 0.068003... | [0.0624170005322, 0.025585... |\n",
      "| [0.0747887119651, 0.040028... | [-0.0308105759323, 0.05083... |\n",
      "| [-0.0129357185215, 0.02059... | [0.0339866839349, -0.10177... |\n",
      "| [0.0897816419601, 0.092595... | [0.00858089141548, 0.05269... |\n",
      "| [0.0898949056864, 0.016020... | [-0.0574618801475, 0.02392... |\n",
      "| [0.0442144423723, 0.022028... | [-0.0158031452447, -0.0220... |\n",
      "| [0.0372946076095, -0.00466... | [0.00567228347063, 0.03291... |\n",
      "| [0.0954717099667, -0.00978... | [-0.0748420953751, -0.0069... |\n",
      "| [0.138465583324, -0.002816... | [-0.0867905467749, -0.0304... |\n",
      "| [0.0602867566049, -0.06274... | [-0.124017260969, -0.00332... |\n",
      "| [0.0052992682904, -0.01609... | [-0.00742531241849, 0.0435... |\n",
      "| [0.123843804002, 0.0522734... | [-0.149921476841, 0.048491... |\n",
      "| [0.108563803136, 0.0889096... | [-0.0650622174144, -0.0066... |\n",
      "| [0.147401019931, -0.019558... | [0.0504663772881, -0.01135... |\n",
      "| [0.0725515410304, 0.061234... | [-0.0847049728036, -0.0313... |\n",
      "| [0.0760545283556, 0.090660... | [-0.0443372763693, 0.10322... |\n",
      "| [0.00463952124119, 0.01966... | [-0.0301370508969, 0.05358... |\n",
      "| [0.0389111340046, 0.020329... | [-0.0716275647283, -0.0343... |\n",
      "| [0.0893837288022, -0.00048... | [-0.0400735288858, 0.04493... |\n",
      "| [0.115134015679, 0.0232971... | [-0.112844496965, 0.012505... |\n",
      "| [0.0516687892377, -0.01793... | [-0.0481378957629, 0.10786... |\n",
      "| [-0.0272525716573, 0.05536... | [-0.10721886903, 0.0046645... |\n",
      "| [-0.0612168833613, 0.00064... | [-0.0400614328682, -0.0232... |\n",
      "| [0.00187381752767, -0.0268... | [-0.0588501691818, -0.0274... |\n",
      "| [0.0159787870944, -0.02671... | [-0.0400235466659, -0.0088... |\n",
      "| [-0.0199026782066, -0.0006... | [-0.104008577764, 0.032565... |\n",
      "| [-0.0300348531455, 0.05153... | [-0.13460457325, -0.066520... |\n",
      "| [0.120457105339, 0.0383040... | [-0.0317094884813, 0.07322... |\n",
      "| [-0.0357645489275, 0.00679... | [0.0194496065378, -0.11097... |\n",
      "| [0.118801735342, 0.1237873... | [-0.102538183331, 0.039267... |\n",
      "| [0.0342131592333, 0.013000... | [-0.0136751504615, -0.0671... |\n",
      "| [-0.0476497523487, 0.04332... | [0.00181763211731, 0.02764... |\n",
      "| [0.0406407192349, 0.044575... | [0.0368278771639, 0.015668... |\n",
      "| [0.0914439707994, 0.047760... | [-0.0573430843651, 0.01691... |\n",
      "| [0.099991068244, 0.0089546... | [-0.00414849584922, -0.043... |\n",
      "| [0.0867816358805, 0.103190... | [-0.108259871602, -0.02567... |\n",
      "| [0.0132867479697, 0.003259... | [-0.0449450798333, 0.05618... |\n",
      "| [-0.0618564523757, 0.01010... | [-0.127568289638, 0.026712... |\n",
      "| [0.0926425158978, 0.056237... | [-0.0690314993262, 0.02343... |\n",
      "| [0.111741781235, -0.052814... | [-0.00809447932988, -0.030... |\n",
      "| [0.10147446394, -0.0271267... | [0.023539731279, -0.033592... |\n",
      "| [0.162768989801, 0.0939823... | [-0.00588364526629, 0.0535... |\n",
      "| [0.0366762802005, 0.012108... | [-0.0264404695481, 0.03342... |\n",
      "| [0.162693753839, 0.0156998... | [-0.152779281139, -0.10079... |\n",
      "| [0.134293466806, 0.0399430... | [-0.0236396938562, -0.0031... |\n",
      "| [0.0551957041025, 0.054328... | [-0.0696168839931, -0.0507... |\n",
      "| [-0.0482872985303, 0.01664... | [0.011687614955, 0.0064238... |\n",
      "| [0.0604298859835, 0.016068... | [-0.0100898258388, 0.00192... |\n",
      "| [-0.0469279550016, -0.0365... | [-0.020039357245, -0.03113... |\n",
      "| [0.136503741145, 0.0004583... | [-0.127609878778, 0.057298... |\n",
      "| [0.0504719354212, 0.039818... | [-0.0850827842951, 0.02221... |\n",
      "| [0.0239549018443, -0.00157... | [-0.00720317382365, 0.0174... |\n",
      "| [0.00675392709672, 0.02066... | [-0.0907266885042, -0.0325... |\n",
      "| [-0.00600930443034, 0.0380... | [0.0065565048717, -0.01040... |\n",
      "| [0.089013427496, 0.0757480... | [-0.0329717583954, -0.0245... |\n",
      "| [0.0923304334283, 0.047845... | [-0.0566820912063, 0.04464... |\n",
      "| [0.0448147915304, 0.044528... | [-0.0847820267081, -0.0162... |\n",
      "| [0.0806686729193, 0.034914... | [-0.0488636679947, -0.0474... |\n",
      "| [0.0163390748203, 0.002520... | [-0.0899409651756, 0.00838... |\n",
      "| [0.0190285611898, -0.02665... | [-0.0307907983661, 0.03693... |\n",
      "| [0.0383166559041, 0.014046... | [0.00055006420007, 0.01861... |\n",
      "| [0.109769538045, 0.0005380... | [-0.0310226771981, -0.0075... |\n",
      "| [-0.0265385396779, 0.03465... | [-0.0727132260799, 0.08212... |\n",
      "| [0.0716057494283, 0.037512... | [0.0164673831314, 0.000148... |\n",
      "| [0.0805361270905, 0.061609... | [-0.0543365068734, 0.05371... |\n",
      "| [0.00654507242143, 0.05004... | [-0.147713065147, -0.04980... |\n",
      "| [0.0986822992563, 0.099681... | [0.0289698336273, -0.02811... |\n",
      "| [0.00362138450146, 0.01925... | [0.0155096845701, 0.002772... |\n",
      "| [0.0398951843381, 0.070748... | [-0.0288582835346, 0.06711... |\n",
      "| [0.0347172170877, 0.041598... | [-0.0286541413516, 0.01771... |\n",
      "| [0.0536781065166, 0.021448... | [-0.00824998039752, 0.0030... |\n",
      "| [0.031580131501, -0.009028... | [-0.0384875498712, -0.0638... |\n",
      "| [0.123884223402, -0.025702... | [-0.107289731503, 0.072847... |\n",
      "| [0.0553863570094, 0.006139... | [-0.0420567169785, -0.0205... |\n",
      "| [0.0107287364081, 0.038497... | [-0.0628749504685, 0.05211... |\n",
      "| [-0.0824421793222, -0.0027... | [-0.015110197477, 0.037749... |\n",
      "| [0.107138991356, -0.133607... | [-0.0453491359949, 0.01026... |\n",
      "| [0.115738920867, -0.087941... | [-0.148734524846, 0.006988... |\n",
      "| [0.112503454089, 0.0410843... | [0.0498917885125, 0.014249... |\n",
      "| [0.100822247565, 0.0685932... | [-0.0216466058046, -0.0363... |\n",
      "| [0.0559836253524, 0.040534... | [-0.0458233468235, -0.0029... |\n",
      "| [0.171316459775, 0.0234743... | [-0.136303052306, -0.04050... |\n",
      "| [0.036456156522, 0.0704887... | [-0.0634521842003, 0.11170... |\n",
      "| [0.058960929513, 0.0689015... | [0.0328996442258, 0.039060... |\n",
      "| [0.185650154948, 0.0568065... | [-0.0212185755372, -0.0949... |\n",
      "| [-0.0326340161264, -0.0252... | [0.00855151657015, 0.03011... |\n",
      "| [-0.00141433509998, -0.035... | [-0.124435827136, -0.04317... |\n",
      "| [0.032900840044, 0.0299094... | [-0.00546560809016, 0.0243... |\n",
      "| [0.0230798255652, 0.044304... | [-0.0133921690285, -0.0046... |\n",
      "| [0.0840777903795, 0.069860... | [-0.0342310555279, 0.00129... |\n",
      "| [0.163026183844, 0.0297715... | [-0.0377180241048, -0.0089... |\n",
      "| [0.00975848268718, 0.04112... | [-0.040051329881, 0.057123... |\n",
      "| [0.106456212699, 0.0183660... | [-0.0598263591528, 0.03069... |\n",
      "| [0.0846844688058, 0.073722... | [-0.0659293234348, 0.05477... |\n",
      "| [0.0602086186409, 0.081270... | [-0.0151858376339, 0.05123... |\n",
      "| [0.14880117774, 0.06557853... | [-0.0348721928895, 0.04381... |\n",
      "| [0.0845233723521, 0.033332... | [-0.119349323213, 0.038423... |\n",
      "| [0.0572250187397, 0.107821... | [-0.0347604006529, 0.06731... |\n",
      "| [0.0767703652382, -0.05363... | [-0.093043923378, -0.09396... |\n",
      "| [-0.00185601250269, 0.0006... | [-0.0407268926501, -0.0749... |\n",
      "| [0.021862719208, -0.000133... | [-0.0449810102582, -0.0139... |\n",
      "| [0.0819378420711, 0.026825... | [0.00577952293679, 0.00457... |\n",
      "| [0.00550261605531, 0.02545... | [0.00799831189215, 0.00698... |\n",
      "| [0.0345849879086, 0.028918... | [-0.0615450292826, 0.00945... |\n",
      "| [0.0646410733461, 0.037191... | [-0.0581789314747, -0.0127... |\n",
      "| [0.0279638282955, 0.003780... | [-0.0628831759095, 0.00177... |\n",
      "| [0.0835594087839, -0.00707... | [-0.15547336638, -0.009694... |\n",
      "| [0.15775193274, 0.04754373... | [-0.0956281721592, -0.0301... |\n",
      "| [0.0889324322343, 0.133494... | [0.0839013382792, 0.057207... |\n",
      "| [0.135650888085, 0.0468959... | [-0.0909791663289, 0.05759... |\n",
      "| [0.121551141143, 0.0938665... | [-0.0663660317659, 0.04686... |\n",
      "| [0.0605899430811, 0.026519... | [-0.0413345396519, 0.00498... |\n",
      "| [0.201625347137, -0.062467... | [-0.133090734482, -0.13420... |\n",
      "| [0.0499690882862, 0.045941... | [0.000593225471675, -0.011... |\n",
      "| [0.0432216525078, 0.074600... | [-0.0325164012611, 0.03339... |\n",
      "| [0.12021869421, -0.0358995... | [-0.132023826241, -0.04307... |\n",
      "| [0.0506272241473, -0.02849... | [-0.108429856598, -0.02343... |\n",
      "| [-0.0252869296819, -0.0433... | [-0.0389261506498, 0.01868... |\n",
      "| [0.0462179780006, 0.029833... | [-0.0478379428387, -0.0714... |\n",
      "| [0.0879946947098, -0.00936... | [0.0275080818683, 0.005066... |\n",
      "| [0.118005305529, -0.026530... | [-0.0786823928356, -0.0149... |\n",
      "| [0.0426518060267, 0.026783... | [-0.0368417985737, 0.02254... |\n",
      "| [0.141383171082, -0.037766... | [-0.101970143616, -0.00297... |\n",
      "| [0.0649816170335, 0.098885... | [0.0193480681628, -0.11054... |\n",
      "| [0.0378896407783, -0.02195... | [-0.13405571878, 0.0426398... |\n",
      "| [0.0425102524459, -0.00393... | [0.0265094842762, 0.023118... |\n",
      "| [0.0703432112932, 0.063389... | [-0.0414600223303, -0.0212... |\n",
      "| [0.0607297681272, 0.065873... | [-0.0476631857455, -0.0477... |\n",
      "| [0.109387248755, -0.038954... | [-0.0728551670909, 0.01254... |\n",
      "| [0.0224410369992, 0.005263... | [0.00102464586962, -0.0099... |\n",
      "| [0.0962359532714, 0.080933... | [-0.0427970848978, 0.00449... |\n",
      "| [0.0187284722924, -0.02739... | [-0.0622581765056, -0.0486... |\n",
      "| [0.0168247316033, 0.028698... | [-0.10455802083, -0.012645... |\n",
      "| [-0.0776744261384, -0.0081... | [-0.0479247458279, 0.04116... |\n",
      "| [0.100606553257, 0.0966513... | [-0.134176120162, 0.122406... |\n",
      "| [0.0492269583046, 0.053114... | [-0.00356664857827, 0.0657... |\n",
      "| [0.0448147915304, 0.044528... | [-0.0847820267081, -0.0162... |\n",
      "| [0.047498177737, -0.025736... | [-0.134140074253, -0.06256... |\n",
      "| [0.04782519117, 0.06033409... | [-0.0222770497203, 0.01396... |\n",
      "| [0.212088420987, 0.0699284... | [-0.0427153110504, -0.0328... |\n",
      "| [0.0373480692506, -0.00656... | [-0.0373855978251, 0.02863... |\n",
      "| [0.100720517337, 0.0363638... | [-0.0717587471008, -0.0319... |\n",
      "| [0.054864179343, 0.0576945... | [-0.0734975114465, 0.03152... |\n",
      "| [0.0932852178812, -0.03697... | [-0.0248047374189, -0.0075... |\n",
      "| [-0.00449765939265, -0.011... | [-0.00300249760039, 0.0090... |\n",
      "| [0.0289562512189, -0.05112... | [0.0611937642097, -0.04627... |\n",
      "| [0.0175907928497, -2.38666... | [-0.0246466789395, -0.1137... |\n",
      "| [0.13560397923, 0.01047624... | [-0.12368080765, -0.024783... |\n",
      "| [0.0713485330343, 0.044041... | [-0.0920603498816, 0.00878... |\n",
      "| [0.117192648351, 0.0239159... | [-0.11784491688, -0.022249... |\n",
      "| [0.168832242489, 0.0272245... | [-0.0248406212777, -0.0024... |\n",
      "| [0.128103345633, -0.026771... | [-0.0339549258351, 0.01778... |\n",
      "| [0.086062297225, 0.0099880... | [-0.0704948604107, 0.03265... |\n",
      "| [-0.000281204789644, 0.088... | [-0.00260891905054, -0.030... |\n",
      "| [-0.0987945273519, -0.0081... | [-0.100981213152, 0.063932... |\n",
      "| [0.0860193967819, 0.034841... | [-0.0559086129069, -0.0157... |\n",
      "| [0.140543878078, -0.031374... | [-0.0281970109791, 0.01957... |\n",
      "| [0.0906044840813, 0.047504... | [0.0194967873394, -0.00444... |\n",
      "| [0.0516763217747, 0.011650... | [-0.121512196958, -0.03715... |\n",
      "| [0.0644848868251, 0.006297... | [-0.110011599958, 0.005280... |\n",
      "| [-0.0705043748021, 0.03532... | [-0.0191950108856, -0.0140... |\n",
      "| [0.0470976606011, -0.02559... | [-0.0813533514738, -0.0347... |\n",
      "| [0.147388473153, 0.0082419... | [-0.116546399891, -0.02726... |\n",
      "| [0.0739592760801, -0.02210... | [0.00693877181038, -0.0091... |\n",
      "| [0.148052126169, 0.0717593... | [-0.0288940872997, 0.02822... |\n",
      "| [0.0986103862524, -0.03404... | [-0.12384147197, 0.0343502... |\n",
      "| [0.106411747634, 0.0294853... | [-0.0513948611915, 0.05602... |\n",
      "| [0.00826187245548, 0.05369... | [0.00346167082898, -0.0212... |\n",
      "| [0.0918313339353, 0.104876... | [-0.155231237411, 0.093401... |\n",
      "| [0.0750747397542, 0.079889... | [-0.0385886132717, 0.00627... |\n",
      "| [0.0610880926251, 0.072156... | [-0.00801097042859, -0.031... |\n",
      "| [0.186701864004, -0.036909... | [-0.0518744364381, 0.05110... |\n",
      "| [-0.0741173401475, -0.0247... | [-0.016558855772, 0.052894... |\n",
      "| [0.0265525188297, 0.019773... | [-0.0249184686691, -0.0775... |\n",
      "| [-0.236925393343, 0.016983... | [0.0461575649679, -0.09090... |\n",
      "| [0.0509794764221, -0.00221... | [0.0391275398433, 0.021463... |\n",
      "| [0.047042619437, -0.008585... | [0.0208697244525, -0.03101... |\n",
      "| [0.0211136117578, -0.00302... | [0.0186220500618, 0.033669... |\n",
      "| [0.0197880398482, 0.048756... | [-0.0315691716969, 0.00649... |\n",
      "| [-0.0320809371769, 0.04036... | [0.0463209673762, 0.034736... |\n",
      "| [0.0215905364603, 0.072911... | [-0.00528509821743, 0.0597... |\n",
      "| [0.039929267019, 0.0673335... | [-0.0628116726875, -0.0059... |\n",
      "| [0.0922877267003, 0.073609... | [0.0506470762193, 0.011558... |\n",
      "| [-0.0402984879911, -0.0470... | [-0.0419899374247, -0.1424... |\n",
      "| [-0.00770078785717, 0.0424... | [-0.127803206444, 0.081630... |\n",
      "| [0.0238788556308, 0.054537... | [0.0338765718043, -0.00229... |\n",
      "| [0.114159703255, 0.0738204... | [-0.0710958391428, 0.09880... |\n",
      "| [0.140050709248, -0.016999... | [-0.087649308145, -0.02841... |\n",
      "| [-0.0382101722062, 0.04672... | [-0.0443196035922, -6.5671... |\n",
      "| [0.0285921171308, 0.018223... | [0.0375352688134, 0.127024... |\n",
      "| [0.113145120442, -0.025239... | [-0.00810715183616, -0.074... |\n",
      "| [0.0287271887064, -0.00169... | [-0.0554183498025, -0.0513... |\n",
      "| [0.00977312028408, 0.04211... | [-0.0374377667904, -0.0778... |\n",
      "| [0.0253151264042, -0.03375... | [0.0390156768262, -0.09379... |\n",
      "| [0.0124820843339, 0.119835... | [-0.0522232465446, 0.04474... |\n",
      "| [0.0564348883927, -0.02328... | [-0.0609393343329, 0.00737... |\n",
      "| [-0.0199030879885, 0.01499... | [-0.0430292785168, 0.02752... |\n",
      "| [0.150586023927, 0.0394405... | [-0.0786501467228, 0.00609... |\n",
      "| [0.0295341946185, -0.03053... | [-0.0736955180764, -0.0058... |\n",
      "| [0.164906084538, -0.027464... | [-0.0797619894147, 0.04102... |\n",
      "| [0.0371321327984, -0.03832... | [-0.0235275998712, -0.0320... |\n",
      "| [-0.00975280161947, 0.0066... | [-0.00367890996858, -0.050... |\n",
      "| [0.0246596969664, 0.070899... | [-0.0157567150891, -0.0048... |\n",
      "| [-0.0167858023196, -0.0306... | [-0.0243251305073, -0.0207... |\n",
      "| [0.0722602903843, 0.048838... | [0.0714136809111, -0.00932... |\n",
      "| [0.125273287296, 0.0227451... | [-0.170420885086, -0.01884... |\n",
      "| [-0.000268361211056, 0.037... | [-0.0394067503512, -0.0387... |\n",
      "| [0.0744532123208, 0.032232... | [0.0168098527938, 0.036552... |\n",
      "| [0.0368175432086, 0.048278... | [-0.0317231863737, 0.01824... |\n",
      "| [-0.0181189458817, 0.01903... | [-0.0579950064421, -0.0223... |\n",
      "| [0.0220087077469, 0.051657... | [-0.0446775741875, 0.02436... |\n",
      "| [0.0926528871059, -0.00927... | [-0.0755148008466, -0.0248... |\n",
      "| [0.0689836591482, 0.018581... | [-0.0384117849171, 0.03999... |\n",
      "| [0.129791155457, 0.0863009... | [-0.102800630033, 0.061082... |\n",
      "| [-0.0110491001979, 0.04059... | [-0.0817290320992, 0.05791... |\n",
      "| [0.0465990267694, -0.03880... | [-0.048153873533, 0.015194... |\n",
      "| [-0.0319351069629, 0.04621... | [-0.128782182932, 0.091671... |\n",
      "| [0.0462847985327, -0.01071... | [-0.0668838992715, -0.0471... |\n",
      "| [0.0593018755317, -0.00781... | [-0.0556268058717, -0.0300... |\n",
      "| [-0.0457326732576, 0.03757... | [-0.0127360671759, 0.04642... |\n",
      "| [0.0236354935914, 0.091518... | [-0.01130502671, 0.1496441... |\n",
      "| [0.0778558328748, 0.023786... | [-0.0615060478449, 0.00814... |\n",
      "| [0.0104048764333, 0.080693... | [0.0381392501295, 0.012157... |\n",
      "| [0.0990164205432, 0.019992... | [-0.0469067245722, 0.04767... |\n",
      "| [0.0371530167758, 0.145652... | [-0.000806928612292, 0.007... |\n",
      "| [-0.0166969429702, 0.01934... | [-0.0872879996896, -0.0607... |\n",
      "| [0.0270560234785, -0.05098... | [-0.00513749243692, 0.1282... |\n",
      "| [-0.0394740030169, 0.03538... | [-0.128029808402, 0.100608... |\n",
      "| [0.0757330432534, 0.115519... | [0.0187007617205, -0.01295... |\n",
      "| [0.0623273327947, 0.015957... | [-0.0386104956269, -0.0299... |\n",
      "| [0.212776690722, 0.0969738... | [-0.0492134317756, 0.06185... |\n",
      "| [0.0307934693992, -0.00046... | [-0.0249706804752, 0.00888... |\n",
      "| [0.0289635341614, 0.042523... | [-0.122153930366, -0.04483... |\n",
      "| [0.109968744218, -0.008929... | [-0.0889867246151, 0.00866... |\n",
      "| [0.1146126315, -0.02484617... | [-0.0542266592383, -0.0096... |\n",
      "| [0.0898616909981, 0.004197... | [-0.066075630486, 0.025661... |\n",
      "| [0.170317873359, 0.0414564... | [0.00942583568394, 0.04185... |\n",
      "| [0.0708517059684, -0.00533... | [0.00654547801241, -0.0133... |\n",
      "| [0.0574431829154, -0.00786... | [-0.0315835364163, -0.0230... |\n",
      "| [0.0756903588772, 0.017551... | [-0.0455748401582, -0.0075... |\n",
      "| [0.0143534559757, 0.060771... | [-0.040267854929, 0.041663... |\n",
      "| [0.0263665560633, 0.019368... | [-0.0169086884707, -0.0211... |\n",
      "| [0.0529702976346, -0.01197... | [-0.0746861994267, 0.04533... |\n",
      "| [0.0747154578567, -0.08489... | [-0.125231683254, -0.08878... |\n",
      "| [0.235693573952, 0.0180868... | [-0.122282296419, -0.05587... |\n",
      "| [0.0605420805514, 0.065281... | [-0.0575769767165, 0.02201... |\n",
      "| [0.079871237278, -0.057180... | [-0.0706871300936, -0.0108... |\n",
      "| [0.129749342799, 0.0928739... | [-0.0591086857021, -0.0210... |\n",
      "| [0.0417427606881, 0.029707... | [-0.0369121916592, 0.04228... |\n",
      "| [0.053000126034, 0.0705508... | [-0.0421138852835, -0.0134... |\n",
      "| [0.203236147761, 0.0518301... | [0.140521466732, -0.007910... |\n",
      "| [0.100853025913, 0.0247420... | [-0.100314766169, 0.023268... |\n",
      "| [0.0524277985096, -0.03561... | [-0.00412112381309, 0.0194... |\n",
      "| [0.0214361995459, -0.03037... | [-0.0365919880569, -0.0512... |\n",
      "| [0.0480110011995, 0.017843... | [-0.0621462874115, 0.04487... |\n",
      "| [0.074514426291, -0.019486... | [-0.0245777592063, 0.02267... |\n",
      "| [0.0790068358183, -0.04524... | [-0.0770814493299, -0.0853... |\n",
      "| [0.0579403676093, 0.067560... | [-0.0410621017218, 0.06026... |\n",
      "| [0.0239074453712, 0.010380... | [0.00296512618661, 0.03746... |\n",
      "| [-0.000749851285946, -0.01... | [-0.061167858541, 0.052761... |\n",
      "| [0.101228378713, 0.0576586... | [-0.102800682187, 0.016869... |\n",
      "| [0.167257234454, 0.1310135... | [-0.00754807982594, 0.0421... |\n",
      "| [0.00574208796024, -0.0159... | [-0.0575670599937, -0.0411... |\n",
      "| [0.00282183289528, -0.0139... | [-0.0140526322648, -0.0488... |\n",
      "| [0.154132679105, 0.1227048... | [0.113785348833, 0.0319196... |\n",
      "| [0.0684042721987, 0.041628... | [-0.000592562078964, -0.04... |\n",
      "| [0.00897742155939, 0.04233... | [-0.0569441206753, 0.03276... |\n",
      "| [0.229028254747, 0.0493598... | [0.0218199975789, -0.00534... |\n",
      "| [-0.0163240861148, 0.01291... | [-0.0216323900968, -0.0149... |\n",
      "| [-0.0936389267445, 0.02915... | [0.00738580338657, 0.03187... |\n",
      "| [0.0628043934703, 0.026787... | [-0.0408481433988, -0.0234... |\n",
      "| [-0.0822860002518, 0.00223... | [-0.114380002022, 0.069485... |\n",
      "| [0.163053750992, 0.0287375... | [-0.0708812102675, 0.04883... |\n",
      "| [-0.0347759537399, 0.05106... | [-0.00177001021802, 0.0321... |\n",
      "| [0.0623453073204, 0.003217... | [-0.0481769293547, 0.02931... |\n",
      "| [0.074557594955, 0.0671757... | [0.0692637339234, 0.030971... |\n",
      "| [0.0184451118112, 0.000864... | [0.009550713934, -0.045708... |\n",
      "| [0.0527863912284, -0.02181... | [-0.0478941388428, -0.0343... |\n",
      "| [0.0205282792449, -0.00910... | [0.0281494762748, -0.05475... |\n",
      "| [0.00269297580235, -0.0201... | [-0.0533668473363, -0.0226... |\n",
      "| [0.105943091214, 0.0426387... | [-0.0582564808428, -0.0678... |\n",
      "| [0.0617477558553, 0.029005... | [0.0103685706854, 0.008307... |\n",
      "| [0.0907120034099, 0.006842... | [-0.00176142365672, -0.028... |\n",
      "| [0.00907680764794, 0.07564... | [-0.015319397673, 0.030734... |\n",
      "| [-0.0313007421792, -0.0056... | [0.013859921135, 0.0260610... |\n",
      "| [0.0302574262023, 0.026075... | [0.0459271110594, 0.026054... |\n",
      "| [0.0741234123707, 0.065596... | [-0.071001239121, 0.023931... |\n",
      "| [0.201031684875, 0.0062241... | [-0.105887837708, 0.055402... |\n",
      "| [0.100543394685, 0.0324598... | [-0.0440127924085, -0.0135... |\n",
      "| [0.00163818523288, 0.07325... | [0.00258366391063, 0.03065... |\n",
      "| [-0.0111932596192, 0.04241... | [0.00854220055044, -0.0064... |\n",
      "| [-0.0676506087184, 0.06908... | [-0.0511249303818, 0.06271... |\n",
      "| [0.123259373009, 0.0196634... | [-0.040301989764, -0.01246... |\n",
      "| [0.13611997664, 0.06439060... | [-0.0108082182705, 0.00470... |\n",
      "| [0.0204437654465, -0.04454... | [-0.037959434092, -0.04207... |\n",
      "| [-0.0129145868123, 0.05920... | [-0.0455581136048, 0.11155... |\n",
      "| [0.0307477470487, 0.005052... | [-0.110414199531, -0.00179... |\n",
      "| [0.0829643383622, 0.042820... | [-0.0434163920581, 0.02460... |\n",
      "| [0.156884133816, 0.1024928... | [0.00674970028922, 0.02312... |\n",
      "| [0.036847576499, 0.0563909... | [-0.0494205281138, 0.02516... |\n",
      "| [0.09639852494, -0.0486703... | [-0.0421989373863, -0.0532... |\n",
      "| [0.0720421224833, -0.00599... | [0.00645740656182, -0.0301... |\n",
      "| [0.0427679903805, 0.060553... | [-0.0864373967052, -0.0132... |\n",
      "| [0.129274457693, 0.0513636... | [-0.174457877874, 0.024054... |\n",
      "| [0.0357083231211, 0.031008... | [-0.0474932789803, -0.0145... |\n",
      "| [0.0395422838628, 0.023403... | [-0.022938368842, -0.01367... |\n",
      "| [0.0445496253669, 0.024880... | [-0.0292386058718, -0.0043... |\n",
      "| [0.0307746175677, 0.014941... | [-0.0978095009923, -0.0115... |\n",
      "| [-0.0179427172989, -0.0334... | [0.0131835089996, -0.09960... |\n",
      "| [0.0942674428225, 0.004340... | [-0.108958862722, -0.00156... |\n",
      "| [-0.0503708310425, 0.14751... | [-0.0243763960898, 0.06376... |\n",
      "| [-0.0249286592007, 0.05088... | [0.057610951364, -0.088731... |\n",
      "| [-0.000374163559172, 0.044... | [-0.0864512994885, 0.00739... |\n",
      "| [0.0526335611939, -0.00051... | [-0.025063611567, 0.006197... |\n",
      "| [0.00663153640926, 0.02778... | [-0.104087486863, 0.024173... |\n",
      "| [-0.0949370190501, 0.03390... | [0.0387844331563, 0.002608... |\n",
      "| [0.13305015862, 0.12894824... | [0.035401172936, 0.0542224... |\n",
      "| [0.0335081294179, 0.072725... | [-0.106432020664, 0.055270... |\n",
      "| [0.205003097653, 0.1229295... | [0.022571478039, 0.0784760... |\n",
      "| [0.0789992958307, 0.046536... | [0.00931936781853, -0.0067... |\n",
      "| [0.152633845806, 0.0406329... | [-0.0981472805142, -0.0149... |\n",
      "| [0.0877893194556, 0.032607... | [0.0113733438775, 0.027843... |\n",
      "| [0.120220154524, 0.0214231... | [-0.0366549044847, -0.0153... |\n",
      "| [0.0204367693514, 0.026664... | [-0.0314375311136, 0.01418... |\n",
      "| [0.100619398057, 0.0829153... | [-0.102301470935, 0.030435... |\n",
      "| [0.122921526432, -0.004567... | [-0.0348535478115, -0.0533... |\n",
      "| [-0.0120444968343, -0.0101... | [-0.0650539621711, 0.04675... |\n",
      "| [-0.0350898243487, -0.0660... | [-0.0187067687511, 0.06095... |\n",
      "| [0.108740210533, 0.0343648... | [-0.0886919498444, -0.0650... |\n",
      "| [0.162440940738, -0.011015... | [-0.0424901992083, -0.0716... |\n",
      "| [0.0960418283939, -0.02512... | [0.0212058518082, 0.027098... |\n",
      "| [0.0077661476098, 0.061179... | [0.0583236552775, 0.041547... |\n",
      "| [0.0670233517885, 0.020198... | [-0.107733510435, -0.01009... |\n",
      "| [0.00586873618886, 0.00238... | [-0.0514991991222, 0.01725... |\n",
      "| [0.0525717437267, 0.030956... | [-0.0871175378561, -0.0053... |\n",
      "| [0.22589096427, 0.00235791... | [0.011698063463, 0.0456161... |\n",
      "| [-0.0140031576157, -0.0102... | [-0.0196136198938, 0.05129... |\n",
      "| [-0.0149457314983, 0.03614... | [0.0209196396172, 0.010508... |\n",
      "| [0.0231073126197, -0.01873... | [-0.0170355290174, -0.0619... |\n",
      "| [0.0106760403141, -0.03589... | [-0.017196636647, 0.017636... |\n",
      "| [0.0475979559124, 0.043773... | [-0.0738250091672, 0.07065... |\n",
      "| [0.0667612478137, 0.033003... | [-0.030498566106, -0.07252... |\n",
      "| [0.041425678879, 0.0428479... | [-0.0675428584218, -0.0119... |\n",
      "| [0.0961557477713, -0.04687... | [-0.111814327538, 0.020894... |\n",
      "| [0.0650061517954, 0.057577... | [-0.0135656297207, 0.02910... |\n",
      "| [0.0429412461817, 0.096304... | [0.0391984805465, 0.055093... |\n",
      "| [-0.0281083583832, 0.03361... | [0.0630896016955, 0.050751... |\n",
      "| [0.143003568053, 0.0731129... | [-0.0661876276135, -0.0362... |\n",
      "| [0.115089826286, -0.133523... | [-0.0172517113388, 0.01967... |\n",
      "| [0.0325312167406, 0.023713... | [-0.00223671994172, -0.046... |\n",
      "| [0.0660203546286, 0.065255... | [0.0557544007897, 0.024436... |\n",
      "| [0.0503943040967, 0.025874... | [-0.0339791327715, -0.0296... |\n",
      "| [-0.0344006642699, -0.0328... | [-0.0620458498597, 0.08986... |\n",
      "| [0.0279102232307, -0.01585... | [0.0140903992578, 0.039128... |\n",
      "| [0.133595764637, -0.058276... | [0.0508729852736, -0.09799... |\n",
      "| [0.007229315117, 0.0020496... | [0.0166342034936, 0.046824... |\n",
      "| [0.121269583702, 0.1291814... | [-0.0824662894011, 0.05938... |\n",
      "| [0.109624281526, -0.029384... | [-0.0752609446645, -0.0336... |\n",
      "| [0.00707390904427, -0.0333... | [-0.087297052145, -0.02687... |\n",
      "| [0.037852909416, 0.0232453... | [-0.0432416535914, -0.0027... |\n",
      "| [0.138505235314, 0.0202522... | [0.00249797105789, 0.00601... |\n",
      "| [0.0562476664782, -0.02988... | [-0.0103973792866, -0.0205... |\n",
      "| [0.0947135165334, 0.033199... | [-0.131564602256, -0.02615... |\n",
      "| [0.0257261544466, 0.024255... | [0.0265622586012, 0.019172... |\n",
      "| [0.0369274765253, 0.025060... | [0.00269733113237, 0.06932... |\n",
      "| [0.0592079199851, 0.005201... | [-0.0462207011878, 0.06177... |\n",
      "| [-0.0423817113042, 0.03050... | [-0.0209733806551, -0.0084... |\n",
      "| [0.12607331574, -0.0101040... | [0.00259897275828, 0.01656... |\n",
      "| [-0.0384789295495, 0.00932... | [-0.055239263922, -0.04126... |\n",
      "| [0.0628081485629, 0.002427... | [-0.0925926789641, -0.0581... |\n",
      "| [-0.00374971865676, 0.0311... | [0.0146793033928, 0.051894... |\n",
      "| [-0.0453805364668, -0.0170... | [0.00769061082974, -0.0910... |\n",
      "| [0.0861588865519, -0.00552... | [-0.0230658873916, 0.03507... |\n",
      "| [0.01867085509, -0.0028244... | [-0.0386060364544, 0.07843... |\n",
      "| [0.116558164358, 0.0370915... | [-0.118992649019, -0.03439... |\n",
      "| [0.0446674712002, -0.01305... | [-0.0317050814629, 0.07334... |\n",
      "| [0.200759634376, 0.0673387... | [-0.0715128183365, 0.01657... |\n",
      "| [0.0919890031219, 0.019129... | [-0.0342242047191, -0.0114... |\n",
      "| [-0.0295886490494, 0.01625... | [-0.00922585278749, -0.008... |\n",
      "| [0.0623150542378, 0.025543... | [-0.0351044051349, 0.04751... |\n",
      "| [-0.0339419692755, 0.01987... | [-0.0233715549111, 0.01246... |\n",
      "| [0.00836451351643, 0.06107... | [-0.0363219864666, 0.08363... |\n",
      "| [0.129100799561, -0.016536... | [-0.100742384791, -0.06748... |\n",
      "| [0.11032666266, 0.00498426... | [-0.0569333322346, -0.0537... |\n",
      "| [-0.0501603521407, 0.02150... | [-0.068449139595, -0.00312... |\n",
      "| [0.198002278805, -0.001731... | [-0.0648123472929, -0.0320... |\n",
      "| [0.10716895014, 0.11445809... | [0.0146925551817, -0.08223... |\n",
      "| [0.00622666953132, -0.0277... | [-0.0131533807144, 0.04093... |\n",
      "| [0.0898292064667, -0.00777... | [-0.0381370894611, 0.03700... |\n",
      "| [0.0163106676191, 0.074505... | [-0.00922801252455, -0.037... |\n",
      "| [0.0251783598214, 0.068307... | [0.0114969424903, -0.02535... |\n",
      "| [0.0300160031766, 0.036310... | [-0.0203896872699, 0.03937... |\n",
      "| [0.0685505867004, 0.003471... | [-0.0708217248321, 0.01300... |\n",
      "| [0.0877037644386, 0.000402... | [0.0275159627199, 0.036369... |\n",
      "| [0.0465025603771, 0.033278... | [-0.000632042181678, 0.067... |\n",
      "| [0.0640282928944, -0.02286... | [-0.0388703495264, 0.03316... |\n",
      "| [0.00713589321822, -0.0023... | [-0.07790466398, 0.0385010... |\n",
      "| [-0.0255252774805, 0.04428... | [-0.00460693519562, -0.009... |\n",
      "| [0.0344837829471, 0.033649... | [-0.033383987844, -0.04268... |\n",
      "| [-0.0533604994416, 0.00404... | [0.0371165052056, 0.045198... |\n",
      "| [-0.039164390415, 0.014153... | [-0.0631232187152, -0.0096... |\n",
      "| [0.014224216342, 0.0195629... | [-0.0403252057731, 0.08232... |\n",
      "| [0.0324010923505, 0.029930... | [-0.0280165951699, 0.03207... |\n",
      "| [0.106136992574, 0.0897235... | [0.0493684224784, -0.04045... |\n",
      "| [0.15006493032, 0.13062998... | [-0.124495588243, 0.044481... |\n",
      "| [0.182604819536, 0.1322038... | [-0.0263245031238, -0.0228... |\n",
      "| [0.0252718832344, 0.049814... | [-0.022931067273, 0.022603... |\n",
      "| [0.0795855894685, 0.012063... | [-0.0820577591658, -0.0770... |\n",
      "| [-0.118080101907, 0.014017... | [0.00676729809493, 0.04466... |\n",
      "| [0.132728636265, -0.044486... | [-0.0481637381017, -0.0874... |\n",
      "| [0.0586962997913, 0.020774... | [0.00490211416036, 0.05282... |\n",
      "| [-0.0134796006605, -0.0329... | [0.00100813026074, -0.0796... |\n",
      "| [0.0732724517584, 0.018173... | [-0.00947911571711, -0.006... |\n",
      "| [0.0299794729799, 0.031878... | [-0.0347666926682, -0.0168... |\n",
      "| [0.0570354573429, 0.001580... | [0.0381179265678, 0.010543... |\n",
      "| [-0.00556052988395, -0.008... | [-0.0225384272635, -0.1013... |\n",
      "| [0.0439150445163, 0.087061... | [-0.0532070733607, -0.0743... |\n",
      "| [0.0479825623333, 0.085777... | [-0.0860209614038, 0.02405... |\n",
      "| [0.0845887959003, 0.011272... | [-0.122913524508, 0.024460... |\n",
      "| [-0.00420900853351, -0.006... | [-0.102153114974, -0.02012... |\n",
      "| [-0.0243055932224, -0.0091... | [-0.047344814986, 0.013520... |\n",
      "| [0.168588101864, 0.0707587... | [-0.0145828872919, 0.00644... |\n",
      "| [0.0495227985084, -0.01327... | [-0.0598235242069, -0.0044... |\n",
      "| [-0.0254818256944, 0.04004... | [-0.106338001788, 0.011851... |\n",
      "| [0.129461199045, 0.1196740... | [-0.0775872021914, -0.0188... |\n",
      "| [0.04366305843, 0.05230921... | [-0.00928513798863, 0.0574... |\n",
      "| [-0.0186640247703, -0.1014... | [-0.00110835279338, -0.036... |\n",
      "| [0.107231877744, -0.104487... | [-0.171050712466, -0.17312... |\n",
      "| [0.0247397366911, -0.00160... | [-0.00364230992272, -0.017... |\n",
      "| [0.0904796123505, -0.03455... | [-0.028000921011, -0.05263... |\n",
      "| [0.0640347972512, 0.033737... | [-0.0676734596491, -0.0008... |\n",
      "| [0.0252814367414, 0.014175... | [-0.0199760869145, 0.03089... |\n",
      "| [0.0413723438978, 0.051801... | [-0.0421677008271, 0.08298... |\n",
      "| [0.11823131144, 0.10469465... | [-0.0327265746891, 0.02856... |\n",
      "| [0.0139921400696, -0.03516... | [-0.114260360599, 0.066339... |\n",
      "| [0.0245990213007, -0.00655... | [-0.0976864770055, 0.02867... |\n",
      "| [-0.0528225488961, -0.0132... | [0.0175140611827, -0.00789... |\n",
      "| [0.0793718993664, 0.042451... | [-0.0958971679211, 0.00173... |\n",
      "| [0.0339921712875, 0.056951... | [-0.00990066491067, 0.0600... |\n",
      "| [0.0679406300187, 0.119783... | [-0.05232013762, 0.0180290... |\n",
      "| [0.13400144875, -0.0015652... | [-0.00556737184525, 0.0172... |\n",
      "| [0.100489608943, -0.011469... | [-0.138352975249, 0.041443... |\n",
      "| [0.0783539935946, 0.066234... | [-0.0804416239262, 0.04011... |\n",
      "| [0.0238362979144, -0.00267... | [-0.0775430873036, 0.01031... |\n",
      "| [-0.00057088478934, -0.082... | [-0.151460036635, -0.15328... |\n",
      "| [0.138150408864, -0.026785... | [-0.0585446469486, 0.01934... |\n",
      "| [0.0433733277023, 0.084200... | [0.0182784758508, 0.040223... |\n",
      "| [0.116885051131, -0.006741... | [-0.00063277373556, -0.022... |\n",
      "| [0.0261017549783, 0.004635... | [-0.0524662584066, 0.07225... |\n",
      "| [0.0829963609576, 0.074331... | [-0.106801703572, 0.116321... |\n",
      "| [0.048252414912, 0.0463794... | [-0.038573615253, -0.03044... |\n",
      "| [0.13228186965, 0.12888576... | [-0.0698223039508, 0.02293... |\n",
      "| [0.0693788006902, 0.059290... | [-0.0940863713622, 0.03617... |\n",
      "| [0.0447876825929, 0.003584... | [0.00913528446108, 0.02107... |\n",
      "| [-0.0169465318322, 0.00558... | [-0.052326772362, 0.004970... |\n",
      "| [0.0330828912556, -0.01981... | [0.0140888402238, -0.02832... |\n",
      "| [0.260926008224, 0.1391316... | [0.00465900776908, 0.00025... |\n",
      "| [0.0507744029164, 0.010707... | [-0.0519581101835, 0.03507... |\n",
      "| [0.0586317107081, 0.005670... | [0.0099355103448, -0.01203... |\n",
      "| [0.0322535522282, -0.02459... | [0.0160552673042, -0.02257... |\n",
      "| [0.200810670853, 0.1116367... | [-0.0228575188667, 0.03107... |\n",
      "| [0.189399674535, 0.0464527... | [-0.134693369269, -0.03504... |\n",
      "| [0.0282689593732, 0.070388... | [-0.121195942163, 0.011467... |\n",
      "| [0.112239278853, 0.0106995... | [0.0199686419219, -0.02969... |\n",
      "| [0.0974476113915, 0.047455... | [-0.0638904124498, 0.05292... |\n",
      "| [0.0440681912005, -0.07455... | [-0.0236967504025, -0.0952... |\n",
      "| [0.126970782876, 0.0342853... | [-0.0506037920713, -0.0291... |\n",
      "| [0.00534401135519, -0.0244... | [-0.0481964573264, 0.07422... |\n",
      "| [-0.00615818798542, -0.003... | [0.0192972514778, -0.03023... |\n",
      "| [0.0204399637878, 0.079450... | [-0.05596126616, -0.032899... |\n",
      "| [0.0260283239186, 0.033009... | [0.0372057296336, 0.076759... |\n",
      "| [0.0881438031793, 0.031848... | [0.000211364036659, 0.0070... |\n",
      "| [0.0603585354984, 0.015843... | [0.000849092553835, 0.0094... |\n",
      "| [0.16372320056, -0.0349017... | [0.00930482894182, -0.0542... |\n",
      "| [0.0507612153888, 0.036313... | [0.0372869893909, -0.01013... |\n",
      "| [0.0623314827681, 0.031048... | [0.000188246369362, -0.011... |\n",
      "| [0.0151330688968, 0.049954... | [-0.033617246896, 0.013463... |\n",
      "| [0.0734688416123, 0.062237... | [-0.100508928299, -0.03262... |\n",
      "| [0.0200406201184, 0.049885... | [0.0103055173531, 0.011104... |\n",
      "| [0.099268220365, 0.0749717... | [-0.0499804727733, 0.07867... |\n",
      "| [-0.000604983826634, 0.031... | [0.0647443458438, 0.059247... |\n",
      "| [0.0273612309247, -0.01491... | [-0.126796782017, 0.025159... |\n",
      "| [0.0641955807805, 0.023768... | [-0.0673337504268, -0.0397... |\n",
      "| [0.0809100940824, 0.008531... | [-0.0459374897182, 0.00889... |\n",
      "| [0.0836975201964, -0.00721... | [-0.0365074463189, -0.0648... |\n",
      "| [0.0733333304524, 0.047290... | [-0.0244213398546, -0.0325... |\n",
      "| [0.136140242219, 0.0531320... | [-0.0117354812101, 0.00908... |\n",
      "| [0.00904357247055, 0.05762... | [-0.0591378211975, 0.00082... |\n",
      "| [0.0300141293555, 0.064663... | [0.0346714965999, 0.006074... |\n",
      "| [0.0619424507022, -0.07656... | [-0.0635324269533, -0.0087... |\n",
      "| [0.128728672862, 0.0659248... | [0.0329491123557, 0.045412... |\n",
      "| [0.131314277649, 0.0645326... | [-0.0325597114861, 0.03489... |\n",
      "| [0.0928487181664, -0.03246... | [-0.0402219034731, -0.0358... |\n",
      "| [0.137572735548, 0.0536537... | [-0.0496398434043, -0.0342... |\n",
      "| [0.166193962097, 0.0476749... | [-0.0944438129663, 0.02375... |\n",
      "| [0.0606363080442, 0.013080... | [-0.0543198026717, -0.0004... |\n",
      "| [0.0571812093258, 0.056051... | [-0.0579672344029, 0.06034... |\n",
      "| [0.173208251595, 0.0216094... | [-0.0544457100332, -0.0675... |\n",
      "| [0.0324134081602, 0.040660... | [-0.0439134538174, -0.0041... |\n",
      "| [0.0774481520057, 0.047877... | [-0.0122695574537, 0.02829... |\n",
      "| [0.0214382521808, 0.010585... | [-0.0844203457236, 0.00043... |\n",
      "| [0.0295724868774, 0.064449... | [0.022021273151, 0.0407860... |\n",
      "| [0.0602838024497, 0.031728... | [-0.0727398619056, 0.01950... |\n",
      "| [0.0431425273418, -0.01717... | [-0.0530769228935, -0.0903... |\n",
      "| [0.108032986522, 0.0527568... | [-0.126238688827, -0.03050... |\n",
      "| [0.0647624507546, 0.037389... | [-0.104709789157, -0.03328... |\n",
      "| [-0.00445623602718, 0.0416... | [-0.0355681553483, -0.0077... |\n",
      "| [0.0274733230472, -0.00963... | [-0.0667653754354, -0.0644... |\n",
      "| [0.078369833529, 0.0798044... | [0.0129036577418, 0.051263... |\n",
      "| [0.123259373009, 0.0196634... | [-0.040301989764, -0.01246... |\n",
      "| [0.0269559528679, -0.02478... | [-0.105268858373, -0.09049... |\n",
      "| [0.0912797376513, 0.030882... | [-0.0191368218511, -0.0272... |\n",
      "| [0.00452446937561, 0.05069... | [0.0185093190521, 0.002253... |\n",
      "| [0.0376674197614, 0.082244... | [-0.112620040774, 0.060084... |\n",
      "| [0.143570706248, -0.053931... | [-0.104861289263, -0.07639... |\n",
      "| [0.0876466110349, 0.049201... | [-0.0205875132233, -0.0238... |\n",
      "| [-0.00671575311571, 0.0566... | [0.0880455970764, -0.03746... |\n",
      "| [0.0710463300347, 0.032923... | [0.00438511604443, -0.0173... |\n",
      "| [0.1645154953, 0.045428361... | [-0.0442339517176, 0.06099... |\n",
      "| [0.0396813601255, -0.02607... | [-0.0393633842468, -0.0198... |\n",
      "| [-0.0316941961646, -0.0055... | [-0.0352925136685, -0.0157... |\n",
      "| [0.0788525417447, 0.054314... | [-0.088430583477, -0.00542... |\n",
      "| [0.0723024830222, -0.05719... | [-0.0980765372515, 0.06759... |\n",
      "| [0.105859436095, 0.0080118... | [-0.0165805052966, 0.05977... |\n",
      "| [0.038278799504, -0.027928... | [0.0139453476295, -0.01799... |\n",
      "| [0.0464682802558, 0.016200... | [-0.0487658008933, 0.02142... |\n",
      "| [0.0345609635115, 0.070713... | [-0.0199321042746, -0.0134... |\n",
      "| [0.00918385758996, 0.03566... | [0.038968835026, -0.011043... |\n",
      "| [0.0762875378132, -0.04668... | [-0.0161974672228, 0.04959... |\n",
      "| [0.0356015600264, -0.05648... | [-0.0329576469958, -0.0615... |\n",
      "| [0.0345851927996, -0.02947... | [-0.0943785905838, -0.0227... |\n",
      "| [-0.0169333443046, -0.0222... | [-0.0293324403465, 0.05783... |\n",
      "| [0.161620676517, 0.0878657... | [-0.0922230109572, 0.02595... |\n",
      "| [0.232889443636, 0.0453842... | [-0.108930319548, -0.07048... |\n",
      "| [0.116382494569, -0.021264... | [-0.0701223164797, -0.0716... |\n",
      "| [0.0368965789676, -0.00038... | [-0.0750454440713, -0.0118... |\n",
      "| [0.0942565500736, 0.007094... | [-0.0914075523615, -0.0537... |\n",
      "| [0.00347400619648, 0.01795... | [0.0323670804501, -0.00990... |\n",
      "| [0.0498482100666, 0.032677... | [-0.129523545504, -0.01843... |\n",
      "| [0.0557777397335, 0.133718... | [-0.00110595521983, -0.060... |\n",
      "| [0.0388406999409, -0.01147... | [-0.0215344373137, -0.0290... |\n",
      "| [0.0444717518985, -0.02409... | [-0.0346285887063, -0.0149... |\n",
      "| [-0.0404047518969, -0.0031... | [-0.0180733781308, 0.01431... |\n",
      "| [-0.0264156088233, -0.0098... | [0.022463530302, 0.0618368... |\n",
      "| [0.0152993882075, -0.01361... | [-0.122449330986, 0.048247... |\n",
      "| [0.101191788912, 0.1020619... | [-0.0237009208649, 0.07249... |\n",
      "| [0.00328333233483, 0.03872... | [-0.00189175829291, -0.005... |\n",
      "| [0.19173052907, 0.02606537... | [0.00456350343302, -0.0472... |\n",
      "| [-0.00756331859156, 0.1209... | [-0.0339031070471, -0.0078... |\n",
      "| [0.0506467483938, 0.019809... | [-0.00497646490112, 0.0541... |\n",
      "| [0.0492804124951, -0.00014... | [-0.0328349880874, 0.04310... |\n",
      "| [0.0512477308512, -0.00052... | [-0.0790698379278, -0.0381... |\n",
      "| [-0.019903101027, 0.043555... | [-0.0196588505059, 0.05062... |\n",
      "| [0.01328902971, 0.04116544... | [0.00135856622364, -0.0045... |\n",
      "| [0.185112178326, -0.011940... | [-0.104338608682, -0.02234... |\n",
      "| [-0.0170110203326, 0.01782... | [-0.0541443675756, 0.03446... |\n",
      "| [0.0233093071729, 0.033589... | [-0.0837873741984, 0.02784... |\n",
      "| [-0.0416813977063, 0.03054... | [-0.0853554978967, 0.08506... |\n",
      "| [-0.00279554724693, 0.0634... | [0.0804759785533, 0.004228... |\n",
      "| [0.075602337718, 0.0716881... | [-0.0262410398573, 0.00253... |\n",
      "| [0.115582488477, -0.018746... | [0.00044970959425, 0.04002... |\n",
      "| [0.0452094860375, 0.006649... | [-0.0377465337515, 0.00995... |\n",
      "| [0.0756808668375, 0.066742... | [-0.0922698527575, 0.03127... |\n",
      "| [0.237695857882, 0.0395229... | [-0.10179374367, -0.000776... |\n",
      "| [-0.04259981215, -0.066103... | [-0.056952316314, -0.00160... |\n",
      "| [0.0623905844986, 0.032462... | [-0.0507042519748, -0.0291... |\n",
      "| [0.0440666265786, 0.070186... | [0.0563729405403, 0.039503... |\n",
      "| [0.0677286684513, 0.006808... | [-0.107381038368, 0.039324... |\n",
      "| [0.0772503465414, -0.01710... | [-0.092671521008, 0.031461... |\n",
      "| [0.0167915038764, 0.010585... | [-0.0407687984407, -0.1001... |\n",
      "| [-0.0100392159075, 0.11527... | [-0.168665900826, 0.033405... |\n",
      "| [0.0618576146662, 0.033011... | [-0.0634552612901, -0.0020... |\n",
      "| [0.0593852475286, 0.002221... | [-0.126897230744, 0.011502... |\n",
      "| [0.0712919980288, 0.062549... | [-0.0189264677465, -0.0290... |\n",
      "| [-0.0274676680565, 0.01560... | [-0.0674933940172, 0.01266... |\n",
      "| [0.00781345367432, 0.04130... | [0.0516900010407, 0.003033... |\n",
      "| [0.0645562559366, 0.025890... | [-0.0083499988541, -0.0600... |\n",
      "| [0.154175579548, -0.017859... | [-0.0105172051117, -0.0744... |\n",
      "| [-0.050859183073, -0.00567... | [-0.0484598875046, -0.0429... |\n",
      "| [0.0170142911375, -0.01109... | [-0.072248108685, 0.055564... |\n",
      "| [0.0412958785892, 0.027598... | [-0.078666806221, -0.01158... |\n",
      "| [-0.0135177141055, 0.01302... | [0.0120543055236, 0.011533... |\n",
      "| [0.235637694597, 0.0713683... | [-0.0430052652955, 0.05029... |\n",
      "| [0.0289635341614, 0.042523... | [-0.122153930366, -0.04483... |\n",
      "| [0.0541388019919, 0.029472... | [-0.10800267756, -0.074134... |\n",
      "| [0.0619402565062, -0.03171... | [-0.0932270660996, -0.0505... |\n",
      "| [0.0164230372757, -0.01309... | [-0.0253785904497, -0.0509... |\n",
      "| [0.0870245471597, 0.016481... | [-0.103729948401, 0.006308... |\n",
      "| [0.0571160949767, 0.036770... | [0.0512067675591, -0.08757... |\n",
      "| [0.134484812617, 0.0023998... | [-0.0929875299335, -0.0028... |\n",
      "| [0.0683683678508, -0.04215... | [-0.00232413783669, 0.0061... |\n",
      "| [0.0509666875005, 0.009130... | [0.00040512040141, -0.0314... |\n",
      "| [0.0977280363441, 0.020590... | [-0.0369945317507, -0.0219... |\n",
      "| [0.13923689723, 0.04590379... | [-0.0508451610804, 0.00099... |\n",
      "| [-0.0121928863227, 0.01069... | [-0.0283769275993, 0.06028... |\n",
      "| [0.0342565514147, 0.054842... | [0.00372061668895, 0.01077... |\n",
      "| [0.0698765665293, 0.051897... | [-0.000683809339534, 0.032... |\n",
      "| [0.0846736133099, 0.022997... | [-0.0851989984512, -0.0157... |\n",
      "| [0.00301116099581, 0.03210... | [0.00818754918873, 0.02841... |\n",
      "| [0.136306479573, 0.0119946... | [-0.089093118906, 0.036586... |\n",
      "| [0.0405173413455, 0.007105... | [-0.0166264940053, -5.5056... |\n",
      "| [0.187399998307, 0.0407762... | [-0.0195897333324, -0.0118... |\n",
      "| [0.0049799894914, -0.02122... | [-0.0339252389967, -0.0185... |\n",
      "| [0.127208024263, 0.0010692... | [-0.0629665404558, -0.0252... |\n",
      "| [-0.0230744667351, 0.00269... | [-0.0333931967616, -0.0570... |\n",
      "| [0.019507072866, -0.024923... | [0.0495052412152, 0.031572... |\n",
      "| [0.103748016059, 0.0126182... | [-0.0650280565023, 0.00714... |\n",
      "| [-0.0305890068412, 0.03610... | [-0.0540780909359, 0.01877... |\n",
      "| [0.111998386681, 0.0902362... | [-0.035428866744, -0.01438... |\n",
      "| [0.0648129731417, 0.033112... | [0.000903004489373, -0.027... |\n",
      "| [0.139265522361, 0.1382476... | [0.00757021456957, -0.0050... |\n",
      "| [0.01804914698, 0.05441901... | [-0.0501813516021, 0.03299... |\n",
      "| [-0.0105911968276, 0.03612... | [-0.0609393715858, -0.0062... |\n",
      "| [0.00078934681369, 0.05184... | [-0.105617858469, -0.12613... |\n",
      "| [0.0603748187423, 0.099136... | [-0.0684701502323, 0.11165... |\n",
      "| [0.116270020604, 0.0872803... | [0.0168637819588, -0.01779... |\n",
      "| [0.0795662403107, 0.029326... | [-0.0123635567725, -0.0755... |\n",
      "| [0.105263888836, 0.1025859... | [-0.0247980840504, 0.03810... |\n",
      "| [0.0413723438978, 0.051801... | [-0.0421677008271, 0.08298... |\n",
      "| [0.00602162489668, 0.01827... | [-0.0673546865582, -0.0123... |\n",
      "| [0.0296633653343, 0.001920... | [0.0188997853547, -0.00361... |\n",
      "| [0.0299254097044, -0.07056... | [-0.0625386014581, -0.0186... |\n",
      "| [0.0159100685269, 0.041641... | [-0.00848133023828, 0.0231... |\n",
      "| [0.102655045688, 0.0290696... | [-0.0298283752054, 0.02239... |\n",
      "| [-0.0915622860193, 0.04676... | [-0.0846383869648, 0.02276... |\n",
      "| [-0.0391287505627, -0.0258... | [0.00405295938253, -0.0153... |\n",
      "| [0.0869574397802, 0.018659... | [-0.09976311028, 0.0002191... |\n",
      "| [0.200310021639, -0.019013... | [-0.0665476098657, -0.1005... |\n",
      "| [-0.0332931242883, 0.01861... | [0.0668389797211, 0.047290... |\n",
      "| [-0.0111145339906, -0.0089... | [-0.0408565737307, 0.02897... |\n",
      "| [0.120493546128, 0.1385127... | [-0.00418422278017, 0.0576... |\n",
      "| [0.117088519037, 0.0848670... | [-0.00429223105311, 0.0450... |\n",
      "| [0.087116882205, 0.0198021... | [-0.102624818683, 0.016453... |\n",
      "| [0.136809080839, 0.0259545... | [-0.0642964169383, -0.0337... |\n",
      "| [0.0308968629688, 0.018208... | [-0.0563344731927, 0.02604... |\n",
      "| [0.0459648706019, 0.088491... | [-0.0323504284024, -0.0156... |\n",
      "| [0.0366135574877, -0.08543... | [0.0301058422774, -0.04740... |\n",
      "| [0.123191028833, 0.1183821... | [-0.00322131067514, 0.0660... |\n",
      "| [0.0129038635641, -0.00627... | [-0.052869386971, -0.05802... |\n",
      "| [0.156135752797, 0.0502346... | [-0.0514879859984, 0.03281... |\n",
      "| [0.0393742546439, 0.009737... | [0.0532517172396, -0.03394... |\n",
      "| [0.10635753721, 0.02201342... | [-0.0963510349393, -0.0171... |\n",
      "| [0.0347391441464, 0.056265... | [-0.0283938441426, -0.0389... |\n",
      "| [0.0448147915304, 0.044528... | [-0.0847820267081, -0.0162... |\n",
      "| [0.0267875250429, 0.093820... | [-0.0465673208237, -0.0123... |\n",
      "| [0.0323578752577, 0.077421... | [0.0123759778216, 0.056492... |\n",
      "| [0.172982320189, 0.0138952... | [-0.090458124876, -0.01755... |\n",
      "| [-0.0770476609468, 0.07398... | [0.0401378273964, -0.06271... |\n",
      "| [0.0574507713318, 0.024219... | [-0.0425010882318, 0.00348... |\n",
      "| [0.106883376837, -0.028370... | [-0.0714514926076, -0.0054... |\n",
      "| [0.0640978589654, 0.024241... | [-0.015761340037, -0.00687... |\n",
      "| [0.189805328846, 0.0892341... | [-0.0145995067433, 0.04611... |\n",
      "| [0.0157291814685, -0.01367... | [-0.00772254681215, 0.0247... |\n",
      "| [0.0857536122203, 0.024450... | [-0.132996529341, 0.039740... |\n",
      "| [0.118387244642, -0.032207... | [-0.0879969149828, -0.0777... |\n",
      "| [0.00103919580579, 0.00852... | [-0.0234773233533, 0.02122... |\n",
      "| [0.0078010559082, 0.027229... | [-0.078895188868, 0.072526... |\n",
      "| [-0.00655063986778, 0.0099... | [0.019521297887, -0.006278... |\n",
      "| [0.195213064551, 0.0389424... | [-0.0675484165549, 0.00290... |\n",
      "| [0.0961551889777, 0.045788... | [-0.00862185750157, 0.0435... |\n",
      "| [0.0872871950269, 0.118073... | [0.0549852699041, -0.02638... |\n",
      "| [0.014249955304, 0.0264615... | [-0.0108373332769, 0.04433... |\n",
      "| [0.0537943206728, 0.008076... | [0.109473198652, 0.0148377... |\n",
      "| [0.0854067951441, 0.093078... | [-0.0394830256701, 0.04466... |\n",
      "| [0.0158654693514, 0.074334... | [-0.0591367073357, 0.06445... |\n",
      "| [0.0457320325077, -0.00715... | [-0.0424227826297, 0.02003... |\n",
      "| [0.0463402792811, 0.014587... | [-0.0277902241796, 0.00127... |\n",
      "| [0.0360869206488, -0.03211... | [-0.0189984291792, -0.0298... |\n",
      "| [0.00539721781388, 0.07479... | [-0.0738788470626, 0.04037... |\n",
      "| [0.0336590744555, -0.01646... | [-0.0702240765095, -0.0365... |\n",
      "| [0.102171160281, 0.1145504... | [-0.0416809767485, 0.12183... |\n",
      "| [-0.0136138545349, -0.0087... | [0.0434794053435, 0.024447... |\n",
      "| [-0.0560791827738, 0.06003... | [-0.0496257208288, -0.0134... |\n",
      "| [0.0675282850862, -0.08454... | [-0.016331974417, 0.009742... |\n",
      "| [0.0228824224323, 0.067569... | [-0.0831509158015, -0.0475... |\n",
      "| [0.0585595034063, 0.017702... | [-0.0785850286484, -0.0117... |\n",
      "| [-0.0554368831217, -0.0255... | [0.0457475818694, 0.034929... |\n",
      "| [0.0404240675271, 0.029509... | [0.00830511935055, -0.0391... |\n",
      "| [-0.0268043223768, 0.03274... | [0.00230889767408, 0.04997... |\n",
      "| [0.107133045793, 0.0290630... | [-0.112989939749, -0.03449... |\n",
      "| [0.117313146591, -0.099964... | [-0.210896357894, 0.034553... |\n",
      "| [-0.0339337326586, -0.0006... | [-0.0727511867881, 0.01836... |\n",
      "| [0.161511257291, 0.1039002... | [-0.19839091599, 0.1353441... |\n",
      "| [0.102790914476, 0.0023653... | [-0.0680563375354, -0.0318... |\n",
      "| [0.0709166303277, -0.01490... | [-0.0736036673188, -0.0081... |\n",
      "| [0.0831534639001, -0.03267... | [-0.170542553067, -0.10128... |\n",
      "| [0.0569027364254, 0.055137... | [-0.0573299862444, 0.05776... |\n",
      "| [0.0311866123229, 0.037457... | [-0.0628423914313, 0.02207... |\n",
      "| [0.0732133388519, 0.005095... | [-0.078637085855, 0.026323... |\n",
      "| [0.120118655264, -0.017884... | [-0.0298847071826, -0.0965... |\n",
      "| [0.0109971426427, -0.00094... | [-0.0507566034794, -0.0083... |\n",
      "| [0.0300470218062, 0.028128... | [-0.00282574258745, 0.0460... |\n",
      "| [0.0358305349946, 0.030839... | [0.0200479514897, 0.058839... |\n",
      "| [0.040064688772, 0.0163467... | [-0.102153897285, -0.03523... |\n",
      "| [0.197261944413, 0.1425442... | [0.0872082412243, 0.017990... |\n",
      "| [0.0184070207179, 0.019603... | [-0.0171403810382, 0.02768... |\n",
      "| [-0.0436540171504, 0.10599... | [-0.0972185134888, 0.10283... |\n",
      "| [-0.0649810507894, -0.0277... | [-0.148307517171, 0.064104... |\n",
      "| [0.174831494689, 0.0828301... | [-0.0143277794123, 0.02271... |\n",
      "| [0.0565667264163, -0.01195... | [-0.0605975240469, -0.0048... |\n",
      "| [0.031744979322, 0.0303809... | [-0.0427162908018, 0.02879... |\n",
      "| [0.146111726761, 0.0736908... | [-0.0359338410199, 0.06988... |\n",
      "| [0.0713090971112, -0.03646... | [-0.0387298166752, -0.0527... |\n",
      "| [0.119053550065, -0.014987... | [-0.114187411964, 0.029690... |\n",
      "| [0.0328545123339, 0.059181... | [0.00915882457048, 0.00921... |\n",
      "| [0.166663378477, -0.051560... | [-0.142434701324, -0.03842... |\n",
      "| [0.0623994097114, 0.033333... | [-0.0357047170401, 0.00833... |\n",
      "| [0.0429041534662, 0.043203... | [-0.00381437083706, -0.032... |\n",
      "| [0.106153778732, -0.026181... | [-0.0719781219959, -0.0297... |\n",
      "| [0.0279284790158, -0.00198... | [-0.0325939320028, -0.0187... |\n",
      "| [0.00125793123152, 0.06818... | [-0.0603894591331, 0.02511... |\n",
      "| [0.0409139953554, 0.034689... | [-0.0133794192225, 0.04022... |\n",
      "| [0.0929493606091, -0.01600... | [-0.101618565619, -0.00291... |\n",
      "| [0.031167652458, 0.0273270... | [0.0118317138404, -0.02150... |\n",
      "| [-0.0142794400454, -0.0133... | [-0.0311428569257, -0.1720... |\n",
      "| [0.0426553152502, 0.060732... | [-0.0457055680454, 0.01543... |\n",
      "| [-0.0108184413984, 0.05991... | [0.00735769374296, -0.0125... |\n",
      "| [0.114537999034, 0.1001447... | [0.0409442745149, -0.00049... |\n",
      "| [0.0447424799204, 0.032157... | [-0.0609556622803, -0.0623... |\n",
      "| [0.149831324816, 0.1004612... | [-0.0746583268046, 0.06205... |\n",
      "| [-0.0343165248632, 0.09794... | [-0.0358356572688, -0.0037... |\n",
      "| [0.167694330215, 0.0676646... | [-0.0438284911215, 0.00372... |\n",
      "| [0.0511435940862, 0.009716... | [0.00755571667105, 0.04259... |\n",
      "| [0.180393442512, 0.0849507... | [-0.131964564323, 0.031373... |\n",
      "| [0.0857873260975, 0.082366... | [-0.0531729087234, 0.02752... |\n",
      "| [0.0503564365208, 0.058158... | [0.00383843784221, 0.04485... |\n",
      "| [0.0485214032233, 0.052084... | [-0.0269914865494, -0.0399... |\n",
      "| [-0.000376204028726, 0.002... | [-0.0614829361439, -0.0188... |\n",
      "| [0.0549932941794, -0.05436... | [-0.0384649895132, 0.04233... |\n",
      "| [0.0210735797882, 0.076800... | [0.0117319803685, -0.03635... |\n",
      "| [-0.00903521291912, -0.113... | [-0.0665931925178, -0.1189... |\n",
      "| [0.0806059390306, 0.030033... | [-0.076660849154, -0.03052... |\n",
      "| [0.112471304834, 0.0254342... | [-0.12241166085, 0.0308037... |\n",
      "| [0.0859218910336, 0.101190... | [-0.110098145902, 0.029715... |\n",
      "| [0.0682699754834, 0.032556... | [-0.0184369757771, 0.03179... |\n",
      "| [0.085462346673, 0.0581449... | [-0.0677977278829, 0.03768... |\n",
      "| [0.116018824279, -0.008679... | [-0.157912045717, -0.02374... |\n",
      "| [0.0774743184447, 0.034703... | [-0.040694437921, -0.01944... |\n",
      "| [-0.0533740185201, 0.04106... | [-0.0566337555647, -0.0117... |\n",
      "| [0.0922518000007, 0.053481... | [-0.0344505086541, 0.00521... |\n",
      "| [0.0460751019418, -0.01965... | [-0.053800355643, -0.08616... |\n",
      "| [0.188279747963, 0.1254271... | [-0.00835796259344, 0.0623... |\n",
      "| [0.0809350311756, 0.042535... | [0.00931603647768, 0.00481... |\n",
      "| [0.0768631696701, 0.081347... | [-0.00715127168223, 0.0760... |\n",
      "| [0.135184600949, 0.0831962... | [-0.0587900280952, -0.0061... |\n",
      "| [0.0437876805663, 0.003845... | [-0.0470253452659, -0.0376... |\n",
      "| [0.0883543565869, -0.02469... | [-0.0873260647058, 0.01970... |\n",
      "| [0.0812121480703, 0.108677... | [-0.0740234926343, 0.05732... |\n",
      "| [0.0476026423275, 0.049066... | [-0.0865153372288, 0.00034... |\n",
      "| [0.138221681118, 0.0569794... | [-0.0745060965419, -0.0292... |\n",
      "| [-0.015343257226, 0.097205... | [-0.00224709766917, -0.021... |\n",
      "| [0.169935062528, 0.0779416... | [-0.107564985752, -0.01360... |\n",
      "| [0.0564978048205, 0.002196... | [0.00319233699702, 0.00612... |\n",
      "| [0.0101049877703, 0.030128... | [0.0229811388999, 0.036254... |\n",
      "| [0.0541688501835, -0.02517... | [-0.161382630467, 0.098148... |\n",
      "| [0.117665491998, 0.0534561... | [-0.0609778240323, 0.03049... |\n",
      "| [0.0486328490078, -0.01842... | [-0.0830877050757, 0.01654... |\n",
      "| [0.0279964357615, 0.040155... | [-0.109867975116, -0.08161... |\n",
      "| [0.0296383295208, 0.030181... | [-0.079211987555, -0.00889... |\n",
      "| [-0.0596332326531, -0.0144... | [-0.0421231761575, 0.01358... |\n",
      "| [0.143058031797, 0.0456707... | [-0.0161477643996, 0.06024... |\n",
      "| [0.194424644113, 0.0117205... | [-0.0228733532131, 0.05525... |\n",
      "| [-0.0253386702389, 0.05387... | [-0.0268028918654, 0.04072... |\n",
      "| [0.0834241136909, 0.029795... | [-0.00341343949549, 0.0396... |\n",
      "| [0.132614046335, 0.0554044... | [-0.0872957333922, -0.0925... |\n",
      "| [0.0402398630977, 0.092704... | [-0.148121505976, 0.029356... |\n",
      "| [0.077458165586, 0.0170608... | [-0.0443691015244, -0.0316... |\n",
      "| [-0.0199026782066, -0.0006... | [-0.104008577764, 0.032565... |\n",
      "| [0.0981516167521, 0.071829... | [-0.00235104141757, 0.0197... |\n",
      "| [0.0255741551518, 0.003741... | [-0.0683005154133, 0.00185... |\n",
      "| [0.00655879545957, -0.0268... | [-0.105434067547, 0.029648... |\n",
      "| [0.080406524241, 0.0446078... | [-0.111086286604, -0.02444... |\n",
      "| [0.194744154811, 0.0470394... | [-0.108013629913, 0.048489... |\n",
      "| [-0.00114322709851, 0.0767... | [0.0279647372663, 0.035952... |\n",
      "| [0.0898637622595, 0.069851... | [-0.107294932008, 0.047993... |\n",
      "| [0.0834546163678, 0.053967... | [-0.109560661018, 0.051032... |\n",
      "| [0.0452625639737, 0.027176... | [0.0367364287376, 0.032912... |\n",
      "| [0.0672824531794, 0.018932... | [-0.129834637046, 0.042403... |\n",
      "| [0.106875650585, 0.0426848... | [-0.0461961850524, -0.0082... |\n",
      "| [-0.0418201088905, 0.04973... | [-0.0618103034794, 0.10987... |\n",
      "| [-0.0291242804378, -0.0259... | [-0.058517575264, 0.014496... |\n",
      "| [0.116802297533, -0.009711... | [-0.134805291891, 0.015771... |\n",
      "| [0.100351043046, 0.0394627... | [0.029308186844, 0.0095209... |\n",
      "| [0.173768565059, 0.0528257... | [-0.0459601581097, 0.04970... |\n",
      "| [0.0461772605777, -0.03792... | [-0.0118287596852, -0.0060... |\n",
      "| [0.0293489918113, 0.001675... | [0.0225415192544, 0.047494... |\n",
      "| [0.0800661146641, 0.032697... | [-0.0692477524281, -0.0549... |\n",
      "| [0.0518258176744, -0.01258... | [0.0113110672683, -0.03877... |\n",
      "| [0.0454208217561, -0.04108... | [0.0514188967645, 0.042043... |\n",
      "| [0.105076856911, 0.0349005... | [-0.0678445771337, -0.0243... |\n",
      "| [0.151428237557, -0.026233... | [-0.0940935015678, -0.1159... |\n",
      "| [0.0271073877811, 0.024954... | [-0.0158598348498, -0.0162... |\n",
      "| [0.00609225593507, 0.00183... | [0.0211733747274, -0.02684... |\n",
      "| [0.0202599205077, -0.01206... | [-0.0514583326876, 0.03976... |\n",
      "| [-0.039956279099, 0.032790... | [-0.0131954764947, -0.0157... |\n",
      "| [0.083656668663, -0.065920... | [-0.0892145857215, -0.1023... |\n",
      "| [-0.103490032256, -0.00852... | [0.00267996010371, -0.0095... |\n",
      "| [-0.0475626066327, -0.0226... | [-0.0206400193274, -0.0160... |\n",
      "| [0.0747988075018, -0.00560... | [-0.0876001194119, 0.02067... |\n",
      "| [0.0579426549375, -0.02023... | [-0.0175721812993, 0.05154... |\n",
      "| [0.11674785614, 0.03794752... | [-0.142141774297, -0.02362... |\n",
      "| [0.088495850563, -0.005535... | [-0.0815854370594, -0.0081... |\n",
      "| [0.18747715652, 0.07238652... | [-0.140544101596, 0.032613... |\n",
      "| [0.163200631738, 0.0914005... | [-0.1019378528, 0.03463523... |\n",
      "| [0.137499496341, 0.0893647... | [-0.00883128214628, -0.011... |\n",
      "| [-0.0360653474927, 0.02018... | [-0.0172486174852, 0.00450... |\n",
      "| [0.026706404984, -0.015465... | [-0.0283893980086, -0.0441... |\n",
      "| [0.0890903919935, 0.011863... | [-0.0194549709558, 0.00690... |\n",
      "| [0.124034993351, 0.0540225... | [-0.0362781360745, 0.01367... |\n",
      "| [0.0674567073584, 0.089205... | [0.0794895738363, 0.118602... |\n",
      "| [0.0674558654428, 0.048218... | [-0.0921966135502, -0.0087... |\n",
      "| [0.0378400757909, -0.02376... | [0.01353002619, 0.01032433... |\n",
      "| [0.0513617880642, 0.040339... | [-0.0508384257555, -0.0391... |\n",
      "| [0.123274408281, 0.1239039... | [-0.051740758121, -0.01029... |\n",
      "| [0.111717700958, 0.0078446... | [-0.0734914168715, -0.0295... |\n",
      "| [0.0314516834915, 0.016667... | [-0.0519697777927, 0.03874... |\n",
      "| [0.0830715671182, 0.009463... | [-0.0274195279926, 0.06477... |\n",
      "| [0.105421774089, 0.0968019... | [0.00250792084262, 0.08285... |\n",
      "| [0.145047277212, 0.0592698... | [-0.0354220122099, 0.02898... |\n",
      "| [0.0485512278974, 0.000861... | [-0.0483661778271, 0.06182... |\n",
      "| [0.0669258385897, 0.075166... | [-0.0741683393717, -0.0155... |\n",
      "| [0.0888004079461, 0.068870... | [-0.115059442818, 0.057944... |\n",
      "| [0.024847175926, -0.018381... | [0.0367229953408, -0.00311... |\n",
      "| [0.0393396317959, 0.023268... | [-0.0544162541628, 0.01795... |\n",
      "| [-0.0422370247543, 0.06825... | [-0.116584978998, 0.080196... |\n",
      "| [0.0851044431329, 0.013168... | [-0.0172054879367, 0.07975... |\n",
      "| [0.054795037955, -0.012718... | [-0.0748368427157, 0.02003... |\n",
      "| [0.0170307699591, 0.024434... | [-0.0345076732337, -0.0599... |\n",
      "| [-0.00619537848979, 0.0277... | [-0.053209900856, 0.040696... |\n",
      "| [0.00620045093819, 0.03623... | [-0.0488468818367, 0.08136... |\n",
      "| [0.04412605986, 0.09380967... | [-0.0645020455122, -0.0376... |\n",
      "| [0.0506210997701, -0.02838... | [-0.0829520374537, -0.0473... |\n",
      "| [-0.034583773464, 0.115568... | [0.0244813673198, 0.048840... |\n",
      "| [0.0282142609358, 0.102078... | [-0.040933303535, -0.02132... |\n",
      "| [0.0736911222339, -0.03640... | [-0.00851653609425, -0.083... |\n",
      "| [0.0782668665051, 0.018312... | [-0.0151221957058, 0.01334... |\n",
      "| [-0.0296139698476, -0.0273... | [-0.0265614818782, -0.0165... |\n",
      "| [0.0440506562591, -0.00155... | [-0.0660208165646, -0.0154... |\n",
      "| [0.122346334159, 0.0573998... | [-0.0875975713134, -0.0348... |\n",
      "| [0.106182418764, 0.0030717... | [-0.0513507500291, -0.0559... |\n",
      "| [0.14259031415, -0.0296717... | [-0.0775806158781, -0.0740... |\n",
      "| [0.0324059724808, 0.045216... | [-0.00700957002118, 0.0207... |\n",
      "| [0.0958141237497, 0.013794... | [-0.0932246595621, 0.02676... |\n",
      "| [-0.0355867445469, 0.03211... | [-0.0521749146283, -0.0292... |\n",
      "+-------------------------------+-------------------------------+\n",
      "+-------------------------------+-------------------------------+\n",
      "|     vectors_neutral_ornot     |           word_count          |\n",
      "+-------------------------------+-------------------------------+\n",
      "| [-0.0611692331731, -0.0014... | {'and': 1L, 'debates.': 1L... |\n",
      "| [-0.0122985923663, -0.0303... | {'tuesday.': 1L, 'england'... |\n",
      "| [-0.015430896543, 0.004648... | {'ami': 1L, 'on': 1L, 'poe... |\n",
      "| [0.041755553335, 0.0509736... | {'be': 1L, '...the': 1L, '... |\n",
      "| [-0.0503308363259, 0.02813... | {'snc': 2L, 'for': 1L, 'vs... |\n",
      "| [-0.0498167611659, -0.0227... | {'heart': 1L, ':)': 1L, 'i... |\n",
      "| [-0.00943487230688, 0.0108... | {'tri': 1L, 'on': 1L, 'man... |\n",
      "| [-0.0208705831319, 0.04485... | {'a': 1L, 'amp;': 1L, 'thu... |\n",
      "| [-0.0344804115593, 0.11037... | {'a': 1L, 'on': 1L, 'for':... |\n",
      "| [-0.01688497141, 0.0223041... | {'and': 1L, 'on': 1L, 'tod... |\n",
      "| [-0.0262641850859, 0.09667... | {'then': 1L, 'parad': 1L, ... |\n",
      "| [-0.023871243, 0.027631470... | {'and': 2L, 'it': 2L, 'at'... |\n",
      "| [0.0147351296619, -0.03213... | {'it': 1L, 'unfurl': 1L, '... |\n",
      "| [-0.00761850038543, -0.028... | {'and': 1L, 'a': 1L, 'them... |\n",
      "| [0.00324351643212, 0.09560... | {'elvi': 1L, 'play': 1L, '... |\n",
      "| [0.0148008866236, 0.016317... | {'investor': 1L, 'on': 1L,... |\n",
      "| [-0.0603638552129, 0.03537... | {'anoth': 1L, 'wisconsin':... |\n",
      "| [-0.0398929454386, 0.00374... | {'and': 1L, 'w/': 1L, '$nu... |\n",
      "| [-0.0209448616952, -0.0259... | {'be': 1L, 'to': 1L, 'mond... |\n",
      "| [-0.0321729294956, 0.04328... | {'a': 1L, 'and': 1L, 'abc'... |\n",
      "| [0.0161550752819, 0.084485... | {'excit': 1L, 'and': 1L, '... |\n",
      "| [-0.0385398492217, 0.06398... | {'a': 1L, 'will': 1L, 'fam... |\n",
      "| [-0.0343153625727, 0.06520... | {'and': 1L, 'houston': 1L,... |\n",
      "| [-0.004495955538, 0.086851... | {'is': 1L, 'grandma': 1L, ... |\n",
      "| [-0.0206903312355, 0.01768... | {'$num:': 1L, 'rosi': 1L, ... |\n",
      "| [-0.0282405093312, 0.01966... | {'a': 2L, 'season.': 1L, '... |\n",
      "| [-0.0445197448134, 0.08697... | {'...': 1L, 'patriot': 2L,... |\n",
      "| [0.00198144651949, 0.05395... | {'and': 1L, 've': 1L, 'we'... |\n",
      "| [-0.0504969134927, -0.0038... | {'and': 2L, 'btw': 1L, 'of... |\n",
      "| [0.0241333246231, 0.021314... | {'northwestern': 1L, 'men'... |\n",
      "| [0.00159945827909, 0.08822... | {'all': 1L, 'liar': 1L, 's... |\n",
      "| [-0.00768602639437, 0.0173... | {'and': 1L, 'night.i': 1L,... |\n",
      "| [-0.0687092840672, 0.12289... | {'are': 1L, 'on': 1L, 'com... |\n",
      "| [-0.0164584107697, 0.19032... | {'it.': 1L, 'on': 2L, 'now... |\n",
      "| [-0.0368973910809, 0.12178... | {'footbal': 1L, 'concord':... |\n",
      "| [-0.00234656920657, 0.0150... | {'a': 1L, 'boy': 1L, 'play... |\n",
      "| [-0.0324404798448, 0.08226... | {'a': 1L, 'the': 3L, 'ther... |\n",
      "| [0.0147988637909, 0.020695... | {'and': 1L, 'over': 1L, 'i... |\n",
      "| [-0.0309663154185, -0.0300... | {'a': 1L, 'current': 1L, '... |\n",
      "| [-0.0308629143983, 0.03610... | {'oiershdjkfwl': 1L, 'toni... |\n",
      "| [-0.00788780674338, 0.1056... | {'a': 2L, 'saturday': 1L, ... |\n",
      "| [-0.0109067540616, 0.07130... | {'then': 1L, 'guess': 1L, ... |\n",
      "| [-0.0463831238449, 0.09197... | {'week': 1L, 'for': 1L, 'u... |\n",
      "| [-0.0465274006128, 0.05958... | {'on': 1L, 'pvr': 1L, 'to'... |\n",
      "| [0.0279976408929, 0.073371... | {'bc,': 1L, 'be': 2L, 'in'... |\n",
      "| [0.0383709259331, 0.096911... | {'and': 1L, 'sun:': 1L, 'p... |\n",
      "| [0.0058635044843, 0.025889... | {'and': 1L, 'on': 1L, 'cel... |\n",
      "| [-0.0749920979142, 0.18523... | {'again': 1L, 'round$num':... |\n",
      "| [-0.124233260751, 0.070298... | {'a': 1L, 'marshal': 1L, '... |\n",
      "| [-0.0121179725975, 0.12072... | {'hqb,': 1L, 'aja,': 1L, '... |\n",
      "| [-0.0195948909968, 0.09662... | {'costume.': 1L, 'at': 1L,... |\n",
      "| [-0.0537320673466, 0.10082... | {'and': 1L, 'cba': 1L, 'ju... |\n",
      "| [-0.0682339966297, 0.09019... | {'and': 1L, 'just': 1L, 's... |\n",
      "| [-0.0550586134195, 0.06404... | {'see': 1L, '(mi': 1L, 'in... |\n",
      "| [-0.073643065989, 0.080724... | {'and': 1L, 'on': 1L, 'you... |\n",
      "| [-0.0449769683182, -0.0213... | {'don': 1L, 'though': 1L, ... |\n",
      "| [-0.0426962524652, 0.05624... | {'and': 1L, 'anatomi': 1L,... |\n",
      "| [-0.0294797215611, 0.08619... | {'heey': 1L, 'dont': 1L, '... |\n",
      "| [-0.040467582643, 0.096396... | {'counti': 1L, 'is': 1L, '... |\n",
      "| [0.00736766494811, 0.10420... | {'and': 1L, 'that': 1L, 'i... |\n",
      "| [-0.035775963217, 0.058317... | {'mish': 1L, 'it': 1L, 'in... |\n",
      "| [-0.0595093220472, 0.03775... | {'schedul': 1L, 'sign': 1L... |\n",
      "| [-0.0161180477589, 0.07765... | {'and': 1L, 'el': 1L, 'it'... |\n",
      "| [0.0050350530073, 0.067384... | {'on': 1L, 'for': 1L, 'in'... |\n",
      "| [-0.0260227322578, 0.06349... | {'night': 1L, 'footbal': 1... |\n",
      "| [-0.042309153825, 0.114253... | {'and': 1L, 'on': 2L, 'set... |\n",
      "| [-0.0177834648639, 0.07527... | {'anyway': 1L, 'at': 1L, '... |\n",
      "| [-0.00608528079465, 0.0192... | {'and': 1L, 'romney': 1L, ... |\n",
      "| [-0.0571280866861, 0.02598... | {'negative.': 1L, 'is': 1L... |\n",
      "| [0.0343977548182, 0.020674... | {'elvi': 1L, 'novemb': 1L,... |\n",
      "| [0.0174297131598, 0.026791... | {'then': 1L, 'to': 1L, 'pa... |\n",
      "| [0.0374517217278, 0.059919... | {'a': 1L, 'about': 1L, 'to... |\n",
      "| [-0.0213100612164, 0.02668... | {'and': 1L, 'golden': 1L, ... |\n",
      "| [0.056328881532, 0.1747023... | {'town': 1L, 'tomorrow?': ... |\n",
      "| [-0.0316448137164, 0.03584... | {'clipper': 1L, 'in': 1L, ... |\n",
      "| [-0.0481898859143, 0.07019... | {'on': 1L, 'of': 1L, 'plea... |\n",
      "| [-0.104547604918, 0.109859... | {'delux': 1L, 'at': 1L, '$... |\n",
      "| [-0.06430914253, -0.000887... | {'just': 1L, 'it': 1L, 'sa... |\n",
      "| [-0.0328916497529, -0.0056... | {'and': 1L, ':)': 1L, 'pla... |\n",
      "| [-0.0443238392472, 0.02408... | {'be': 2L, 'ch': 1L, 'pain... |\n",
      "| [-0.0296524669975, 0.00064... | {'cbb': 1L, 'do': 1L, 'now... |\n",
      "| [-0.0186006482691, 0.03801... | {'1st': 1L, 'credits,but..... |\n",
      "| [-0.0349591560662, 0.00895... | {'and': 1L, 'tonight,': 1L... |\n",
      "| [-0.0660058706999, 0.04002... | {'and': 1L, 'footbal': 1L,... |\n",
      "| [-0.0185516774654, -0.0045... | {'king': 1L, 'luther': 1L,... |\n",
      "| [-0.0181480254978, 0.31140... | {'tiger': 1L, 'url': 1L, '... |\n",
      "| [-0.0614239126444, 0.13183... | {'and': 1L, 'see': 1L, 'ge... |\n",
      "| [-0.0567832253873, 0.02619... | {'and': 1L, 'is': 1L, 'at'... |\n",
      "| [-0.0505589358509, -0.0013... | {'a': 1L, 'still': 1L, 'i'... |\n",
      "| [-0.0537160746753, 0.12406... | {'excit': 1L, ':)': 1L, 'f... |\n",
      "| [-0.0440470501781, 0.14011... | {'do': 1L, 'we': 1L, 'gone... |\n",
      "| [-0.00526963034645, 0.2459... | {'and': 1L, 'healthcar': 1... |\n",
      "| [-0.00868854951113, 0.0908... | {'dad': 1L, 'march': 1L, '... |\n",
      "| [-0.0604977048934, 0.05588... | {'on': 1L, 'via': 1L, 'las... |\n",
      "| [-0.0815594419837, -0.0558... | {'a': 1L, 'the': 2L, 'marc... |\n",
      "| [-0.0253734085709, 0.09048... | {'for': 1L, 'ipod': 1L, 't... |\n",
      "| [-0.015848422423, 0.112502... | {'into': 1L, 'at': 2L, 'ho... |\n",
      "| [0.0138513911515, 0.031671... | {'and': 1L, 'it': 1L, 'all... |\n",
      "| [-0.00824808236212, 0.1265... | {'from': 1L, 'film': 1L, '... |\n",
      "| [0.0491882711649, 0.017114... | {'liar': 1L, 'pretti': 1L,... |\n",
      "| [-0.113510012627, 0.009496... | {'and': 1L, 'be': 1L, 'ann... |\n",
      "| [0.0548252463341, 0.150051... | {'devil': 1L, 'insid': 1L,... |\n",
      "| [0.00239949417301, -0.0083... | {'a': 1L, 'thursday:': 1L,... |\n",
      "| [-0.0498959831893, 0.08911... | {'and': 1L, 'toni': 1L, 'r... |\n",
      "| [-0.040891777724, 0.016087... | {'and': 1L, 'onli': 1L, 'p... |\n",
      "| [-0.0380180589855, 0.02983... | {'all': 1L, 'mornin': 1L, ... |\n",
      "| [-0.0434358157218, 0.07138... | {'a': 1L, 'pas': 1L, 'actu... |\n",
      "| [-0.0527029857039, -0.0332... | {'stat': 1L, 'sure': 1L, '... |\n",
      "| [-0.0303401891142, 0.08041... | {':)': 1L, 'cb': 1L, 'mind... |\n",
      "| [-0.0743826627731, 0.04726... | {'no,': 1L, 'and': 1L, 'bo... |\n",
      "| [0.0120500614867, 0.086169... | {'are': 1L, 'a': 1L, 'engl... |\n",
      "| [-0.0708399116993, 0.02717... | {'and': 1L, 'just': 1L, 's... |\n",
      "| [0.0137801468372, 0.039713... | {'and': 1L, 'gari': 1L, 'm... |\n",
      "| [-0.0237505547702, -0.0021... | {'past': 1L, 'semest': 1L,... |\n",
      "| [-0.000671353889629, 0.078... | {'a': 1L, 'ugli': 1L, 'loo... |\n",
      "| [-0.0781316235662, 0.04232... | {'and': 1L, 'within': 1L, ... |\n",
      "| [-0.0961941555142, 0.07304... | {'low': 1L, 'then': 1L, 'a... |\n",
      "| [-0.0516706109047, -0.0466... | {'nba.': 1L, 'honestli': 1... |\n",
      "| [-0.0451164543629, 0.04831... | {'qtr': 1L, '$numth': 1L, ... |\n",
      "| [0.0203456487507, 0.051990... | {'all': 1L, 'fuck': 1L, 'n... |\n",
      "| [-0.0414124391973, -0.0228... | {'outweigh': 1L, '...': 1L... |\n",
      "| [-0.0206907000393, 0.04502... | {'have': 1L, 'to': 1L, 'pa... |\n",
      "| [0.022561782971, -0.008916... | {'taylor': 1L, 've': 1L, '... |\n",
      "| [-0.0555992424488, 0.01337... | {'for': 1L, 'januari': 1L,... |\n",
      "| [-0.00825204700232, 0.0094... | {'all': 1L, 'scorer': 1L, ... |\n",
      "| [-0.0236247777939, -0.0497... | {'$num.sh': 2L, '$numthing... |\n",
      "| [-0.00285540870391, 0.1032... | {'i': 1L, 'anderson': 1L, ... |\n",
      "| [0.0160865858197, 0.104785... | {'a': 1L, 'wwwyki': 1L, 'b... |\n",
      "| [0.0529015175998, 0.050946... | {'and': 1L, 'rank': 1L, 's... |\n",
      "| [-0.0309637747705, 0.05043... | {'sound': 1L, 'on': 1L, 'l... |\n",
      "| [-0.0197199471295, 0.09243... | {'texan': 1L, 'i': 1L, 'm'... |\n",
      "| [0.0415176264942, -0.00600... | {'anatomi': 1L, 'on': 1L, ... |\n",
      "| [0.0314589776099, 0.027570... | {'is': 1L, 'at': 1L, 'if':... |\n",
      "| [-0.0483122356236, -0.0217... | {'a': 1L, 'huge': 1L, 'hea... |\n",
      "| [-0.0323772914708, 0.09620... | {'a': 1L, 'twitpic': 1L, '... |\n",
      "| [0.0177300963551, 0.017328... | {'a': 1L, 'do': 1L, 'all':... |\n",
      "| [0.0206923689693, 0.006975... | {'tri': 1L, 'on': 1L, 'ari... |\n",
      "| [0.0121359052137, 0.105052... | {'sunday?': 1L, 'from': 1L... |\n",
      "| [-0.0481849461794, 0.05950... | {'direct': 1L, 'one': 1L, ... |\n",
      "| [0.0516584515572, 0.335502... | {'chalmer': 1L, 'tomorrow?... |\n",
      "| [-0.0623956173658, 0.23109... | {'excit': 1L, 'on': 1L, 'o... |\n",
      "| [0.0266149695963, 0.114851... | {'webb': 1L, 'help': 1L, '... |\n",
      "| [-0.0369256734848, 0.02004... | {'n.j./boston,': 1L, 'on':... |\n",
      "| [-0.0222587026656, 0.01825... | {'saturday': 1L, 'sir': 1L... |\n",
      "| [-0.0759382098913, 0.16534... | {'on': 1L, 'get': 1L, 'gon... |\n",
      "| [-0.0501229353249, 0.05158... | {'reveal': 1L, 'th...': 1L... |\n",
      "| [-0.084177814424, 0.211225... | {'on': 1L, 'tcu': 1L, 't':... |\n",
      "| [0.0728168040514, 0.017986... | {'and': 1L, 'manor.': 1L, ... |\n",
      "| [0.0247106030583, 0.085470... | {'no.': 1L, 'gonn': 1L, 's... |\n",
      "| [0.0219979528338, -0.01616... | {'throw.': 1L, 'made': 1L,... |\n",
      "| [-0.0333748012781, 0.06907... | {'a': 1L, 'on': 1L, 'brown... |\n",
      "| [-0.0234171524644, 0.10729... | {'it': 1L, 'ard': 1L, 'in'... |\n",
      "| [0.0219364073128, 0.014255... | {'ali': 1L, 'into': 1L, 't... |\n",
      "| [-0.0473302565515, 0.01068... | {'is': 1L, 'it': 1L, 'want... |\n",
      "| [0.0130185838789, 0.156320... | {'a': 1L, 'about': 1L, 'ks... |\n",
      "| [-0.121276237071, -0.00351... | {'on': 1L, 'life': 1L, 'li... |\n",
      "| [-0.050560452044, 0.031471... | {'coach': 1L, 'on': 1L, 'm... |\n",
      "| [0.0130011616275, 0.059981... | {'is': 1L, 'novemb': 1L, '... |\n",
      "| [-0.0384186170995, 0.10866... | {'earli': 1L, 'do': 1L, ':... |\n",
      "| [-0.0522058978677, 0.04993... | {'a': 1L, 'despit': 1L, '-... |\n",
      "| [-0.0450854338706, 0.05423... | {'mike': 1L, 'willingham':... |\n",
      "| [-0.0538582317531, 0.12671... | {'and': 1L, '@': 1L, 'tues... |\n",
      "| [-0.0726679041982, 0.07862... | {'tuesday.': 1L, 'wisconsi... |\n",
      "| [0.0460533760488, 0.021083... | {'and': 1L, 'chalmer': 1L,... |\n",
      "| [0.0162874832749, -0.02445... | {'school.': 1L, 'me.': 1L,... |\n",
      "| [-0.0614145994186, 0.06750... | {'a': 1L, 'prejudic': 1L, ... |\n",
      "| [0.00159669597633, 0.04101... | {'the': 1L, 'apewalkin': 1... |\n",
      "| [-0.0131154870614, -0.0917... | {'be': 1L, 'most': 1L, 'ma... |\n",
      "| [0.0232024881989, 0.014549... | {'play': 1L, 'sure': 1L, '... |\n",
      "| [0.0280452296138, 0.115809... | {'the': 1L, 'begin': 1L, '... |\n",
      "| [-0.0379933640361, 0.08950... | {'and': 1L, 'houston': 1L,... |\n",
      "| [-0.0167708639055, 0.04866... | {'a': 2L, 'monday': 1L, 's... |\n",
      "| [-0.0502530485392, 0.08580... | {'a': 1L, 'kathmandu,': 1L... |\n",
      "| [-0.0286532156169, -0.0979... | {'and': 1L, 'qtr': 1L, 'to... |\n",
      "| [0.038479860872, 0.0689156... | {'on': 1L, 'pacer': 1L, 't... |\n",
      "| [-0.0678229108453, 0.00953... | {'and': 1L, 'on': 1L, 'th.... |\n",
      "| [0.0329398736358, 0.124274... | {'sunderland': 1L, 'at': 1... |\n",
      "| [0.00711381155998, 0.03051... | {'on': 1L, 'swansea': 1L, ... |\n",
      "| [-0.0109461611137, 0.05231... | {'our...': 1L, 'movi': 1L,... |\n",
      "| [-0.00707151787356, 0.0250... | {'is': 1L, 'me.': 1L, 'ora... |\n",
      "| [-0.0247596595436, -0.0137... | {'and': 1L, 'love': 1L, 't... |\n",
      "| [-0.0431097745895, 0.17462... | {'there': 1L, 'no': 1L, 't... |\n",
      "| [0.0148008866236, 0.016317... | {'investor': 1L, '...': 1L... |\n",
      "| [-0.0338329635561, 0.03765... | {'le': 1L, 'i': 1L, 'ill':... |\n",
      "| [-0.0430157147348, 0.03506... | {'coach': 1L, 'just': 1L, ... |\n",
      "| [0.0514028482139, 0.166801... | {'at': 2L, 'in': 1L, 'go':... |\n",
      "| [-0.0167429186404, 0.14235... | {'on': 1L, 'get': 1L, 'par... |\n",
      "| [0.022954121232, 0.0157054... | {'tuesday.': 1L, ':/': 1L,... |\n",
      "| [-0.0270753502846, 0.09258... | {'shoot': 1L, 'will': 1L, ... |\n",
      "| [0.0108904326335, 0.137812... | {'on': 1L, 'devil': 1L, 'r... |\n",
      "| [0.0455414652824, 0.015473... | {'tonight': 1L, 'about': 1... |\n",
      "| [-0.0199679937214, 0.01662... | {'cane': 1L, 'orang': 1L, ... |\n",
      "| [-0.0380886942148, -0.0147... | {'on': 1L, 'liar': 1L, '**... |\n",
      "| [-0.0477378182113, 0.09303... | {'cbc': 1L, 'on': 1L, 'els... |\n",
      "| [-0.0593592710793, 0.07800... | {'georgia': 1L, 'url': 1L,... |\n",
      "| [-0.0415348485112, 0.01273... | {'and': 2L, 'jason': 1L, '... |\n",
      "| [-0.0859031900764, 0.06229... | {'mention': 1L, 'rob': 1L,... |\n",
      "| [-0.0181899759918, 0.07918... | {'see': 1L, 'are': 1L, 'go... |\n",
      "| [-0.0378844738007, 0.06154... | {'...': 1L, 'friends:': 1L... |\n",
      "| [-0.00453892536461, 0.0406... | {'a': 1L, 'you.': 1L, 'wee... |\n",
      "| [0.00355677539483, -0.0081... | {'a': 1L, '...': 1L, 'diff... |\n",
      "| [-0.00166837824509, 0.0829... | {'of': 1L, 'lax-': 1L, 'gt... |\n",
      "| [0.00147625349928, 0.05020... | {'anatomi': 1L, 'and': 1L,... |\n",
      "| [-0.0247721560299, 0.08352... | {'be': 2L, 'we': 1L, ':-)'... |\n",
      "| [0.0125198252499, 0.089563... | {'user': 1L, 'an': 1L, 'at... |\n",
      "| [-0.00161526049487, 0.0040... | {'all': 1L, 've': 2L, 'lfc... |\n",
      "| [-0.00721378624439, 0.0314... | {'shop': 1L, 'busi': 1L, '... |\n",
      "| [-0.0267585460097, -0.0531... | {'a': 1L, 'on': 1L, 'eyes,... |\n",
      "| [0.0128834918141, 0.086479... | {'and': 1L, 'columbia,': 1... |\n",
      "| [-0.0385023318231, 0.02899... | {'just': 1L, 'in': 2L, 'ou... |\n",
      "| [-0.0354197695851, 0.01663... | {'be': 2L, 'laugh....but':... |\n",
      "| [-0.00944831781089, 0.1406... | {'do': 1L, 'don': 1L, 'for... |\n",
      "| [0.00232201698236, 0.07737... | {'model': 1L, 'bottl': 1L,... |\n",
      "| [-0.00189959851559, 0.1452... | {'and': 1L, 'on': 1L, 'sch... |\n",
      "| [-0.0613267682493, 0.04999... | {'and': 1L, 'all': 1L, 'du... |\n",
      "| [-0.0594992078841, -0.0173... | {'an': 1L, 'team': 1L, 'st... |\n",
      "| [-0.0593225322664, 0.01047... | {'the': 1L, 'we': 1L, 'cho... |\n",
      "| [0.021857528016, 0.0638446... | {'me': 1L, 'eddi': 1L, 'th... |\n",
      "| [0.0383777618408, 0.037366... | {'a': 2L, 'mountain': 1L, ... |\n",
      "| [0.00602770736441, -0.0461... | {'into': 1L, 'house.': 1L,... |\n",
      "| [-0.0467856749892, -0.0101... | {'mom:': 1L, 'rang': 1L, '... |\n",
      "| [-0.062638990581, -0.00471... | {'choic': 1L, 'c': 1L, 'i'... |\n",
      "| [-0.0455922298133, 0.05686... | {'we': 1L, 'twitter,': 1L,... |\n",
      "| [-0.0217750221491, 0.06930... | {'and': 2L, 'then': 1L, 'e... |\n",
      "| [-0.0571315772831, -0.0197... | {'webb': 1L, 'on': 2L, 'ar... |\n",
      "| [-0.0550208389759, 0.07270... | {'be': 1L, 'do': 1L, 'flor... |\n",
      "| [0.00114824064076, 0.05734... | {'all': 2L, 'brings.': 1L,... |\n",
      "| [-0.0421308614314, 0.02313... | {'ali': 1L, 'muhammad': 1L... |\n",
      "| [-0.00937065854669, 0.0422... | {'dwyan': 1L, 'fantasi': 1... |\n",
      "| [-0.035954516381, 0.068109... | {'return': 1L, 'look': 1L,... |\n",
      "| [0.00510449195281, 0.06101... | {'and': 1L, 'hurricane.': ... |\n",
      "| [-0.0589256547391, 0.07995... | {'choic': 1L, 'hey,': 1L, ... |\n",
      "| [-0.0422638133168, -0.0987... | {'though,': 1L, 'later..':... |\n",
      "| [-0.0327122323215, 0.01905... | {'brookfield.': 1L, 'and':... |\n",
      "| [-0.0624617561698, 0.05637... | {'and': 1L, 'copper': 1L, ... |\n",
      "| [-0.0231299530715, 0.08608... | {'at': 1L, '$numth': 1L, '... |\n",
      "| [-0.066377595067, 0.017087... | {'bodi': 1L, 'power': 1L, ... |\n",
      "| [-0.0142755778506, -9.4726... | {'and': 1L, 'all': 1L, 'ju... |\n",
      "| [-0.107601836324, -0.01266... | {'on': 1L, 'malibu': 1L, '... |\n",
      "| [0.031563706696, 0.0806484... | {'concert': 1L, 'url': 1L,... |\n",
      "| [0.0392020866275, 0.151952... | {'be': 1L, 'drink': 1L, 'o... |\n",
      "| [0.00321801146492, 0.08341... | {'ran': 1L, 'it': 1L, 'mia... |\n",
      "| [-0.0539445057511, -0.0082... | {'all': 1L, 'concert': 1L,... |\n",
      "| [0.0129013229162, 0.038833... | {'excit': 1L, 'pre-fashion... |\n",
      "| [-0.0424652583897, -0.0342... | {'wiki': 1L, 'dont': 1L, '... |\n",
      "| [-0.0285013914108, -0.0259... | {'a': 3L, 'be': 1L, 'for':... |\n",
      "| [-0.0692483633757, -0.0212... | {'a': 1L, 'good': 1L, 'lar... |\n",
      "| [-0.1493691504, 0.12307546... | {'clipper': 1L, 'beat': 1L... |\n",
      "| [-0.0358489342034, 0.07086... | {'billion': 1L, 'classic':... |\n",
      "| [0.0229071173817, 0.035730... | {'a': 1L, 'kind': 1L, 'i':... |\n",
      "| [-0.0209683496505, 0.00188... | {'bho': 1L, 'romney': 1L, ... |\n",
      "| [-0.0277325026691, 0.02579... | {'school': 1L, 'le': 1L, '... |\n",
      "| [0.0118630155921, 0.090441... | {'and': 1L, 'a': 1L, 'matt... |\n",
      "| [-0.0222915988415, 0.12802... | {'el': 1L, 'is': 1L, 'back... |\n",
      "| [0.0162243247032, 0.093819... | {'tri': 1L, 'the': 2L, 'go... |\n",
      "| [-0.0449375919998, 0.05137... | {'a': 1L, 'mike': 1L, 'dev... |\n",
      "| [-0.0159028191119, 0.15472... | {'me': 1L, 'and': 1L, 'hoo... |\n",
      "| [-0.0691620409489, 0.03681... | {'on': 2L, 'now': 1L, 'ari... |\n",
      "| [-0.0260338839144, -0.0376... | {'$numpm-$numpm.': 1L, 'is... |\n",
      "| [-0.0382589325309, 0.03749... | {'yeah': 1L, 'georgia': 1L... |\n",
      "| [-0.0499980710447, 0.05040... | {'all': 1L, 'av': 1L, 'age... |\n",
      "| [-0.0183355975896, 0.09474... | {'and': 1L, 'at': 1L, '$nu... |\n",
      "| [-0.0905553996563, 0.00863... | {'even': 1L, 'el': 1L, 'of... |\n",
      "| [-0.0390683040023, 0.09164... | {'again': 1L, 'we': 1L, ':... |\n",
      "| [0.0190718453377, 0.067503... | {'steel': 1L, 'tuesday.': ... |\n",
      "| [-0.0224700048566, 0.04144... | {'a': 1L, 'index': 1L, 'co... |\n",
      "| [-0.0786083340645, 0.11862... | {'...': 1L, 'anim': 2L, 'f... |\n",
      "| [-0.0812315791845, 0.00704... | {'be': 2L, 'the...': 1L, '... |\n",
      "| [-0.0429613552988, 0.09512... | {'webb': 1L, 'save': 1L, '... |\n",
      "| [-0.0775408968329, 0.03340... | {'and': 1L, 'both': 1L, 't... |\n",
      "| [-0.0227977372706, -8.2394... | {'c': 1L, 'google,': 1L, '... |\n",
      "| [-0.0292471423745, 0.02520... | {'and': 1L, 'on': 1L, 'sur... |\n",
      "| [-0.0362385623157, 0.05672... | {'in': 2L, 'and': 1L, 'cen... |\n",
      "| [-0.0473119430244, 1.89181... | {'ago': 1L, 'on': 2L, 'wiz... |\n",
      "| [-0.0895557552576, -0.0274... | {'it': 1L, '$numth': 1L, '... |\n",
      "| [-0.0396156273782, 0.02113... | {'fuck': 1L, 'see': 1L, 'd... |\n",
      "| [-0.043849054724, 0.292630... | {'a': 1L, 'do': 1L, 'want'... |\n",
      "| [-0.062793366611, -0.03484... | {'and': 1L, 'all': 1L, 'ha... |\n",
      "| [-0.00190573500004, 0.0992... | {'and': 1L, 'the': 1L, 'al... |\n",
      "| [0.00898650474846, 0.03531... | {'and': 1L, 'good': 1L, 'w... |\n",
      "| [-0.0488154292107, 0.06584... | {'it': 1L, 'at': 1L, 'foll... |\n",
      "| [-0.0815523788333, 0.11967... | {'over': 1L, 'it': 1L, 'at... |\n",
      "| [-0.00421002553776, 0.1264... | {'wont': 1L, 'ive': 1L, 'p... |\n",
      "| [0.0337110459805, 0.120688... | {'$num': 1L, 'cuban': 1L, ... |\n",
      "| [-0.112952888012, 0.105612... | {'e$num': 1L, 'caus': 1L, ... |\n",
      "| [0.0094662187621, 0.251668... | {'and': 1L, 'wifff': 1L, '... |\n",
      "| [0.0492523871362, 0.049600... | {'a': 1L, 'from': 1L, 'doe... |\n",
      "| [-0.057756382972, -0.03664... | {'it.': 1L, 'love': 1L, 'm... |\n",
      "| [-0.0577867776155, 0.01760... | {'mucha': 1L, 'all': 1L, '... |\n",
      "| [-0.0616859979928, 0.11682... | {'at': 2L, 'in': 1L, 'thei... |\n",
      "| [-0.027486519888, 0.067216... | {'addicted...i': 1L, 'im':... |\n",
      "| [-0.0604408495128, 0.06479... | {'a': 1L, 'on': 1L, 'last'... |\n",
      "| [-0.0365670137107, 0.02795... | {'clipper': 1L, 'on': 1L, ... |\n",
      "| [-0.0714586600661, 0.12066... | {'$num': 1L, 'all': 1L, 'c... |\n",
      "| [-0.090039588511, -0.02811... | {'and': 1L, 'week': 1L, 'o... |\n",
      "| [-0.00634918548167, 0.0112... | {'crunch': 1L, 'had': 1L, ... |\n",
      "| [-0.0318430252373, 0.02588... | {'load': 1L, 'everi': 1L, ... |\n",
      "| [-0.056011557579, 0.018304... | {'reap': 1L, '(bcs)': 1L, ... |\n",
      "| [-0.0615633167326, 0.04789... | {'golden': 1L, 'globe': 1L... |\n",
      "| [-0.00880001299083, 0.0067... | {'opposit': 1L, 'dure': 1L... |\n",
      "| [-0.0312273986638, 0.06801... | {'liar': 1L, 'marlen': 1L,... |\n",
      "| [-0.148102983832, 0.058263... | {'and': 1L, 'all': 1L, 'in... |\n",
      "| [-0.0643607825041, 0.03558... | {'have': 2L, 'we': 1L, 's$... |\n",
      "| [0.0178718157113, 0.125091... | {'a': 1L, 'wiki': 1L, 'her... |\n",
      "| [-0.0295387674123, 0.03654... | {'feedback': 1L, 'just': 1... |\n",
      "| [-0.0346180386841, 0.10068... | {'and': 1L, 'me,': 1L, 'is... |\n",
      "| [-0.094345793128, 0.334306... | {'for': 1L, 'can': 1L, 'se... |\n",
      "| [-0.135188341141, 0.063304... | {'me': 1L, 'on': 1L, 'love... |\n",
      "| [0.0273869726807, -0.00613... | {'and': 1L, 'boy': 1L, 'fe... |\n",
      "| [0.0230792220682, 0.047137... | {'a': 1L, 'on': 1L, 'hold'... |\n",
      "| [-0.130336135626, 0.006480... | {'play': 1L, 'never': 1L, ... |\n",
      "| [-0.0821606963873, 0.06975... | {'ricki': 1L, 'be': 1L, 'm... |\n",
      "| [0.0281613990664, 0.049694... | {'song': 1L, 'friday': 1L,... |\n",
      "| [-0.0682339966297, 0.09019... | {'and': 1L, 'just': 1L, 's... |\n",
      "| [-0.0348236635327, 0.04697... | {'be': 1L, 'luther': 1L, '... |\n",
      "| [-0.0935243591666, 0.08365... | {'a': 1L, 'newcastl': 1L, ... |\n",
      "| [-0.025514382869, 0.027454... | {'and': 1L, 'joy': 1L, 'in... |\n",
      "| [-0.0151431318372, 0.21246... | {'end': 1L, 'that': 1L, ':... |\n",
      "| [-0.0491836369038, 0.14230... | {'even': 1L, 'use': 1L, 'p... |\n",
      "| [-0.0432638041675, 0.10857... | {'and': 1L, 'norwich': 1L,... |\n",
      "| [-0.032421451062, 0.087982... | {'and': 1L, 'from': 1L, 'j... |\n",
      "| [-0.00637843459845, 0.0073... | {'on': 1L, 'pacer': 1L, 'w... |\n",
      "| [0.0111371865496, 0.054100... | {'all': 1L, 'four,th': 1L,... |\n",
      "| [-0.0891445279121, -0.0375... | {'and': 1L, 'week': 1L, 'a... |\n",
      "| [-0.0566440597177, 0.04870... | {'a': 2L, 'on': 1L, 'engla... |\n",
      "| [-0.00873961113393, 0.1297... | {'and': 1L, 'always.': 1L,... |\n",
      "| [-0.0744456723332, 0.03317... | {'durant': 1L, 'point': 1L... |\n",
      "| [-0.0493814945221, 0.02093... | {'ami': 1L, 'golden': 1L, ... |\n",
      "| [-0.0311196297407, 0.04568... | {'is': 1L, 'at': 1L, 'girl... |\n",
      "| [0.0101407626644, 0.000448... | {'round.': 1L, 'a': 1L, 'm... |\n",
      "| [0.00175061239861, 0.03747... | {'a': 1L, 'on': 1L, 'i': 2... |\n",
      "| [0.0518035069108, 0.068702... | {'be': 1L, 'matt': 1L, 'fi... |\n",
      "| [-0.0740896016359, 0.15171... | {'fifa': 1L, 'el': 1L, 'my... |\n",
      "| [-0.0324969962239, 0.03805... | {'and': 1L, 'chip.': 1L, '... |\n",
      "| [-0.0637900456786, 0.03060... | {'alabama': 1L, 'or': 1L, ... |\n",
      "| [-0.0338892526925, 0.04197... | {'till': 1L, 'at': 2L, '$n... |\n",
      "| [-0.0727734565735, 0.09928... | {'el': 1L, 'lock': 1L, 'it... |\n",
      "| [-0.0744018256664, -0.0125... | {'mile': 1L, 'coach': 1L, ... |\n",
      "| [-0.0769541487098, -0.0299... | {'and': 1L, 'la.': 1L, 'dw... |\n",
      "| [-0.0894424691796, 0.00373... | {'a': 1L, 'pas': 1L, 'just... |\n",
      "| [0.0155551182106, 0.021780... | {'lamar': 1L, 'for': 1L, '... |\n",
      "| [0.00802935659885, 0.10079... | {'and': 1L, 'golden': 1L, ... |\n",
      "| [-0.0618068240583, 0.17622... | {'you': 1L, 'so..': 1L, 'p... |\n",
      "| [-0.0221680365503, 0.07303... | {'and': 1L, 'is': 1L, 'som... |\n",
      "| [-0.0229056794196, -0.0241... | {'a': 1L, 'of': 2L, 'john'... |\n",
      "| [-0.0860818699002, 0.06104... | {'for': 1L, '$numrd': 1L, ... |\n",
      "| [-0.00258550420403, 0.0416... | {'sscx': 1L, '$num:': 1L, ... |\n",
      "| [-0.0142871756107, -0.0046... | {'a': 1L, 'texan': 1L, 'fo... |\n",
      "| [-0.00834351219237, 0.0404... | {'becaus': 1L, 'tumblr': 1... |\n",
      "| [-0.0412339344621, 0.16856... | {'a': 1L, 'anatomi': 1L, '... |\n",
      "| [-0.0723815187812, 0.01291... | {'chalmer': 1L, 'foul': 1L... |\n",
      "| [-0.060642618686, 0.045068... | {'album': 2L, 'itun': 1L, ... |\n",
      "| [-0.0325547754765, 0.04063... | {'box': 1L, 'then': 1L, 'a... |\n",
      "| [-0.00802580174059, 0.0378... | {'a': 2L, 'internet': 1L, ... |\n",
      "| [0.0142890419811, 0.097975... | {'i': 2L, 'january.': 1L, ... |\n",
      "| [-0.0101366462186, 0.09792... | {'and': 1L, 'the': 3L, 'gu... |\n",
      "| [-0.0847741290927, 0.04363... | {'back': 1L, 'a': 1L, ',':... |\n",
      "| [-0.000912654446438, 0.166... | {'a': 1L, 'newcastl': 1L, ... |\n",
      "| [-0.0487545132637, 0.04851... | {'will': 1L, 'we': 1L, 'la... |\n",
      "| [-0.0255273301154, 0.03810... | {'aw': 1L, 'it': 1L, 'at':... |\n",
      "| [-0.0380601994693, 0.07253... | {'$num-$num,': 1L, 'on': 1... |\n",
      "| [-0.00762757426128, -0.126... | {'be': 1L, 'celtic': 1L, '... |\n",
      "| [-0.0329280532897, 0.05287... | {'be': 1L, 'said': 1L, 'wo... |\n",
      "| [0.0158559959382, 0.013763... | {'be': 1L, 'tomorrow': 1L,... |\n",
      "| [-0.0459963046014, 0.20493... | {'michael': 1L, 'dc': 1L, ... |\n",
      "| [0.00535284494981, 0.03378... | {'tonight,': 1L, 'by': 1L,... |\n",
      "| [-0.0204818882048, 0.13329... | {'bowl?': 1L, 'georgia': 1... |\n",
      "| [-0.0824794322252, 0.02057... | {'be': 1L, 'luther': 1L, '... |\n",
      "| [-0.042861700058, 0.148648... | {'a': 2L, 'then': 1L, 'for... |\n",
      "| [0.00435890862718, 0.05674... | {'a': 1L, 'on': 1L, 'page,... |\n",
      "| [-0.0422769673169, -0.0253... | {'awar': 1L, 'check': 1L, ... |\n",
      "| [-0.0599208176136, -0.0620... | {'just': 1L, 'is': 1L, 'ha... |\n",
      "| [-0.0238769687712, 0.06428... | {'love': 1L, 'movi': 1L, '... |\n",
      "| [0.000162042677402, 0.0171... | {'golden': 1L, '$num-$num.... |\n",
      "| [-0.0653506815434, 0.14137... | {'be': 1L, ':)': 1L, 'i': ... |\n",
      "| [0.043871242553, 0.1091278... | {'and': 1L, ':(': 1L, 'had... |\n",
      "| [-0.0762208551168, -0.0807... | {'and': 1L, 'on': 1L, 'abo... |\n",
      "| [0.051543161273, 0.0421932... | {'and': 1L, ':(': 1L, 'som... |\n",
      "| [-0.134363934398, 0.219072... | {'a': 1L, 'on': 1L, 'sunda... |\n",
      "| [0.0189971663058, 0.076052... | {'mirror': 1L, 'one': 1L, ... |\n",
      "| [-0.0529408790171, 0.00108... | {'nwc': 1L, 'it': 1L, 'nov... |\n",
      "| [-0.0396797657013, 0.03570... | {'and': 1L, 'just': 1L, '.... |\n",
      "| [0.000715745612979, 0.1225... | {'sound': 1L, 'them': 1L, ... |\n",
      "| [0.0110860746354, 0.134724... | {'so': 1L, 'wanna': 1L, 'i... |\n",
      "| [-0.0656841471791, -0.0157... | {'and': 1L, 'golden': 1L, ... |\n",
      "| [0.0521953888237, 0.053865... | {'and': 1L, 'is': 1L, 'bec... |\n",
      "| [-0.0840977802873, 0.07763... | {'on': 1L, 'liverpool': 1L... |\n",
      "| [-0.0276213567704, 0.13560... | {'on': 1L, 'pain': 1L, 'th... |\n",
      "| [-0.022842625156, -0.11651... | {'and': 1L, 'finchelor.': ... |\n",
      "| [-0.116751529276, 0.065018... | {'be': 1L, 'obama': 1L, 'a... |\n",
      "| [-0.0105456411839, 0.06412... | {'and': 1L, 'on': 1L, 'ami... |\n",
      "| [-0.0297262705863, 0.01203... | {'januari': 1L, 'parad': 1... |\n",
      "| [-0.0729345008731, 0.01962... | {'a': 1L, 'rumbl': 1L, 'ap... |\n",
      "| [-0.105400644243, -0.08053... | {'no,': 1L, 'and': 1L, 'al... |\n",
      "| [0.0446121022105, 0.029543... | {'again': 1L, 'tonight.': ... |\n",
      "| [-0.0198208615184, -0.0248... | {'sound': 1L, 'be': 2L, 'h... |\n",
      "| [0.0154598122463, 0.109236... | {'just': 1L, 'becaus': 1L,... |\n",
      "| [-0.0580459348857, 0.01160... | {'a': 1L, 'on': 2L, 'that'... |\n",
      "| [-0.0546106509864, 0.02649... | {'even': 1L, 'on': 1L, 'fo... |\n",
      "| [-0.122732840478, 0.140825... | {'sunday:': 1L, 'cm': 1L, ... |\n",
      "| [-0.0070849750191, 0.00992... | {'kidrauhl': 1L, 'c': 1L, ... |\n",
      "| [-0.0301091223955, 0.02763... | {'absolut': 2L, 'c': 1L, '... |\n",
      "| [0.0328027047217, 0.021031... | {'and': 1L, 'both': 1L, 'f... |\n",
      "| [-0.0554527267814, 0.09800... | {'newcastl': 1L, 'x)': 1L,... |\n",
      "| [-0.0207700319588, 0.02096... | {'a': 2L, 'of...': 1L, 'ma... |\n",
      "| [-0.0271319746971, 0.04812... | {'and': 1L, 'season.': 1L,... |\n",
      "| [-0.058125525713, 0.261960... | {'play': 1L, 'for': 1L, 'd... |\n",
      "| [-0.0367707982659, 0.09310... | {'and': 2L, ':(': 1L, 'hav... |\n",
      "| [0.0127541609108, 0.224338... | {'eduardo': 1L, 'thursday'... |\n",
      "| [-0.0408950075507, 0.06829... | {'it.': 1L, 'didn': 1L, 't... |\n",
      "| [0.00516738975421, 0.16895... | {'street': 1L, 'at': 3L, '... |\n",
      "| [-0.00309587595984, 0.0084... | {'a': 1L, 'king': 1L, 'lut... |\n",
      "| [-0.066857740283, 0.099862... | {'teen': 1L, 'can': 1L, 'i... |\n",
      "| [-0.0651089474559, 0.04935... | {'and': 1L, 'torress': 1L,... |\n",
      "| [-0.0124725233763, 0.05896... | {'a': 1L, 'c': 1L, 'don': ... |\n",
      "| [0.0252090077847, -0.01800... | {'and': 1L, 'we': 1L, 'mad... |\n",
      "| [-0.0346241183579, 0.06038... | {'awar': 1L, 'breed.': 1L,... |\n",
      "| [-0.0816091150045, 0.01796... | {'respond': 1L, 'feed': 1L... |\n",
      "| [-0.00302427192219, 0.0317... | {'em': 1L, 'ive': 1L, 'wat... |\n",
      "| [0.0243079587817, 0.010119... | {'and': 1L, 'just': 1L, 'g... |\n",
      "| [-0.0743171572685, 0.14369... | {':(': 1L, 'is': 1L, 'in':... |\n",
      "| [-0.0700479447842, 0.04743... | {'be': 1L, 'human': 1L, 'j... |\n",
      "| [-0.0310575868934, 0.05284... | {'be': 1L, 'giant': 1L, 'e... |\n",
      "| [-0.00182231143117, 0.0328... | {'aug.': 1L, 'photo': 1L, ... |\n",
      "| [0.0134599385783, 0.051267... | {'the': 1L, ':(': 1L, 'phi... |\n",
      "| [0.0171637833118, 0.023910... | {'me': 1L, 'boy': 1L, 'hea... |\n",
      "| [-0.050327822566, -0.04517... | {'now,': 1L, 'be': 1L, 'an... |\n",
      "| [-0.00772584509104, 0.0672... | {'and': 1L, 'friday,': 1L,... |\n",
      "| [-0.065128698945, -0.01519... | {'scutaro': 2L, 'hip': 1L,... |\n",
      "| [-0.0913192853332, 0.01139... | {'premiers,': 1L, 'afrojac... |\n",
      "| [0.0205189790577, -0.03086... | {'and': 1L, 'over': 1L, 'a... |\n",
      "| [1.57025133376e-05, 0.0846... | {'green.': 1L, 'concert': ... |\n",
      "| [-0.00954212900251, -0.017... | {'brazil': 1L, 'the': 1L, ... |\n",
      "| [-0.05380673334, -0.060411... | {'b': 1L, 'may': 1L, 'is':... |\n",
      "| [-0.0815905183554, 0.14306... | {'lmfaoooo': 1L, 'at': 2L,... |\n",
      "| [-0.0010717822006, 0.08503... | {'start': 1L, 'of': 1L, 'l... |\n",
      "| [0.00928697921336, 0.05097... | {'even': 1L, 'me': 1L, 'al... |\n",
      "| [0.020456219092, 0.0659141... | {'serv': 1L, 'and': 1L, 'f... |\n",
      "| [-0.0197348631918, -0.0051... | {'about': 1L, 'wa': 1L, 'h... |\n",
      "| [-0.0442237891257, 0.02323... | {'move': 1L, 'southampton'... |\n",
      "| [-0.0128804882988, -0.0192... | {'tide': 1L, 'begin': 1L, ... |\n",
      "| [-0.0365856587887, 0.10821... | {'and': 1L, 'el': 2L, 'at'... |\n",
      "| [0.00899633765221, 0.00523... | {'lfc': 1L, 'emot': 1L, '$... |\n",
      "| [0.0343910232186, 0.197875... | {'and': 2L, 'play': 1L, 'j... |\n",
      "| [-0.0504638329148, 0.15943... | {'see': 1L, 'at': 1L, 'in'... |\n",
      "| [0.021306283772, 0.1188459... | {'do': 1L, ':-)': 1L, 'hea... |\n",
      "| [-0.0402150936425, 0.02918... | {'a': 1L, 'do': 1L, 'and':... |\n",
      "| [-0.0150678437203, 0.09550... | {'a': 1L, 'daboswinneyprob... |\n",
      "| [-0.0324529670179, 0.02966... | {'me': 1L, 'be': 1L, 'on':... |\n",
      "| [-0.0484945289791, 0.05851... | {'we': 1L, 'episod': 1L, '... |\n",
      "| [-0.030599810183, 0.040139... | {'the': 2L, 'onli': 1L, ':... |\n",
      "| [-0.00214464077726, -0.049... | {'a': 1L, 'and': 1L, 'actu... |\n",
      "| [-0.0545872300863, 0.03600... | {'at': 1L, 'in': 1L, 'if':... |\n",
      "| [0.0179383605719, 0.052865... | {'and': 1L, 'golden': 1L, ... |\n",
      "| [-0.0741953775287, 0.07814... | {'a': 3L, 'don': 1L, 'to':... |\n",
      "| [-0.0315528921783, 0.10465... | {'user': 1L, 'on': 1L, 'ur... |\n",
      "| [-0.0203472804278, 0.04953... | {'school': 1L, 'moc-floyd'... |\n",
      "| [-0.0780061632395, 0.12295... | {'go,': 1L, 'play': 1L, 'r... |\n",
      "| [0.00268582254648, 0.12532... | {'on': 1L, 'to': 1L, 'url'... |\n",
      "| [0.025825612247, 0.0157832... | {':)': 1L, 'some': 1L, 'ev... |\n",
      "| [-0.00960778445005, 0.0265... | {'even': 1L, 'chalmer': 1L... |\n",
      "| [-0.0130428085104, 0.03174... | {'on': 1L, 'play': 1L, 'ta... |\n",
      "| [-0.0523699373007, 0.03855... | {'week': 1L, 'cbb': 1L, 's... |\n",
      "| [0.0398712866008, 0.004429... | {'be': 1L, 'asleep': 1L, '... |\n",
      "| [-0.0432371795177, 0.02354... | {'and': 2L, 'impact': 1L, ... |\n",
      "| [-0.0637994110584, 0.06202... | {'a': 1L, 'on': 1L, 'no.':... |\n",
      "| [0.0191837288439, -0.01678... | {'trainer': 1L, 'and': 1L,... |\n",
      "| [-0.0359298251569, 0.05655... | {'ami': 1L, 'golden': 1L, ... |\n",
      "| [0.0792124122381, 0.056159... | {'me': 1L, 'a': 2L, 'small... |\n",
      "| [0.0193088967353, -0.01516... | {'even': 1L, 'king': 1L, '... |\n",
      "| [-0.0242350604385, 0.05727... | {'url': 1L, 'park': 1L, 'n... |\n",
      "| [0.0264861732721, -0.01823... | {'love': 1L, 'xoxox': 1L, ... |\n",
      "| [-0.0368678830564, 0.03116... | {'and': 2L, 'all': 1L, 'ty... |\n",
      "| [-0.118714563549, 0.112924... | {'and': 1L, 'on': 1L, 'abo... |\n",
      "| [0.0497824884951, 0.055314... | {'closer': 1L, 'huntsville... |\n",
      "| [-0.0550668239594, -0.0316... | {'it.': 1L, 'on': 1L, 'hon... |\n",
      "| [-0.0383427739143, 0.01774... | {'a': 1L, 'on': 1L, 'liver... |\n",
      "| [-0.0216367729008, 0.03572... | {'tri': 2L, 'on': 1L, 'for... |\n",
      "| [-0.0998618006706, 0.12082... | {'a': 1L, 'on': 1L, 'monda... |\n",
      "| [-0.0530989430845, 0.01770... | {'thebachelor': 1L, 'excit... |\n",
      "| [-0.0335507057607, 0.12515... | {'onli': 1L, 'sorry...': 1... |\n",
      "| [0.0225276853889, 0.055721... | {'look': 1L, 'movi': 1L, '... |\n",
      "| [-0.0751526504755, 0.14166... | {'tuesday': 1L, 'at': 2L, ... |\n",
      "| [-0.0307407174259, 0.13864... | {'//': 1L, 'on': 1L, 'it,'... |\n",
      "| [-0.0130402659997, 0.04024... | {'costum': 1L, 'googl': 1L... |\n",
      "| [0.0296571198851, 0.132644... | {'and': 3L, 'do': 1L, 'thi... |\n",
      "| [-0.0592928305268, -0.0479... | {'and': 1L, 'the': 2L, 'br... |\n",
      "| [-0.0599036663771, 0.07184... | {'huge': 1L, 'stone': 1L, ... |\n",
      "| [-0.0239307377487, 0.04672... | {'onli': 1L, 'thi': 1L, 'c... |\n",
      "| [-0.139378875494, 0.090977... | {'prediction:': 1L, 'bold'... |\n",
      "| [0.00857042428106, 0.12385... | {'and': 1L, 'tomorrow.': 1... |\n",
      "| [-0.0487755946815, 0.05351... | {'respond': 1L, 'be': 1L, ... |\n",
      "| [0.00296388752759, 0.00491... | {'grime,': 1L, 'on': 1L, '... |\n",
      "| [-0.0474674366415, 0.05767... | {'and': 3L, 'all': 1L, 'an... |\n",
      "| [0.0259777847677, 0.051809... | {'contract?': 1L, 'price.'... |\n",
      "| [-0.0282002203166, 0.12425... | {'hqb,': 1L, 'aja,': 1L, '... |\n",
      "| [-0.00819054991007, 0.0343... | {'stat': 1L, 'nigga': 1L, ... |\n",
      "| [-0.0507114641368, 0.03062... | {'el': 1L, 'about': 1L, 'p... |\n",
      "| [0.0433351323009, -0.08887... | {'on': 1L, 'patriot': 1L, ... |\n",
      "| [-0.0427974946797, 0.04078... | {'me': 1L, 'and': 1L, 'ana... |\n",
      "| [0.0155212860554, 0.189155... | {'notes..': 1L, 'brown': 1... |\n",
      "| [-0.0193113554269, -0.0519... | {'(and': 1L, 'madonna': 1L... |\n",
      "| [0.00457054236904, 0.05031... | {'on': 1L, 'we': 2L, 'swan... |\n",
      "| [0.00976726412773, 0.03243... | {'and': 2L, 'then': 1L, 'l... |\n",
      "| [-0.0448298491538, 0.08557... | {'holla': 1L, 'poker': 1L,... |\n",
      "| [-0.00817367434502, 0.0926... | {'and': 2L, 'fuck.': 1L, '... |\n",
      "| [-0.0581041760743, 0.09245... | {'and': 1L, 'charlott': 1L... |\n",
      "| [-0.0565429367125, -0.0264... | {'and': 1L, 'king': 1L, 'c... |\n",
      "| [-0.00697827059776, 0.0440... | {'drop': 1L, 'user': 1L, '... |\n",
      "| [-0.0100315539166, 0.02296... | {':)': 1L, 'is': 1L, 'some... |\n",
      "| [-0.0290440637618, 0.06010... | {'via': 1L, 'an': 1L, 'lsu... |\n",
      "| [-0.0145923914388, 0.01023... | {'and': 1L, 'is': 2L, 'fro... |\n",
      "| [0.00194977014326, 0.01730... | {'a': 3L, 'jan': 1L, 'ariz... |\n",
      "| [0.0210653878748, 0.083216... | {'saturday': 1L, 'nffc': 1... |\n",
      "| [-0.0306738708168, 0.23289... | {'and': 1L, 'on': 1L, ':('... |\n",
      "| [-0.00517556909472, -0.041... | {'a': 1L, 'be': 1L, 'may':... |\n",
      "| [-0.0737115591764, 0.00668... | {'recently,': 1L, 'homegro... |\n",
      "| [-0.0647903829813, 0.01503... | {'time...': 1L, 'at': 1L, ... |\n",
      "| [-0.0382035039365, -0.0775... | {'...': 1L, 'would': 1L, '... |\n",
      "| [0.0368801355362, 0.060749... | {'tomorrow': 1L, 'fantasyf... |\n",
      "| [0.0268437862396, 0.043786... | {'user': 1L, 'on': 1L, 'sp... |\n",
      "| [-0.0951878577471, 0.15599... | {'earli': 1L, 'a': 1L, 'ma... |\n",
      "| [-0.0503620542586, 0.14632... | {'on': 1L, ':-)': 1L, 'ret... |\n",
      "| [-0.0679892599583, 0.08462... | {'and': 1L, 'we': 1L, 'in'... |\n",
      "| [-0.0705554783344, 0.16322... | {'el': 1L, 'have': 1L, 'la... |\n",
      "| [-0.0654674097896, 0.01997... | {'and': 1L, 'yet?': 1L, 's... |\n",
      "| [-0.0319362767041, 0.17876... | {'me': 1L, 'i': 1L, 'flori... |\n",
      "| [-0.0561401732266, 0.05888... | {'are': 1L, 'on': 1L, 'and... |\n",
      "| [0.00729086715728, 0.12572... | {'and': 1L, 'tinchi': 1L, ... |\n",
      "| [-0.0381912253797, -0.0077... | {'gone': 1L, 'said': 1L, '... |\n",
      "| [-0.041028149426, 0.115232... | {'town': 1L, 'on': 1L, 'av... |\n",
      "| [-0.0282177291811, 0.05133... | {'and': 2L, 'aha': 1L, 'ye... |\n",
      "| [-0.0246204268187, 0.03060... | {'riverside,': 1L, 'be': 1... |\n",
      "| [-0.06210635975, 0.1007103... | {'and': 1L, 'el': 1L, 'jus... |\n",
      "| [-0.0409106090665, 0.10443... | {'it': 2L, '$num-$num': 1L... |\n",
      "| [-0.0256296414882, 0.13284... | {'the': 1L, 'monday': 1L, ... |\n",
      "| [-0.0842637419701, 0.00124... | {'it.': 1L, 'a': 1L, 'don'... |\n",
      "| [-0.0756014734507, -0.0548... | {'week': 1L, 'and': 2L, 'a... |\n",
      "| [-0.00176012096927, 0.0958... | {'play': 1L, 'said': 1L, '... |\n",
      "| [0.02239382267, 0.11410196... | {'championship': 1L, 'want... |\n",
      "| [-0.0631031543016, 0.04491... | {'game': 1L, '$num': 1L, '... |\n",
      "| [-0.0443431921303, 0.02844... | {'a': 1L, '$numrd,': 1L, '... |\n",
      "| [0.00892918929458, 0.10845... | {'excit': 1L, 'houston': 1... |\n",
      "| [-0.0247384980321, 0.13026... | {'a': 1L, 'we': 1L, ':)': ... |\n",
      "| [-0.0467160195112, 0.04845... | {'event.': 1L, 'chapter': ... |\n",
      "| [-0.043659415096, -0.01124... | {'last': 1L, 'wa': 1L, 'no... |\n",
      "| [-0.0312202628702, -0.0004... | {'a': 1L, 'portray': 1L, '... |\n",
      "| [-0.0797911807895, -0.0415... | {'a': 1L, 'be': 1L, 'may':... |\n",
      "| [-0.0514363236725, 0.01472... | {'and': 1L, 'love': 1L, 'a... |\n",
      "| [-0.0133600588888, -0.0322... | {'move': 1L, 'it': 1L, 'pa... |\n",
      "| [-0.0209064427763, 0.03262... | {'(:': 1L, 'be': 2L, 'wssu... |\n",
      "| [0.0215123761445, 0.068466... | {'and': 1L, 'do': 1L, 'abo... |\n",
      "| [0.0203878544271, 0.033470... | {'and': 1L, '...': 1L, 'va... |\n",
      "| [-0.0025587209966, 0.01884... | {'wasn': 1L, 'it': 1L, 'ye... |\n",
      "| [-0.0488340258598, 0.04291... | {'and': 1L, ':)': 1L, 'is'... |\n",
      "| [-0.0828461721539, 0.04086... | {'on': 1L, 'work': 1L, 'lo... |\n",
      "| [-0.0203958377242, 0.11863... | {'listen': 1L, 'ne,': 2L, ... |\n",
      "| [0.012601624243, 0.1029427... | {'ralli': 1L, 'jr': 1L, 'p... |\n",
      "| [0.0127997109666, 0.076613... | {'and': 1L, 'a': 1L, 'go':... |\n",
      "| [0.02470895648, 0.18965820... | {'we': 1L, 'nin': 1L, ':)'... |\n",
      "| [-0.0612034909427, 0.03389... | {'choic': 1L, 'of...': 1L,... |\n",
      "| [0.00269170687534, 0.03138... | {'and': 1L, 'eo': 1L, 'tho... |\n",
      "| [-0.00849851500243, 0.0138... | {'and': 1L, 'do': 1L, 'eve... |\n",
      "| [-0.0110466368496, 0.05677... | {'i': 1L, '-': 1L, 'm': 1L... |\n",
      "| [-0.0656746402383, 0.05314... | {'iu': 1L, 'see': 1L, '$nu... |\n",
      "| [-0.0467931516469, 5.75866... | {'me': 1L, 'rey': 1L, 'lan... |\n",
      "| [-0.0943487137556, -0.0156... | {'aston': 1L, 'sure': 1L, ... |\n",
      "| [0.00899275485426, 0.10032... | {'everi': 1L, 'for': 1L, '... |\n",
      "| [-0.0757739767432, 0.20675... | {'on': 1L, 't': 1L, 'to': ... |\n",
      "| [-0.0728662759066, 0.09180... | {'on': 2L, 'set': 2L, '...... |\n",
      "| [-0.114768840373, -0.01946... | {'page.': 1L, 'on': 1L, 'a... |\n",
      "| [-0.0783655792475, 0.16307... | {'cbb...monday': 1L, 'is':... |\n",
      "| [0.0593746975064, 0.091712... | {'on': 1L, 'i': 1L, 'broth... |\n",
      "| [0.000388496293453, 0.0735... | {'golden': 1L, 'globe': 1L... |\n",
      "| [-0.0220839884132, 0.04352... | {'and': 1L, 'taxi': 1L, 't... |\n",
      "| [0.00387054588646, 0.13525... | {'and': 1L, 'all': 1L, 'al... |\n",
      "| [-0.0179627034813, 0.07723... | {'on': 2L, 'itun': 1L, 'wh... |\n",
      "| [-0.0357877761126, 0.03898... | {'...': 1L, 'niners:': 1L,... |\n",
      "| [-0.0319625101984, -0.0442... | {'money': 1L, 'doesn': 1L,... |\n",
      "| [-0.0220376905054, 0.00920... | {'and': 1L, 'be': 1L, 'it'... |\n",
      "| [-0.0278850402683, -0.0071... | {'control': 1L, 'and': 2L,... |\n",
      "| [-0.0343231707811, 0.08303... | {'watch': 1L, '$num': 1L, ... |\n",
      "| [0.0431643612683, 0.021052... | {'everi': 1L, 'from': 1L, ... |\n",
      "| [-0.049443602562, 0.046444... | {'and': 1L, 'lock': 1L, 'a... |\n",
      "| [-0.0197455007583, 0.00561... | {'and': 1L, 've': 1L, 'gir... |\n",
      "| [-0.0146848550066, 0.13434... | {'run': 1L, 'for': 1L, 'of... |\n",
      "| [0.0133251128718, 0.073945... | {'king': 1L, 'luther': 1L,... |\n",
      "| [-0.0116544421762, -0.0511... | {'kidrauhl': 1L, 'and': 1L... |\n",
      "| [-0.0488992705941, 0.04887... | {'and': 1L, 'ah': 1L, 'it'... |\n",
      "| [0.0354352369905, 0.169397... | {'webb': 1L, 'on': 1L, 'op... |\n",
      "| [-0.111116334796, 0.031763... | {'igot': 1L, 'via': 1L, 'i... |\n",
      "| [-0.0292005147785, 0.07884... | {'gahhhhden': 1L, 'and': 1... |\n",
      "| [-0.0620219632983, -0.0668... | {'a': 1L, 'be': 2L, 'about... |\n",
      "| [-0.0446837991476, 0.07614... | {'and': 1L, 'al': 1L, 'jd'... |\n",
      "| [-0.0812287181616, -0.0633... | {'be': 1L, 'we': 1L, 'may'... |\n",
      "| [-0.0477280430496, -0.0018... | {'and': 1L, 'it': 1L, 'say... |\n",
      "| [-0.00891729537398, -0.016... | {'feel': 1L, 'we': 1L, 'se... |\n",
      "| [-0.00393006298691, 0.2266... | {'to': 1L, 'xx': 1L, 'morn... |\n",
      "| [-0.0448569208384, 0.03588... | {'be': 1L, 'about': 1L, 'l... |\n",
      "| [-0.137941196561, 0.079296... | {'glamour': 1L, 'el': 1L, ... |\n",
      "| [-0.0880702063441, 0.15978... | {'dwight': 1L, 'that': 1L,... |\n",
      "| [0.0417433343828, 0.037680... | {'drunken': 1L, 'a': 1L, '... |\n",
      "| [0.0252773500979, 0.027225... | {'and': 1L, 'set': 1L, 'po... |\n",
      "| [-0.0129646360874, 0.03853... | {'and': 1L, '$num-$num:$nu... |\n",
      "| [-0.0112716844305, -0.0549... | {'aliens,': 1L, 'backflip'... |\n",
      "| [-0.0360692739487, 0.12996... | {'celtic': 1L, 'gotta': 1L... |\n",
      "| [-0.0549148991704, 0.03891... | {'and': 1L, 'program': 1L,... |\n",
      "| [-0.00213093566708, 0.0239... | {'fiesta': 1L, 'surpris': ... |\n",
      "| [-0.0511738099158, -0.0002... | {'a': 2L, 'golden': 1L, 'h... |\n",
      "| [-0.0121896285564, -0.0070... | {'a': 1L, 'the': 2L, 'can'... |\n",
      "| [-0.00577672338113, 0.0569... | {'and': 1L, 'year': 1L, 'b... |\n",
      "| [-0.0925708413124, 0.11637... | {'play': 1L, 'via': 1L, 'f... |\n",
      "| [-0.0282002203166, 0.12425... | {'hqb,': 1L, 'aja,': 1L, '... |\n",
      "| [-0.00362866488285, 0.0214... | {'up.': 1L, 'a': 2L, 'now'... |\n",
      "| [-0.0472977086902, 0.01261... | {'and': 1L, 'cfc': 1L, 'br... |\n",
      "| [-0.00416147941723, -0.083... | {'again': 1L, 'said': 1L, ... |\n",
      "| [0.018507450819, 0.0681680... | {'and': 1L, 'lfc': 1L, 'an... |\n",
      "| [-0.00599196180701, -0.015... | {'and': 1L, 'on': 1L, 'le'... |\n",
      "| [-0.031304333359, 0.031432... | {'back': 2L, 'heard': 1L, ... |\n",
      "| [-0.0151192713529, 0.00744... | {'justin.': 1L, 'belieb': ... |\n",
      "| [0.033587757498, 0.0600819... | {'is': 2L, 'am': 1L, 'it':... |\n",
      "| [-0.000895387609489, 0.046... | {'and': 1L, 'golden': 1L, ... |\n",
      "| [-0.0407392568886, 0.10489... | {'and': 1L, 'accustom': 1L... |\n",
      "| [0.0187967251986, 0.015622... | {'a': 2L, 'withstand': 1L,... |\n",
      "| [-0.00879068858922, 0.0582... | {'it.': 1L, 'begin': 1L, '... |\n",
      "| [-0.0686931237578, -0.0017... | {'be': 1L, 'may': 1L, 'ali... |\n",
      "| [-0.0413490571082, 0.04494... | {'power': 1L, 'ne': 1L, 'o... |\n",
      "| [-0.0343536026776, 0.00687... | {'manger': 1L, 'art': 1L, ... |\n",
      "| [0.104943454266, -0.029753... | {'do': 1L, 'no': 1L, 'i': ... |\n",
      "| [-0.0206576641649, 0.10764... | {'a': 1L, 'loss.': 1L, 'ge... |\n",
      "| [-0.0420241132379, 0.09167... | {'then': 1L, 'down': 2L, '... |\n",
      "| [-0.0316644534469, -0.0100... | {'chaplin': 1L, 'a': 1L, '... |\n",
      "| [-0.0760777816176, -0.0781... | {'angeles,': 1L, 'knightle... |\n",
      "| [-0.0254618059844, 0.02574... | {'hotel,': 1L, 'will': 1L,... |\n",
      "| [-0.0725064724684, 0.07536... | {'coach': 1L, 'po': 1L, 'm... |\n",
      "| [0.0010839626193, 0.082452... | {'and': 1L, 'at': 1L, '$nu... |\n",
      "| [-0.0302661471069, 0.01859... | {'versu': 1L, 'yes,': 1L, ... |\n",
      "| [-0.0215118303895, 0.01997... | {'preview:': 1L, 'sta...':... |\n",
      "| [0.0623091012239, 0.169810... | {'tri': 1L, 'on': 1L, 'van... |\n",
      "| [-0.036030460149, 0.156354... | {'about': 1L, 'you,': 1L, ... |\n",
      "| [-0.095754519105, 0.002532... | {'next': 1L, 'earn': 1L, '... |\n",
      "| [-0.0227274857461, 0.08512... | {'klaas-jan': 1L, 'through... |\n",
      "| [0.0473954863846, 0.012313... | {'and': 1L, 'georgia': 1L,... |\n",
      "| [-0.0252649243921, 0.05113... | {'broke?': 1L, 'be': 2L, '... |\n",
      "| [-0.109524428844, 0.000482... | {'houston': 1L, 'award.': ... |\n",
      "| [0.0120095629245, 0.146242... | {'a': 1L, 'her': 1L, 'sam'... |\n",
      "| [0.0078591927886, 0.081545... | {'we': 1L, 'ablett': 1L, '... |\n",
      "| [-0.00158019037917, 0.1079... | {'and': 1L, 'all': 1L, 'ha... |\n",
      "| [0.0407618433237, 0.049049... | {'rebecca': 1L, 'her': 1L,... |\n",
      "| [-0.0528539940715, 0.03375... | {'and': 2L, 'on': 1L, 'don... |\n",
      "| [0.00961825717241, 0.04122... | {'a': 1L, 'atl': 1L, 'po':... |\n",
      "| [-0.00796709489077, 0.1087... | {'justin': 1L, 'i': 1L, 'm... |\n",
      "| [-0.0245401337743, 0.09009... | {'(:': 1L, 'for': 1L, 'mon... |\n",
      "| [-0.0322622433305, 0.16596... | {'and': 1L, 'bestprimetime... |\n",
      "| [-0.030218699947, -0.00865... | {'and': 1L, 'king': 1L, 'l... |\n",
      "| [-0.0398146025836, 0.02341... | {'and': 1L, 'via': 1L, 'ed... |\n",
      "| [-0.0611391179264, 0.04379... | {'just': 1L, 'is': 1L, 'mi... |\n",
      "| [-0.0577972307801, 0.12319... | {'and': 1L, ':)': 1L, 'it'... |\n",
      "| [-0.0455506742001, 0.00643... | {'a': 1L, 'may': 1L, 'fami... |\n",
      "| [-0.103875026107, 0.060206... | {'a': 1L, 'neck': 1L, 'dri... |\n",
      "| [-0.0437644049525, 0.10220... | {'and': 1L, 'giant': 1L, '... |\n",
      "| [-0.0231515057385, 0.02102... | {'carri': 1L, 'sex': 1L, '... |\n",
      "| [-0.00667528575286, 0.1234... | {'on': 1L, 'in': 1L, 'befo... |\n",
      "| [-0.0309970546514, 0.05610... | {'a': 2L, 'after': 1L, 'so... |\n",
      "| [0.0082679418847, 0.199664... | {'cpfc': 1L, 'and': 1L, 'd... |\n",
      "| [-0.0127224260941, 0.06560... | {'a': 1L, 'be': 1L, 'and':... |\n",
      "| [0.0211759358644, 0.175731... | {'are': 1L, 'all': 1L, 'in... |\n",
      "| [-0.00674222270027, 0.0742... | {'and': 1L, 'exorcis': 1L,... |\n",
      "| [-0.137920841575, 0.005709... | {'then': 1L, 'nov/$num/$nu... |\n",
      "| [0.0183716453612, 0.136866... | {'a': 1L, 'feb': 1L, 'for'... |\n",
      "| [0.0150683317333, 0.084039... | {'ohh': 1L, 'ani': 1L, 'at... |\n",
      "| [-0.0475184805691, 0.04535... | {'angel': 1L, 'is': 1L, 'w... |\n",
      "| [-0.117296487093, -0.03605... | {'may': 1L, 'to': 2L, 'jus... |\n",
      "| [0.022715382278, -0.032830... | {'commercials.': 1L, 'tayl... |\n",
      "| [-0.0970534309745, 0.00108... | {'me': 1L, 'they': 1L, 'te... |\n",
      "| [-0.0339469872415, 0.09137... | {'have': 1L, 'sam': 1L, 't... |\n",
      "| [-0.0794613882899, 0.01005... | {'and': 3L, 'letter': 1L, ... |\n",
      "| [-0.0410982966423, 0.15596... | {'and': 1L, 'side.': 1L, '... |\n",
      "| [-0.0551517345011, 0.00531... | {'and': 1L, 'all': 2L, 'do... |\n",
      "| [-0.0758402869105, 0.04892... | {'coach': 1L, 'lillard,': ... |\n",
      "| [-0.0401665978134, 0.01259... | {'a': 2L, 'on': 1L, 'liver... |\n",
      "| [0.000710432534106, -0.010... | {'tim': 1L, 'urban': 1L, '... |\n",
      "| [-0.0770014971495, 0.08862... | {':(': 1L, 'in': 2L, 'see'... |\n",
      "| [-0.0637593790889, 0.06897... | {'german': 1L, 'at': 2L, '... |\n",
      "| [-0.0720506906509, -0.0004... | {'benghazi': 1L, 'and': 1L... |\n",
      "| [-0.0297263283283, 0.03991... | {'and': 1L, 'set': 1L, 'is... |\n",
      "| [-0.0113180726767, 0.12994... | {'(:': 1L, 'and': 2L, 'mat... |\n",
      "| [-0.0650722682476, -0.0142... | {'and': 1L, 'me': 2L, 'som... |\n",
      "| [-0.0515781193972, -0.0024... | {'and': 1L, 'a': 1L, 'emil... |\n",
      "| [-0.00272377580404, 0.0855... | {'lfc': 1L, 'we': 1L, 'in'... |\n",
      "| [0.02573405765, 0.13449604... | {'on': 1L, 'saturday?': 1L... |\n",
      "| [-0.00975237879902, 0.0301... | {'a': 1L, 'to': 2L, 'i': 1... |\n",
      "| [-0.0282002203166, 0.12425... | {'hqb,': 1L, 'aja,': 1L, '... |\n",
      "| [-0.0137684326619, -0.0107... | {'and': 1L, 'happy,': 1L, ... |\n",
      "| [0.0228510387242, 0.102389... | {'a': 1L, 'to': 1L, 'may':... |\n",
      "| [-0.00575702870265, 0.1047... | {'just': 1L, 'is': 1L, 'fr... |\n",
      "| [-0.0195447187871, -0.0247... | {'your': 1L, 'that': 1L, '... |\n",
      "| [-0.0418014004827, 0.07948... | {'swim': 1L, 'on': 1L, 'ur... |\n",
      "| [-0.0476715750992, 0.03757... | {'and': 1L, 'german': 1L, ... |\n",
      "| [-0.0529586710036, 0.00083... | {'awar': 1L, 'real.': 1L, ... |\n",
      "| [-0.0378320813179, 0.05519... | {'ve': 1L, 'premier': 1L, ... |\n",
      "| [-0.0013854324352, 0.01827... | {'and': 1L, 'have': 1L, 'j... |\n",
      "| [-0.0953176319599, -0.0303... | {'novemb': 1L, 'at': 1L, '... |\n",
      "| [0.0133558260277, 0.015970... | {'on': 1L, 'all': 1L, '$nu... |\n",
      "| [-0.0354859568179, 0.05802... | {'kenda': 1L, 'feel': 1L, ... |\n",
      "| [-0.0508556775749, 0.04567... | {'saturday': 1L, 'on': 1L,... |\n",
      "| [-0.0916784927249, 0.05089... | {'just': 1L, 'for': 1L, 'i... |\n",
      "| [-0.0398720130324, 0.06486... | {'and': 2L, 'factori': 1L,... |\n",
      "| [0.0171149671078, 0.090468... | {'messages.': 1L, 'tumblr'... |\n",
      "| [-0.020467499271, 0.043947... | {'the': 1L, 'c': 1L, 'six'... |\n",
      "| [-0.0523625761271, 0.01540... | {'on': 1L, 'move': 1L, 'to... |\n",
      "| [-0.0322616174817, 0.01407... | {'india': 1L, 'have': 1L, ... |\n",
      "| [-0.0751453414559, 0.10436... | {'el': 1L, 'set': 1L, 'vic... |\n",
      "| [-0.0269659049809, 0.10114... | {'all': 1L, 'wayne,': 1L, ... |\n",
      "| [-0.0686322674155, 0.00023... | {'orlean': 1L, 'about': 1L... |\n",
      "| [-0.0360095053911, 0.05238... | {'arsen': 1L, 'espn': 2L, ... |\n",
      "| [-0.00268807681277, 0.0320... | {'and': 1L, 'movi': 1L, 'o... |\n",
      "| [-0.0810153931379, 0.04812... | {'and': 1L, 'all': 1L, 'tu... |\n",
      "| [-0.0812277421355, 0.07894... | {'a': 1L, 'all': 1L, 'come... |\n",
      "| [-0.0776819884777, -0.0232... | {'choic': 1L, 'we': 1L, 'o... |\n",
      "| [-0.0406782440841, 0.10583... | {'just': 1L, 'at': 2L, 'bu... |\n",
      "| [-0.00289464159869, 0.0398... | {'hotel,': 1L, 'report:': ... |\n",
      "| [-0.0394725985825, 0.03872... | {'and': 1L, 'on': 1L, 'ful... |\n",
      "| [0.0333463773131, 0.018807... | {'there,': 1L, 'on': 1L, '... |\n",
      "| [-0.039831623435, 0.084893... | {'limp': 1L, 'problem...':... |\n",
      "| [-0.0679454728961, 0.13932... | {'aint': 1L, 'blazer': 1L,... |\n",
      "| [-0.0655845403671, 0.11541... | {'on': 1L, 'all': 1L, 'buy... |\n",
      "| [-0.0961526334286, 0.07235... | {'me': 1L, 'be': 1L, 'all'... |\n",
      "| [-0.0578277595341, 0.01591... | {'be': 2L, 'never': 1L, 'p... |\n",
      "| [0.00459300121292, -0.0160... | {'and': 1L, 'on': 1L, 'the... |\n",
      "| [-0.0684670880437, 0.02517... | {'week': 1L, '$num': 1L, '... |\n",
      "| [-0.00732926046476, -0.005... | {'and': 1L, 'do': 1L, 'dis... |\n",
      "| [-0.0389817766845, 0.06195... | {'is': 1L, 'ard': 1L, '$nu... |\n",
      "| [-0.114729657769, 0.049183... | {'ll': 1L, 'don': 1L, 'i':... |\n",
      "| [-0.00215075025335, 0.0509... | {'and': 1L, 'gari': 1L, 'o... |\n",
      "| [-0.00600191298872, 0.0198... | {'me': 1L, 'on': 1L, 'and'... |\n",
      "| [0.00017159160052, 0.04048... | {'a': 1L, 'beliv': 1L, 'th... |\n",
      "| [-0.0569411963224, 0.02358... | {'me': 1L, 'next': 1L, 'we... |\n",
      "| [0.00951693113893, 0.14907... | {'on': 1L, 'we': 1L, 'ben'... |\n",
      "| [0.0444801487029, -0.04477... | {'a': 1L, 'be': 1L, 'we': ... |\n",
      "| [-0.115469746292, 0.088646... | {'@': 1L, 'great': 1L, '(p... |\n",
      "| [-0.0261711236089, 0.02011... | {'all': 1L, 'finish': 1L, ... |\n",
      "| [-0.029386786744, 0.309227... | {'exactly,': 1L, 'buck': 1... |\n",
      "| [-0.0829619020224, 0.03310... | {'webb': 1L, 'on': 1L, 'ol... |\n",
      "| [-0.076569236815, 0.076320... | {'pat': 1L, 'haven': 1L, '... |\n",
      "| [0.00483624869958, 0.08515... | {'and': 1L, 'hook': 1L, 'a... |\n",
      "| [0.0148866856471, -0.00976... | {'over,': 1L, 'it': 2L, 's... |\n",
      "| [0.000820414163172, -0.042... | {'origin': 1L, 'on': 1L, '... |\n",
      "| [-0.00113202841021, 0.0220... | {'down.': 1L, 'osu': 1L, '... |\n",
      "| [0.0126481549814, 0.028545... | {'user': 1L, 'england': 1L... |\n",
      "| [0.0420362986624, 0.073685... | {'clipper': 1L, 'on': 1L, ... |\n",
      "| [0.0474588871002, 0.019579... | {'kingdom': 1L, 'light': 1... |\n",
      "| [-0.0350128859282, 0.03948... | {'and': 1L, 'almost': 1L, ... |\n",
      "| [-0.0810274630785, 0.02911... | {'awar': 1L, 'via': 1L, 'a... |\n",
      "| [-0.0942114442587, -0.0543... | {'may': 1L, 'is': 1L, 'nev... |\n",
      "| [-0.00909718219191, 0.1405... | {'and': 1L, 'franc': 1L, '... |\n",
      "| [0.00904993154109, -0.0157... | {'a': 1L, 'be': 1L, 'about... |\n",
      "| [-0.0656289309263, -0.0405... | {'a': 1L, '...': 1L, 'rush... |\n",
      "| [0.0516725443304, 0.126416... | {'a': 1L, 'town': 1L, 'fro... |\n",
      "| [-0.0107872812077, 0.02775... | {'don': 1L, 'know': 1L, 't... |\n",
      "| [0.0648245587945, 0.026831... | {'feel': 1L, 'everi': 1L, ... |\n",
      "| [-0.137142956257, 0.006461... | {'on': 1L, 'won': 1L, 'cla... |\n",
      "| [-0.0827430412173, 0.01553... | {'yeah': 1L, 'year': 1L, '... |\n",
      "| [-0.0170592907816, 0.10106... | {'and': 1L, 'gone': 1L, 'b... |\n",
      "| [-0.00566595001146, 0.0427... | {'thur': 1L, 'one': 1L, 'a... |\n",
      "| [-0.038894020021, -0.01591... | {'the': 1L, '$numth': 1L, ... |\n",
      "| [-0.0588425733149, 0.04710... | {'last': 1L, 'thi': 1L, 'w... |\n",
      "| [-0.0574175156653, 0.02378... | {'mile': 1L, 'on': 1L, 'co... |\n",
      "| [-0.0267918668687, 0.03153... | {'rack.': 1L, 'at': 1L, 't... |\n",
      "| [-0.010079389438, 0.118417... | {'a': 2L, 'pre': 1L, 'last... |\n",
      "| [-0.0218015350401, 0.12621... | {'and': 1L, 'plays.': 1L, ... |\n",
      "| [-0.043512839824, 0.080912... | {'el': 1L, 'ellai,': 1L, '... |\n",
      "| [0.00358266872354, 0.07227... | {'for': 1L, 'tail': 1L, 't... |\n",
      "| [-0.0454517751932, -0.0446... | {'impact': 1L, 'a': 2L, 'h... |\n",
      "| [-0.0511712990701, -0.0038... | {'it.': 1L, 'teen': 1L, 'a... |\n",
      "| [-0.0927109792829, 0.09923... | {'a': 1L, 'begin': 1L, 'ta... |\n",
      "| [-0.0182183682919, -0.0133... | {'and': 1L, 'sunday:': 1L,... |\n",
      "| [-0.0268259029835, 0.05491... | {'saturday': 1L, 'champion... |\n",
      "| [0.049923196435, 0.0776512... | {'toronto': 1L, 'ne': 3L, ... |\n",
      "| [-0.0122529082, 0.09442878... | {'central': 1L, 'po': 1L, ... |\n",
      "| [0.0535954721272, 0.116171... | {'a': 1L, 'school': 1L, 'g... |\n",
      "| [-0.0702910721302, 0.00760... | {'td.': 1L, 'w/': 1L, 'it'... |\n",
      "| [-0.0251902099699, 0.02824... | {'and': 2L, '$numth': 1L, ... |\n",
      "| [-0.0621904619038, 0.07340... | {'and': 1L, 'quarter,': 1L... |\n",
      "| [-0.00989351421595, 0.1535... | {'that,': 1L, 'yaaaaay': 1... |\n",
      "| [-0.0309773813933, 0.04326... | {'redundant?': 1L, 'georgi... |\n",
      "| [0.00169056956656, 0.22632... | {'kentucki': 1L, 'justin':... |\n",
      "| [-0.0085741430521, -0.0074... | {'is': 1L, 'second': 1L, '... |\n",
      "| [-0.0486565679312, 0.07672... | {'hqb,': 1L, 'aja,': 1L, '... |\n",
      "| [-0.024517243728, 0.117454... | {'concert': 1L, 'see': 1L,... |\n",
      "| [-0.0216413028538, 0.04135... | {'a': 1L, 'o.$num': 1L, 's... |\n",
      "| [0.00350876408629, 0.11683... | {'concord': 1L, 'trade': 1... |\n",
      "| [-0.0458659604192, 0.16208... | {'sell': 1L, 'again': 1L, ... |\n",
      "| [-0.0254260431975, 0.08634... | {'point': 1L, 'tim': 1L, '... |\n",
      "| [-0.0612135492265, 0.02896... | {'sun-dri': 1L, '[pic]:': ... |\n",
      "| [-0.0539697445929, 0.03802... | {'and': 1L, 'wiki': 1L, 'a... |\n",
      "| [0.0304600410163, 0.077553... | {'be': 1L, 'school': 1L, '... |\n",
      "| [0.0282369237393, 0.058394... | {'(honey': 1L, 'thunderup'... |\n",
      "| [-0.0297042336315, 0.03585... | {'thingy,': 1L, 'is': 1L, ... |\n",
      "| [-0.0485206581652, 0.04077... | {'just': 1L, 'is': 1L, 'it... |\n",
      "| [-0.0617710240185, 0.08511... | {'a': 1L, 'on': 1L, 'pictu... |\n",
      "| [-0.00515753310174, 0.1169... | {'user': 1L, 'play': 1L, '... |\n",
      "| [-0.0680927485228, 0.00058... | {'a': 1L, 'be': 1L, 'from'... |\n",
      "| [-0.0527199320495, 0.05272... | {'back': 1L, 'brown': 1L, ... |\n",
      "| [-0.0507997348905, 0.03885... | {'be': 1L, 'twitter': 2L, ... |\n",
      "| [-0.03993229568, 0.0649738... | {'on': 2L, 'open': 1L, 'wa... |\n",
      "| [-0.00607449468225, 0.1680... | {'on': 1L, 'tucson': 1L, '... |\n",
      "| [-0.00442097848281, 0.1036... | {'is': 1L, 'see': 1L, 'at'... |\n",
      "| [-0.0369197204709, 0.00474... | {'wife,': 1L, 'love': 1L, ... |\n",
      "| [0.0220429990441, 0.008200... | {'and': 1L, 'set': 1L, 'ba... |\n",
      "| [-0.0710274651647, 0.06901... | {'on': 1L, 'bitches.': 1L,... |\n",
      "| [-0.0189541354775, -0.0332... | {'depart': 1L, 'have': 1L,... |\n",
      "| [0.0132076665759, 0.007039... | {'and': 1L, 'foreman': 1L,... |\n",
      "| [0.0114206522703, 0.130093... | {'on': 1L, 'use': 1L, 'for... |\n",
      "| [-0.0543443411589, 0.05435... | {'a': 1L, 'underworld': 1L... |\n",
      "| [-0.0195815935731, 0.07875... | {'and': 2L, 'capit': 1L, '... |\n",
      "| [0.0466518215835, 0.033849... | {'return': 1L, 'white': 1L... |\n",
      "| [-0.0223587434739, 0.22129... | {'myself': 1L, 'from': 1L,... |\n",
      "| [-0.0160686206073, -0.0449... | {'a': 1L, 'be': 1L, 'look'... |\n",
      "| [0.0127637470141, 0.017495... | {'dc.': 1L, 'yrs.': 1L, 'r... |\n",
      "| [0.01987670362, 0.30556723... | {'a': 1L, 'from': 1L, 'tha... |\n",
      "| [0.0194169208407, 0.027389... | {'oper': 1L, 'bring': 1L, ... |\n",
      "| [-0.0123978909105, 0.08139... | {'nesav': 1L, 'last': 1L, ... |\n",
      "| [-0.0515330545604, 0.24896... | {'and': 1L, '$num': 1L, 'd... |\n",
      "| [-0.00967715028673, 0.0324... | {'a': 1L, 'week': 1L, 'cas... |\n",
      "| [-0.00528434477746, 0.0867... | {'and': 1L, 'play': 2L, 's... |\n",
      "| [-0.0502284504473, 0.02612... | {'doodl': 1L, 'odyssea': 1... |\n",
      "| [0.133304670453, 0.1443740... | {'on': 1L, 'baylor': 1L, '... |\n",
      "| [0.00563444430009, 0.11545... | {'and': 1L, 'a': 1L, 'sour... |\n",
      "| [-0.000551425619051, -0.01... | {'onli': 1L, 'would': 1L, ... |\n",
      "| [-0.0349632054567, 0.07129... | {'teen': 1L, 'onli': 1L, '... |\n",
      "| [-0.0390346720815, 0.03864... | {'the': 1L, 'over': 1L, 'm... |\n",
      "| [0.0228107515723, 0.011355... | {'a': 1L, 'cmu.': 1L, 'nam... |\n",
      "| [-0.0866406559944, -0.0193... | {'a': 1L, 'golden': 1L, 'a... |\n",
      "| [-0.0169855505228, -0.0298... | {'on': 1L, 'plu': 1L, 'fro... |\n",
      "| [-0.0155210886151, 0.01981... | {'rush': 1L, 'for': 1L, 't... |\n",
      "| [0.0342304930091, 0.057360... | {'of...': 1L, 'landfal': 1... |\n",
      "| [-0.0313067361712, 0.02271... | {'and': 1L, 'inspir': 1L, ... |\n",
      "| [-0.0212388597429, 0.06808... | {'king': 1L, 'luther': 1L,... |\n",
      "| [-0.0341915264726, 0.03507... | {'ali': 1L, 'midnight': 1L... |\n",
      "| [-0.0140592157841, 0.04382... | {'be': 1L, 'hart:': 1L, 'p... |\n",
      "| [0.0185972135514, 0.145456... | {'flames.': 1L, 'is': 1L, ... |\n",
      "| [-0.0313143394887, 0.09064... | {'be': 1L, 'death': 1L, 'i... |\n",
      "| [-0.0156375076622, 0.22772... | {'all': 1L, 'to': 1L, 'wit... |\n",
      "| [-0.0453616715968, 0.09771... | {'and': 2L, 'just': 1L, 'b... |\n",
      "| [-0.114098936319, 0.062516... | {'a': 1L, 'on': 1L, 'sunda... |\n",
      "| [0.00995584949851, 0.05910... | {'kirk': 1L, 'out': 1L, 'j... |\n",
      "| [0.0076368348673, -0.05795... | {'a': 1L, 'be': 1L, 'patri... |\n",
      "| [-0.0192018561065, 0.06434... | {'and': 1L, 'vampir': 1L, ... |\n",
      "| [-0.0309070982039, 0.15077... | {'sun:': 1L, 'newcastl': 1... |\n",
      "| [-0.0861047133803, -0.0363... | {'on': 1L, 'ing': 1L, 'for... |\n",
      "| [-0.0948735997081, 0.04678... | {'past': 1L, 'swfc': 1L, '... |\n",
      "| [-0.0338704846799, -0.0245... | {'wisconsin.': 1L, 'share'... |\n",
      "| [-0.0464321374893, 0.03293... | {'ar...': 1L, 'on': 1L, 'm... |\n",
      "| [-0.0308488830924, 0.26406... | {'and': 1L, 'again': 1L, '... |\n",
      "| [-0.0626926794648, 0.11795... | {'webb': 1L, 'loan': 1L, '... |\n",
      "| [-0.0789661258459, 0.04232... | {'google.': 1L, 'of': 2L, ... |\n",
      "| [-0.0283823292702, 0.03458... | {'then.': 1L, 'may': 1L, '... |\n",
      "| [-0.0353035144508, 0.02685... | {'be': 1L, 'wa': 1L, 'that... |\n",
      "| [0.0449386015534, 0.071598... | {'rebecca': 1L, 'be': 1L, ... |\n",
      "| [-0.00408264202997, 0.1189... | {'both': 1L, 'we': 3L, 'am... |\n",
      "| [-0.0233719106764, 0.06739... | {':)': 1L, 'it': 1L, 'an':... |\n",
      "| [-0.00935129914433, 0.0394... | {'pop': 1L, 'one': 1L, 'in... |\n",
      "| [-0.022859280929, 0.060792... | {'high': 1L, 'flight': 1L,... |\n",
      "| [-0.0914109125733, 0.01393... | {'week': 1L, 'on': 1L, 'we... |\n",
      "| [-0.0107405232266, 0.02377... | {'and': 1L, 'sander': 1L, ... |\n",
      "| [-0.0649503841996, -0.0225... | {'on': 1L, 'end': 1L, 'for... |\n",
      "| [0.0252201128751, 0.025473... | {'a': 1L, 'be': 1L, 'from'... |\n",
      "| [-0.000735358858947, 0.054... | {'and': 1L, 'w/': 1L, 'hon... |\n",
      "| [0.0143163856119, 0.056442... | {'we': 1L, 'except': 1L, '... |\n",
      "| [-0.100666791201, 0.011510... | {'and': 1L, 'on': 1L, 'pat... |\n",
      "| [0.0231269784272, 0.003909... | {'penalti': 1L, 'on': 1L, ... |\n",
      "| [0.00655092811212, 0.14751... | {'school': 2L, 'think': 1L... |\n",
      "| [-0.0484873838723, 0.01795... | {'a': 1L, 'on': 1L, 'fight... |\n",
      "| [-0.0330639854074, 0.19050... | {'don': 2L, 'concord': 1L,... |\n",
      "| [-0.0330231450498, 0.10669... | {'and': 1L, 'yes.': 1L, 'f... |\n",
      "| [-0.0474275238812, 0.08259... | {'and': 1L, 'do': 1L, 'dev... |\n",
      "| [-0.021174499765, 0.138584... | {'user': 1L, 'offici': 1L,... |\n",
      "| [0.0198563449085, 0.051626... | {'a': 1L, 'on': 2L, 'volvo... |\n",
      "| [-0.0171155352145, 0.09654... | {'we': 1L, 'see': 1L, 'i':... |\n",
      "| [-0.0510949268937, 0.08449... | {'a': 1L, 'about': 1L, 'ho... |\n",
      "| [-0.0355827994645, 0.01142... | {'a': 1L, 'york': 1L, 'i':... |\n",
      "| [-0.116797983646, -0.13423... | {'me': 1L, 'aaliyah,': 1L,... |\n",
      "| [-0.0623197928071, -0.0603... | {'a': 1L, 'ive': 1L, 'and'... |\n",
      "| [-0.00958249345422, 0.1349... | {'a': 1L, 'on': 1L, 'and':... |\n",
      "| [-0.196025088429, 0.043978... | {'on': 1L, 'nov': 1L, '$nu... |\n",
      "| [0.0118373800069, 0.056507... | {'kind': 1L, 'but': 1L, 'j... |\n",
      "| [-0.0115562686697, 0.08887... | {'and': 1L, 'dont': 1L, 'n... |\n",
      "| [-0.0752232670784, 0.01225... | {'recipi': 1L, 'be': 1L, '... |\n",
      "| [-0.0269745774567, 0.01435... | {'and': 1L, 'just': 1L, 'r... |\n",
      "| [0.00387516012415, 0.08177... | {'and': 1L, 'it': 1L, 'at'... |\n",
      "| [-0.0297654941678, 0.07982... | {'be': 1L, 'huntsman,': 1L... |\n",
      "| [-0.044661488384, 0.049501... | {'point': 1L, 'is': 1L, 'i... |\n",
      "| [-0.0136597622186, 0.05472... | {'and': 1L, 'v': 1L, 'toni... |\n",
      "| [-0.0110933529213, 0.01476... | {'skirt': 1L, 'mention$num... |\n",
      "| [-0.0145599003881, -0.0183... | {'some': 1L, 'it': 1L, 'ha... |\n",
      "| [-0.0456154383719, 0.13603... | {'upto': 1L, 'and': 1L, 'f... |\n",
      "| [-0.0269416254014, -0.0266... | {'ve': 1L, 'brown': 1L, 'n... |\n",
      "| [0.0306553766131, 0.116461... | {'one': 1L, 'a': 1L, 'po':... |\n",
      "| [-0.0311799608171, 0.14158... | {'(ankle)': 1L, 'qualifi':... |\n",
      "| [-0.0505835562944, 0.05115... | {'opposit': 1L, 'it': 1L, ... |\n",
      "| [-0.0762040913105, 0.02130... | {'be': 1L, 'think': 1L, 'i... |\n",
      "| [0.00435468787327, 0.02026... | {'feel': 1L, 'one,': 1L, '... |\n",
      "| [-0.0248619578779, 0.01412... | {'on': 1L, 'tuesday,': 1L,... |\n",
      "| [-0.0833972916007, -0.1052... | {'angeles,': 1L, 't...': 1... |\n",
      "| [-0.0434425808489, 0.00181... | {'gari': 1L, 'in': 1L, 'jo... |\n",
      "| [-0.0061211027205, 0.09508... | {'just': 1L, 'onc': 1L, 'y... |\n",
      "| [-0.116366237402, 0.034609... | {'league.': 1L, 'competit'... |\n",
      "| [-0.0345034003258, 0.10135... | {'even': 1L, 'them': 1L, '... |\n",
      "| [0.0152005273849, 0.081605... | {'a': 1L, 'the': 1L, 'geor... |\n",
      "| [-0.0149780092761, -0.0423... | {'googl': 1L, 'pictur': 1L... |\n",
      "| [0.0226081609726, -0.02806... | {'and': 1L, 'it': 1L, 'are... |\n",
      "| [-0.214918449521, 0.194837... | {'knick': 1L, 'for': 1L, '... |\n",
      "| [-0.0956999287009, 0.06316... | {'anim': 1L, 'practic': 1L... |\n",
      "| [-0.0114658316597, -0.0504... | {'onc': 1L, 'is': 1L, '$nu... |\n",
      "| [-0.0406839847565, 0.02238... | {'almost.': 1L, 'last': 1L... |\n",
      "| [-0.0857155844569, 0.15139... | {'tryna': 1L, 'we': 1L, 'a... |\n",
      "| [0.0455517023802, -0.02916... | {'taylor': 1L, 'a.j.': 1L,... |\n",
      "| [0.0317500159144, 0.065122... | {'and': 2L, 'dc.': 1L, 'au... |\n",
      "| [-0.0716507956386, -0.0007... | {'a': 1L, 'on': 1L, 'le': ... |\n",
      "| [0.021399019286, -0.026867... | {'a': 2L, 'be': 1L, 'say,'... |\n",
      "| [0.0332418344915, 0.075290... | {'want': 1L, 'to': 2L, 'i'... |\n",
      "| [0.0128495451063, 0.108539... | {':)': 1L, 'then': 1L, 've... |\n",
      "| [0.0487388186157, 0.068400... | {'and': 1L, 'shop': 1L, 'l... |\n",
      "| [-0.0758413821459, 0.03327... | {'inning': 1L, 'england': ... |\n",
      "| [-0.0720011740923, 0.02645... | {'week': 1L, '...': 1L, 'n... |\n",
      "| [0.0106208110228, 0.029886... | {'talkn': 1L, 'is': 1L, 'i... |\n",
      "| [-0.0195546615869, -0.0700... | {'dec,': 1L, 'on': 1L, 'ac... |\n",
      "| [-0.0610578469932, 0.00662... | {'awar': 1L, '-': 1L, 'off... |\n",
      "| [-0.0118078105152, 0.10410... | {'and': 1L, 'qtr': 1L, 've... |\n",
      "| [-0.138866141438, 0.010619... | {'on': 1L, 'await': 1L, 't... |\n",
      "| [-0.0552681759, -0.0514873... | {'a': 1L, 'plea': 1L, 'may... |\n",
      "| [-0.0607030652463, 0.05098... | {'on': 1L, 'don': 1L, 'for... |\n",
      "| [0.00606088992208, 0.07471... | {'feb': 1L, 'i': 1L, '-': ... |\n",
      "| [-0.035822711885, -0.00227... | {'and': 1L, 've': 1L, 'at'... |\n",
      "| [0.020852630958, 0.0401680... | {'orang': 1L, 'doesn': 1L,... |\n",
      "| [-0.014873303473, 0.028962... | {'and': 1L, 'is': 3L, 'ir'... |\n",
      "| [0.00510431453586, 0.01809... | {'and': 1L, 'paranorm': 1L... |\n",
      "| [-0.0158789940178, -0.0469... | {'and': 1L, 'hoes?': 1L, '... |\n",
      "| [-0.0513081327081, 0.00653... | {'be': 1L, 'lb': 1L, 'chri... |\n",
      "| [-0.0620290786028, 0.02695... | {'pow': 1L, 'is': 1L, 'it'... |\n",
      "| [0.0385132543743, 0.063182... | {'are': 1L, 'kind': 1L, 'u... |\n",
      "| [-0.0569033101201, 0.08558... | {'brown': 1L, 'bad-weath':... |\n",
      "| [-0.0142905060202, 0.05392... | {'on': 1L, 'play': 1L, 'ma... |\n",
      "| [0.0433083474636, 0.109648... | {'shop': 1L, 'el': 1L, 'fo... |\n",
      "| [-0.00131855637301, 0.0790... | {'onc': 1L, 'share': 1L, '... |\n",
      "| [-0.0105646150187, -0.0041... | {'definit': 1L, 'in': 1L, ... |\n",
      "| [0.0166644528508, 0.055465... | {'and': 1L, 'transfer.': 1... |\n",
      "| [0.00863910745829, 0.10177... | {'a': 1L, 'clipper': 1L, '... |\n",
      "| [-0.0902850478888, 0.05109... | {'golden': 1L, 'sorri': 1L... |\n",
      "| [-0.00238516321406, 0.0995... | {'just': 1L, 'is': 1L, 'sk... |\n",
      "| [-0.0112661318853, 0.02653... | {'all': 1L, 'angeles.': 1L... |\n",
      "| [-0.0524748153985, 0.05124... | {'angeles.': 1L, 'at': 1L,... |\n",
      "| [-0.0394056290388, 0.05822... | {'week': 1L, 'patriot': 1L... |\n",
      "| [-0.112185820937, 0.078135... | {'we': 1L, 'disast': 1L, '... |\n",
      "| [-0.0445960275829, 0.01751... | {'ve': 1L, 'concord': 1L, ... |\n",
      "| [-0.0134496800601, 0.01941... | {'webb': 1L, 'into': 1L, '... |\n",
      "| [-0.0136704491451, 0.06155... | {'season.': 1L, 'anymore.'... |\n",
      "| [-0.0176222007722, 0.12070... | {'claro': 1L, 'in': 1L, 'a... |\n",
      "| [-0.0224174447358, 0.09004... | {'i': 2L, 'm': 1L, 'wrong.... |\n",
      "| [0.0176894851029, 0.202853... | {'aj': 1L, 'pacer': 1L, 'i... |\n",
      "| [-0.0418710038066, 0.07922... | {'knick': 1L, 'gunna': 1L,... |\n",
      "| [-0.0096617359668, 0.06202... | {'a': 1L, 'be': 1L, 'rft':... |\n",
      "| [-0.0190108977258, 0.05038... | {'me': 1L, 'on': 2L, 'luth... |\n",
      "| [-0.0685985907912, -0.0442... | {'checked,': 1L, 'held': 1... |\n",
      "| [-0.000823402777314, 0.033... | {'we': 1L, 'find': 1L, 'be... |\n",
      "| [0.034594155848, 0.0572107... | {'exorc': 1L, 'just': 1L, ... |\n",
      "| [-0.0100131081417, -0.0412... | {'a': 1L, 'on': 1L, 'such'... |\n",
      "| [-0.0746499970555, 0.04852... | {'may': 1L, 'i': 1L, 'work... |\n",
      "| [-0.0295334942639, 0.07863... | {'qtr': 1L, 'mishandl': 1L... |\n",
      "| [0.00656464323401, 0.03505... | {'besi': 1L, 'old': 1L, 'i... |\n",
      "| [-0.0771802514791, -0.0205... | {'manchest': 1L, '$num:': ... |\n",
      "| [-0.0500014312565, 0.06624... | {'january,': 1L, 'liar': 1... |\n",
      "| [-0.0671772584319, 0.02221... | {'(bcs)': 1L, 'foxbusiness... |\n",
      "| [-0.0453400351107, 0.05436... | {'and': 1L, 'on': 2L, 'the... |\n",
      "| [-0.0312404315919, -0.0044... | {'day.': 1L, 'on': 1L, 'pl... |\n",
      "| [0.00618916749954, 0.00150... | {'wa': 1L, 'blew': 1L, 'li... |\n",
      "| [-0.0482003986835, 0.11059... | {'a': 1L, 'on': 1L, 'and':... |\n",
      "| [0.0271940529346, 0.117938... | {'and': 1L, 'on': 1L, 'wea... |\n",
      "| [-0.0351203046739, -0.0275... | {'and': 1L, 'wisconsin,': ... |\n",
      "| [-0.0985963493586, 0.16115... | {'wizard': 1L, 'door': 1L,... |\n",
      "| [0.0478675439954, 0.135789... | {'for': 1L, 'atl': 1L, 'un... |\n",
      "| [-0.0298958867788, -0.0692... | {'you...': 1L, 'video': 1L... |\n",
      "| [0.10226765275, -0.0617107... | {'anatomy.': 1L, 'of': 1L,... |\n",
      "| [-0.0258266143501, 0.09752... | {':)': 1L, 'is': 1L, 'it':... |\n",
      "| [-0.0689646974206, -0.0278... | {'lfc': 1L, 'ne': 1L, 'at'... |\n",
      "| [-0.02048946172, 0.0567210... | {'and': 1L, 'about': 1L, '... |\n",
      "| [0.0259206853807, 0.076615... | {'user': 1L, 'you.': 1L, '... |\n",
      "| [-0.0504638329148, 0.15943... | {'can': 1L, 'see': 1L, 'at... |\n",
      "| [0.0269594993442, 0.096896... | {'chix': 1L, 'w/': 1L, 'at... |\n",
      "| [0.00309838051908, 0.05852... | {'and': 2L, 'then': 1L, 'v... |\n",
      "| [-0.0399522073567, 0.03533... | {'on': 1L, 'play': 1L, '$n... |\n",
      "| [0.0167268868536, 0.009207... | {'a': 1L, 'it-yo': 1L, 'go... |\n",
      "| [-0.0547384247184, 0.03685... | {'hooiser': 1L, 'love': 1L... |\n",
      "| [-0.101838693023, 0.094785... | {'a': 1L, 'life': 1L, 'tow... |\n",
      "| [-0.0635076910257, 0.08789... | {'hit': 1L, 'for': 1L, 'pa... |\n",
      "| [0.0110897338018, 0.120635... | {'and': 1L, 'is': 2L, 'at'... |\n",
      "| [-0.0888650715351, 0.04343... | {'come': 1L, 'on': 1L, 're... |\n",
      "| [-0.0189441666007, 0.09637... | {'and': 1L, 'at': 1L, 'sat... |\n",
      "| [0.00861909426749, 0.06373... | {'toronto': 1L, 'tdot': 1L... |\n",
      "| [-0.0452816560864, -0.0532... | {'and': 1L, 'given': 1L, '... |\n",
      "| [0.0109553234652, 0.032212... | {'rey': 1L, 'lana': 1L, 'm... |\n",
      "| [0.00995858386159, 0.08338... | {'on': 1L, 'onli': 1L, 'fr... |\n",
      "| [0.0157063771039, -0.05208... | {':(': 1L, 'monday...': 1L... |\n",
      "| [0.0503054261208, -0.02205... | {'and': 1L, ':(': 1L, 'sch... |\n",
      "| [0.00994499213994, 0.09635... | {'a': 1L, 'georgia': 1L, '... |\n",
      "| [-0.0252406913787, 0.06823... | {'notifi': 1L, 'just': 1L,... |\n",
      "| [-0.0963904336095, 0.10276... | {'tonight': 1L, 'acoust': ... |\n",
      "| [-0.0310575868934, 0.05284... | {'play': 1L, 'be': 1L, 'gi... |\n",
      "| [-0.0194554924965, -0.0183... | {'rush': 1L, 'to': 1L, 'fo... |\n",
      "| [-0.0316593237221, 0.05253... | {'elvi': 1L, 'are': 1L, 'i... |\n",
      "| [0.013527312316, 0.0756405... | {'escobar': 1L, 'jewelry.r... |\n",
      "| [-0.0142198922113, 0.30996... | {'do': 1L, '$numth?': 1L, ... |\n",
      "| [0.0167628061026, -0.02987... | {'tree.': 1L, 'go': 1L, 's... |\n",
      "| [-0.00122711958829, 0.0425... | {'and': 1L, 'then': 1L, 'o... |\n",
      "| [-0.0405936390162, 0.05619... | {'a': 1L, 'el': 1L, 'all':... |\n",
      "| [-0.0161650869995, 0.19662... | {'cbb': 1L, 'upgrad': 1L, ... |\n",
      "| [-0.0517301186919, 0.08643... | {'webb': 1L, 'manchest': 1... |\n",
      "| [-0.0542725399137, 0.09510... | {'a': 1L, 'we': 1L, 'footb... |\n",
      "| [-0.0462206751108, 0.06536... | {'be': 1L, 'swag': 1L, 'in... |\n",
      "| [-0.0622398145497, 0.07526... | {'shine': 1L, 'ball': 1L, ... |\n",
      "| [0.0166190229356, 0.005334... | {'me': 1L, 'made': 1L, 'sa... |\n",
      "| [-0.0503425933421, 0.06668... | {'a': 1L, 'manchest': 1L, ... |\n",
      "| [0.0887439772487, 0.045558... | {'a': 1L, 'on': 1L, 'side,... |\n",
      "| [0.0237735193223, -0.00017... | {'and': 1L, 'be': 1L, 'pla... |\n",
      "| [-0.0270568206906, 0.08101... | {'tough': 1L, 'is': 1L, 's... |\n",
      "| [-0.0249012950808, -0.0838... | {'be': 1L, 'onli': 1L, 'pe... |\n",
      "| [0.0303244497627, 0.067580... | {'and': 1L, 'then': 1L, 'a... |\n",
      "| [-0.0364255160093, 0.06935... | {'and': 1L, 'am.': 1L, 'nd... |\n",
      "| [-0.00257513020188, 0.0299... | {'streak': 1L, 'on': 1L, '... |\n",
      "| [-0.0276275482029, 0.06769... | {'control': 1L, 'it,': 1L,... |\n",
      "| [-0.000544603564776, 0.029... | {'all': 1L, 're': 1L, '$nu... |\n",
      "| [-0.00348183372989, 0.0385... | {'me': 1L, 'nugget': 1L, '... |\n",
      "| [-0.0576094649732, 0.10017... | {')': 1L, 'it': 1L, 'frida... |\n",
      "| [0.0620236024261, 0.067024... | {'a': 1L, 'we': 1L, 'mayb'... |\n",
      "| [0.0492795035243, 0.137450... | {'tri': 1L, 'do': 1L, 'par... |\n",
      "| [-0.0112899532542, 0.10760... | {'some': 1L, 'it': 1L, 're... |\n",
      "| [-0.0513892881572, -0.0152... | {'friday': 1L, 'jason': 1L... |\n",
      "| [-0.0446754768491, 0.04695... | {'ad': 1L, 'just': 1L, 'it... |\n",
      "| [0.00248796935193, 0.02174... | {'case': 1L, 'a': 1L, 'hid... |\n",
      "| [0.0280894432217, 0.084975... | {'see': 1L, 'at': 2L, 'in'... |\n",
      "| [0.0146768866107, 0.094293... | {'ahead': 1L, 'short': 1L,... |\n",
      "| [-0.0555023662746, 0.12174... | {'and': 2L, 'give': 1L, 'y... |\n",
      "| [-0.0635424628854, 0.05925... | {'and': 1L, 'you.': 1L, 'b... |\n",
      "| [-0.0159813463688, -0.0110... | {'opposit': 1L, 'is': 2L, ... |\n",
      "| [-0.0316389352083, 0.01696... | {'awar': 1L, 'all': 1L, 'a... |\n",
      "| [-0.0372001454234, 0.15806... | {'wiki': 1L, 'the': 1L, 'w... |\n",
      "| [-0.0589884966612, 0.03177... | {'and': 1L, 'tv': 1L, 'of'... |\n",
      "| [-0.000836766790599, 0.018... | {'right': 1L, 'see': 1L, '... |\n",
      "| [-0.0371038280427, 0.07832... | {'mizzou': 1L, 'a': 1L, 'v... |\n",
      "| [-0.00174770480953, 0.1654... | {'me': 1L, 'on': 1L, 'and'... |\n",
      "| [-0.0261798594147, -0.0063... | {'diego': 1L, 'rush': 1L, ... |\n",
      "| [-0.0206416361034, 0.04726... | {'be': 1L, 'we': 1L, 'on':... |\n",
      "| [-0.0617061257362, 0.03580... | {'on': 1L, '...:': 1L, 'fo... |\n",
      "| [0.0244931280613, -0.01262... | {'kstate': 1L, 'onli': 1L,... |\n",
      "| [0.0161391049623, 0.134429... | {'give': 1L, 'booki': 1L, ... |\n",
      "| [-0.0414617061615, 0.05418... | {'flight': 1L, 'montreal.'... |\n",
      "| [0.00198763608932, 0.03569... | {'on': 1L, 'all': 1L, 'gia... |\n",
      "| [-0.0384904146194, 0.06550... | {'me': 1L, 'malik?:)': 1L,... |\n",
      "| [-0.0441392064095, -0.0224... | {'ago': 1L, 'on': 2L, 'wiz... |\n",
      "| [-0.118088506162, 0.066269... | {'$num:$nump': 1L, 'url': ... |\n",
      "| [-0.0615045055747, 0.00219... | {'right': 1L, 'over': 1L, ... |\n",
      "| [0.00513558741659, 0.00916... | {'lfc': 1L, 'is': 1L, 'wel... |\n",
      "| [-0.00785000715405, 0.0604... | {'and': 1L, 'is': 1L, 'see... |\n",
      "| [-0.0385639518499, 0.02944... | {'even': 1L, 'be': 1L, 'he... |\n",
      "| [-0.0192018561065, 0.06434... | {'and': 1L, 'vampir': 1L, ... |\n",
      "| [0.0201306026429, -0.03053... | {'4th': 1L, 'search': 1L, ... |\n",
      "| [-0.0371874906123, 0.10872... | {'shalwar': 1L, 'aalim': 1... |\n",
      "| [-0.0209024753422, -0.0368... | {'it.': 1L, 'and': 1L, 'we... |\n",
      "| [-0.0510071739554, 0.01843... | {'a': 1L, 'on': 1L, 'fight... |\n",
      "| [-0.0425248257816, -0.0061... | {'group': 1L, 'elizabeth':... |\n",
      "| [-0.0280409734696, -0.0315... | {'enter': 1L, 'second': 1L... |\n",
      "| [-0.0907103642821, 0.01060... | {'a': 1L, 'on': 1L, 'about... |\n",
      "| [-0.0686215162277, 0.15292... | {'a': 1L, 'tuesday': 1L, '... |\n",
      "| [-0.065206207335, 0.196448... | {'a': 1L, 'and': 1L, 'e.':... |\n",
      "| [0.0336341410875, 0.030288... | {'a': 1L, 'the': 4L, 'frid... |\n",
      "| [-0.0449182838202, -0.0204... | {'episod': 1L, 'an': 1L, '... |\n",
      "| [0.0508290119469, 0.120187... | {'and': 1L, 'beverag': 1L,... |\n",
      "| [-0.0847124382854, -0.0263... | {'even': 1L, 'and': 1L, 'm... |\n",
      "| [-0.0715316459537, 0.05447... | {'concert.': 1L, 'twitter.... |\n",
      "| [0.0468519516289, -0.04941... | {'and': 1L, 'may': 1L, 'he... |\n",
      "| [-0.0106023689732, 0.04217... | {'a': 1L, 'by': 1L, 'mid':... |\n",
      "| [-0.0310080572963, 0.09125... | {'just': 2L, 'see': 1L, 's... |\n",
      "| [-0.0153183210641, 0.04977... | {'and': 1L, '...': 1L, 're... |\n",
      "| [-0.0473811663687, 0.02396... | {'and': 1L, 'is': 2L, 'we'... |\n",
      "| [0.0493879318237, -0.03648... | {'and': 1L, 'philippines.'... |\n",
      "| [-0.0253249630332, 0.04743... | {'and': 1L, 'garden,': 1L,... |\n",
      "| [-0.0825007408857, -0.0661... | {'me': 1L, 'a': 1L, 'again... |\n",
      "| [-0.0618958547711, 0.16900... | {'me': 1L, 'over': 1L, 'to... |\n",
      "| [0.0365997776389, 0.038032... | {'$numrd': 1L, 'of': 1L, '... |\n",
      "| [-0.0548530928791, 0.10297... | {'after': 1L, 'uk': 1L, 'i... |\n",
      "| [-0.0647986829281, 0.03288... | {'vicar': 1L, 'is': 1L, 'i... |\n",
      "| [0.0247982516885, 0.056437... | {'a': 1L, 'all': 1L, 'rear... |\n",
      "| [-0.0581980757415, 0.01828... | {'all': 1L, 'is': 1L, 'dow... |\n",
      "| [-0.0665286406875, -0.0216... | {'a': 1L, 'be': 1L, 'may':... |\n",
      "| [-0.044071931392, 0.085974... | {'newcastl': 1L, 'victori'... |\n",
      "| [-0.01419270318, 0.0273250... | {'a': 1L, 'defens': 1L, 't... |\n",
      "| [-0.0590115599334, 0.00773... | {'choic': 1L, 'finish': 1L... |\n",
      "| [-0.0142370313406, 0.04137... | {'w/': 1L, 'it': 1L, 'play... |\n",
      "| [-0.00767300836742, -0.036... | {'about': 2L, 'sure': 1L, ... |\n",
      "| [-0.0310733579099, -0.0025... | {'a': 2L, 'on': 1L, 'liver... |\n",
      "| [0.0327895358205, 0.233047... | {'then': 1L, 'ummm': 1L, '... |\n",
      "| [0.0237853433937, 0.049207... | {'and': 1L, 'at': 1L, 'in'... |\n",
      "| [-0.0065059941262, 0.09939... | {'excit': 1L, ':)': 1L, 's... |\n",
      "| [0.0392005108297, 0.008393... | {'a': 1L, 'on': 1L, 'bit':... |\n",
      "| [0.0184266716242, 0.149156... | {'on': 1L, 'play': 1L, 'be... |\n",
      "| [-0.0119741344824, 9.73244... | {'and': 1L, 'be': 1L, 'poe... |\n",
      "| [-0.100087098777, 0.023424... | {'all': 1L, 'is': 2L, 'pla... |\n",
      "| [0.0201311986893, 0.090014... | {'on': 1L, 'crap': 1L, 'fo... |\n",
      "| [0.00519597530365, -0.0215... | {'a': 1L, 'gone': 1L, 'can... |\n",
      "| [-0.104454800487, 0.028766... | {'on': 1L, 'letterman,': 1... |\n",
      "| [0.000673448026646, 0.0796... | {'brazil': 1L, 'style': 1L... |\n",
      "| [-0.0382778570056, 0.08102... | {'all': 1L, 'over': 1L, 'c... |\n",
      "| [-0.129993736744, 0.084862... | {'and': 3L, 'both': 1L, '$... |\n",
      "| [0.00322541920468, 0.06812... | {'and': 1L, 'sanchez': 1L,... |\n",
      "| [0.0383285917342, 0.094892... | {'the': 1L, 'see': 1L, 'at... |\n",
      "| [-0.0568851232529, 0.03072... | {'clipper': 1L, 'and': 1L,... |\n",
      "| [0.0104088066146, 0.112462... | {'at': 1L, 'onli': 1L, 'fr... |\n",
      "| [-0.0460471026599, 0.04706... | {'madonna': 2L, 'be': 1L, ... |\n",
      "| [-0.00404243776575, 0.1212... | {'see': 1L, 'url': 1L, 'we... |\n",
      "| [-0.0284374952316, -0.0209... | {'and': 1L, 'notredam': 1L... |\n",
      "| [0.0252469088882, 0.085152... | {'cbb': 1L, '$numth': 1L, ... |\n",
      "| [-0.0525603927672, 0.08361... | {'it.': 1L, 'it,': 1L, 'fi... |\n",
      "| [0.0700938627124, -0.02113... | {'and': 1L, 'on': 1L, 'rec... |\n",
      "| [-0.0432029254735, -0.0093... | {'a': 1L, 'on': 1L, 'chels... |\n",
      "| [-0.0474475584924, 0.02899... | {'week': 1L, 'on': 1L, 'el... |\n",
      "| [-0.0378128662705, 0.02017... | {'on': 1L, 'for': 1L, 'big... |\n",
      "| [0.0044842180796, 0.040340... | {'own': 1L, 'see': 1L, 'pr... |\n",
      "| [0.0047317808494, 0.048337... | {'and': 1L, 'doesn': 2L, '... |\n",
      "| [-0.0379001051188, -0.0185... | {'all': 1L, 'liar': 1L, 'p... |\n",
      "| [-0.0293046794832, -0.0408... | {'classic': 1L, 'nhl': 1L,... |\n",
      "| [0.00467254314572, -0.0124... | {'and': 1L, 'golden': 1L, ... |\n",
      "| [0.0229380652308, 0.027954... | {'flex': 1L, 'matt': 1L, '... |\n",
      "| [-0.000159069895744, 0.058... | {'give': 1L, 'is': 1L, 'cr... |\n",
      "| [-0.0738156810403, 0.03637... | {'one': 1L, 'on': 1L, 'sta... |\n",
      "| [-0.00388072803617, 0.0441... | {'yeah': 1L, 'in': 1L, 'go... |\n",
      "| [-0.0748166888952, 0.10137... | {'is': 1L, 'at': 1L, '$num... |\n",
      "| [-0.026755547151, 0.029121... | {'the': 2L, 'least': 1L, '... |\n",
      "| [-0.0217938795686, 0.21893... | {'be': 1L, '$num?': 1L, 'c... |\n",
      "| [-0.0486565679312, 0.07672... | {'hqb,': 1L, 'aja,': 1L, '... |\n",
      "| [-0.0338211134076, -0.0108... | {'a': 1L, 'hop:': 1L, 'eli... |\n",
      "| [-0.108280621469, -0.01695... | {'golden': 1L, 'that.': 1L... |\n",
      "| [-0.0198026467115, -0.0586... | {'and': 1L, 'twitter': 1L,... |\n",
      "| [-0.0275172498077, 0.11385... | {'is': 1L, 'osu': 1L, 'cha... |\n",
      "| [0.042688138783, -0.015440... | {'and': 1L, 'use': 1L, 'pr... |\n",
      "| [-0.0952632427216, -0.0279... | {'info': 1L, 'awar': 1L, '... |\n",
      "| [-0.0607975125313, -0.0361... | {'is': 1L, '(and': 1L, 'ha... |\n",
      "| [-0.0249518249184, 0.08285... | {'sound': 1L, 'on': 1L, 'd... |\n",
      "| [-0.0630148798227, 0.05734... | {'and': 1L, 'down': 1L, 's... |\n",
      "| [-0.0737391486764, 0.13542... | {'concert': 1L, 'indonesia... |\n",
      "| [-0.0438061282039, -0.0215... | {'a': 2L, 'bless': 1L, 'al... |\n",
      "| [0.0146647207439, 0.058697... | {'downton': 1L, 'it': 1L, ... |\n",
      "| [-0.0916639864445, 0.07094... | {'deal': 2L, 'hour': 1L, '... |\n",
      "| [-0.0547083877027, 0.02723... | {'fair': 1L, 'anyway': 1L,... |\n",
      "| [-0.0456022545695, 0.12461... | {'don': 1L, 'to': 1L, 'url... |\n",
      "| [0.00516768079251, 0.20664... | {'el': 1L, 'is': 1L, 'it':... |\n",
      "| [-0.0791201740503, 0.00486... | {'on': 1L, 'url': 1L, 'tit... |\n",
      "| [-0.094783835113, 0.149562... | {'and': 1L, 'on': 1L, 'vam... |\n",
      "| [-0.123017325997, -0.02583... | {'week': 1L, 'fantasi': 1L... |\n",
      "| [-0.0786746740341, 0.03172... | {'ratings:': 1L, 'nba,': 1... |\n",
      "| [-0.0544984303415, 0.06268... | {'and': 1L, 'friend': 1L, ... |\n",
      "| [0.0271020047367, 0.032770... | {'a': 1L, 'bummer': 1L, 'w... |\n",
      "| [0.0116248037666, 0.105616... | {'town.': 1L, 'into': 1L, ... |\n",
      "| [-0.0277927443385, 0.02050... | {'and': 1L, 'a': 1L, 'all'... |\n",
      "| [-0.0296621154994, 0.06590... | {'underworld': 1L, 'for': ... |\n",
      "| [0.0113586243242, 0.043551... | {'readi': 1L, 'some': 1L, ... |\n",
      "| [-0.0116898026317, 0.24723... | {'me': 1L, 'thank': 1L, 'f... |\n",
      "| [-0.0612898468971, 0.02498... | {'testifyhisgreat': 1L, 'j... |\n",
      "| [-0.0506925396621, 0.07805... | {'footbal': 1L, 'chicago':... |\n",
      "| [0.01125488244, 0.10814599... | {'a': 1L, 'patriot': 1L, '... |\n",
      "| [-0.154468163848, 0.112157... | {'cowboy': 1L, 'i': 1L, 'i... |\n",
      "| [-0.097098313272, 0.009254... | {'patriot': 1L, 'i': 1L, '... |\n",
      "| [-0.00173147616442, -0.013... | {'a': 1L, 'thursday:': 1L,... |\n",
      "| [-0.0125352619216, 0.08372... | {'we': 1L, 'devil': 1L, 'i... |\n",
      "| [-0.0504638329148, 0.15943... | {'see': 1L, 'at': 1L, 'in'... |\n",
      "| [-0.0444347262383, 0.10428... | {'liar': 1L, 'just': 1L, '... |\n",
      "| [0.0263396687806, 0.031581... | {'discussion,': 1L, 'homes... |\n",
      "| [-0.0532385595143, 0.13435... | {'a': 1L, 'el': 1L, 'march... |\n",
      "| [-0.0528151132166, 0.11053... | {'analysi': 1L, 'on': 1L, ... |\n",
      "| [-0.0547167807817, 0.08639... | {'and': 1L, 'on': 2L, 'com... |\n",
      "| [-0.0408292077482, 0.01544... | {'on': 1L, 'last': 1L, 'po... |\n",
      "| [0.0083324322477, -0.05238... | {'dad': 1L, 'all': 1L, 'bu... |\n",
      "| [-0.0331176556647, 0.07275... | {'reproduct': 1L, 'join': ... |\n",
      "| [-0.0766057595611, 0.05978... | {'...': 1L, '$num:': 1L, '... |\n",
      "| [0.0461126938462, 0.042832... | {'and': 1L, 'about': 1L, '... |\n",
      "| [0.0254941117018, 0.057612... | {'tomorrow': 1L, 'i': 1L, ... |\n",
      "| [-0.0725430175662, 0.23079... | {'again': 1L, 'good': 1L, ... |\n",
      "| [-0.0485547147691, 0.10085... | {'and': 1L, 'on': 1L, 'abo... |\n",
      "| [-0.0348019786179, 0.07695... | {'a': 1L, 'and': 1L, 'list... |\n",
      "| [-0.0850331261754, -0.0016... | {'cfn': 1L, 'a': 1L, 'cham... |\n",
      "| [-0.0462045669556, 0.06766... | {'choic': 1L, 'gt;': 1L, '... |\n",
      "| [0.00738889491186, 0.09007... | {'wa': 2L, 'her': 1L, 'tha... |\n",
      "| [0.0166831985116, 0.031785... | {'point': 1L, 'for': 1L, '... |\n",
      "| [0.0630436465144, 0.373876... | {'ce': 1L, 'to': 1L, 'user... |\n",
      "| [-0.0726847723126, 0.03784... | {'is': 1L, 'it': 1L, 'one'... |\n",
      "| [0.0423260815442, 0.189681... | {'you.': 1L, 'look': 1L, '... |\n",
      "| [-0.0139838121831, 0.06085... | {'user': 1L, 'the': 1L, 'e... |\n",
      "| [-0.02421409823, -0.004676... | {'and': 1L, 'on': 1L, 'dif... |\n",
      "| [-0.0603634789586, 0.01733... | {'mile': 1L, 'le': 1L, '[a... |\n",
      "| [-0.0282002203166, 0.12425... | {'hqb,': 1L, 'aja,': 1L, '... |\n",
      "| [-0.0420682951808, 0.08867... | {'set': 1L, 'soon': 1L, 'o... |\n",
      "| [-0.00427719717845, 0.1278... | {'tuesday': 1L, 'sunderlan... |\n",
      "| [-0.0221665129066, 0.14626... | {'saturday': 1L, 'afrojack... |\n",
      "| [0.0219979528338, -0.01616... | {'throw.': 1L, '$num': 3L,... |\n",
      "| [0.00580156827345, 0.04530... | {'...': 1L, 'anytim': 1L, ... |\n",
      "| [-0.0180559232831, 0.01330... | {'on': 1L, 'abc': 1L, 'omg... |\n",
      "| [-0.0327971018851, 0.09119... | {'and': 1L, 'golden': 1L, ... |\n",
      "| [0.0177782438695, 0.148796... | {'a': 1L, 'and': 2L, 'mw$n... |\n",
      "| [-0.0428780093789, -0.0222... | {'then': 1L, 'sure': 1L, '... |\n",
      "| [-0.0226254947484, 0.06509... | {'me': 1L, 'on': 1L, 'and'... |\n",
      "| [0.0521953888237, 0.053865... | {'and': 1L, 'is': 1L, 'bec... |\n",
      "| [-0.0212691538036, 0.03727... | {'and': 2L, 'feel': 1L, 'i... |\n",
      "| [0.0060169599019, 0.030747... | {'fiesta': 1L, 'be': 1L, '... |\n",
      "| [-0.0171080771834, -0.0434... | {'religi': 1L, 'amend': 1L... |\n",
      "| [-0.0806336179376, 0.05207... | {'and': 1L, 'have': 1L, 't... |\n",
      "| [0.00322664598934, 0.11504... | {'and': 1L, 'all': 1L, 'xc... |\n",
      "| [-0.0199465919286, 0.01459... | {'...': 1L, 'irish': 1L, '... |\n",
      "| [-0.026869609952, 0.074562... | {'on': 1L, 'liverpool': 1L... |\n",
      "| [0.00752510875463, 0.15718... | {'el': 1L, 'gona': 1L, 'is... |\n",
      "| [-0.00677921855822, 0.1338... | {'just': 1L, 'yeah': 1L, '... |\n",
      "| [-0.00710024684668, 0.0940... | {'now.': 1L, 'good': 1L, '... |\n",
      "| [-0.054567374289, 0.006762... | {'kidrauhl': 1L, 'on': 1L,... |\n",
      "| [-0.0393457226455, -0.0941... | {'be': 2L, 'lamar': 1L, 'm... |\n",
      "| [-0.00624624779448, 0.0069... | {'ambassador': 1L, 'just':... |\n",
      "| [-0.0036826597061, -0.0501... | {'play': 1L, 'manu': 1L, '... |\n",
      "| [-0.0532266311347, -0.0151... | {'now,': 1L, 'n.j./boston,... |\n",
      "| [0.0703854188323, 0.161960... | {'anatomi': 1L, 'but': 1L,... |\n",
      "| [0.0135523453355, -0.02958... | {'sometimes,': 1L, 'choic'... |\n",
      "| [-0.0369323566556, -0.0337... | {'hi': 1L, 'career': 1L, '... |\n",
      "| [-0.0107458624989, -0.0252... | {'is': 1L, 'it': 1L, 'wow.... |\n",
      "| [-0.0963511019945, -0.0202... | {'a': 1L, 'on': 2L, 'stude... |\n",
      "| [-0.0117991631851, -0.0525... | {'and': 1L, 'be': 1L, 'eve... |\n",
      "| [0.0361617095768, -0.08349... | {'and': 1L, 'downs.': 1L, ... |\n",
      "| [-0.0564541742206, -0.0001... | {'be': 1L, 'paramount': 1L... |\n",
      "| [-0.0271239262074, 0.10369... | {'a': 1L, 'and': 2L, 'grea... |\n",
      "| [-0.0170628018677, 0.06087... | {'wa': 1L, 'tonight': 1L, ... |\n",
      "| [-0.11112511158, 0.1485261... | {'do': 1L, 'we': 1L, 'don'... |\n",
      "| [-0.0435979813337, 0.06985... | {'falcons)': 1L, 'it': 1L,... |\n",
      "| [-0.100787937641, 0.108950... | {'don': 1L, 'forget': 1L, ... |\n",
      "| [0.00502714375034, -0.0013... | {'preorder': 1L, 'pre-sold... |\n",
      "| [-0.0682948082685, 0.04297... | {'and': 1L, 'dream,': 1L, ... |\n",
      "| [0.00660694390535, 0.02181... | {'and': 1L, ':(': 1L, 'giv... |\n",
      "| [-0.00719352113083, 0.0954... | {'and': 1L, 'photo': 1L, '... |\n",
      "| [-0.0552898794413, 0.03624... | {'now,': 1L, 'tri': 1L, 'g... |\n",
      "| [0.0398758500814, 0.053529... | {'gotti': 1L, 'concert': 1... |\n",
      "| [-0.0466288551688, 0.04758... | {'@': 1L, 'from': 1L, 'hum... |\n",
      "| [-0.0488582290709, 0.06317... | {'and': 2L, 'it': 1L, 'at'... |\n",
      "| [-0.019716154784, 0.110529... | {'no,': 1L, 'a': 1L, 'play... |\n",
      "| [0.0298666860908, 0.087725... | {'be': 1L, 'iowach': 1L, '... |\n",
      "| [0.0106367282569, 0.015540... | {'be': 1L, 'it': 1L, 'stam... |\n",
      "| [-0.0155995534733, 0.23123... | {'c': 1L, 'compar': 1L, 'j... |\n",
      "| [-0.0536147020757, 0.04629... | {'a': 1L, 'head': 1L, 'you... |\n",
      "| [-0.167370811105, 0.079609... | {'gone': 1L, 'cowboy': 1L,... |\n",
      "| [-0.0612402670085, 0.04587... | {'besti': 1L, 'for': 1L, '... |\n",
      "| [-0.0368993319571, 0.16987... | {'a': 1L, 'me': 1L, 'get':... |\n",
      "| [-0.00752246240154, 0.0219... | {'and': 1L, 'anatomi': 1L,... |\n",
      "| [0.00156987912487, 0.06707... | {'heart': 1L, 'all': 1L, '... |\n",
      "| [0.0169710163027, 0.265670... | {'ami': 1L, 'do': 1L, ':('... |\n",
      "| [-0.0546191670001, -0.0285... | {'just': 1L, 'it': 1L, 'us... |\n",
      "| [-0.0436277389526, 0.07559... | {'a': 1L, 'from': 1L, 'cam... |\n",
      "| [0.000712105014827, 0.0200... | {'natur': 1L, 'some': 1L, ... |\n",
      "| [-0.0458983704448, 0.05005... | {'and': 1L, 'on': 1L, 'fro... |\n",
      "| [-0.0515365451574, 0.09801... | {'see': 1L, 'suspens': 1L,... |\n",
      "| [-0.0452983789146, -0.0088... | {'a': 1L, 'shot': 1L, 'goa... |\n",
      "| [-0.00059600174427, 0.0313... | {'becaus': 1L, 'are': 2L, ... |\n",
      "| [-0.0294942073524, -0.0243... | {'claim': 1L, 'march': 1L,... |\n",
      "| [-0.0370598956943, 0.06499... | {'me': 1L, 'play': 1L, 'no... |\n",
      "| [-0.0392938107252, 0.07163... | {'is': 1L, 'lsu': 1L, 'at'... |\n",
      "| [0.0100948931649, 0.040399... | {'@': 1L, 'tomorrow': 1L, ... |\n",
      "| [-0.0341402664781, 0.07155... | {'a': 1L, '$numth': 1L, 'r... |\n",
      "| [-0.0421022064984, -0.0261... | {'and': 1L, 'grade': 1L, '... |\n",
      "| [0.00299019366503, 0.05383... | {'and': 1L, 'on': 1L, 'sta... |\n",
      "| [0.01304128021, 0.00252963... | {'ve': 1L, 'at': 1L, 'in':... |\n",
      "| [0.0595271140337, 0.194192... | {'a': 1L, 'tripl': 1L, 'an... |\n",
      "| [0.0465187355876, -0.00703... | {'the': 1L, 'we': 1L, 'omg... |\n",
      "| [-0.00352617353201, 0.2004... | {'movi': 1L, 'of': 1L, 'wi... |\n",
      "| [-0.0385875552893, 0.03217... | {'are': 1L, 'in': 2L, 'you... |\n",
      "| [0.00236145406961, 0.16850... | {'be': 1L, 'just': 1L, 'fo... |\n",
      "| [-0.0476576089859, 0.04163... | {'the': 1L, 'to': 1L, 'may... |\n",
      "| [-0.0606384798884, 0.20824... | {'me': 1L, ':dtake': 1L, '... |\n",
      "| [-0.0108897713944, 0.09141... | {'and': 1L, 'sparkingat$nu... |\n",
      "| [-0.0754919797182, 0.07442... | {'a': 1L, 'great': 1L, 'le... |\n",
      "| [-0.037527281791, 0.064014... | {'sf.': 1L, 'good': 1L, 'a... |\n",
      "| [-0.0531123615801, 0.07653... | {'co': 1L, 'booked.': 1L, ... |\n",
      "| [0.0041641057469, 0.083770... | {'all': 1L, 'm': 1L, 'is':... |\n",
      "| [0.0741297230124, 0.070909... | {'and': 1L, 'sergei': 1L, ... |\n",
      "| [-0.125868275762, -0.08267... | {'cyber': 1L, 'ngayon': 1L... |\n",
      "| [-0.0339785031974, 0.08966... | {'a': 1L, 'lennox-gastaut'... |\n",
      "| [-0.0819241553545, 0.03219... | {'babe': 1L, 'just': 1L, '... |\n",
      "| [0.0285201556981, 0.108340... | {'be': 1L, 'tide': 1L, 'an... |\n",
      "| [0.00370031199418, 0.07444... | {'our': 1L, 'all': 1L, 'li... |\n",
      "| [-0.0540356636047, 0.04498... | {'awesome.': 1L, 'pas': 1L... |\n",
      "| [-0.10462655127, 0.0837644... | {'user': 1L, 'on': 2L, ':-... |\n",
      "| [-0.0352489724755, 0.04562... | {'on': 1L, 'set': 1L, 'app... |\n",
      "| [-0.0516516305506, 0.08102... | {'a': 1L, 'great': 1L, 'co... |\n",
      "| [-0.0972990393639, 0.11135... | {'...': 1L, 'underworld': ... |\n",
      "| [-0.0440042205155, -0.0246... | {'ago': 1L, 'on': 2L, 'cha... |\n",
      "| [-0.0247086584568, 0.31163... | {'even': 1L, 'all': 1L, 'f... |\n",
      "| [0.00250282138586, 0.03651... | {'a': 1L, 'guess': 1L, 'di... |\n",
      "| [-0.105530902743, -0.01330... | {'a': 1L, 'nigga': 1L, 'de... |\n",
      "| [-0.0623981133103, 0.04372... | {'rocket': 1L, 'today,hawk... |\n",
      "| [0.0367067046463, 0.078001... | {'ever.': 1L, 'is': 1L, 'm... |\n",
      "| [-0.0110420053825, 0.07163... | {'and': 1L, 'angel': 1L, '... |\n",
      "| [-0.0997103303671, 0.11980... | {'and': 1L, 'movi': 1L, 'a... |\n",
      "| [0.0490299500525, 0.050069... | {'and': 1L, 'all': 1L, 'tu... |\n",
      "| [-0.0793770253658, 0.13412... | {'origin': 1L, 'sat': 1L, ... |\n",
      "| [-0.00841114483774, 0.0654... | {'and': 1L, 'all': 1L, 'bo... |\n",
      "| [-0.0538837388158, 0.07297... | {'a': 1L, 'then': 1L, 'mai... |\n",
      "| [-0.0120419533923, 0.05027... | {'and': 2L, 'is': 1L, 'bac... |\n",
      "| [-0.00487822201103, 0.0647... | {'ne': 1L, 'realli': 1L, '... |\n",
      "| [-0.138730764389, 0.177305... | {'phantom': 1L, 'listen': ... |\n",
      "| [-0.0131269963458, 0.16935... | {'user': 1L, 'cute': 1L, '... |\n",
      "| [-0.00289021502249, 0.0161... | {'sound': 1L, 'actual': 1L... |\n",
      "| [-0.0317923389375, 0.05208... | {'for': 1L, 'get': 1L, 'pa... |\n",
      "| [-0.0275386553258, 0.02725... | {'up.': 1L, 'devil': 1L, '... |\n",
      "| [-0.0485580228269, -0.0463... | {'be': 1L, 'devil': 1L, 'v... |\n",
      "| [-0.101525716484, 0.059540... | {':(': 1L, 'classic': 1L, ... |\n",
      "| [-0.0960975885391, 0.19209... | {'just': 1L, 'user': 1L, '... |\n",
      "| [-0.0387542098761, 0.00929... | {'and': 1L, 'is': 1L, 'are... |\n",
      "| [-0.0250204000622, 0.08185... | {'and': 1L, 'cb': 1L, 'one... |\n",
      "| [-0.0864805281162, 0.06384... | {'ksu': 1L, 'and': 1L, 'us... |\n",
      "| [0.00160776672419, -0.0045... | {'me': 1L, 'be.': 1L, 'tha... |\n",
      "| [0.0263861324638, 0.054311... | {'set': 1L, 'it': 1L, 'at'... |\n",
      "| [-0.0343231707811, 0.08303... | {'...': 1L, 'for': 1L, 'wa... |\n",
      "| [0.0829064771533, 0.181072... | {'tomorrow.': 1L, 'her.': ... |\n",
      "| [0.0338502712548, 0.080839... | {'all': 1L, 'it': 1L, 'one... |\n",
      "| [0.0405461713672, 0.029067... | {'play': 1L, '...': 1L, 'o... |\n",
      "| [-0.0322831198573, 0.05262... | {'it': 1L, 'at': 1L, 'satu... |\n",
      "| [-0.0307890586555, 0.11241... | {'me': 1L, 'houston': 1L, ... |\n",
      "| [-0.039907682687, 0.006505... | {'about': 2L, 'school': 1L... |\n",
      "| [-0.0603586174548, -0.0185... | {'and': 1L, 'houston': 1L,... |\n",
      "| [-0.0743705406785, 0.02107... | {'then': 1L, 'school': 1L,... |\n",
      "| [-0.0664405971766, 0.05203... | {'am': 1L, 'wit': 1L, 'are... |\n",
      "| [0.0264775101095, 0.125179... | {'and': 3L, 'geaux': 1L, '... |\n",
      "| [0.0256558172405, 0.220207... | {'the': 2L, 'all': 1L, 'to... |\n",
      "| [-0.1162507236, 0.10209759... | {'listen,': 1L, 'all': 1L,... |\n",
      "| [-0.117559231818, 0.026256... | {'and': 1L, 'earli': 1L, '... |\n",
      "| [-0.0690108239651, 0.08875... | {'and': 1L, 'it': 1L, 'at'... |\n",
      "| [-0.0379595123231, 0.08986... | {'and': 1L, 'on': 1L, 'con... |\n",
      "| [-0.0528682880104, 0.12115... | {'all': 1L, 'weeknd': 1L, ... |\n",
      "| [-0.0782618299127, -0.0590... | {'be': 1L, 'gotti': 1L, 'r... |\n",
      "| [0.0254891198128, 0.085092... | {'cbb': 1L, 'is': 2L, 'say... |\n",
      "| [-0.0905310586095, -0.0011... | {'a': 1L, 'championship': ... |\n",
      "| [-0.00615442078561, 0.0126... | {'meme': 1L, 'on': 1L, 'fe... |\n",
      "| [-0.0325028263032, 0.11024... | {'all': 1L, 'too.': 1L, 'a... |\n",
      "| [-0.0202564876527, 0.05596... | {'rva': 1L, 'set': 1L, 'ha... |\n",
      "| [-0.0320738106966, 0.03717... | {'king': 1L, 'luther': 1L,... |\n",
      "| [-0.0397757291794, 0.03196... | {'and': 1L, 'luther': 1L, ... |\n",
      "| [-0.00376738910563, 0.0894... | {'anatomi': 1L, 'be': 1L, ... |\n",
      "| [-0.0376836620271, 0.03138... | {'em': 1L, 'and': 1L, 'the... |\n",
      "| [-0.0287641715258, -0.0334... | {'gt;i': 1L, 'player': 1L,... |\n",
      "| [-0.0939719900489, 0.03355... | {'and': 1L, 'eye': 1L, 'di... |\n",
      "| [-0.0335368663073, -0.0566... | {'ve': 1L, 'over': 1L, 'kn... |\n",
      "| [-0.0678060725331, 0.00934... | {'ha...': 1L, 'date': 1L, ... |\n",
      "| [-0.0755510479212, 0.01990... | {'tournament': 1L, 'four':... |\n",
      "| [0.0506073571742, 0.047461... | {'and': 2L, 'hw.': 1L, 'is... |\n",
      "| [-0.000350405403879, 0.002... | {'...': 1L, 'giant': 2L, '... |\n",
      "| [0.00539120472968, 0.06913... | {'the': 1L, 'movi': 1L, 'y... |\n",
      "| [-0.0488809533417, 0.16943... | {'i': 1L, 'mlk': 1L, 'shou... |\n",
      "| [-0.0462133958936, 0.08582... | {'excit': 1L, 'with': 1L, ... |\n",
      "| [-0.00839949212968, 0.1333... | {'be': 1L, 'ani': 1L, 'con... |\n",
      "| [-0.0198074672371, 0.06418... | {'and': 1L, 'midnight': 1L... |\n",
      "| [-0.0329628922045, 0.02292... | {'cutom': 1L, 'at': 1L, '$... |\n",
      "| [-0.0193206556141, 0.10999... | {'at': 1L, 'want': 1L, 'in... |\n",
      "| [-0.0454073511064, 0.21013... | {'saturday': 1L, ':)': 1L,... |\n",
      "| [0.0131960120052, 0.018920... | {'zumba': 1L, 'feel': 1L, ... |\n",
      "| [-0.0188775770366, 0.14411... | {'arena': 1L, '@': 2L, 'bu... |\n",
      "| [-0.020801814273, 0.030456... | {'even': 1L, 'and': 2L, 't... |\n",
      "| [0.053026072681, 0.1257151... | {'and': 1L, ':)': 1L, 'are... |\n",
      "| [-0.0636398643255, 0.16040... | {'boy': 1L, 'eli': 1L, 'be... |\n",
      "| [-0.0327904112637, -0.0070... | {'even': 1L, 'medium': 1L,... |\n",
      "| [-0.0275649353862, 0.01660... | {'just': 1L, 'pa$num': 1L,... |\n",
      "| [0.0126928268, 0.079793699... | {'and': 1L, 'poor': 1L, 'w... |\n",
      "| [-0.101505503058, 0.158162... | {'them': 1L, 'outta': 1L, ... |\n",
      "| [-0.0509973578155, 0.07095... | {'i': 1L, 'm': 1L, 'game':... |\n",
      "| [-0.104525193572, 0.124047... | {'are': 1L, 'down': 1L, 's... |\n",
      "| [-0.0424499772489, 0.08890... | {'monday': 1L, 'to': 1L, '... |\n",
      "| [-0.0386058315635, 0.12736... | {':)': 1L, 'tuesday': 1L, ... |\n",
      "| [0.023630797863, 0.0086736... | {'and': 1L, 'omg': 1L, 'wa... |\n",
      "| [-0.00459069106728, 0.1023... | {'shop': 1L, 'gone': 1L, '... |\n",
      "| [-0.0345543473959, 0.10696... | {'and': 1L, 'play': 1L, 'r... |\n",
      "| [0.0162200089544, 0.161308... | {'tomorrow..somebodi': 1L,... |\n",
      "| [-0.024665845558, 0.066228... | {'a': 1L, 'the': 1L, 'last... |\n",
      "| [0.0312065836042, 0.006840... | {'pull': 1L, 'wa': 1L, 'mo... |\n",
      "| [0.0047944500111, -0.02401... | {'er': 1L, 'tonight,': 1L,... |\n",
      "| [0.00247607612982, 0.07517... | {'and': 1L, '^lb': 1L, 'he... |\n",
      "| [-0.0228994954377, 0.07483... | {'it': 1L, 'at': 1L, 'have... |\n",
      "| [-0.0761801600456, 0.01905... | {'app...': 1L, 'court': 2L... |\n",
      "| [0.00162885338068, 0.08701... | {'result': 1L, 'will': 2L,... |\n",
      "| [-0.0708051100373, 0.13865... | {'rt': 1L, 'and': 1L, 'pat... |\n",
      "| [-0.0398091487586, 0.03381... | {'and': 2L, 'fifa$num': 1L... |\n",
      "| [-0.00657891621813, 0.0815... | {'on': 1L, 'hey,': 1L, 'lo... |\n",
      "| [-0.0786978378892, 0.00330... | {'high': 1L, 'well,': 1L, ... |\n",
      "| [-0.0377023816109, 0.04869... | {'williams..': 1L, 'art': ... |\n",
      "| [-0.10548633337, 0.0733597... | {'again.': 1L, 'url': 1L, ... |\n",
      "| [-0.00405161455274, -0.014... | {'episod': 1L, 'tweet.': 1... |\n",
      "| [-0.0589959546924, 0.00103... | {'a': 1L, 'king': 1L, 'lut... |\n",
      "| [0.0163234882057, 0.091713... | {'are': 2L, 'on': 1L, 'we'... |\n",
      "| [-0.0535659715533, 0.18609... | {'are': 1L, 'on': 1L, 'off... |\n",
      "| [-0.0737252607942, 0.06472... | {'gone': 1L, 'all': 1L, 't... |\n",
      "+-------------------------------+-------------------------------+\n",
      "+-------------------------------+----------+-------------+-------------+----------------+\n",
      "|             tfidf             | pos_neg  | pos_neutral | neg_neutral | negative_ornot |\n",
      "+-------------------------------+----------+-------------+-------------+----------------+\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'tuesday.': 5.64332450561... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'ami': 4.687813060591651,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'snc': 14.505524836106375... | positive |   positive  |   neutral   |       0        |\n",
      "| {'heart': 6.55961523749324... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'tri': 4.480173695813406,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'then': 3.615176258326801... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 2.853524621345474,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'it': 1.7193729293256672,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'elvi': 5.461002948825133... | positive |   positive  |   neutral   |       0        |\n",
      "| {'investor': 6.55961523749... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'anoth': 5.30685226899787... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'be': 1.9152243383518694,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'excit': 5.05553784071696... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'is': 1.6959343563536495,... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'$num:': 5.17332087637335... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 2.6785188248298346, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'...': 4.161719964694871,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 2.853524621345474,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'northwestern': 6.5596152... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'all': 2.8339218102565895... | negative |   positive  |   negative  |       1        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'are': 3.0480697986622216... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'it.': 4.257030144499196,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'footbal': 4.767855768265... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'oiershdjkfwl': 7.2527624... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 2.6785188248298346, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'then': 3.615176258326801... | negative |   positive  |   negative  |       1        |\n",
      "| {'week': 4.161719964694871... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       1        |\n",
      "| {'bc,': 7.252762418053187,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   positive  |   negative  |       1        |\n",
      "| {'again': 4.54471221695097... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'hqb,': 5.306852268997874... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'costume.': 7.25276241805... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'see': 2.7094676357831835... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'don': 3.563882963939251,... | positive |   positive  |   negative  |       1        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'heey': 7.252762418053187... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'counti': 7.2527624180531... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'mish': 7.252762418053187... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'schedul': 6.154150129385... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       1        |\n",
      "| {'night': 2.94869732484901... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'anyway': 5.6433245056190... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'negative.': 7.2527624180... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'elvi': 5.461002948825133... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'then': 3.615176258326801... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'town': 5.643324505619087... | positive |   positive  |   neutral   |       0        |\n",
      "| {'clipper': 5.173320876373... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'delux': 7.25276241805318... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'just': 2.657642567918597... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'be': 3.830448676703739, ... | negative |   positive  |   neutral   |       0        |\n",
      "| {'cbb': 5.643324505619087,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'1st': 6.559615237493242,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'king': 4.419549073996971... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'tiger': 4.95017732505914... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'excit': 5.05553784071696... | positive |   positive  |   neutral   |       0        |\n",
      "| {'do': 3.3209367853288616,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'dad': 6.154150129385077,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'for': 1.64696035175719, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'into': 4.419549073996971... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'from': 3.004267176003828... | positive |   positive  |   neutral   |       0        |\n",
      "| {'liar': 4.950177325059141... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'devil': 4.16171996469487... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'all': 2.8339218102565895... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | negative |   positive  |   neutral   |       0        |\n",
      "| {'stat': 5.643324505619087... | positive |   positive  |   neutral   |       0        |\n",
      "| {':)': 3.6151762583268017,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'no,': 6.154150129385077,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'are': 3.0480697986622216... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'past': 5.866468056933296... | positive |   positive  |   negative  |       0        |\n",
      "| {'a': 1.3392594124149173, ... | negative |   positive  |   neutral   |       1        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'low': 7.252762418053187,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'nba.': 7.252762418053187... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'qtr': 5.8664680569332965... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'all': 2.8339218102565895... | positive |   positive  |   neutral   |       0        |\n",
      "| {'outweigh': 6.55961523749... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'have': 2.408575331594596... | negative |   positive  |   neutral   |       1        |\n",
      "| {'taylor': 5.4610029488251... | positive |   positive  |   negative  |       0        |\n",
      "| {'for': 1.64696035175719, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'all': 2.8339218102565895... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'$num.sh': 14.50552483610... | negative |   positive  |   neutral   |       0        |\n",
      "| {'i': 1.2341692035569527, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'sound': 5.46100294882513... | positive |   positive  |   neutral   |       0        |\n",
      "| {'texan': 5.86646805693329... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'anatomi': 4.419549073996... | positive |   positive  |   neutral   |       0        |\n",
      "| {'is': 1.6959343563536495,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | negative |   positive  |   neutral   |       1        |\n",
      "| {'tri': 4.480173695813406,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'sunday?': 5.643324505619... | positive |   positive  |   neutral   |       0        |\n",
      "| {'direct': 5.4610029488251... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'chalmer': 5.643324505619... | positive |   positive  |   neutral   |       0        |\n",
      "| {'excit': 5.05553784071696... | positive |   positive  |   neutral   |       0        |\n",
      "| {'webb': 5.173320876373351... | positive |   positive  |   neutral   |       0        |\n",
      "| {'n.j./boston,': 6.5596152... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'saturday': 2.87073578337... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'reveal': 5.8664680569332... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'no.': 6.154150129385077,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'throw.': 6.5596152374932... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'it': 1.7193729293256672,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'ali': 5.306852268997874,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'is': 1.6959343563536495,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | negative |   positive  |   neutral   |       0        |\n",
      "| {'coach': 5.46100294882513... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'is': 1.6959343563536495,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'earli': 5.17332087637335... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'mike': 6.154150129385077... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'tuesday.': 5.64332450561... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'school.': 6.154150129385... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'the': 0.4900329111213081... | negative |   neutral   |   negative  |       1        |\n",
      "| {'be': 1.9152243383518694,... | negative |   neutral   |   negative  |       0        |\n",
      "| {'play': 3.141888553879876... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'the': 0.4900329111213081... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 2.6785188248298346, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'sunderland': 4.950177325... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'our...': 7.2527624180531... | positive |   positive  |   neutral   |       0        |\n",
      "| {'is': 1.6959343563536495,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'there': 3.58920077192354... | positive |   positive  |   neutral   |       0        |\n",
      "| {'investor': 6.55961523749... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'le': 4.362390660157023, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'coach': 5.46100294882513... | positive |   positive  |   neutral   |       0        |\n",
      "| {'at': 1.5794659222650347,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'tuesday.': 5.64332450561... | negative |   positive  |   negative  |       0        |\n",
      "| {'shoot': 6.55961523749324... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'tonight': 4.117268202124... | positive |   positive  |   neutral   |       0        |\n",
      "| {'cane': 7.252762418053187... | positive |   positive  |   neutral   |       1        |\n",
      "| {'on': 1.3556085504164468,... | negative |   positive  |   neutral   |       1        |\n",
      "| {'cbc': 7.252762418053187,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'georgia': 5.055537840716... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 2.853524621345474,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'mention': 5.866468056933... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'see': 2.7094676357831835... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'...': 4.161719964694871,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'of': 1.7154281510346507,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'anatomi': 4.419549073996... | positive |   positive  |   neutral   |       0        |\n",
      "| {'be': 3.830448676703739, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'user': 1.004719543544758... | positive |   positive  |   neutral   |       0        |\n",
      "| {'all': 2.8339218102565895... | positive |   positive  |   neutral   |       0        |\n",
      "| {'shop': 5.866468056933296... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'just': 2.657642567918597... | positive |   positive  |   neutral   |       0        |\n",
      "| {'be': 3.830448676703739, ... | negative |   positive  |   neutral   |       0        |\n",
      "| {'do': 3.3209367853288616,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'model': 6.55961523749324... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       1        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'an': 3.563882963939251, ... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'the': 0.4900329111213081... | positive |   positive  |   neutral   |       0        |\n",
      "| {'me': 2.8960535913635956,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 2.6785188248298346, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'into': 4.419549073996971... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'mom:': 7.252762418053187... | negative |   positive  |   neutral   |       1        |\n",
      "| {'choic': 4.68781306059165... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'we': 2.6576425679185975,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 2.853524621345474,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'webb': 5.173320876373351... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'all': 5.667843620513179,... | positive |   positive  |   negative  |       0        |\n",
      "| {'ali': 5.306852268997874,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'dwyan': 7.25276241805318... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'return': 4.8548671452548... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   positive  |   neutral   |       1        |\n",
      "| {'choic': 4.68781306059165... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'though,': 7.252762418053... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'brookfield.': 7.25276241... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'at': 0.7897329611325173,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'bodi': 7.252762418053187... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   positive  |   negative  |       1        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'concert': 4.950177325059... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'ran': 7.252762418053187,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'all': 2.8339218102565895... | positive |   positive  |   neutral   |       0        |\n",
      "| {'excit': 5.05553784071696... | positive |   positive  |   neutral   |       0        |\n",
      "| {'wiki': 5.461002948825133... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 4.017778237244752, '... | negative |   positive  |   negative  |       1        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'clipper': 5.173320876373... | positive |   positive  |   neutral   |       0        |\n",
      "| {'billion': 7.252762418053... | negative |   positive  |   negative  |       1        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'bho': 7.252762418053187,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'school': 4.0747085877052... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'el': 4.208239980329765, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'tri': 4.480173695813406,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'me': 2.8960535913635956,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'on': 2.7112171008328936,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'$numpm-$numpm.': 7.25276... | positive |   positive  |   neutral   |       0        |\n",
      "| {'yeah': 5.055537840716968... | positive |   positive  |   neutral   |       0        |\n",
      "| {'all': 2.8339218102565895... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   positive  |   neutral   |       0        |\n",
      "| {'even': 4.117268202124038... | positive |   positive  |   neutral   |       0        |\n",
      "| {'again': 4.54471221695097... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'steel': 7.25276241805318... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | negative |   positive  |   neutral   |       1        |\n",
      "| {'...': 4.161719964694871,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'be': 3.830448676703739, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'webb': 5.173320876373351... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'c': 4.687813060591651, '... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'in': 2.5681097161356434,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'ago': 5.8664680569332965... | positive |   positive  |   neutral   |       0        |\n",
      "| {'it': 1.7193729293256672,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'fuck': 5.306852268997874... | negative |   positive  |   negative  |       1        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'it': 1.7193729293256672,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'over': 4.362390660157023... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'wont': 6.154150129385077... | positive |   positive  |   neutral   |       0        |\n",
      "| {'$num': 1.868267355264098... | positive |   positive  |   neutral   |       0        |\n",
      "| {'e$num': 7.25276241805318... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'it.': 4.257030144499196,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'mucha': 7.25276241805318... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'at': 1.5794659222650347,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'addicted...i': 7.2527624... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'clipper': 5.173320876373... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'$num': 1.868267355264098... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'crunch': 7.2527624180531... | negative |   neutral   |   negative  |       0        |\n",
      "| {'load': 7.252762418053187... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'reap': 6.559615237493242... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'golden': 4.2570301444991... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'opposit': 6.154150129385... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'liar': 4.950177325059141... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'have': 4.817150663189192... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'feedback': 7.25276241805... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'for': 1.64696035175719, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'me': 2.8960535913635956,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'play': 3.141888553879876... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'ricki': 5.64332450561908... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'song': 5.306852268997874... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'end': 4.6137050884379285... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'even': 4.117268202124038... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'all': 2.8339218102565895... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 2.6785188248298346, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'durant': 7.2527624180531... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'ami': 4.687813060591651,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'is': 1.6959343563536495,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'round.': 7.2527624180531... | negative |   neutral   |   negative  |       1        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'fifa': 7.252762418053187... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'alabama': 4.613705088437... | positive |   positive  |   neutral   |       0        |\n",
      "| {'till': 4.950177325059141... | positive |   positive  |   neutral   |       0        |\n",
      "| {'el': 4.208239980329765, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'mile': 4.854867145254817... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'lamar': 5.64332450561908... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'you': 1.7762988661216768... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'for': 1.64696035175719, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'sscx': 7.252762418053187... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'becaus': 4.2082399803297... | negative |   positive  |   negative  |       1        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'chalmer': 5.643324505619... | negative |   positive  |   negative  |       1        |\n",
      "| {'album': 12.3083002587701... | positive |   positive  |   neutral   |       0        |\n",
      "| {'box': 5.8664680569332965... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 2.6785188248298346, ... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'i': 2.4683384071139054, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'back': 3.381561407145296... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'will': 2.786854299398603... | positive |   positive  |   neutral   |       0        |\n",
      "| {'aw': 6.154150129385077, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'$num-$num,': 6.154150129... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | negative |   neutral   |   negative  |       1        |\n",
      "| {'be': 1.9152243383518694,... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'be': 1.9152243383518694,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'michael': 6.154150129385... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'tonight,': 5.46100294882... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'bowl?': 6.55961523749324... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 2.6785188248298346, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'awar': 4.854867145254817... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'just': 2.657642567918597... | negative |   positive  |   neutral   |       0        |\n",
      "| {'love': 4.544712216950978... | positive |   positive  |   neutral   |       0        |\n",
      "| {'golden': 4.2570301444991... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'mirror': 7.2527624180531... | positive |   positive  |   neutral   |       0        |\n",
      "| {'nwc': 7.252762418053187,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'sound': 5.46100294882513... | positive |   positive  |   neutral   |       0        |\n",
      "| {'so': 2.821945619209874, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       1        |\n",
      "| {'be': 1.9152243383518694,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'januari': 4.074708587705... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'no,': 6.154150129385077,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'again': 4.54471221695097... | positive |   positive  |   neutral   |       0        |\n",
      "| {'sound': 5.46100294882513... | positive |   positive  |   negative  |       0        |\n",
      "| {'just': 2.657642567918597... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'even': 4.117268202124038... | negative |   positive  |   neutral   |       1        |\n",
      "| {'sunday:': 6.559615237493... | positive |   positive  |   neutral   |       0        |\n",
      "| {'kidrauhl': 5.30685226899... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'absolut': 11.28664901123... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'newcastl': 4.76785576826... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 2.6785188248298346, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'play': 3.141888553879876... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 2.853524621345474,... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'eduardo': 7.252762418053... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'it.': 4.257030144499196,... | negative |   positive  |   neutral   |       0        |\n",
      "| {'street': 6.5596152374932... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'teen': 4.950177325059141... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'awar': 4.854867145254817... | positive |   positive  |   neutral   |       0        |\n",
      "| {'respond': 6.559615237493... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'em': 6.559615237493242, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {':(': 4.480173695813406, ... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'aug.': 7.252762418053187... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'the': 0.4900329111213081... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'me': 2.8960535913635956,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'now,': 6.154150129385077... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'scutaro': 14.50552483610... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'premiers,': 7.2527624180... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'green.': 7.2527624180531... | positive |   positive  |   neutral   |       0        |\n",
      "| {'brazil': 5.4610029488251... | positive |   positive  |   neutral   |       0        |\n",
      "| {'b': 5.8664680569332965, ... | negative |   neutral   |   negative  |       1        |\n",
      "| {'lmfaoooo': 7.25276241805... | negative |   positive  |   neutral   |       0        |\n",
      "| {'start': 3.78702651525346... | positive |   positive  |   neutral   |       0        |\n",
      "| {'even': 4.117268202124038... | negative |   neutral   |   negative  |       1        |\n",
      "| {'serv': 7.252762418053187... | positive |   positive  |   neutral   |       0        |\n",
      "| {'about': 3.19231940750676... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'move': 5.055537840716968... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'tide': 5.306852268997874... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'lfc': 5.306852268997874,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 2.853524621345474,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'see': 2.7094676357831835... | positive |   positive  |   neutral   |       0        |\n",
      "| {'do': 3.3209367853288616,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'me': 2.8960535913635956,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'we': 2.6576425679185975,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'the': 0.9800658222426162... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'at': 0.7897329611325173,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 4.017778237244752, '... | positive |   positive  |   neutral   |       0        |\n",
      "| {'user': 1.004719543544758... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'school': 4.0747085877052... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'go,': 6.559615237493242,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   positive  |   neutral   |       0        |\n",
      "| {':)': 3.6151762583268017,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'even': 4.117268202124038... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'week': 4.161719964694871... | positive |   positive  |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | negative |   positive  |   negative  |       0        |\n",
      "| {'and': 2.853524621345474,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'trainer': 7.252762418053... | positive |   positive  |   neutral   |       0        |\n",
      "| {'ami': 4.687813060591651,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'me': 2.8960535913635956,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'even': 4.117268202124038... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'url': 1.6655137596529377... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'love': 4.544712216950978... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 2.853524621345474,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'closer': 7.2527624180531... | positive |   positive  |   neutral   |       0        |\n",
      "| {'it.': 4.257030144499196,... | negative |   positive  |   negative  |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'tri': 8.960347391626811,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       1        |\n",
      "| {'thebachelor': 7.25276241... | positive |   positive  |   neutral   |       0        |\n",
      "| {'onli': 3.994665880031705... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'look': 3.641844505408963... | positive |   positive  |   neutral   |       0        |\n",
      "| {'tuesday': 4.074708587705... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'//': 7.252762418053187, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'costum': 5.6433245056190... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 4.28028693201821, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'huge': 5.461002948825133... | positive |   positive  |   neutral   |       0        |\n",
      "| {'onli': 3.994665880031705... | positive |   positive  |   neutral   |       0        |\n",
      "| {'prediction:': 7.25276241... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'respond': 6.559615237493... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'grime,': 7.2527624180531... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 4.28028693201821, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'contract?': 7.2527624180... | positive |   positive  |   neutral   |       0        |\n",
      "| {'hqb,': 5.306852268997874... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'stat': 5.643324505619087... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'el': 4.208239980329765, ... | negative |   positive  |   negative  |       1        |\n",
      "| {'on': 1.3556085504164468,... | negative |   neutral   |   negative  |       1        |\n",
      "| {'me': 2.8960535913635956,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'notes..': 7.252762418053... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'(and': 6.559615237493242... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 2.853524621345474,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'holla': 7.25276241805318... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 2.853524621345474,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'drop': 5.643324505619087... | negative |   neutral   |   negative  |       0        |\n",
      "| {':)': 3.6151762583268017,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'via': 5.173320876373351,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 4.017778237244752, '... | negative |   neutral   |   negative  |       1        |\n",
      "| {'saturday': 2.87073578337... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'recently,': 7.2527624180... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'time...': 6.559615237493... | positive |   positive  |   neutral   |       0        |\n",
      "| {'...': 4.161719964694871,... | negative |   positive  |   negative  |       0        |\n",
      "| {'tomorrow': 2.03240659297... | positive |   positive  |   neutral   |       0        |\n",
      "| {'user': 1.004719543544758... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'earli': 5.17332087637335... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'el': 4.208239980329765, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'me': 2.8960535913635956,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'are': 3.0480697986622216... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'gone': 5.055537840716968... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'town': 5.643324505619087... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 2.853524621345474,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'riverside,': 7.252762418... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'it': 3.4387458586513344,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'the': 0.4900329111213081... | positive |   positive  |   neutral   |       0        |\n",
      "| {'it.': 4.257030144499196,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'week': 4.161719964694871... | positive |   positive  |   neutral   |       0        |\n",
      "| {'play': 3.141888553879876... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'championship': 5.3068522... | positive |   positive  |   neutral   |       0        |\n",
      "| {'game': 2.618033429823551... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'excit': 5.05553784071696... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'event.': 7.2527624180531... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'last': 3.563882963939251... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'move': 5.055537840716968... | positive |   neutral   |   neutral   |       1        |\n",
      "| {'(:': 5.643324505619087, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'wasn': 6.154150129385077... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'listen': 5.1733208763733... | negative |   neutral   |   negative  |       0        |\n",
      "| {'ralli': 6.55961523749324... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'we': 2.6576425679185975,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'choic': 4.68781306059165... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'i': 1.2341692035569527, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'iu': 6.154150129385077, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'me': 2.8960535913635956,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'aston': 6.55961523749324... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'everi': 4.85486714525481... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 2.7112171008328936,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'page.': 7.25276241805318... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'cbb...monday': 7.2527624... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'golden': 4.2570301444991... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 2.7112171008328936,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'...': 4.161719964694871,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'money': 5.46100294882513... | negative |   positive  |   negative  |       1        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'control': 6.154150129385... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'watch': 2.78685429939860... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'everi': 4.85486714525481... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'and': 1.426762310672737,... | negative |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'run': 4.480173695813406,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'king': 4.419549073996971... | positive |   positive  |   neutral   |       0        |\n",
      "| {'kidrauhl': 5.30685226899... | negative |   positive  |   negative  |       1        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'webb': 5.173320876373351... | positive |   positive  |   neutral   |       0        |\n",
      "| {'igot': 7.252762418053187... | positive |   positive  |   neutral   |       0        |\n",
      "| {'gahhhhden': 7.2527624180... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'feel': 4.767855768265187... | positive |   positive  |   neutral   |       0        |\n",
      "| {'to': 0.9124031143254353,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'glamour': 7.252762418053... | positive |   positive  |   neutral   |       0        |\n",
      "| {'dwight': 5.1733208763733... | positive |   positive  |   neutral   |       0        |\n",
      "| {'drunken': 7.252762418053... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'aliens,': 7.252762418053... | positive |   positive  |   neutral   |       0        |\n",
      "| {'celtic': 5.1733208763733... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'fiesta': 5.0555378407169... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 2.6785188248298346, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'play': 3.141888553879876... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'hqb,': 5.306852268997874... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'up.': 5.643324505619087,... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'again': 4.54471221695097... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'back': 6.763122814290593... | negative |   neutral   |   negative  |       1        |\n",
      "| {'justin.': 7.252762418053... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'is': 3.391868712707299, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 2.6785188248298346, ... | negative |   neutral   |   negative  |       1        |\n",
      "| {'it.': 4.257030144499196,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | negative |   neutral   |   negative  |       1        |\n",
      "| {'power': 5.64332450561908... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'manger': 7.2527624180531... | positive |   positive  |   neutral   |       0        |\n",
      "| {'do': 3.3209367853288616,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'then': 3.615176258326801... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'chaplin': 7.252762418053... | negative |   positive  |   negative  |       0        |\n",
      "| {'angeles,': 6.55961523749... | negative |   positive  |   neutral   |       1        |\n",
      "| {'hotel,': 6.1541501293850... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'coach': 5.46100294882513... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'versu': 6.55961523749324... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'preview:': 7.25276241805... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'tri': 4.480173695813406,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'about': 3.19231940750676... | positive |   positive  |   neutral   |       0        |\n",
      "| {'next': 3.956925552048858... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'klaas-jan': 7.2527624180... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'broke?': 7.2527624180531... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'houston': 5.461002948825... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'we': 2.6576425679185975,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'rebecca': 5.461002948825... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 2.853524621345474,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'justin': 4.9501773250591... | positive |   positive  |   neutral   |       0        |\n",
      "| {'(:': 5.643324505619087, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'just': 2.657642567918597... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'carri': 6.55961523749324... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 2.6785188248298346, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'cpfc': 7.252762418053187... | negative |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'are': 3.0480697986622216... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'then': 3.615176258326801... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'ohh': 7.252762418053187,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'angel': 5.86646805693329... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'may': 2.171358053068724,... | negative |   positive  |   neutral   |       0        |\n",
      "| {'commercials.': 7.2527624... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'me': 2.8960535913635956,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'have': 2.408575331594596... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 4.28028693201821, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'coach': 5.46100294882513... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 2.6785188248298346, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'tim': 4.687813060591651,... | negative |   positive  |   negative  |       1        |\n",
      "| {':(': 4.480173695813406, ... | negative |   positive  |   negative  |       1        |\n",
      "| {'german': 6.5596152374932... | positive |   positive  |   neutral   |       0        |\n",
      "| {'benghazi': 6.55961523749... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'(:': 5.643324505619087, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       1        |\n",
      "| {'lfc': 5.306852268997874,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'hqb,': 5.306852268997874... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'just': 2.657642567918597... | positive |   positive  |   neutral   |       0        |\n",
      "| {'your': 3.491562302359625... | positive |   positive  |   neutral   |       0        |\n",
      "| {'swim': 7.252762418053187... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'awar': 4.854867145254817... | positive |   positive  |   neutral   |       0        |\n",
      "| {'ve': 4.161719964694871, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   negative  |       0        |\n",
      "| {'novemb': 3.9946658800317... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'kenda': 7.25276241805318... | positive |   positive  |   neutral   |       0        |\n",
      "| {'saturday': 2.87073578337... | positive |   positive  |   neutral   |       0        |\n",
      "| {'just': 2.657642567918597... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 2.853524621345474,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'messages.': 7.2527624180... | positive |   positive  |   neutral   |       0        |\n",
      "| {'the': 0.4900329111213081... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'india': 7.25276241805318... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'el': 4.208239980329765, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'all': 2.8339218102565895... | negative |   positive  |   neutral   |       0        |\n",
      "| {'orlean': 7.2527624180531... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'arsen': 5.64332450561908... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'choic': 4.68781306059165... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'just': 2.657642567918597... | positive |   positive  |   neutral   |       0        |\n",
      "| {'hotel,': 6.1541501293850... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'there,': 5.6433245056190... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'limp': 7.252762418053187... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'aint': 7.252762418053187... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'me': 2.8960535913635956,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'be': 3.830448676703739, ... | negative |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       1        |\n",
      "| {'week': 4.161719964694871... | negative |   positive  |   negative  |       1        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'is': 1.6959343563536495,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'ll': 3.515092799769819, ... | negative |   neutral   |   negative  |       1        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'me': 2.8960535913635956,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'me': 2.8960535913635956,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'@': 4.767855768265187, '... | positive |   positive  |   neutral   |       0        |\n",
      "| {'all': 2.8339218102565895... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'exactly,': 7.25276241805... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'webb': 5.173320876373351... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'pat': 4.950177325059141,... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'over,': 7.25276241805318... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'origin': 6.1541501293850... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'down.': 6.55961523749324... | positive |   positive  |   neutral   |       0        |\n",
      "| {'user': 1.004719543544758... | positive |   positive  |   neutral   |       0        |\n",
      "| {'clipper': 5.173320876373... | positive |   positive  |   neutral   |       0        |\n",
      "| {'kingdom': 7.252762418053... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'awar': 4.854867145254817... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'may': 2.171358053068724,... | negative |   neutral   |   negative  |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | negative |   positive  |   negative  |       1        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'don': 3.563882963939251,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'feel': 4.767855768265187... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'yeah': 5.055537840716968... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'thur': 5.866468056933296... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'the': 0.4900329111213081... | positive |   neutral   |   neutral   |       1        |\n",
      "| {'last': 3.563882963939251... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'mile': 4.854867145254817... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'rack.': 7.25276241805318... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 2.6785188248298346, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'el': 4.208239980329765, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'for': 1.64696035175719, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'impact': 6.5596152374932... | positive |   positive  |   neutral   |       0        |\n",
      "| {'it.': 4.257030144499196,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | negative |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'saturday': 2.87073578337... | positive |   positive  |   neutral   |       0        |\n",
      "| {'toronto': 6.559615237493... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'central': 5.306852268997... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'td.': 7.252762418053187,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 2.853524621345474,... | negative |   positive  |   neutral   |       1        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'that,': 7.25276241805318... | positive |   positive  |   neutral   |       0        |\n",
      "| {'redundant?': 7.252762418... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'kentucki': 6.55961523749... | positive |   positive  |   neutral   |       0        |\n",
      "| {'is': 1.6959343563536495,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'hqb,': 5.306852268997874... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'concert': 4.950177325059... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'concord': 5.173320876373... | positive |   positive  |   neutral   |       0        |\n",
      "| {'sell': 6.154150129385077... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'point': 5.46100294882513... | positive |   positive  |   neutral   |       0        |\n",
      "| {'sun-dri': 7.252762418053... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | negative |   neutral   |   negative  |       1        |\n",
      "| {'(honey': 7.2527624180531... | negative |   neutral   |   negative  |       1        |\n",
      "| {'thingy,': 7.252762418053... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'just': 2.657642567918597... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'user': 1.004719543544758... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'back': 3.381561407145296... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 2.7112171008328936,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'is': 1.6959343563536495,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'wife,': 7.25276241805318... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   positive  |   neutral   |       1        |\n",
      "| {'on': 1.3556085504164468,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'depart': 7.2527624180531... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 2.853524621345474,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'return': 4.8548671452548... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'myself': 5.4610029488251... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'dc.': 6.559615237493242,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'oper': 7.252762418053187... | positive |   positive  |   neutral   |       0        |\n",
      "| {'nesav': 7.25276241805318... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'doodl': 7.25276241805318... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'onli': 3.994665880031705... | negative |   positive  |   neutral   |       0        |\n",
      "| {'teen': 4.950177325059141... | positive |   positive  |   neutral   |       0        |\n",
      "| {'the': 0.4900329111213081... | negative |   positive  |   neutral   |       1        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       1        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'rush': 5.866468056933296... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'of...': 5.86646805693329... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'king': 4.419549073996971... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'ali': 5.306852268997874,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'flames.': 7.252762418053... | positive |   positive  |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'all': 2.8339218102565895... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 2.853524621345474,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'kirk': 5.306852268997874... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'a': 1.3392594124149173, ... | negative |   positive  |   negative  |       1        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'sun:': 6.559615237493242... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'past': 5.866468056933296... | positive |   positive  |   neutral   |       0        |\n",
      "| {'wisconsin.': 7.252762418... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'ar...': 7.25276241805318... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'webb': 5.173320876373351... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'google.': 7.252762418053... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'then.': 6.55961523749324... | positive |   positive  |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'rebecca': 5.461002948825... | positive |   positive  |   neutral   |       0        |\n",
      "| {'both': 4.767855768265187... | positive |   positive  |   neutral   |       0        |\n",
      "| {':)': 3.6151762583268017,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'pop': 6.154150129385077,... | negative |   positive  |   negative  |       1        |\n",
      "| {'high': 4.544712216950978... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'week': 4.161719964694871... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | negative |   neutral   |   negative  |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   negative  |       0        |\n",
      "| {'we': 2.6576425679185975,... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'penalti': 6.559615237493... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'school': 8.1494171754104... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'don': 7.127765927878502,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'user': 1.004719543544758... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'we': 2.6576425679185975,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'me': 2.8960535913635956,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'kind': 5.643324505619087... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   positive  |   negative  |       1        |\n",
      "| {'recipi': 7.2527624180531... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'point': 5.46100294882513... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'skirt': 7.25276241805318... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'some': 3.381561407145296... | negative |   positive  |   negative  |       1        |\n",
      "| {'upto': 7.252762418053187... | positive |   positive  |   neutral   |       0        |\n",
      "| {'ve': 4.161719964694871, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'one': 3.4685727841349263... | positive |   positive  |   neutral   |       0        |\n",
      "| {'(ankle)': 7.252762418053... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'opposit': 6.154150129385... | negative |   neutral   |   negative  |       1        |\n",
      "| {'be': 1.9152243383518694,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'feel': 4.767855768265187... | positive |   positive  |   negative  |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'angeles,': 6.55961523749... | negative |   positive  |   neutral   |       0        |\n",
      "| {'gari': 5.461002948825133... | positive |   positive  |   neutral   |       0        |\n",
      "| {'just': 2.657642567918597... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'league.': 7.252762418053... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'even': 4.117268202124038... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'googl': 5.17332087637335... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'knick': 6.55961523749324... | positive |   positive  |   neutral   |       0        |\n",
      "| {'anim': 6.154150129385077... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'onc': 5.8664680569332965... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'almost.': 7.252762418053... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'tryna': 6.55961523749324... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'taylor': 5.4610029488251... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 2.853524621345474,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 2.6785188248298346, ... | negative |   positive  |   neutral   |       0        |\n",
      "| {'want': 3.402614816343129... | positive |   neutral   |   neutral   |       0        |\n",
      "| {':)': 3.6151762583268017,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'inning': 7.2527624180531... | positive |   positive  |   neutral   |       0        |\n",
      "| {'week': 4.161719964694871... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'talkn': 7.25276241805318... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'dec,': 7.252762418053187... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'awar': 4.854867145254817... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'feb': 6.154150129385077,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   positive  |   negative  |       1        |\n",
      "| {'orang': 4.95017732505914... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'pow': 7.252762418053187,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'are': 3.0480697986622216... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'brown': 5.46100294882513... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'shop': 5.866468056933296... | positive |   positive  |   neutral   |       0        |\n",
      "| {'onc': 5.8664680569332965... | positive |   positive  |   neutral   |       0        |\n",
      "| {'definit': 5.643324505619... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       1        |\n",
      "| {'golden': 4.2570301444991... | positive |   positive  |   neutral   |       0        |\n",
      "| {'just': 2.657642567918597... | positive |   positive  |   neutral   |       0        |\n",
      "| {'all': 2.8339218102565895... | positive |   positive  |   neutral   |       0        |\n",
      "| {'angeles.': 6.55961523749... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'week': 4.161719964694871... | positive |   positive  |   neutral   |       0        |\n",
      "| {'we': 2.6576425679185975,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'ve': 4.161719964694871, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'webb': 5.173320876373351... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'season.': 5.643324505619... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'claro': 7.25276241805318... | positive |   positive  |   neutral   |       0        |\n",
      "| {'i': 2.4683384071139054, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'aj': 7.252762418053187, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'knick': 6.55961523749324... | negative |   neutral   |   negative  |       1        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'me': 2.8960535913635956,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'checked,': 7.25276241805... | positive |   positive  |   neutral   |       0        |\n",
      "| {'we': 2.6576425679185975,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'exorc': 7.25276241805318... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'may': 2.171358053068724,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'qtr': 5.8664680569332965... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'besi': 7.252762418053187... | positive |   positive  |   neutral   |       0        |\n",
      "| {'manchest': 5.05553784071... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'january,': 6.15415012938... | positive |   positive  |   neutral   |       0        |\n",
      "| {'(bcs)': 6.55961523749324... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'day.': 5.173320876373351... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'wa': 2.7984151217996796,... | negative |   positive  |   neutral   |       1        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'wizard': 4.9501773250591... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'for': 1.64696035175719, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'you...': 6.5596152374932... | positive |   positive  |   neutral   |       0        |\n",
      "| {'anatomy.': 7.25276241805... | positive |   neutral   |   neutral   |       0        |\n",
      "| {':)': 3.6151762583268017,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'lfc': 5.306852268997874,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'user': 1.004719543544758... | positive |   positive  |   neutral   |       0        |\n",
      "| {'can': 2.58932332394112, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'chix': 7.252762418053187... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 2.853524621345474,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'hooiser': 7.252762418053... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'hit': 5.461002948825133,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'come': 3.033254712877081... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'toronto': 6.559615237493... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'rey': 5.173320876373351,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   positive  |   neutral   |       0        |\n",
      "| {':(': 4.480173695813406, ... | negative |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'notifi': 7.2527624180531... | positive |   positive  |   neutral   |       0        |\n",
      "| {'tonight': 4.117268202124... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'play': 3.141888553879876... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'rush': 5.866468056933296... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'elvi': 5.461002948825133... | positive |   positive  |   neutral   |       0        |\n",
      "| {'escobar': 7.252762418053... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'do': 3.3209367853288616,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'tree.': 6.15415012938507... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | negative |   neutral   |   negative  |       1        |\n",
      "| {'cbb': 5.643324505619087,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'webb': 5.173320876373351... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'shine': 6.55961523749324... | positive |   positive  |   neutral   |       0        |\n",
      "| {'me': 2.8960535913635956,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   positive  |   neutral   |       0        |\n",
      "| {'tough': 7.25276241805318... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | negative |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   positive  |   neutral   |       1        |\n",
      "| {'streak': 6.5596152374932... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'control': 6.154150129385... | positive |   positive  |   neutral   |       0        |\n",
      "| {'all': 2.8339218102565895... | positive |   positive  |   neutral   |       0        |\n",
      "| {'me': 2.8960535913635956,... | positive |   positive  |   negative  |       0        |\n",
      "| {')': 5.8664680569332965, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'tri': 4.480173695813406,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'some': 3.381561407145296... | positive |   positive  |   neutral   |       0        |\n",
      "| {'friday': 3.2097111502186... | negative |   positive  |   neutral   |       0        |\n",
      "| {'ad': 7.252762418053187, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'case': 6.154150129385077... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'see': 2.7094676357831835... | positive |   positive  |   neutral   |       0        |\n",
      "| {'ahead': 5.64332450561908... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 2.853524621345474,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'opposit': 6.154150129385... | negative |   positive  |   negative  |       1        |\n",
      "| {'awar': 4.854867145254817... | positive |   positive  |   neutral   |       0        |\n",
      "| {'wiki': 5.461002948825133... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'right': 5.86646805693329... | positive |   positive  |   neutral   |       0        |\n",
      "| {'mizzou': 7.2527624180531... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'me': 2.8960535913635956,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'diego': 7.25276241805318... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'kstate': 7.2527624180531... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'give': 4.480173695813406... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'flight': 5.8664680569332... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | negative |   neutral   |   negative  |       1        |\n",
      "| {'me': 2.8960535913635956,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'ago': 5.8664680569332965... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'$num:$nump': 7.252762418... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'right': 5.86646805693329... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'lfc': 5.306852268997874,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'even': 4.117268202124038... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'4th': 6.154150129385077,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'shalwar': 7.252762418053... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'it.': 4.257030144499196,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'group': 6.15415012938507... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'enter': 6.55961523749324... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'episod': 4.8548671452548... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'even': 4.117268202124038... | positive |   positive  |   neutral   |       0        |\n",
      "| {'concert.': 6.15415012938... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   positive  |   negative  |       1        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'just': 5.315285135837195... | positive |   positive  |   negative  |       1        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'me': 2.8960535913635956,... | negative |   positive  |   negative  |       1        |\n",
      "| {'me': 2.8960535913635956,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'$numrd': 3.4026148163431... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'after': 3.66924347959707... | positive |   positive  |   neutral   |       0        |\n",
      "| {'vicar': 7.25276241805318... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'all': 2.8339218102565895... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'newcastl': 4.76785576826... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'choic': 4.68781306059165... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'w/': 4.419549073996971, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'about': 6.38463881501353... | negative |   neutral   |   negative  |       1        |\n",
      "| {'a': 2.6785188248298346, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'then': 3.615176258326801... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   positive  |   negative  |       1        |\n",
      "| {'excit': 5.05553784071696... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | negative |   neutral   |   negative  |       1        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'all': 2.8339218102565895... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'brazil': 5.4610029488251... | positive |   positive  |   neutral   |       0        |\n",
      "| {'all': 2.8339218102565895... | negative |   positive  |   neutral   |       0        |\n",
      "| {'and': 4.28028693201821, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'the': 0.4900329111213081... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'clipper': 5.173320876373... | positive |   positive  |   neutral   |       0        |\n",
      "| {'at': 0.7897329611325173,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'madonna': 10.34664175274... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'see': 2.7094676357831835... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'cbb': 5.643324505619087,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'it.': 4.257030144499196,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'week': 4.161719964694871... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'own': 6.559615237493242,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'all': 2.8339218102565895... | positive |   positive  |   neutral   |       0        |\n",
      "| {'classic': 5.643324505619... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'flex': 7.252762418053187... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'give': 4.480173695813406... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'one': 3.4685727841349263... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'yeah': 5.055537840716968... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'is': 1.6959343563536495,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'the': 0.9800658222426162... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'hqb,': 5.306852268997874... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'golden': 4.2570301444991... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'is': 1.6959343563536495,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'info': 7.252762418053187... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'is': 1.6959343563536495,... | negative |   positive  |   neutral   |       1        |\n",
      "| {'sound': 5.46100294882513... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'concert': 4.950177325059... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 2.6785188248298346, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'downton': 5.306852268997... | negative |   positive  |   neutral   |       1        |\n",
      "| {'deal': 12.30830025877015... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'fair': 7.252762418053187... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'don': 3.563882963939251,... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'el': 4.208239980329765, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'week': 4.161719964694871... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'ratings:': 6.55961523749... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   negative  |       1        |\n",
      "| {'town.': 6.55961523749324... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'underworld': 5.866468056... | positive |   positive  |   neutral   |       0        |\n",
      "| {'readi': 6.55961523749324... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'me': 2.8960535913635956,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'testifyhisgreat': 7.2527... | positive |   positive  |   neutral   |       0        |\n",
      "| {'footbal': 4.767855768265... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'cowboy': 5.1733208763733... | positive |   positive  |   neutral   |       0        |\n",
      "| {'patriot': 4.613705088437... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'we': 2.6576425679185975,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'see': 2.7094676357831835... | positive |   positive  |   neutral   |       0        |\n",
      "| {'liar': 4.950177325059141... | positive |   positive  |   neutral   |       0        |\n",
      "| {'discussion,': 7.25276241... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'analysi': 7.252762418053... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'dad': 6.154150129385077,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'reproduct': 7.2527624180... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'...': 4.161719964694871,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'tomorrow': 2.03240659297... | positive |   positive  |   neutral   |       0        |\n",
      "| {'again': 4.54471221695097... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'cfn': 7.252762418053187,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'choic': 4.68781306059165... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'wa': 5.596830243599359, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'point': 5.46100294882513... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'ce': 5.461002948825133, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'is': 1.6959343563536495,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'you.': 5.173320876373351... | positive |   positive  |   neutral   |       0        |\n",
      "| {'user': 1.004719543544758... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'mile': 4.854867145254817... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'hqb,': 5.306852268997874... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'set': 4.767855768265187,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'tuesday': 4.074708587705... | positive |   positive  |   neutral   |       0        |\n",
      "| {'saturday': 2.87073578337... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'throw.': 6.5596152374932... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'...': 4.161719964694871,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'then': 3.615176258326801... | positive |   positive  |   neutral   |       0        |\n",
      "| {'me': 2.8960535913635956,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 2.853524621345474,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'fiesta': 5.0555378407169... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'religi': 7.2527624180531... | negative |   neutral   |   negative  |       1        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'...': 4.161719964694871,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'el': 4.208239980329765, ... | negative |   positive  |   neutral   |       0        |\n",
      "| {'just': 2.657642567918597... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'now.': 5.461002948825133... | positive |   positive  |   neutral   |       0        |\n",
      "| {'kidrauhl': 5.30685226899... | positive |   positive  |   neutral   |       1        |\n",
      "| {'be': 3.830448676703739, ... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'ambassador': 7.252762418... | positive |   positive  |   neutral   |       0        |\n",
      "| {'play': 3.141888553879876... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'now,': 6.154150129385077... | positive |   positive  |   neutral   |       0        |\n",
      "| {'anatomi': 4.419549073996... | positive |   positive  |   neutral   |       0        |\n",
      "| {'sometimes,': 7.252762418... | negative |   neutral   |   negative  |       1        |\n",
      "| {'hi': 3.360942119942561, ... | negative |   positive  |   negative  |       1        |\n",
      "| {'is': 1.6959343563536495,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'be': 1.9152243383518694,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'wa': 2.7984151217996796,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'do': 3.3209367853288616,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'falcons)': 7.25276241805... | positive |   positive  |   neutral   |       0        |\n",
      "| {'don': 3.563882963939251,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'preorder': 7.25276241805... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'now,': 6.154150129385077... | positive |   positive  |   neutral   |       0        |\n",
      "| {'gotti': 5.30685226899787... | positive |   positive  |   neutral   |       0        |\n",
      "| {'@': 4.767855768265187, '... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 2.853524621345474,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'no,': 6.154150129385077,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'c': 4.687813060591651, '... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'gone': 5.055537840716968... | positive |   positive  |   neutral   |       0        |\n",
      "| {'besti': 7.25276241805318... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'heart': 6.55961523749324... | positive |   positive  |   neutral   |       0        |\n",
      "| {'ami': 4.687813060591651,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'just': 2.657642567918597... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'natur': 6.55961523749324... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'see': 2.7094676357831835... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'becaus': 4.2082399803297... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'claim': 5.86646805693329... | negative |   neutral   |   negative  |       1        |\n",
      "| {'me': 2.8960535913635956,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'is': 1.6959343563536495,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'@': 4.767855768265187, '... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   positive  |   negative  |       1        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'ve': 4.161719964694871, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'the': 0.4900329111213081... | positive |   positive  |   neutral   |       0        |\n",
      "| {'movi': 4.480173695813406... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'are': 3.0480697986622216... | positive |   positive  |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'the': 0.4900329111213081... | positive |   positive  |   neutral   |       0        |\n",
      "| {'me': 2.8960535913635956,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'sf.': 7.252762418053187,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'co': 6.559615237493242, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'all': 2.8339218102565895... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'cyber': 7.25276241805318... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'babe': 7.252762418053187... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'be': 1.9152243383518694,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'our': 3.9205579078779835... | positive |   positive  |   neutral   |       0        |\n",
      "| {'awesome.': 7.25276241805... | positive |   positive  |   neutral   |       0        |\n",
      "| {'user': 1.004719543544758... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'...': 4.161719964694871,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'ago': 5.8664680569332965... | positive |   positive  |   neutral   |       0        |\n",
      "| {'even': 4.117268202124038... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'rocket': 6.5596152374932... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'ever.': 5.86646805693329... | positive |   positive  |   negative  |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'origin': 6.1541501293850... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 2.853524621345474,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'ne': 1.5863357299407552,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'phantom': 7.252762418053... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'user': 1.004719543544758... | positive |   positive  |   neutral   |       0        |\n",
      "| {'sound': 5.46100294882513... | negative |   positive  |   negative  |       1        |\n",
      "| {'for': 1.64696035175719, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'up.': 5.643324505619087,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | negative |   neutral   |   negative  |       1        |\n",
      "| {':(': 4.480173695813406, ... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'just': 2.657642567918597... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   positive  |   negative  |       1        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'ksu': 7.252762418053187,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'me': 2.8960535913635956,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'set': 4.767855768265187,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'...': 4.161719964694871,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'tomorrow.': 3.5892007719... | positive |   positive  |   neutral   |       0        |\n",
      "| {'all': 2.8339218102565895... | positive |   positive  |   negative  |       0        |\n",
      "| {'play': 3.141888553879876... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'it': 1.7193729293256672,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'me': 2.8960535913635956,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'about': 6.38463881501353... | negative |   neutral   |   negative  |       1        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'then': 3.615176258326801... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'am': 4.544712216950978, ... | negative |   positive  |   neutral   |       0        |\n",
      "| {'and': 4.28028693201821, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'the': 0.9800658222426162... | positive |   positive  |   neutral   |       0        |\n",
      "| {'listen,': 7.252762418053... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'all': 2.8339218102565895... | positive |   positive  |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | negative |   positive  |   neutral   |       1        |\n",
      "| {'cbb': 5.643324505619087,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'meme': 7.252762418053187... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'all': 2.8339218102565895... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'rva': 6.559615237493242,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'king': 4.419549073996971... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'anatomi': 4.419549073996... | positive |   positive  |   neutral   |       0        |\n",
      "| {'em': 6.559615237493242, ... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'gt;i': 7.252762418053187... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'ve': 4.161719964694871, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'ha...': 6.55961523749324... | negative |   positive  |   neutral   |       0        |\n",
      "| {'tournament': 7.252762418... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 2.853524621345474,... | negative |   positive  |   neutral   |       0        |\n",
      "| {'...': 4.161719964694871,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'the': 0.4900329111213081... | positive |   positive  |   neutral   |       0        |\n",
      "| {'i': 1.2341692035569527, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'excit': 5.05553784071696... | positive |   positive  |   neutral   |       0        |\n",
      "| {'be': 1.9152243383518694,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'cutom': 7.25276241805318... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'at': 0.7897329611325173,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'saturday': 2.87073578337... | positive |   positive  |   neutral   |       0        |\n",
      "| {'zumba': 5.30685226899787... | positive |   positive  |   neutral   |       0        |\n",
      "| {'arena': 6.55961523749324... | positive |   positive  |   neutral   |       0        |\n",
      "| {'even': 4.117268202124038... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'boy': 5.055537840716968,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'even': 4.117268202124038... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'just': 2.657642567918597... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   positive  |   neutral   |       1        |\n",
      "| {'them': 4.687813060591651... | negative |   positive  |   neutral   |       0        |\n",
      "| {'i': 1.2341692035569527, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'are': 3.0480697986622216... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'monday': 3.1752249741474... | positive |   neutral   |   neutral   |       0        |\n",
      "| {':)': 3.6151762583268017,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'shop': 5.866468056933296... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'tomorrow..somebodi': 7.2... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | negative |   positive  |   neutral   |       1        |\n",
      "| {'pull': 6.154150129385077... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'er': 6.559615237493242, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.426762310672737,... | negative |   neutral   |   negative  |       1        |\n",
      "| {'it': 1.7193729293256672,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'app...': 7.2527624180531... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'result': 6.1541501293850... | positive |   positive  |   neutral   |       0        |\n",
      "| {'rt': 6.559615237493242, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 2.853524621345474,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.3556085504164468,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'high': 4.544712216950978... | positive |   positive  |   neutral   |       0        |\n",
      "| {'williams..': 7.252762418... | positive |   positive  |   neutral   |       0        |\n",
      "| {'again.': 5.8664680569332... | positive |   positive  |   neutral   |       0        |\n",
      "| {'episod': 4.8548671452548... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3392594124149173, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'are': 6.096139597324443,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'are': 3.0480697986622216... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'gone': 5.055537840716968... | positive |   positive  |   neutral   |       0        |\n",
      "+-------------------------------+----------+-------------+-------------+----------------+\n",
      "[1412 rows x 18 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_tweets.print_rows(num_rows=1400, num_columns=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_tweets['pos_neg_neutral'] = model_pos_neg_neutral.predict(test_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##End of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tweets2['word_count'] = graphlab.text_analytics.count_words(test_tweets2['Tweet'])\n",
    "tfidf = graphlab.text_analytics.tf_idf(test_tweets2['word_count'])\n",
    "test_tweets2['tfidf'] = tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_tweets2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-b70a4ec2eb7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#test_tweets2['neg_ornot'] = model_negative_ornot.predict(test_tweets2['Tweet','vectors_neg_ornot'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_tweets2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_rows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_tweets2' is not defined"
     ]
    }
   ],
   "source": [
    "#test_tweets2['neg_ornot'] = model_negative_ornot.predict(test_tweets2['Tweet','vectors_neg_ornot'])\n",
    "test_tweets2.print_rows(num_rows=1000, num_columns=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('C:\\\\Users\\OmarAbdelwahab\\\\Documents\\\\RESEARCH\\\\SEMEVAL2016\\\\TwitterSentimentAnalysisTask\\\\Train-Trial-Data\\\\2016Data\\\\Train+dev-2016\\\\Needed\\\\Train\\\\clf.pickle', 'rb')\n",
    "classifier_word2vec = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Creating a validation set from 5 percent of training data. This may take a while.\n",
      "          You can set ``validation_set=None`` to disable validation tracking.\n",
      "\n",
      "PROGRESS: The following methods are available for this type of problem.\n",
      "PROGRESS: LogisticClassifier, SVMClassifier\n",
      "PROGRESS: The returned model will be chosen according to validation accuracy.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 5155\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 157648\n",
      "PROGRESS: Number of coefficients    : 157649\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy | Validation-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | 1         | 6        | 0.000012  | 0.685459     | 0.757517          | 0.712177            |\n",
      "PROGRESS: | 2         | 8        | 1.000000  | 0.990663     | 0.843647          | 0.715867            |\n",
      "PROGRESS: | 3         | 9        | 1.000000  | 1.183790     | 0.998448          | 0.811808            |\n",
      "PROGRESS: | 4         | 10       | 1.000000  | 1.374920     | 0.998060          | 0.778598            |\n",
      "PROGRESS: | 5         | 11       | 1.000000  | 1.565048     | 0.997866          | 0.760148            |\n",
      "PROGRESS: | 6         | 12       | 1.000000  | 1.748167     | 0.999418          | 0.752768            |\n",
      "PROGRESS: | 10        | 16       | 1.000000  | 2.532690     | 0.999418          | 0.752768            |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: SVM:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 5155\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 6\n",
      "PROGRESS: Number of unpacked features : 157648\n",
      "PROGRESS: Number of coefficients    : 157649\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy | Validation-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | 1         | 7        | 0.000004  | 0.623418     | 0.757517          | 0.712177            |\n",
      "PROGRESS: | 2         | 11       | 3.000000  | 1.091727     | 0.981571          | 0.771218            |\n",
      "PROGRESS: | 3         | 12       | 3.000000  | 1.262841     | 0.537536          | 0.309963            |\n",
      "PROGRESS: | 4         | 14       | 1.000000  | 1.530021     | 0.999030          | 0.804428            |\n",
      "PROGRESS: | 5         | 15       | 1.000000  | 1.708140     | 0.999224          | 0.797048            |\n",
      "PROGRESS: | 6         | 16       | 1.000000  | 1.889259     | 0.999418          | 0.793358            |\n",
      "PROGRESS: | 10        | 20       | 1.000000  | 2.611741     | 0.999612          | 0.793358            |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Model selection based on validation accuracy:\n",
      "PROGRESS: ---------------------------------------------\n",
      "PROGRESS: LogisticClassifier              : 0.752768\n",
      "PROGRESS: SVMClassifier                   : 0.793358\n",
      "PROGRESS: ---------------------------------------------\n",
      "PROGRESS: Selecting SVMClassifier based on validation set performance.\n"
     ]
    }
   ],
   "source": [
    "feature_set_pos_neg_neutral = ['tfidf','1gram features','2gram features','3gram features','vectors_neg_ornot','vectors_pos_neg']\n",
    "svm_model = graphlab.classifier.create(train_data_pos_neg, target='Sentiment',\n",
    "                                 features=feature_set_pos_neg_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8344774980930587, 'confusion_matrix': Columns:\n",
       " \ttarget_label\tstr\n",
       " \tpredicted_label\tstr\n",
       " \tcount\tint\n",
       " \n",
       " Rows: 4\n",
       " \n",
       " Data:\n",
       " +--------------+-----------------+-------+\n",
       " | target_label | predicted_label | count |\n",
       " +--------------+-----------------+-------+\n",
       " |   negative   |     positive    |  170  |\n",
       " |   negative   |     negative    |  143  |\n",
       " |   positive   |     negative    |   47  |\n",
       " |   positive   |     positive    |  951  |\n",
       " +--------------+-----------------+-------+\n",
       " [4 rows x 3 columns], 'f1_score': 0.8975932043416706, 'precision': 0.848349687778769, 'recall': 0.9529058116232465}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model.evaluate(test_data_pos_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Creating a validation set from 5 percent of training data. This may take a while.\n",
      "          You can set ``validation_set=None`` to disable validation tracking.\n",
      "\n",
      "PROGRESS: The following methods are available for this type of problem.\n",
      "PROGRESS: LogisticClassifier, SVMClassifier\n",
      "PROGRESS: The returned model will be chosen according to validation accuracy.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 11188\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 4\n",
      "PROGRESS: Number of unpacked features : 305069\n",
      "PROGRESS: Number of coefficients    : 305070\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy | Validation-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | 1         | 3        | 0.000089  | 0.221149     | 0.923758          | 0.873770            |\n",
      "PROGRESS: | 2         | 5        | 1.000000  | 0.367244     | 0.998481          | 0.873770            |\n",
      "PROGRESS: | 3         | 6        | 1.000000  | 0.488326     | 0.999285          | 0.873770            |\n",
      "PROGRESS: | 4         | 7        | 1.000000  | 0.641431     | 0.999553          | 0.873770            |\n",
      "PROGRESS: | 5         | 8        | 1.000000  | 0.763509     | 0.999374          | 0.873770            |\n",
      "PROGRESS: | 6         | 10       | 1.000000  | 0.930625     | 0.999642          | 0.873770            |\n",
      "PROGRESS: | 10        | 20       | 1.492381  | 1.565048     | 0.999642          | 0.873770            |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: SVM:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 11188\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 4\n",
      "PROGRESS: Number of unpacked features : 305069\n",
      "PROGRESS: Number of coefficients    : 305070\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy | Validation-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | 1         | 3        | 0.000089  | 0.829555     | 0.923758          | 0.873770            |\n",
      "PROGRESS: | 2         | 5        | 1.000000  | 0.951635     | 0.994011          | 0.873770            |\n",
      "PROGRESS: | 3         | 6        | 1.000000  | 1.057706     | 0.998927          | 0.873770            |\n",
      "PROGRESS: | 4         | 7        | 1.000000  | 1.155774     | 0.999285          | 0.873770            |\n",
      "PROGRESS: | 5         | 9        | 1.000000  | 1.294867     | 0.999464          | 0.873770            |\n",
      "PROGRESS: | 6         | 10       | 1.000000  | 1.368916     | 0.999553          | 0.873770            |\n",
      "PROGRESS: | 10        | 14       | 1.000000  | 2.309548     | 0.999642          | 0.873770            |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Model selection based on validation accuracy:\n",
      "PROGRESS: ---------------------------------------------\n",
      "PROGRESS: LogisticClassifier              : 0.87377\n",
      "PROGRESS: SVMClassifier                   : 0.87377\n",
      "PROGRESS: ---------------------------------------------\n",
      "PROGRESS: Selecting LogisticClassifier based on validation set performance.\n"
     ]
    }
   ],
   "source": [
    "#feature_set_negative_ornot = ['tfidf','1gram features','2gram features','3gram features','vectors_neg_ornot','vectors_doc2vec_tweetsonly_dm']\n",
    "#feature_set_negative_ornot = ['tfidf','1gram features','2gram features','3gram features','vectors_neg_ornot']\n",
    "feature_set_negative_ornot = ['tfidf','1gram features','2gram features','3gram features']\n",
    "#feature_set_negative_ornot = ['tfidf','1gram features','2gram features','3gram features','vectors_doc2vec_tweetsonly_dm']\n",
    "model_negative_ornot_svm =graphlab.classifier.create(tweets,\n",
    "                                                     target='NegativeorNot',\n",
    "                                                     features=feature_set_negative_ornot,\n",
    "                                                    \n",
    "                                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Creating a validation set from 5 percent of training data. This may take a while.\n",
      "          You can set ``validation_set=None`` to disable validation tracking.\n",
      "\n",
      "PROGRESS: The following methods are available for this type of problem.\n",
      "PROGRESS: LogisticClassifier, SVMClassifier\n",
      "PROGRESS: The returned model will be chosen according to validation accuracy.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 6411\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 4\n",
      "PROGRESS: Number of unpacked features : 188877\n",
      "PROGRESS: Number of coefficients    : 188878\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy | Validation-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | 1         | 4        | 0.000078  | 0.181125     | 0.977383          | 0.769939            |\n",
      "PROGRESS: | 2         | 6        | 1.000000  | 0.280188     | 0.998596          | 0.776074            |\n",
      "PROGRESS: | 3         | 7        | 1.000000  | 0.348233     | 0.999532          | 0.776074            |\n",
      "PROGRESS: | 4         | 8        | 1.000000  | 0.412275     | 0.999688          | 0.776074            |\n",
      "PROGRESS: | 5         | 9        | 1.000000  | 0.465315     | 0.999688          | 0.776074            |\n",
      "PROGRESS: | 6         | 10       | 1.000000  | 0.514343     | 0.999688          | 0.776074            |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: SVM:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 6411\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 4\n",
      "PROGRESS: Number of unpacked features : 188877\n",
      "PROGRESS: Number of coefficients    : 188878\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy | Validation-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | 1         | 3        | 0.000156  | 0.099066     | 0.977383          | 0.769939            |\n",
      "PROGRESS: | 2         | 5        | 1.000000  | 0.169114     | 0.998284          | 0.769939            |\n",
      "PROGRESS: | 3         | 6        | 1.000000  | 0.212141     | 0.999376          | 0.769939            |\n",
      "PROGRESS: | 4         | 7        | 1.000000  | 0.254171     | 0.999688          | 0.769939            |\n",
      "PROGRESS: | 5         | 8        | 1.000000  | 0.293195     | 0.999688          | 0.769939            |\n",
      "PROGRESS: | 6         | 14       | 1.286039  | 0.468316     | 0.999532          | 0.769939            |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Model selection based on validation accuracy:\n",
      "PROGRESS: ---------------------------------------------\n",
      "PROGRESS: LogisticClassifier              : 0.788344\n",
      "PROGRESS: SVMClassifier                   : 0.769939\n",
      "PROGRESS: ---------------------------------------------\n",
      "PROGRESS: Selecting LogisticClassifier based on validation set performance.\n"
     ]
    }
   ],
   "source": [
    "#feature_set_pos_neg = ['tfidf','1gram features','2gram features','3gram features','vectors_pos_neg','vectors_pos_neutral','vectors_pos_ornot','vectors_neg_ornot','vectors_doc2vec_tweetsonly_dm']\n",
    "#feature_set_pos_neg = ['tfidf','1gram features','2gram features','3gram features','vectors_pos_neg','vectors_pos_neutral','vectors_pos_ornot','vectors_neg_ornot']\n",
    "feature_set_pos_neg = ['tfidf','1gram features','2gram features','3gram features']\n",
    "#feature_set_pos_neg = ['tfidf','1gram features','2gram features','3gram features','vectors_doc2vec_tweetsonly_dm']\n",
    "model_pos_neg_svm =graphlab.classifier.create(tweets_pos_neg,\n",
    "                                                     target='Sentiment',\n",
    "                                                     features=feature_set_pos_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Creating a validation set from 5 percent of training data. This may take a while.\n",
      "          You can set ``validation_set=None`` to disable validation tracking.\n",
      "\n",
      "PROGRESS: The following methods are available for this type of problem.\n",
      "PROGRESS: LogisticClassifier, SVMClassifier\n",
      "PROGRESS: The returned model will be chosen according to validation accuracy.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 9658\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 7\n",
      "PROGRESS: Number of unpacked features : 268905\n",
      "PROGRESS: Number of coefficients    : 268906\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy | Validation-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | 1         | 3        | 0.000104  | 0.964644     | 0.990992          | 0.649895            |\n",
      "PROGRESS: | 2         | 5        | 1.000000  | 1.858242     | 0.997826          | 0.654088            |\n",
      "PROGRESS: | 3         | 6        | 1.000000  | 2.401604     | 0.998447          | 0.660377            |\n",
      "PROGRESS: | 4         | 7        | 1.000000  | 2.974986     | 0.998861          | 0.660377            |\n",
      "PROGRESS: | 5         | 8        | 1.000000  | 3.531354     | 0.999068          | 0.662474            |\n",
      "PROGRESS: | 6         | 9        | 1.000000  | 4.127752     | 0.999068          | 0.662474            |\n",
      "PROGRESS: | 10        | 14       | 1.000000  | 6.798535     | 0.999068          | 0.666667            |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: SVM:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 9658\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 7\n",
      "PROGRESS: Number of unpacked features : 268905\n",
      "PROGRESS: Number of coefficients    : 268906\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy | Validation-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | 1         | 3        | 0.000104  | 1.088725     | 0.990992          | 0.649895            |\n",
      "PROGRESS: | 2         | 5        | 1.000000  | 2.058373     | 0.996687          | 0.662474            |\n",
      "PROGRESS: | 3         | 6        | 1.000000  | 2.733827     | 0.998136          | 0.654088            |\n",
      "PROGRESS: | 4         | 7        | 1.000000  | 3.380255     | 0.998343          | 0.647799            |\n",
      "PROGRESS: | 5         | 9        | 1.000000  | 4.327884     | 0.998861          | 0.645702            |\n",
      "PROGRESS: | 6         | 10       | 1.000000  | 4.954302     | 0.998447          | 0.666667            |\n",
      "PROGRESS: | 10        | 16       | 1.000000  | 8.179451     | 0.997101          | 0.660377            |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Model selection based on validation accuracy:\n",
      "PROGRESS: ---------------------------------------------\n",
      "PROGRESS: LogisticClassifier              : 0.666667\n",
      "PROGRESS: SVMClassifier                   : 0.660377\n",
      "PROGRESS: ---------------------------------------------\n",
      "PROGRESS: Selecting LogisticClassifier based on validation set performance.\n"
     ]
    }
   ],
   "source": [
    "feature_set_pos_neutral = ['tfidf','1gram features','2gram features','3gram features','vectors_pos_neutral','vectors_pos_ornot','vectors_neutral_ornot']\n",
    "model_pos_neutral_svm =graphlab.classifier.create(tweets_pos_neutral,\n",
    "                                                     target='Sentiment',\n",
    "                                                     features=feature_set_pos_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Creating a validation set from 5 percent of training data. This may take a while.\n",
      "          You can set ``validation_set=None`` to disable validation tracking.\n",
      "\n",
      "PROGRESS: The following methods are available for this type of problem.\n",
      "PROGRESS: LogisticClassifier, SVMClassifier\n",
      "PROGRESS: The returned model will be chosen according to validation accuracy.\n",
      "PROGRESS: Logistic regression:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 6395\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 7\n",
      "PROGRESS: Number of unpacked features : 196209\n",
      "PROGRESS: Number of coefficients    : 196210\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy | Validation-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | 1         | 7        | 0.000003  | 1.756173     | 0.752306          | 0.768997            |\n",
      "PROGRESS: | 2         | 10       | 5.000000  | 3.423286     | 0.991400          | 0.802432            |\n",
      "PROGRESS: | 3         | 11       | 5.000000  | 4.333888     | 0.990618          | 0.784195            |\n",
      "PROGRESS: | 4         | 12       | 5.000000  | 4.832223     | 0.995622          | 0.762918            |\n",
      "PROGRESS: | 5         | 13       | 5.000000  | 5.223485     | 0.996873          | 0.778116            |\n",
      "PROGRESS: | 6         | 14       | 5.000000  | 5.600735     | 0.997342          | 0.808511            |\n",
      "PROGRESS: | 10        | 19       | 1.000000  | 7.286861     | 0.999375          | 0.820669            |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: SVM:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 6395\n",
      "PROGRESS: Number of classes           : 2\n",
      "PROGRESS: Number of feature columns   : 7\n",
      "PROGRESS: Number of unpacked features : 196209\n",
      "PROGRESS: Number of coefficients    : 196210\n",
      "PROGRESS: Starting L-BFGS\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | Iteration | Passes   | Step size | Elapsed Time | Training-accuracy | Validation-accuracy |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: | 1         | 7        | 0.000003  | 1.919282     | 0.752306          | 0.768997            |\n",
      "PROGRESS: | 2         | 11       | 3.000000  | 2.938964     | 0.963878          | 0.768997            |\n",
      "PROGRESS: | 3         | 12       | 3.000000  | 3.306205     | 0.304457          | 0.234043            |\n",
      "PROGRESS: | 4         | 16       | 3.000000  | 4.221815     | 0.749648          | 0.768997            |\n",
      "PROGRESS: | 5         | 17       | 3.000000  | 4.561047     | 0.512119          | 0.255319            |\n",
      "PROGRESS: | 6         | 18       | 3.000000  | 4.909276     | 0.749961          | 0.768997            |\n",
      "PROGRESS: | 10        | 26       | 1.000000  | 7.043697     | 0.992338          | 0.778116            |\n",
      "PROGRESS: +-----------+----------+-----------+--------------+-------------------+---------------------+\n",
      "PROGRESS: TERMINATED: Iteration limit reached.\n",
      "PROGRESS: This model may not be optimal. To improve it, consider increasing `max_iterations`.\n",
      "PROGRESS: Model selection based on validation accuracy:\n",
      "PROGRESS: ---------------------------------------------\n",
      "PROGRESS: LogisticClassifier              : 0.820669\n",
      "PROGRESS: SVMClassifier                   : 0.778116\n",
      "PROGRESS: ---------------------------------------------\n",
      "PROGRESS: Selecting LogisticClassifier based on validation set performance.\n"
     ]
    }
   ],
   "source": [
    "feature_set_neutral_neg = ['tfidf','1gram features','2gram features','3gram features','vectors_neutral_neg','vectors_neg_ornot','vectors_neutral_ornot']\n",
    "model_neutral_neg_lr_svm =graphlab.classifier.create(tweets_neutral_neg,\n",
    "                                                     target='Sentiment',\n",
    "                                                     features=feature_set_neutral_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "negative\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "negative\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "positive\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "positive\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n",
      "neutral\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "final_sentimentlist = []\n",
    "for i in test_tweets:\n",
    "    pos=0\n",
    "    neg=0\n",
    "    neutral=0\n",
    "    final_sentiment = 'unknown'\n",
    "    if i['pos_neg'] == 'positive':\n",
    "        pos = math.fsum([pos,1])\n",
    "    \n",
    "    if i['pos_neg'] == 'negative':\n",
    "        neg= math.fsum([neg,1])\n",
    "    if i['pos_neutral'] == 'positive':\n",
    "        pos = math.fsum([pos,1])\n",
    "    if i['pos_neutral'] == 'neutral':\n",
    "        neutral = math.fsum([neutral,1])\n",
    "    if i['neg_neutral'] == 'negative':\n",
    "        neg = math.fsum([neg,1])\n",
    "    if i['neg_neutral'] == 'neutral':\n",
    "        neutral = math.fsum([neutral,1])\n",
    "    if i['negative_ornot']==1:\n",
    "        final_sentiment = 'negative'\n",
    "    if(final_sentiment!='negative' and pos>neg and pos>neutral):\n",
    "        final_sentiment='positive'\n",
    "    if(final_sentiment!='negative' and neg>pos and neg>neutral):\n",
    "        final_sentiment='negative'\n",
    "    if(final_sentiment!='negative' and neutral>pos and neutral>neg):\n",
    "        final_sentiment='neutral'\n",
    "    if(final_sentiment == 'negative'):\n",
    "        i['end_sentiment'] = 'negative'\n",
    "    if(final_sentiment == 'positive'):\n",
    "        i['end_sentiment'] = 'positive'\n",
    "    if(final_sentiment == 'neutral'):\n",
    "        i['end_sentiment'] = 'neutral'\n",
    "    if(i['negative_ornot']==0 and pos==neg and pos==neutral and neutral==neg):\n",
    "        final_sentiment = i['pos_neg_neutral']\n",
    "    if(final_sentiment=='unknown'):\n",
    "        final_sentiment = i['pos_neg_neutral']\n",
    "    print final_sentiment\n",
    "    final_sentimentlist.append(final_sentiment)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_tweets['last_sentiment'] = final_sentimentlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">ID</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Sentiment</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">10936</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">ye i am go from school<br>have class till $NUM can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">11051</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">can u tape the match for<br>me? i\\u$NUMl rush over ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">10966</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">too mani peopl at my hous<br>my rel are here a po ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">11211</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">yea i have spoken to him<br>liao. inde he is ne over ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">11350</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">haha... i want to see. e<br>macdonald here cheaper. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">10539</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">i\\u$NUMm go down now<br>liao\\u$NUMc with my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">10544</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">ya\\u$NUMc ok for me...<br>erm can let me know the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">11463</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">he told u i\\u$NUMm consid<br>liao mah. i duno\\u$NU ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">11062</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">will you be po on sunday<br>night? i just met ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">11035</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">tom\\u$NUMc u think it is<br>a relative\\u$NUM hous or ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[10 rows x 3 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tID\tint\n",
       "\tSentiment\tstr\n",
       "\tTweet\tstr\n",
       "\n",
       "Rows: 10\n",
       "\n",
       "Data:\n",
       "+-------+-----------+-------------------------------+\n",
       "|   ID  | Sentiment |             Tweet             |\n",
       "+-------+-----------+-------------------------------+\n",
       "| 10936 |  neutral  | ye i am go from school hav... |\n",
       "| 11051 |  neutral  | can u tape the match for m... |\n",
       "| 10966 |  neutral  | too mani peopl at my hous ... |\n",
       "| 11211 |  negative | yea i have spoken to him l... |\n",
       "| 11350 |  positive | haha... i want to see. e m... |\n",
       "| 10539 |  neutral  | i\\u$NUMm go down now liao\\... |\n",
       "| 10544 |  neutral  | ya\\u$NUMc ok for me... erm... |\n",
       "| 11463 |  negative | he told u i\\u$NUMm consid ... |\n",
       "| 11062 |  positive | will you be po on sunday n... |\n",
       "| 11035 |  neutral  | tom\\u$NUMc u think it is a... |\n",
       "+-------+-----------+-------------------------------+\n",
       "[10 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-------------------------------+\n",
      "| ID | Sentiment |             Tweet             |\n",
      "+----+-----------+-------------------------------+\n",
      "| 1  |  UNKNOWN  | i m done write code for th... |\n",
      "| 2  |  UNKNOWN  | seven penni stock on the m... |\n",
      "| 3  |  UNKNOWN  | dec $NUMst $NUM will be kn... |\n",
      "| 4  |  UNKNOWN  | yar he quit po but aft man... |\n",
      "| 5  |  UNKNOWN  | yeah we have thin lizzi he... |\n",
      "| 6  |  UNKNOWN  | mt AT USER syria\\u$NUMc de... |\n",
      "| 7  |  UNKNOWN  | AT USER ne my life, becaus... |\n",
      "| 8  |  UNKNOWN  | ard $NUM\\u$NUMc $NUM time ... |\n",
      "| 9  |  UNKNOWN  | po for fri\\u$NUMcnot so i ... |\n",
      "| 10 |  UNKNOWN  | just got the ic updat from... |\n",
      "| 11 |  UNKNOWN  | check out our ladi peac at... |\n",
      "| 12 |  UNKNOWN  | hmm... mon afternoon i got... |\n",
      "| 13 |  UNKNOWN  | just thought about the fac... |\n",
      "| 14 |  UNKNOWN  | bedtime. go to dayton at $... |\n",
      "| 15 |  UNKNOWN  | we eat at science\\u$NUMc u... |\n",
      "| 16 |  UNKNOWN  | sign of the time for onc p... |\n",
      "| 17 |  UNKNOWN  | hello from the foundat tre... |\n",
      "| 18 |  UNKNOWN  | tomorrow we will be po to ... |\n",
      "| 19 |  UNKNOWN  | impuls bought resid evil: ... |\n",
      "| 20 |  UNKNOWN  | it sound realli ne but it ... |\n",
      "| 21 |  UNKNOWN  | got my AT USER ticket in t... |\n",
      "| 22 |  UNKNOWN  | just arriv in madrid from ... |\n",
      "| 23 |  UNKNOWN  | sopho is the lamest viru e... |\n",
      "| 24 |  UNKNOWN  | ok i also wan $NUM watch e... |\n",
      "| 25 |  UNKNOWN  | AT USER wooooooo\\u$NUMc ge... |\n",
      "| 26 |  UNKNOWN  | AT USER on mtv hit in $NUM... |\n",
      "| 27 |  UNKNOWN  | AT USER you\\u$NUMr not com... |\n",
      "| 28 |  UNKNOWN  | nataaashaann   you been? d... |\n",
      "| 29 |  UNKNOWN  | jose iglesia / igleisa sta... |\n",
      "| 30 |  UNKNOWN  | wasn\\u$NUMt there an inter... |\n",
      "| 31 |  UNKNOWN  | game $NUM of san beda v le... |\n",
      "| 32 |  UNKNOWN  | cainer wa given a key to t... |\n",
      "| 33 |  UNKNOWN  | we\\u$NUMr partner with our... |\n",
      "| 34 |  UNKNOWN  | know that AT USER is at ru... |\n",
      "| 35 |  UNKNOWN  | see you at the houston imp... |\n",
      "| 36 |  UNKNOWN  | good po to AT USER tomorro... |\n",
      "| 37 |  UNKNOWN  | are you? still on bed blur... |\n",
      "| 38 |  UNKNOWN  | u r just po my friends? i ... |\n",
      "| 39 |  UNKNOWN  | the stori featur the same ... |\n",
      "| 40 |  UNKNOWN  | a. is taylor kitsch seriou... |\n",
      "| 41 |  UNKNOWN  | haha... hope can hear the ... |\n",
      "| 42 |  UNKNOWN  | due to the hurricane\\u$NUM... |\n",
      "| 43 |  UNKNOWN  | just pack for a weekend in... |\n",
      "| 44 |  UNKNOWN  | your plan of attend the gr... |\n",
      "| 45 |  UNKNOWN  | hey...   come u rent out u... |\n",
      "| 46 |  UNKNOWN  | espindola doubl rock galax... |\n",
      "| 47 |  UNKNOWN  | rent in u up $NUM betw jan... |\n",
      "| 48 |  UNKNOWN  | by the end of today i will... |\n",
      "| 49 |  UNKNOWN  | check out thi week the AT ... |\n",
      "| 50 |  UNKNOWN  | just chked my email onli..... |\n",
      "+----+-----------+-------------------------------+\n",
      "+-------------------------------+-------------------------------+\n",
      "|         1gram features        |         2gram features        |\n",
      "+-------------------------------+-------------------------------+\n",
      "| {'code': 1L, 've': 1L, 'nu... | {'po we': 1L, 'for the': 2... |\n",
      "| {'volume': 1L, 'on': 1L, '... | {'heavi volume': 1L, 'move... |\n",
      "| {'a': 1L, 'be': 1L, 'end':... | {'will be': 1L, 'end of': ... |\n",
      "| {'aiya': 1L, 'bring': 1L, ... | {'thk leona': 1L, 'leona s... |\n",
      "| {'we': 1L, 'i': 1L, 'infor... | {'have thin': 1L, 'the inf... |\n",
      "| {'ali': 1L, 'al': 1L, 'num... | {'a soldier': 1L, 'at user... |\n",
      "| {'soon': 1L, 'see': 1L, 'a... | {'at user': 1L, 'festiv on... |\n",
      "| {'a': 1L, 'num': 2L, 'conf... | {'num u': 1L, 'numc num': ... |\n",
      "| {'and': 1L, 'numcnot': 1L,... | {'display and': 1L, 'at th... |\n",
      "| {'just': 1L, 'centr': 1L, ... | {'numc so': 1L, 'from soni... |\n",
      "| {'and': 1L, 'summer': 1L, ... | {'gotta po': 1L, 'and spen... |\n",
      "| {'sch': 1L, 'back': 2L, 'o... | {'sch for': 1L, 'for a': 1... |\n",
      "| {'and': 1L, 'just': 1L, 's... | {'prom wow': 1L, 'just tho... |\n",
      "| {'a': 2L, 'head': 1L, 'pri... | {'at num': 1L, 'head of': ... |\n",
      "| {'we': 1L, 'science': 1L, ... | {'science u': 1L, 'at scie... |\n",
      "| {'march': 1L, 'shirt': 1L,... | {'the time': 1L, 'time for... |\n",
      "| {'up': 1L, 'trek': 1L, 'fr... | {'the foundat': 1L, 'trekk... |\n",
      "| {'hetton': 1L, 'be': 1L, '... | {'will be': 1L, 'south het... |\n",
      "| {'oper': 1L, 'it': 1L, 'nu... | {'with num': 1L, 'much edi... |\n",
      "| {'sound': 1L, 'just': 1L, ... | {'couldnt have': 1L, 'but ... |\n",
      "| {'ticket': 1L, 'drop': 1L,... | {'user ticket': 1L, 'mail ... |\n",
      "| {'from': 1L, 'just': 1L, '... | {'hvar croatia': 1L, 'to h... |\n",
      "| {'viru': 1L, 'url': 1L, 'e... | {'becam the': 1L, 'sopho p... |\n",
      "| {'wan': 1L, 'e': 1L, 'i': ... | {'ok i': 1L, 'e num': 1L, ... |\n",
      "| {'a': 2L, 'night': 1L, 'nu... | {'night with': 1L, 'at use... |\n",
      "| {'take': 1L, 'on': 1L, 'hi... | {'the sun': 1L, 'sun video... |\n",
      "| {'numt': 1L, 'not': 1L, 'a... | {'at user': 1L, 'ne you': ... |\n",
      "| {'free': 1L, 'about': 1L, ... | {'free yn': 1L, 'weekend i... |\n",
      "| {'mlb': 1L, 'jose': 1L, 'f... | {'mlb bo': 1L, 'start at':... |\n",
      "| {'numt': 1L, 'numth': 1L, ... | {'numt there': 1L, 'u numt... |\n",
      "| {'on': 1L, 'beda': 1L, 'le... | {'will be': 1L, 'game num'... |\n",
      "| {'cainer': 1L, 'sfgiant': ... | {'also forev': 1L, 'key to... |\n",
      "| {'ten': 1L, 'at': 1L, 'in'... | {'in green': 1L, 'june num... |\n",
      "| {'control': 1L, 'me': 1L, ... | {'rum runner': 1L, 'ne for... |\n",
      "| {'a': 1L, 'houston': 1L, '... | {'thi tuesday': 1L, 'numc ... |\n",
      "| {'gobobcat': 1L, 'champion... | {'user tomorrow': 1L, 'goo... |\n",
      "| {'on': 1L, 'bed': 1L, 'are... | {'are you': 1L, 'still on'... |\n",
      "| {'just': 1L, 'feel': 1L, '... | {'they cry': 1L, 'will u':... |\n",
      "| {'on': 1L, 'featur': 1L, '... | {'stori featur': 1L, 'do n... |\n",
      "| {'a': 1L, 'be': 1L, 'taylo... | {'taylor kitsch': 1L, 'is ... |\n",
      "| {'sound': 1L, 'receipt': 1... | {'hear the': 1L, 'haha hop... |\n",
      "| {'move': 1L, 'num': 3L, 'c... | {'nov num': 2L, 'the easte... |\n",
      "| {'a': 2L, 'on': 1L, 'and':... | {'just pack': 1L, 'sunday ... |\n",
      "| {'about': 1L, 'may': 1L, '... | {'about url': 1L, 'been wa... |\n",
      "| {'again': 1L, 'hse': 1L, '... | {'hse again': 1L, 'hey com... |\n",
      "| {'stud': 1L, 'doubl': 1L, ... | {'the star': 1L, 'more gam... |\n",
      "| {'sound': 1L, 'betw': 1L, ... | {'up num': 2L, 'at user': ... |\n",
      "| {'and': 1L, 'done': 1L, 'h... | {'by the': 1L, 'hot jam': ... |\n",
      "| {'week': 1L, 'on': 1L, 'bb... | {'check out': 1L, 'the thu... |\n",
      "| {'onli': 1L, 'just': 1L, '... | {'just chked': 1L, 'onli h... |\n",
      "+-------------------------------+-------------------------------+\n",
      "+-------------------------------+-------------------------------+\n",
      "|         3gram features        |        vectors_pos_neg        |\n",
      "+-------------------------------+-------------------------------+\n",
      "| {'premer num num': 1L, 'de... | [0.029617883265, 0.0410644... |\n",
      "| {'u numc april': 1L, 'penn... | [-0.0555990971625, -0.0205... |\n",
      "| {'dec numst num': 1L, 'bab... | [0.100304581225, -0.003832... |\n",
      "| {'guess lor he': 1L, 'he g... | [-0.00112453661859, 0.0492... |\n",
      "| {'hate the informerci': 1L... | [0.0188240408897, 0.034142... |\n",
      "| {'wa a soldier': 1L, 'wedn... | [-0.030581291765, 0.008444... |\n",
      "| {'po me to': 1L, 'saturday... | [0.0350846275687, 0.023929... |\n",
      "| {'they alreadi confirm': 1... | [-0.0614000074565, -0.0298... |\n",
      "| {'fri u numcnot': 1L, 'but... | [-0.0698166191578, 0.02203... |\n",
      "| {'just got the': 1L, 'i on... | [0.0192862413824, 0.008397... |\n",
      "| {'hope volleybal tomorrow'... | [0.0178822204471, -0.01563... |\n",
      "| {'or u wanna': 1L, 'hmm mo... | [-0.0346294008195, 0.00955... |\n",
      "| {'about the fact': 1L, 'th... | [-0.139080971479, 0.000473... |\n",
      "| {'num tomorrow to': 1L, 'a... | [-0.00363515829667, -0.051... |\n",
      "| {'u want to': 1L, 'at scie... | [-0.111045718193, -0.00973... |\n",
      "| {'sign of the': 1L, 'shirt... | [0.0515226572752, -0.00072... |\n",
      "| {'hadrian s wall': 1L, 'up... | [0.0562008842826, 0.002701... |\n",
      "| {'po to play': 1L, 'south ... | [-0.0570899359882, -0.0577... |\n",
      "| {'evil oper raccoon': 1L, ... | [-0.0501008890569, -0.0219... |\n",
      "| {'been ne time': 1L, 'real... | [0.0182196740061, 0.033076... |\n",
      "| {'puck hockeyisback url': ... | [0.0615613311529, -0.01122... |\n",
      "| {'go to hvar': 1L, 'arriv ... | [-0.0988371372223, -0.0409... |\n",
      "| {'is the lamest': 1L, 'lam... | [-0.0156447254121, -0.0452... |\n",
      "| {'wan num watch': 1L, 'i a... | [0.021398531273, 0.0491851... |\n",
      "| {'ne on a': 1L, 'on a wedn... | [-0.0426867417991, 0.00149... |\n",
      "| {'behind the scene': 1L, '... | [-0.0434446185827, -0.0200... |\n",
      "| {'you u numr': 1L, 'ne you... | [-0.0695780962706, 0.02170... |\n",
      "| {'weekend in grand': 1L, '... | [-0.0214471127838, 0.02851... |\n",
      "| {'iglesia igleisa start': ... | [-0.0570781491697, -0.0396... |\n",
      "| {'the numth of': 1L, 'numt... | [-0.0349449329078, -0.0475... |\n",
      "| {'smart araneta coliseum':... | [0.0273808278143, -0.03721... |\n",
      "| {'forev be known': 1L, 'da... | [-0.019853612408, 0.009940... |\n",
      "| {'our friend at': 1L, 'in ... | [-0.0481648854911, -0.0668... |\n",
      "| {'po for tomorrow': 1L, 'm... | [0.0340162180364, -0.03365... |\n",
      "| {'houston improv u': 1L, '... | [-0.0556669421494, -0.0111... |\n",
      "| {'at the mac': 1L, 'champi... | [0.0192593429238, -0.02566... |\n",
      "| {'still on bed': 1L, 'are ... | [-0.167103335261, -0.06466... |\n",
      "| {'i hope not': 1L, 'friend... | [-0.0170571058989, -0.0059... |\n",
      "| {'featur the same': 1L, 'n... | [0.0809615179896, 0.055309... |\n",
      "| {'chariti game tomorrow': ... | [-0.0211136806756, 0.03056... |\n",
      "| {'sound gd luck': 1L, 'hah... | [0.0653564482927, -0.00781... |\n",
      "| {'in ny on': 1L, 'u numc g... | [-0.0449222438037, -0.0699... |\n",
      "| {'just pack for': 1L, 'mat... | [0.0056297830306, -0.08104... |\n",
      "| {'out becaus of': 1L, 'of ... | [0.0761785656214, -0.05641... |\n",
      "| {'come u rent': 1L, 'out u... | [-0.0350277163088, -0.0078... |\n",
      "| {'galaxi the star': 1L, 'g... | [0.0213013645262, -0.03315... |\n",
      "| {'num in calif': 1L, 'user... | [-0.0584652647376, -0.0361... |\n",
      "| {'today i will': 1L, 'grou... | [0.0167315285653, 0.030335... |\n",
      "| {'thi week the': 1L, 'back... | [0.0195683259517, -0.00060... |\n",
      "| {'just chked my': 1L, 'ema... | [-0.0436212718487, -0.0452... |\n",
      "+-------------------------------+-------------------------------+\n",
      "+-------------------------------+-------------------------------+\n",
      "|      vectors_pos_neutral      |      vectors_neutral_neg      |\n",
      "+-------------------------------+-------------------------------+\n",
      "| [-0.0290144644678, 0.08835... | [0.0121533488855, -0.03010... |\n",
      "| [0.0096508488059, 0.078141... | [-0.0345708578825, -0.0176... |\n",
      "| [-0.00812429282814, 0.0497... | [-0.0433184877038, -0.0526... |\n",
      "| [-0.0263857822865, -0.0417... | [-0.0326886810362, -0.0569... |\n",
      "| [-0.0148033946753, -0.0085... | [0.00757849076763, 0.01856... |\n",
      "| [0.0943678170443, 0.111308... | [-0.00724790384993, -0.025... |\n",
      "| [0.0416637063026, 0.065840... | [-0.00408355146646, 0.0104... |\n",
      "| [0.0201957114041, 0.104727... | [-0.0136035261676, -0.0177... |\n",
      "| [-0.000824085669592, 0.113... | [-0.0720481649041, -0.0316... |\n",
      "| [-0.000682450307067, 0.027... | [0.0371816009283, -0.08020... |\n",
      "| [0.0412441268563, 0.093050... | [-0.00466080568731, 0.0278... |\n",
      "| [-0.01083769463, 0.1028614... | [-0.0949276909232, -0.0264... |\n",
      "| [0.031493421644, 0.1921428... | [-0.140288189054, -0.09784... |\n",
      "| [0.0137084275484, 0.019054... | [0.0128374164924, 0.006268... |\n",
      "| [0.120492659509, 0.2447140... | [-0.198759168386, -0.00364... |\n",
      "| [0.13203150034, 0.08975888... | [0.0528291501105, 0.046643... |\n",
      "| [0.0343991369009, 0.090250... | [-0.0490359887481, 0.06748... |\n",
      "| [0.00268019456416, -0.0085... | [0.042070299387, 0.0290968... |\n",
      "| [0.0152527242899, 0.044916... | [-0.0398802421987, -0.0374... |\n",
      "| [-0.0806241855025, 0.11620... | [-0.0158817525953, 0.05638... |\n",
      "| [0.135566875339, 0.0624418... | [0.0357806272805, 0.004534... |\n",
      "| [0.0336371585727, 0.097710... | [-0.00716948881745, 0.0288... |\n",
      "| [0.0611828193069, 0.152368... | [-0.0258643943816, 0.01207... |\n",
      "| [-0.0615649558604, -0.0333... | [0.031308721751, 0.0126101... |\n",
      "| [0.0855587795377, 0.134879... | [-0.0450666844845, 0.00135... |\n",
      "| [0.0832929089665, 0.162822... | [0.0016154932091, 0.008442... |\n",
      "| [0.0785728394985, 0.190372... | [-0.0965683311224, -0.0362... |\n",
      "| [-0.0538172125816, -0.0120... | [-0.024050636217, 0.035733... |\n",
      "| [0.0242298450321, 0.207880... | [0.0520509183407, 0.095128... |\n",
      "| [0.0372045040131, 0.113818... | [-0.0645427703857, -0.0462... |\n",
      "| [0.114473104477, 0.0969141... | [0.0610523596406, 0.061656... |\n",
      "| [-0.0539053156972, 0.01016... | [0.0293359458447, -0.06929... |\n",
      "| [0.0644232109189, 0.134826... | [-0.0101435342804, -0.0160... |\n",
      "| [0.0490756519139, 0.082858... | [-0.0174901708961, 0.03492... |\n",
      "| [0.0613584406674, 0.072671... | [-0.0341614335775, -0.0139... |\n",
      "| [0.120074376464, 0.0805153... | [0.0445488318801, 0.064592... |\n",
      "| [-0.0106687080115, 0.11756... | [-0.146499216557, 0.065013... |\n",
      "| [-0.0441002175212, 0.12076... | [-0.0366151444614, -0.0693... |\n",
      "| [0.0443531684577, 0.054377... | [0.0584111660719, 0.012651... |\n",
      "| [0.110192939639, 0.1025733... | [-0.00301864673384, 0.0466... |\n",
      "| [-0.0139516443014, 0.05316... | [0.00197029602714, -0.0059... |\n",
      "| [0.0180492773652, 0.080055... | [0.0021143425256, -0.04513... |\n",
      "| [0.00773802213371, 0.02841... | [0.0306056011468, -0.03224... |\n",
      "| [0.0610545873642, -0.00318... | [-0.0030966643244, 0.04425... |\n",
      "| [0.087095938623, 0.0499123... | [-0.0362793877721, -0.0190... |\n",
      "| [0.0317956246436, 0.063388... | [0.0350339785218, -0.00739... |\n",
      "| [0.114071957767, 0.1491330... | [-0.0556780584157, -0.0274... |\n",
      "| [-0.0401507206261, 0.13078... | [0.0337794795632, -0.02400... |\n",
      "| [0.0263004899025, 0.133957... | [0.0171179566532, 0.044205... |\n",
      "| [0.0705065503716, -0.00464... | [0.0693051442504, -0.05185... |\n",
      "+-------------------------------+-------------------------------+\n",
      "+-------------------------------+-------------------------------+\n",
      "|       vectors_pos_ornot       |       vectors_neg_ornot       |\n",
      "+-------------------------------+-------------------------------+\n",
      "| [0.00712243979797, -0.0160... | [0.0128603372723, 0.005069... |\n",
      "| [-0.067061662674, 0.024024... | [-0.0736088827252, 0.01394... |\n",
      "| [-0.00592513522133, -0.033... | [-0.124480850995, -0.05459... |\n",
      "| [0.0067810039036, -0.00257... | [0.00837434455752, 0.01860... |\n",
      "| [0.0217609088868, 0.006003... | [0.0592337138951, -0.00422... |\n",
      "| [0.0514046177268, 0.002267... | [-0.0657162740827, 0.02150... |\n",
      "| [0.0460254512727, 0.015733... | [-0.0465408526361, 0.01371... |\n",
      "| [-0.0615297779441, 0.02238... | [0.014056799002, 0.0661702... |\n",
      "| [-0.0611166022718, 0.00019... | [-0.0438014827669, 0.00398... |\n",
      "| [0.0468381159008, -0.07540... | [-0.0329575762153, 0.00414... |\n",
      "| [0.0890830680728, 0.068216... | [-0.096286393702, 0.000855... |\n",
      "| [-0.0273836925626, 0.03037... | [-0.083178088069, 0.073267... |\n",
      "| [-0.120552368462, -0.01043... | [0.0013272757642, 0.136869... |\n",
      "| [0.110326312482, -0.042884... | [-0.0840290263295, -0.0145... |\n",
      "| [-0.169902831316, -0.02385... | [0.0103087667376, 0.136735... |\n",
      "| [0.125059187412, -0.008161... | [-0.035734847188, -0.02931... |\n",
      "| [0.0905310362577, 0.054495... | [-0.0366470068693, -0.0214... |\n",
      "| [0.051489200443, 0.0052940... | [-0.0957410112023, 0.06000... |\n",
      "| [0.0325215272605, -0.00772... | [-0.0400345660746, 0.03462... |\n",
      "| [-0.123726308346, 0.076361... | [0.0533507801592, 0.054173... |\n",
      "| [0.131529986858, 0.0805242... | [-0.0670272111893, -0.0621... |\n",
      "| [0.108728870749, 0.0635747... | [-0.171882197261, -0.01185... |\n",
      "| [0.00635326327756, 0.05598... | [-0.0927780196071, 0.01887... |\n",
      "| [0.108406312764, -0.051229... | [-0.0529350154102, 0.03522... |\n",
      "| [-0.0119351111352, 0.01276... | [-0.0453838557005, 0.05175... |\n",
      "| [-0.0319828242064, 0.01273... | [0.0163072850555, 0.024068... |\n",
      "| [-0.0478520020843, 0.02897... | [-0.0254554953426, 0.05492... |\n",
      "| [0.0719074159861, 0.102528... | [-0.127700611949, 0.033438... |\n",
      "| [0.0640659853816, 0.094970... | [-0.133677020669, 0.016099... |\n",
      "| [-0.0280364211649, 0.02694... | [-0.072274364531, 0.083710... |\n",
      "| [0.214400097728, 0.0441058... | [-0.175374358892, -0.06084... |\n",
      "| [0.060157623142, -0.047601... | [-0.0320936590433, -0.0176... |\n",
      "| [0.0172668229789, 0.015489... | [-0.112343370914, 0.038675... |\n",
      "| [0.0223061237484, 0.090482... | [0.0586883164942, 0.022090... |\n",
      "| [0.0467031337321, 0.008991... | [-0.0688711628318, 0.01981... |\n",
      "| [0.0533763244748, 0.019326... | [-0.0094429589808, 0.07894... |\n",
      "| [-0.160777390003, 0.000373... | [0.152821779251, 0.0480462... |\n",
      "| [-0.0881888270378, 0.01372... | [-0.0121696526185, 0.05239... |\n",
      "| [0.00923128519207, -0.0216... | [-0.0892741158605, -0.0069... |\n",
      "| [0.103603251278, 0.0518593... | [-0.063418880105, 0.041964... |\n",
      "| [-0.0113387154415, -0.0323... | [-0.0348374955356, -0.0914... |\n",
      "| [-0.0179256424308, -0.0069... | [-0.0874012410641, -0.0518... |\n",
      "| [-0.02007920295, 0.0243212... | [-0.0281688664109, -0.0870... |\n",
      "| [0.0551196336746, 0.104737... | [-0.0882593542337, 0.02121... |\n",
      "| [0.0326225049794, 0.056185... | [-0.0480501800776, 0.05331... |\n",
      "| [0.122698903084, -0.067994... | [-0.0395422540605, -0.0112... |\n",
      "| [-0.0181057266891, 0.00526... | [0.0257075056434, 0.065540... |\n",
      "| [0.051606874913, 0.0526003... | [-0.0634313374758, 0.00383... |\n",
      "| [0.0265220087022, 0.032185... | [-0.046862795949, -0.04665... |\n",
      "| [0.165012076497, 0.0445467... | [0.0693249627948, -0.00562... |\n",
      "+-------------------------------+-------------------------------+\n",
      "+-------------------------------+-------------------------------+\n",
      "|     vectors_neutral_ornot     |           word_count          |\n",
      "+-------------------------------+-------------------------------+\n",
      "| [-0.0552591644228, -0.0057... | {'code': 1L, 've': 1L, 'do... |\n",
      "| [-0.0920937433839, 0.07679... | {'heavi': 1L, 'seven': 1L,... |\n",
      "| [-0.00281179347076, -0.012... | {'a': 1L, '$num': 1L, 'end... |\n",
      "| [-0.0100535033271, 0.04140... | {'wat.': 1L, 'aiya': 1L, '... |\n",
      "| [0.0414763055742, -0.02369... | {'we': 1L, 'i': 1L, 'hate'... |\n",
      "| [-0.0219900626689, 0.11260... | {'army.': 1L, 'ali': 1L, '... |\n",
      "| [-0.00284150848165, 0.0747... | {'back': 1L, 'see': 1L, 'a... |\n",
      "| [-0.012296882458, 0.083136... | {'a': 1L, 'wk.': 1L, '$num... |\n",
      "| [0.0320414341986, 0.055016... | {'and': 1L, 'at': 1L, 'hav... |\n",
      "| [-0.037889201194, -0.03133... | {'just': 1L, 'centr': 1L, ... |\n",
      "| [-0.0152581213042, 0.01473... | {'and': 1L, 'summer': 1L, ... |\n",
      "| [0.0190621912479, 0.078466... | {'sch': 1L, 'back': 2L, 'o... |\n",
      "| [0.0833906829357, 0.177781... | {'a': 1L, 'about': 1L, 'be... |\n",
      "| [-0.0616335645318, -0.0071... | {'a': 2L, 'bedtime.': 1L, ... |\n",
      "| [0.139558076859, 0.1386564... | {'eat?': 1L, 'we': 1L, 'to... |\n",
      "| [0.0100468657911, 0.071208... | {'march': 1L, 'cosatu': 1L... |\n",
      "| [0.00827411282808, 0.08108... | {'trek': 1L, ':-)': 1L, 'f... |\n",
      "| [-0.143962904811, 0.133843... | {'hetton': 1L, 'be': 1L, '... |\n",
      "| [-0.012994187884, -0.01500... | {'oper': 1L, 'city.': 1L, ... |\n",
      "| [0.0111178820953, 0.036225... | {'sound': 1L, 'just': 1L, ... |\n",
      "| [-0.0299943685532, 0.09709... | {'the': 2L, 'drop': 1L, 't... |\n",
      "| [-0.0528119169176, 0.10137... | {'from': 1L, 'just': 1L, '... |\n",
      "| [0.00310249743052, 0.03465... | {'start': 1L, 'url': 1L, '... |\n",
      "| [-0.112876079977, 0.077977... | {'...': 1L, 'e': 1L, 'wan'... |\n",
      "| [0.0227140840143, 0.121179... | {'a': 2L, 'on': 1L, 'true.... |\n",
      "| [-0.0122541850433, 0.11300... | {'take': 1L, 'on': 1L, 'hi... |\n",
      "| [0.0377861075103, 0.222126... | {':(': 1L, 'want': 1L, 'yo... |\n",
      "| [-0.127101793885, 0.029895... | {'yn$numj': 1L, '(it': 1L,... |\n",
      "| [-0.00461556063965, 0.0080... | {'mlb': 1L, 'jose': 1L, 'f... |\n",
      "| [0.00260364520364, 0.13863... | {'on': 1L, 'march': 1L, 'o... |\n",
      "| [-0.128973647952, 0.009905... | {'$num': 1L, 'beda': 1L, '... |\n",
      "| [0.0222071036696, 0.031206... | {'cainer': 1L, 'sfgiant': ... |\n",
      "| [-0.0210541691631, 0.09116... | {'partner': 1L, 'on': 1L, ... |\n",
      "| [0.0135323526338, 0.099385... | {'control': 1L, 'me': 1L, ... |\n",
      "| [-0.0300039406866, 0.09696... | {'houston': 1L, 'registr':... |\n",
      "| [-0.0529442541301, 0.14866... | {'gobobcat': 1L, 'good': 1... |\n",
      "| [0.0150051815435, -0.08402... | {'blur-blur.': 1L, 'on': 1... |\n",
      "| [0.0995685905218, 0.059829... | {'cry?': 1L, 'just': 1L, '... |\n",
      "| [-0.0729058459401, -0.0156... | {'on': 1L, 'featur': 1L, '... |\n",
      "| [-0.034676540643, 0.083778... | {'be': 1L, 'taylor': 1L, '... |\n",
      "| [-0.0477235466242, 0.03643... | {'sound...': 1L, 'receipt'... |\n",
      "| [-0.0560847483575, 0.03878... | {'move': 1L, 'conf': 1L, '... |\n",
      "| [-0.0228154025972, -0.0337... | {'a': 2L, 'on': 1L, 'and':... |\n",
      "| [-0.0379589162767, 0.04662... | {'great': 1L, 'may': 1L, '... |\n",
      "| [0.0517118833959, 0.078101... | {'again': 1L, 'ah...': 1L,... |\n",
      "| [-0.0896876975894, 0.00461... | {'galaxi:': 1L, 'angel': 1... |\n",
      "| [0.031338904053, 0.1602204... | {'sound': 1L, '$num': 1L, ... |\n",
      "| [-0.0324531570077, 0.02351... | {'and': 1L, 'done': 1L, 'h... |\n",
      "| [-0.0141438590363, 0.08142... | {'week': 1L, 'on': 1L, 'lo... |\n",
      "| [-0.095892123878, -0.02427... | {'just': 1L, 'onli...': 1L... |\n",
      "+-------------------------------+-------------------------------+\n",
      "+-------------------------------+----------+-------------+-------------+----------------+\n",
      "|             tfidf             | pos_neg  | pos_neutral | neg_neutral | negative_ornot |\n",
      "+-------------------------------+----------+-------------+-------------+----------------+\n",
      "| {'code': 8.071187299244452... | positive |   positive  |   neutral   |       0        |\n",
      "| {'heavi': 6.58958275832023... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3272459055005288, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'wat.': 8.29433085055866,... | positive |   positive  |   neutral   |       0        |\n",
      "| {'we': 2.858427821058061, ... | positive |   positive  |   negative  |       0        |\n",
      "| {'army.': 10.3737723922384... | positive |   positive  |   neutral   |       0        |\n",
      "| {'back': 3.635619897642539... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 1.3272459055005288, ... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.4707727303772793... | positive |   positive  |   neutral   |       0        |\n",
      "| {'just': 2.538984284850302... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'and': 1.4707727303772793... | positive |   positive  |   neutral   |       0        |\n",
      "| {'sch': 6.847411867622335,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'a': 1.3272459055005288, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 2.6544918110010576, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'eat?': 8.764334479804397... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'march': 4.65346061563108... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'trek': 8.764334479804397... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'hetton': 10.373772392238... | positive |   positive  |   neutral   |       0        |\n",
      "| {'oper': 7.665722191136287... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'sound': 5.42501250186032... | negative |   neutral   |   negative  |       1        |\n",
      "| {'the': 1.1257873428700633... | positive |   positive  |   neutral   |       0        |\n",
      "| {'from': 2.923111596026957... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'start': 3.85167959406834... | positive |   positive  |   neutral   |       0        |\n",
      "| {'...': 3.9987475724104002... | positive |   positive  |   neutral   |       0        |\n",
      "| {'a': 2.6544918110010576, ... | positive |   positive  |   neutral   |       0        |\n",
      "| {'take': 3.695430277584164... | positive |   neutral   |   neutral   |       0        |\n",
      "| {':(': 5.830477609968493, ... | negative |   neutral   |   neutral   |       1        |\n",
      "| {'yn$numj': 10.37377239223... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'mlb': 6.069707299034327,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'on': 1.4648074198460972,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'$num': 1.909979977549374... | positive |   positive  |   neutral   |       0        |\n",
      "| {'cainer': 10.373772392238... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'partner': 8.071187299244... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'control': 6.461749386810... | positive |   positive  |   neutral   |       0        |\n",
      "| {'houston': 6.908036489438... | positive |   positive  |   neutral   |       0        |\n",
      "| {'gobobcat': 10.3737723922... | positive |   positive  |   neutral   |       0        |\n",
      "| {'blur-blur.': 10.37377239... | positive |   positive  |   neutral   |       0        |\n",
      "| {'cry?': 10.37377239223849... | positive |   positive  |   neutral   |       0        |\n",
      "| {'on': 1.4648074198460972,... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'be': 1.906820417259006, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'sound...': 10.3737723922... | positive |   positive  |   neutral   |       0        |\n",
      "| {'move': 5.12674832007801,... | negative |   neutral   |   neutral   |       0        |\n",
      "| {'a': 2.6544918110010576, ... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'great': 5.26782691833791... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'again': 4.72128321196984... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'galaxi:': 10.37377239223... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'sound': 5.42501250186032... | positive |   positive  |   neutral   |       0        |\n",
      "| {'and': 1.4707727303772793... | positive |   neutral   |   neutral   |       0        |\n",
      "| {'week': 4.700449125067004... | positive |   positive  |   neutral   |       0        |\n",
      "| {'just': 2.538984284850302... | negative |   neutral   |   neutral   |       0        |\n",
      "+-------------------------------+----------+-------------+-------------+----------------+\n",
      "+-----------------+----------------+\n",
      "| pos_neg_neutral | last_sentiment |\n",
      "+-----------------+----------------+\n",
      "|     positive    |    positive    |\n",
      "|     neutral     |    neutral     |\n",
      "|     positive    |    positive    |\n",
      "|     positive    |    positive    |\n",
      "|     positive    |    positive    |\n",
      "|     positive    |    positive    |\n",
      "|     positive    |    positive    |\n",
      "|     neutral     |    neutral     |\n",
      "|     positive    |    positive    |\n",
      "|     positive    |    neutral     |\n",
      "|     positive    |    positive    |\n",
      "|     neutral     |    neutral     |\n",
      "|     positive    |    positive    |\n",
      "|     neutral     |    neutral     |\n",
      "|     positive    |    neutral     |\n",
      "|     neutral     |    neutral     |\n",
      "|     neutral     |    neutral     |\n",
      "|     positive    |    positive    |\n",
      "|     neutral     |    neutral     |\n",
      "|     negative    |    negative    |\n",
      "|     neutral     |    positive    |\n",
      "|     neutral     |    neutral     |\n",
      "|     positive    |    positive    |\n",
      "|     positive    |    positive    |\n",
      "|     positive    |    positive    |\n",
      "|     neutral     |    neutral     |\n",
      "|     neutral     |    negative    |\n",
      "|     neutral     |    neutral     |\n",
      "|     neutral     |    neutral     |\n",
      "|     neutral     |    neutral     |\n",
      "|     neutral     |    positive    |\n",
      "|     neutral     |    neutral     |\n",
      "|     positive    |    neutral     |\n",
      "|     positive    |    positive    |\n",
      "|     positive    |    positive    |\n",
      "|     positive    |    positive    |\n",
      "|     neutral     |    positive    |\n",
      "|     positive    |    positive    |\n",
      "|     neutral     |    neutral     |\n",
      "|     neutral     |    neutral     |\n",
      "|     positive    |    positive    |\n",
      "|     neutral     |    neutral     |\n",
      "|     positive    |    neutral     |\n",
      "|     neutral     |    neutral     |\n",
      "|     neutral     |    neutral     |\n",
      "|     neutral     |    neutral     |\n",
      "|     neutral     |    positive    |\n",
      "|     positive    |    neutral     |\n",
      "|     positive    |    positive    |\n",
      "|     neutral     |    neutral     |\n",
      "+-----------------+----------------+\n",
      "[32009 rows x 20 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_tweets.print_rows(num_rows=50, num_columns=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del test_tweets['Sentiment']\n",
    "del test_tweets['Tweet']\n",
    "del test_tweets['1gram features']\n",
    "del test_tweets['2gram features']\n",
    "del test_tweets['3gram features']\n",
    "del test_tweets['vectors_pos_neg']\n",
    "del test_tweets['vectors_pos_neutral']\n",
    "del test_tweets['vectors_pos_ornot']\n",
    "del test_tweets['vectors_neg_ornot']\n",
    "del test_tweets['vectors_neutral_ornot']\n",
    "del test_tweets['word_count']\n",
    "del test_tweets['tfidf']\n",
    "del test_tweets['pos_neg']\n",
    "del test_tweets['pos_neutral']\n",
    "del test_tweets['neg_neutral']\n",
    "del test_tweets['negative_ornot']\n",
    "del test_tweets['pos_neg_neutral']\n",
    "del test_tweets['vectors_neutral_neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">ye i am go from school<br>have class till $NUM can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">can u tape the match for<br>me? i\\u$NUMl rush over ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">too mani peopl at my hous<br>my rel are here a po ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">yea i have spoken to him<br>liao. inde he is ne over ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">haha... i want to see. e<br>macdonald here cheaper. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">i\\u$NUMm go down now<br>liao\\u$NUMc with my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">ya\\u$NUMc ok for me...<br>erm can let me know the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">he told u i\\u$NUMm consid<br>liao mah. i duno\\u$NU ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">will you be po on sunday<br>night? i just met ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">tom\\u$NUMc u think it is<br>a relative\\u$NUM hous or ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[10 rows x 1 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tTweet\tstr\n",
       "\n",
       "Rows: 10\n",
       "\n",
       "Data:\n",
       "+-------------------------------+\n",
       "|             Tweet             |\n",
       "+-------------------------------+\n",
       "| ye i am go from school hav... |\n",
       "| can u tape the match for m... |\n",
       "| too mani peopl at my hous ... |\n",
       "| yea i have spoken to him l... |\n",
       "| haha... i want to see. e m... |\n",
       "| i\\u$NUMm go down now liao\\... |\n",
       "| ya\\u$NUMc ok for me... erm... |\n",
       "| he told u i\\u$NUMm consid ... |\n",
       "| will you be po on sunday n... |\n",
       "| tom\\u$NUMc u think it is a... |\n",
       "+-------------------------------+\n",
       "[10 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del test_tweets['Sentiment']\n",
    "del test_tweets['ID']\n",
    "test_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tweets.save('C:\\\\Users\\\\OmarAbdelwahab\\\\Documents\\\\RESEARCH\\\\SEMEVAL2016\\\\TwitterSentimentAnalysisTask\\\\Train-Trial-Data\\\\2016Data\\\\Train+dev-2016\\\\Needed\\\\Train\\\\local_results_worst.csv',format='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_tweets.save('testing.csv',format='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## SEMEVAL 2017 preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">TweetID</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Sentiment</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">264183816548130000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">ga by my hous hit<br>$NUM.$NUM i m go to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">264249301910310000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">iranian gener say israel<br>s iron dome can t deal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">264105751826538000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">with j davlar $NUMth.<br>main rival are team ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">264094586689953000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">talk about act s amp;<br>amp; sat s, decid   i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">254941790757601000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">they may have a superbowl<br>in dalla, but dalla a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">264169034155696000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">im bring the ne load of<br>candi tomorrow, i just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">263192091700654000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">appl software, retail<br>chief out in overhaul: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">263398998675693000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">AT USER AT USER AT USER i<br>just watch it sridevi s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">260200142420992000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">neutral</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">livewir nadal confirm for<br>mexican open in febru ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">264087629237202000</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">AT USER i didnt want to<br>just pop up... but ye ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[11798 rows x 3 columns]<br/>Note: Only the head of the SFrame is printed.<br/>You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tTweetID\tint\n",
       "\tSentiment\tstr\n",
       "\tTweet\tstr\n",
       "\n",
       "Rows: 11798\n",
       "\n",
       "Data:\n",
       "+--------------------+-----------+-------------------------------+\n",
       "|      TweetID       | Sentiment |             Tweet             |\n",
       "+--------------------+-----------+-------------------------------+\n",
       "| 264183816548130000 |  positive | ga by my hous hit $NUM.$NU... |\n",
       "| 264249301910310000 |  negative | iranian gener say israel s... |\n",
       "| 264105751826538000 |  positive | with j davlar $NUMth. main... |\n",
       "| 264094586689953000 |  negative | talk about act s amp; amp;... |\n",
       "| 254941790757601000 |  negative | they may have a superbowl ... |\n",
       "| 264169034155696000 |  neutral  | im bring the ne load of ca... |\n",
       "| 263192091700654000 |  neutral  | appl software, retail chie... |\n",
       "| 263398998675693000 |  positive | AT USER AT USER AT USER i ... |\n",
       "| 260200142420992000 |  neutral  | livewir nadal confirm for ... |\n",
       "| 264087629237202000 |  positive | AT USER i didnt want to ju... |\n",
       "+--------------------+-----------+-------------------------------+\n",
       "[11798 rows x 3 columns]\n",
       "Note: Only the head of the SFrame is printed.\n",
       "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_tweets = tweets[tweets['Sentiment'] == 'positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_tweets = tweets[tweets['Sentiment'] == 'negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del pos_tweets['ID']\n",
    "del pos_tweets['Sentiment']\n",
    "del pos_tweets['Tweet']\n",
    "del pos_tweets['clean_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">cut_tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Microsoft I may not<br>prefer your gaming br ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Just ordered my 1st ever<br>tablet Microsoft Surface ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Sunday morning quiet day<br>so time to welcome in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Innovation for jobs is<br>just around the corner ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Vote for AIESEC to become<br>the 10th Global non ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Top 5 most searched for<br>BacktoSchool topics  the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">truckersquigz Microsoft<br>MISpeedway nationwide88 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">ScottArbeit GabeAul<br>Microsoft isntall the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">taehongmin1 We have an<br>IOT workshop by Micro ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">ProfessorF gilwuvsyou<br>Microsoft LivioDeLaCruz ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[2814 rows x 1 columns]<br/>Note: Only the head of the SFrame is printed.<br/>You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tcut_tweet\tstr\n",
       "\n",
       "Rows: 2814\n",
       "\n",
       "Data:\n",
       "+-------------------------------+\n",
       "|           cut_tweet           |\n",
       "+-------------------------------+\n",
       "| Microsoft I may not prefer... |\n",
       "| Just ordered my 1st ever t... |\n",
       "| Sunday morning quiet day s... |\n",
       "| Innovation for jobs is jus... |\n",
       "| Vote for AIESEC to become ... |\n",
       "| Top 5 most searched for Ba... |\n",
       "| truckersquigz Microsoft MI... |\n",
       "| ScottArbeit GabeAul Micros... |\n",
       "| taehongmin1 We have an IOT... |\n",
       "| ProfessorF gilwuvsyou Micr... |\n",
       "+-------------------------------+\n",
       "[2814 rows x 1 columns]\n",
       "Note: Only the head of the SFrame is printed.\n",
       "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns."
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del neg_tweets['ID']\n",
    "del neg_tweets['Sentiment']\n",
    "del neg_tweets['Tweet']\n",
    "del neg_tweets['clean_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">ID</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Sentiment</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Tweet</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">clean_tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">628949369883000832</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">dear @Microsoft the<br>newOoffice for Mac is ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">dear Microsoft the<br>newOoffice for Mac is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">628976607420645377</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">@Microsoft how about you<br>make a system that ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Microsoft how about you<br>make a system that do ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">629345637155360768</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">@MikeWolf1980 @Microsoft<br>I will be downgrading ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">MikeWolf1980 Microsoft I<br>will be downgrading and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">629394528336637953</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">@Microsoft 2nd computer<br>with same error!!! ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Microsoft 2nd computer<br>with same error ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">629797991826722816</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">After attempting a<br>reinstall, it still ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">After attempting a<br>reinstall it still br ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">630542330827771904</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Did @Microsoft break<br>Windows 10? Was working ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Did Microsoft break<br>Windows 10 Was working ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">630636736746422272</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">@MSAU @Microsoft spent<br>over 40 mins &amp;amp; gave ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">MSAU Microsoft spent over<br>40 mins amp gave up coz ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">631104156187627520</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">For the 1st time @Skype<br>has a \"High Startup ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">For the 1st time Skype<br>has a High Startup im ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">631223085476261890</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">#teens @BillGates 1st<br>company failed misera ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">teens BillGates 1st<br>company failed miserably ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">631543121407442946</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">negative</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">@Microsoft support for<br>365 has been terrible ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Microsoft support for 365<br>has been terrible On the ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "<table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">cut_tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">dear Microsoft the<br>newOoffice for Mac is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Microsoft how about you<br>make a system that do ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">MikeWolf1980 Microsoft I<br>will be downgrading and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Microsoft 2nd computer<br>with same error ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">After attempting a<br>reinstall it still br ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Did Microsoft break<br>Windows 10 Was working ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">MSAU Microsoft spent over<br>40 mins amp gave up coz ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">For the 1st time Skype<br>has a High Startup im ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">teens BillGates 1st<br>company failed miserably ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Microsoft support for 365<br>has been terrible On the ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[775 rows x 5 columns]<br/>Note: Only the head of the SFrame is printed.<br/>You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tID\tint\n",
       "\tSentiment\tstr\n",
       "\tTweet\tstr\n",
       "\tclean_tweet\tstr\n",
       "\tcut_tweet\tstr\n",
       "\n",
       "Rows: 775\n",
       "\n",
       "Data:\n",
       "+--------------------+-----------+-------------------------------+\n",
       "|         ID         | Sentiment |             Tweet             |\n",
       "+--------------------+-----------+-------------------------------+\n",
       "| 628949369883000832 |  negative | dear @Microsoft the newOof... |\n",
       "| 628976607420645377 |  negative | @Microsoft how about you m... |\n",
       "| 629345637155360768 |  negative | @MikeWolf1980 @Microsoft I... |\n",
       "| 629394528336637953 |  negative | @Microsoft 2nd computer wi... |\n",
       "| 629797991826722816 |  negative | After attempting a reinsta... |\n",
       "| 630542330827771904 |  negative | Did @Microsoft break Windo... |\n",
       "| 630636736746422272 |  negative | @MSAU @Microsoft spent ove... |\n",
       "| 631104156187627520 |  negative | For the 1st time @Skype ha... |\n",
       "| 631223085476261890 |  negative | #teens @BillGates 1st comp... |\n",
       "| 631543121407442946 |  negative | @Microsoft support for 365... |\n",
       "+--------------------+-----------+-------------------------------+\n",
       "+-------------------------------+-------------------------------+\n",
       "|          clean_tweet          |           cut_tweet           |\n",
       "+-------------------------------+-------------------------------+\n",
       "| dear Microsoft the newOoff... | dear Microsoft the newOoff... |\n",
       "| Microsoft how about you ma... | Microsoft how about you ma... |\n",
       "| MikeWolf1980 Microsoft I w... | MikeWolf1980 Microsoft I w... |\n",
       "| Microsoft 2nd computer wit... | Microsoft 2nd computer wit... |\n",
       "| After attempting a reinsta... | After attempting a reinsta... |\n",
       "| Did Microsoft break Window... | Did Microsoft break Window... |\n",
       "| MSAU Microsoft spent over ... | MSAU Microsoft spent over ... |\n",
       "| For the 1st time Skype has... | For the 1st time Skype has... |\n",
       "| teens BillGates 1st compan... | teens BillGates 1st compan... |\n",
       "| Microsoft support for 365 ... | Microsoft support for 365 ... |\n",
       "+-------------------------------+-------------------------------+\n",
       "[775 rows x 5 columns]\n",
       "Note: Only the head of the SFrame is printed.\n",
       "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tweets.save('pos_tweets.csv',format='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_tweets.save('neg_tweets.csv',format='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cut_text(tex):\n",
    "    counter = 0\n",
    "    word_counter = 0\n",
    "    for c in tex:\n",
    "        counter=counter+1\n",
    "        if c==' ':\n",
    "            word_counter=word_counter + 1\n",
    "        if word_counter == 56:\n",
    "            break\n",
    "    return tex[:counter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_tweets['cut_tweet'] = pos_tweets['clean_tweet'].apply(lambda x: cut_text(x))\n",
    "neg_tweets['cut_tweet'] = neg_tweets['clean_tweet'].apply(lambda x: cut_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_tweets['clean_tweet'] = neg_tweets['Tweet'].apply(lambda x: cut_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del pos_tweets['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del neg_tweets['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clean_tweet': 'AT USER AT USER AT USER i just watch it sridevi s comeback.... u rememb her from the $NUMs?? sun morn on nta ;)'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tweets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\OmarAbdelwahab\\PositiveTweets-A-NA.tsv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\OmarAbdelwahab\\PositiveTweets-A-NA.tsv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.025501 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.025501 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[long,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\OmarAbdelwahab\\PositiveTweets-A-NA.tsv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\OmarAbdelwahab\\PositiveTweets-A-NA.tsv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 2814 lines in 0.021509 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 2814 lines in 0.021509 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\OmarAbdelwahab\\NegativeTweets-A-NA.tsv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\OmarAbdelwahab\\NegativeTweets-A-NA.tsv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.020999 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.020999 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[long,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\OmarAbdelwahab\\NegativeTweets-A-NA.tsv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\OmarAbdelwahab\\NegativeTweets-A-NA.tsv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 775 lines in 0.018001 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 775 lines in 0.018001 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_tweets = graphlab.SFrame('PositiveTweets-A-NA.tsv')\n",
    "neg_tweets = graphlab.SFrame('NegativeTweets-A-NA.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">ID</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Sentiment</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">Tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">629226490152914944</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Microsoft, I may not<br>prefer your gaming br ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">629650766580609026</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Just ordered my 1st ever<br>tablet; @Microsoft ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">630159517058142208</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Sunday morning, quiet day<br>so time to welcome in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">630818265799921664</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Innovation for jobs is<br>just around the corner - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">631368262979297281</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">#Vote for @AIESEC to<br>become the 10th Global ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">631521079245307904</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Top 5 most searched for<br>Back-to-School topics -- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">631696872323850240</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">@trucker_squigz<br>@Microsoft @MISpeedway ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">631842974268305408</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">@ScottArbeit @GabeAul<br>@Microsoft isntall the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">631843393971204097</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">@taehongmin1 We have an<br>IOT workshop by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">633628599271190528</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">positive</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">@ProfessorF @gilwuvsyou<br>@Microsoft @LivioDeLa ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[2814 rows x 3 columns]<br/>Note: Only the head of the SFrame is printed.<br/>You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tID\tint\n",
       "\tSentiment\tstr\n",
       "\tTweet\tstr\n",
       "\n",
       "Rows: 2814\n",
       "\n",
       "Data:\n",
       "+--------------------+-----------+-------------------------------+\n",
       "|         ID         | Sentiment |             Tweet             |\n",
       "+--------------------+-----------+-------------------------------+\n",
       "| 629226490152914944 |  positive | Microsoft, I may not prefe... |\n",
       "| 629650766580609026 |  positive | Just ordered my 1st ever t... |\n",
       "| 630159517058142208 |  positive | Sunday morning, quiet day ... |\n",
       "| 630818265799921664 |  positive | Innovation for jobs is jus... |\n",
       "| 631368262979297281 |  positive | #Vote for @AIESEC to becom... |\n",
       "| 631521079245307904 |  positive | Top 5 most searched for Ba... |\n",
       "| 631696872323850240 |  positive | @trucker_squigz @Microsoft... |\n",
       "| 631842974268305408 |  positive | @ScottArbeit @GabeAul @Mic... |\n",
       "| 631843393971204097 |  positive | @taehongmin1 We have an IO... |\n",
       "| 633628599271190528 |  positive | @ProfessorF @gilwuvsyou @M... |\n",
       "+--------------------+-----------+-------------------------------+\n",
       "[2814 rows x 3 columns]\n",
       "Note: Only the head of the SFrame is printed.\n",
       "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns."
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    import string\n",
    "    return text.translate(None, string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_tweets['clean_tweet'] = pos_tweets['Tweet'].apply(lambda x: remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_tweets['clean_tweet'] = neg_tweets['Tweet'].apply(lambda x: remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tweets['cut_tweet'] = pos_tweets['clean_tweet'].apply(lambda x: cut_text(x))\n",
    "neg_tweets['cut_tweet'] = neg_tweets['clean_tweet'].apply(lambda x: cut_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = pos_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = tweets.append(neg_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.save('tweets17.csv',format='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del tweets['ID']\n",
    "del tweets['Sentiment']\n",
    "del tweets['Tweet']\n",
    "del tweets['clean_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.save('tweets17_raw.csv',format='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
